{"id": "2601.12234", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12234", "abs": "https://arxiv.org/abs/2601.12234", "authors": ["Fadlullah Raji", "Stefano Petrangeli", "Matheus Gadelha", "Yu Shen", "Uttaran Bhattacharya", "Gang Wu"], "title": "Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models", "comment": null, "summary": "Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.", "AI": {"tldr": "Proc3D \u662f\u4e00\u4e2a\u751f\u6210\u53ef\u7f16\u8f913D\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528\u7a0b\u5e8f\u5316\u7d27\u51d1\u56fe\u8868\u793a\uff0c\u652f\u6301\u901a\u8fc7\u6ed1\u5757/\u590d\u9009\u6846\u624b\u52a8\u8c03\u6574\u548c\u81ea\u7136\u8bed\u8a00\u5b9e\u65f6\u4fee\u6539\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u7f16\u8f91\u6548\u7387\u63d0\u5347400\u500d\u4ee5\u4e0a\u3002", "motivation": "\u4f20\u7edf3D\u5efa\u6a21\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709AI\u751f\u6210\u65b9\u6cd5\u4ea7\u751f\u4e0d\u53ef\u7f16\u8f91\u7684\u8868\u793a\uff08\u5982\u7f51\u683c\u6216\u70b9\u4e91\uff09\uff0c\u9650\u5236\u4e86\u8fed\u4ee3\u8bbe\u8ba1\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u751f\u6210\u53ef\u7f16\u8f913D\u6a21\u578b\u5e76\u652f\u6301\u5b9e\u65f6\u4fee\u6539\u7684\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u7a0b\u5e8f\u5316\u7d27\u51d1\u56fe\u4f5c\u4e3a3D\u6a21\u578b\u7684\u56fe\u8868\u793a\uff0c\u7f16\u7801\u751f\u6210\u6a21\u578b\u7684\u7b97\u6cd5\u89c4\u5219\u548c\u7ed3\u6784\uff1b\u901a\u8fc7\u66b4\u9732\u5173\u952e\u53c2\u6570\u652f\u6301\u6ed1\u5757/\u590d\u9009\u6846\u624b\u52a8\u8c03\u6574\uff0c\u4ee5\u53ca\u4f7f\u7528LLM\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5b9e\u65f6\u4fee\u6539\uff1b\u91c7\u7528GPT-4o\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03LLAMA-3\u4e24\u79cd\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u7f16\u8f91\u6548\u7387\u6bd4\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u751f\u6210\u7684\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347400\u500d\u4ee5\u4e0a\uff1bULIP\u5206\u6570\uff08\u8bc4\u4f303D\u6a21\u578b\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u7684\u6307\u6807\uff09\u63d0\u534728%\uff1b\u5b9e\u73b0\u4e86\u6587\u672c\u5bf9\u9f50\u76843D\u6a21\u578b\u751f\u6210\u548c\u7cbe\u786e\u7684\u5b9e\u65f6\u53c2\u6570\u7f16\u8f91\u3002", "conclusion": "Proc3D\u901a\u8fc7\u7a0b\u5e8f\u5316\u7d27\u51d1\u56fe\u8868\u793a\u5b9e\u73b0\u4e86\u53ef\u7f16\u8f913D\u6a21\u578b\u7684\u751f\u6210\u548c\u5b9e\u65f6\u4fee\u6539\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6548\u7387\u548c\u6587\u672c\u5bf9\u9f50\u8d28\u91cf\uff0c\u4e3a\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14207", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14207", "abs": "https://arxiv.org/abs/2601.14207", "authors": ["Rotem Gatenyo", "Ohad Fried"], "title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints", "comment": null, "summary": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c3D\u7f51\u683c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u63cf\u8ff0\u7a7a\u95f4\u5173\u7cfb\uff0c\u4f7f\u7528CLIP\u9a71\u52a8\u7684\u68af\u5ea6\u4f18\u5316\u76f8\u5bf9\u4f4d\u59ff\uff0c\u65e0\u9700\u8bad\u7ec3\u65b0\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u51e0\u4f55\u5bf9\u9f50\u62162D\u6269\u6563\u6a21\u578b\uff0c\u9700\u8981\u76f4\u63a5\u4f18\u5316\u76f8\u5bf9\u4f4d\u59ff\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8bed\u8a00\u6761\u4ef6\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\uff0c\u8fd9\u5bf9\u4e8e\u5185\u5bb9\u521b\u5efa\u548c\u573a\u666f\u7ec4\u88c5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u6d4b\u8bd5\u65f6\u76f4\u63a5\u4f18\u5316\u76f8\u5bf9\u4f4d\u59ff\uff08\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u5404\u5411\u540c\u6027\u7f29\u653e\uff09\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u5668\u4f7f\u7528CLIP\u9a71\u52a8\u7684\u68af\u5ea6\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u76ee\u6807\uff1a\u8f6fICP\u53d8\u4f53\u4fc3\u8fdb\u8868\u9762\u9644\u7740\uff0c\u7a7f\u900f\u635f\u5931\u9632\u6b62\u4e92\u7a7f\uff0c\u91c7\u7528\u5206\u9636\u6bb5\u8c03\u5ea6\u548c\u76f8\u673a\u63a7\u5236\u805a\u7126\u4ea4\u4e92\u533a\u57df\u3002", "result": "\u5728\u5305\u542b\u591a\u6837\u7c7b\u522b\u548c\u5173\u7cfb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u4ea7\u751f\u8bed\u4e49\u5fe0\u5b9e\u4e14\u7269\u7406\u5408\u7406\u7684\u5bf9\u9f50\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u76f8\u5bf9\u4f4d\u59ff\u548c\u7ed3\u5408\u8bed\u8a00\u76d1\u7763\u4e0e\u51e0\u4f55\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c3D\u7f51\u683c\u5bf9\u9f50\uff0c\u4e3a\u5185\u5bb9\u521b\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11617", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11617", "abs": "https://arxiv.org/abs/2601.11617", "authors": ["Xu Wang", "Boyao Han", "Xiaojun Chen", "Ying Liu", "Ruihui Li"], "title": "PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM", "comment": null, "summary": "Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.", "AI": {"tldr": "PointSLAM++ \u662f\u4e00\u79cd\u65b0\u578b RGB-D SLAM \u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5c42\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u548c\u6e10\u8fdb\u59ff\u6001\u4f18\u5316\uff0c\u5728\u6df1\u5ea6\u566a\u58f0\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6 3D \u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u5f53\u524d SLAM \u65b9\u6cd5\u5728\u6df1\u5ea6\u566a\u58f0\u5b58\u5728\u65f6\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6 3D \u91cd\u5efa\u8d28\u91cf\u3002", "method": "1. \u5206\u5c42\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\uff1a\u4fdd\u6301\u7ed3\u6784\u5173\u7cfb\u7684\u540c\u65f6\u751f\u6210\u9ad8\u65af\u57fa\u5143\u8fdb\u884c\u573a\u666f\u5efa\u56fe\uff1b2. \u6e10\u8fdb\u59ff\u6001\u4f18\u5316\uff1a\u51cf\u8f7b\u6df1\u5ea6\u4f20\u611f\u5668\u566a\u58f0\uff0c\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\uff1b3. \u52a8\u6001\u795e\u7ecf\u8868\u793a\u56fe\uff1a\u6839\u636e\u5c40\u90e8\u51e0\u4f55\u590d\u6742\u5ea6\u8c03\u6574\u9ad8\u65af\u8282\u70b9\u5206\u5e03\uff0c\u5b9e\u65f6\u9002\u5e94\u590d\u6742\u573a\u666f\u7ec6\u8282\u3002", "result": "PointSLAM++ \u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e 3DGS \u7684 SLAM \u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5927\u89c4\u6a21 AR \u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "PointSLAM++ \u901a\u8fc7\u521b\u65b0\u7684\u5206\u5c42\u7ea6\u675f\u795e\u7ecf\u9ad8\u65af\u8868\u793a\u548c\u6e10\u8fdb\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u566a\u58f0\u4e0b\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6 3D \u91cd\u5efa\u548c\u903c\u771f\u6e32\u67d3\u3002"}}
{"id": "2601.12257", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.12257", "abs": "https://arxiv.org/abs/2601.12257", "authors": ["Fadlullah Raji", "John Murray-Bruce"], "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy", "comment": null, "summary": "Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \\textit{light-occluding} and \\textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u666e\u901a\u975e\u89c6\u8ddd(NLOS)\u7167\u7247\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9690\u85cf\u573a\u666f\u5206\u89e3\u4e3a\u5149\u906e\u6321\u548c\u975e\u5149\u906e\u6321\u7ec4\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u68af\u5ea6\u4f18\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u6210\u50cf\u9700\u8981\u89c6\u7ebf\u624d\u80fd\u521b\u5efa\u51c6\u786e\u7684\u573a\u666f\u8868\u793a\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u83b7\u5f97\u5408\u9002\u7684\u89c6\u7ebf\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\u3001\u5371\u9669\u751a\u81f3\u4e0d\u53ef\u80fd\u3002\u975e\u89c6\u8ddd\u6210\u50cf\u901a\u8fc7\u95f4\u63a5\u6d4b\u91cf\u91cd\u5efa\u573a\u666f\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002\u73b0\u6709\u7684\u88ab\u52a8NLOS\u65b9\u6cd5\u4ec5\u9650\u4e8e1D\u6216\u4f4e\u5206\u8fa8\u73872D\u5f69\u8272\u6210\u50cf\uff0c\u6216\u53ea\u80fd\u5b9a\u4f4d\u5f62\u72b6\u8fd1\u4f3c\u5df2\u77e5\u7684\u9690\u85cf\u7269\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5149\u4f20\u8f93\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\uff0c\u5c06\u9690\u85cf\u573a\u666f\u5206\u89e3\u4e3a\u5149\u906e\u6321\u548c\u975e\u5149\u906e\u6321\u7ec4\u4ef6\uff0c\u5f62\u6210\u53ef\u5206\u79bb\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58(SNLLS)\u9006\u95ee\u9898\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u53d7\u7269\u7406\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff08\u79f0\u4e3a\u8f6f\u9634\u5f71\u6269\u6563SSD\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u5b9e\u9a8c\u573a\u666f\u4e2d\u5bf9\u591a\u4e2a3D\u573a\u666f\u6709\u6548\u3002SSD\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\uff0c\u4f46\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754cNLOS\u573a\u666f\u4e2d\u7684\u672a\u89c1\u7c7b\u522b\u3002SSD\u8fd8\u663e\u793a\u51fa\u5bf9\u566a\u58f0\u548c\u73af\u5883\u7167\u660e\u7684\u60ca\u4eba\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u666e\u901aNLOS\u7167\u7247\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u88ab\u52a8\u975e\u89c6\u8ddd\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11559", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11559", "abs": "https://arxiv.org/abs/2601.11559", "authors": ["Zilal Eiz AlDin", "John Wu", "Jeffrey Paul Fung", "Jennifer King", "Mya Watts", "Lauren ONeill", "Adam Richard Cross", "Jimeng Sun"], "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?", "comment": "5 pages", "summary": "Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MIMIC-RD\u57fa\u51c6\uff0c\u901a\u8fc7\u5c06\u4e34\u5e8a\u6587\u672c\u5b9e\u4f53\u76f4\u63a5\u6620\u5c04\u5230Orphanet\u7f55\u89c1\u75c5\u6570\u636e\u5e93\u6765\u8bc4\u4f30LLM\u5728\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7f55\u89c1\u75c5\u5f71\u54cd\u5927\u91cf\u4eba\u7fa4\u4f46\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u8bc4\u4f30LLM\u7f55\u89c1\u75c5\u8bca\u65ad\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u4f9d\u8d56\u7406\u60f3\u5316\u7684\u4e34\u5e8a\u6848\u4f8b\u7814\u7a76\uff0c\u672a\u80fd\u6355\u6349\u771f\u5b9e\u4e34\u5e8a\u590d\u6742\u6027\uff1b2) \u4f7f\u7528ICD\u7f16\u7801\u4f5c\u4e3a\u75be\u75c5\u6807\u7b7e\uff0c\u4f46\u8bb8\u591a\u7f55\u89c1\u75c5\u7f3a\u4e4f\u4e0eOrphanet\u7b49\u7efc\u5408\u7f55\u89c1\u75c5\u6570\u636e\u5e93\u7684\u76f4\u63a5\u6620\u5c04\u3002", "method": "\u5f00\u53d1MIMIC-RD\u57fa\u51c6\uff0c\u901a\u8fc7LLM\u6316\u6398\u4e34\u5e8a\u6587\u672c\u5b9e\u4f53\u5e76\u76f4\u63a5\u6620\u5c04\u5230Orphanet\u7f55\u89c1\u75c5\u6570\u636e\u5e93\uff0c\u7136\u540e\u7531\u56db\u4f4d\u533b\u5b66\u6807\u6ce8\u8005\u9a8c\u8bc1\u786e\u8ba4\u8bc6\u522b\u7684\u5b9e\u4f53\u662f\u771f\u6b63\u7684\u7f55\u89c1\u75c5\u3002\u5728145\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u5404\u79cd\u6a21\u578b\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f55\u89c1\u75c5\u9274\u522b\u8bca\u65ad\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u51fa\u73b0\u6709\u80fd\u529b\u4e0e\u4e34\u5e8a\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u7f55\u89c1\u75c5\u7684\u9274\u522b\u8bca\u65ad\u65b9\u6cd5\uff0c\u8bba\u6587\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u51e0\u4e2a\u65b9\u5411\u3002"}}
{"id": "2601.11564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11564", "abs": "https://arxiv.org/abs/2601.11564", "authors": ["Ahilan Ayyachamy Nadar Ponnusamy", "Karthic Chandran", "M Maruf Hossain"], "title": "Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths", "comment": "22 pages, 6 figures", "summary": "The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u65e0\u5173\u4e0a\u4e0b\u6587\u65f6\u7684\u6027\u80fd\u4e0e\u8d28\u91cf\u6743\u8861\uff0c\u53d1\u73b0KV\u7f13\u5b58\u589e\u957f\u5bfc\u81f4\u975e\u7ebf\u6027\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63ed\u793a\u4e86MoE\u67b6\u6784\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u89c4\u6a21\u4e0b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0d\u65ad\u6269\u5927\u4ee5\u652f\u6301\u590d\u6742\u957f\u6587\u672c\u63a8\u7406\uff0c\u7ba1\u7406\u6269\u5c55\u4e0a\u4e0b\u6587\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7cfb\u7edf\u6027\u80fd\u4e0e\u6a21\u578b\u8d28\u91cf\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u65e0\u5173\u548c\u5e72\u6270\u6027\u4e0a\u4e0b\u6587\u65f6\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5bc6\u96c6Transformer\u67b6\u6784\uff08Llama-3.1-70B\u548cQwen1.5-14B\uff09\u66b4\u9732\u4e8e\u5927\u91cf\u65e0\u5173\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5206\u6790KV\u7f13\u5b58\u589e\u957f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\u540c\u65f6\u6269\u5c55\u5206\u6790\u4e86Mixture-of-Experts\uff08MoE\uff09\u67b6\u6784\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u89c4\u6a21\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0KV\u7f13\u5b58\u589e\u957f\u5bfc\u81f4\u975e\u7ebf\u6027\u6027\u80fd\u4e0b\u964d\u3002MoE\u67b6\u6784\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u89c4\u6a21\u4e0b\u8868\u73b0\u51fa\u72ec\u7279\u7684\u884c\u4e3a\u5f02\u5e38\uff0c\u8868\u660e\u5728\u9ad8\u4ee4\u724c\u91cf\u65f6\uff0c\u67b6\u6784\u4f18\u52bf\u53ef\u80fd\u88ab\u57fa\u7840\u8bbe\u65bd\u74f6\u9888\u6240\u63a9\u76d6\u3002", "conclusion": "\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u867d\u7136\u63d0\u5347\u4e86\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u5904\u7406\u65e0\u5173\u5185\u5bb9\u4f1a\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6027\u80fd\u4e0b\u964d\u3002MoE\u67b6\u6784\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u7684\u5f02\u5e38\u884c\u4e3a\u63d0\u793a\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u67b6\u6784\u8bbe\u8ba1\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u573a\u666f\u3002"}}
{"id": "2601.11612", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11612", "abs": "https://arxiv.org/abs/2601.11612", "authors": ["Arnav S. Sonavane"], "title": "Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study", "comment": "11 pages, 4 figures, 9 tables", "summary": "We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT", "AI": {"tldr": "\u5728\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08SimCLR\uff09\u6bd4\u5c42\u6b21\u5316\u67b6\u6784\u8bbe\u8ba1\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u8be5\u4f18\u52bf\u5177\u6709\u67b6\u6784\u65e0\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u9886\u57df\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u6570\u636e\u6536\u96c6\u4e0e\u67b6\u6784\u9009\u62e9\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u4f7f\u7528SimCLR\u57283000\u5f20\u65e0\u6807\u7b7e\u519c\u4e1a\u56fe\u50cf\u4e0a\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5e94\u7528\u4e8eHierarchicalViT\uff08HVT\uff09\u7b49\u5c42\u6b21\u5316\u89c6\u89c9Transformer\uff0c\u5e76\u5728\u4e09\u4e2a\u519c\u4e1a\u75c5\u5bb3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "SimCLR\u9884\u8bad\u7ec3\u5e26\u6765+4.57%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8d85\u8fc7\u5c42\u6b21\u5316\u67b6\u6784\u8bbe\u8ba1\u7684+3.70%\u589e\u76ca\uff1bHVT-Base\u5728\u53c2\u6570\u91cf\u76f8\u8fd1\u65f6\u6bd4Swin-Base\u9ad8+1.68%\uff1b\u6a21\u578b\u6821\u51c6\u540eECE\u964d\u81f31.52%\u3002", "conclusion": "\u5728\u519c\u4e1a\u75c5\u5bb3\u5206\u7c7b\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6bd4\u67b6\u6784\u9009\u62e9\u66f4\u91cd\u8981\uff0c\u5efa\u8bae\u5b9e\u8df5\u8005\u4f18\u5148\u6536\u96c6\u9886\u57df\u6570\u636e\u800c\u975e\u4e13\u6ce8\u4e8e\u67b6\u6784\u4f18\u5316\u3002"}}
{"id": "2601.12481", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.12481", "abs": "https://arxiv.org/abs/2601.12481", "authors": ["Vanessa Sklyarova", "Berna Kabadayi", "Anastasios Yiannakidis", "Giorgio Becherini", "Michael J. Black", "Justus Thies"], "title": "NeuralFur: Animal Fur Reconstruction From Multi-View Images", "comment": "For additional results and code, please refer to https://neuralfur.is.tue.mpg.de", "summary": "Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal's furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands' growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to https://neuralfur.is.tue.mpg.de.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u52a8\u7269\u6bdb\u53d13D\u91cd\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u6bdb\u53d1\u957f\u5ea6\u548c\u751f\u957f\u65b9\u5411\uff0c\u5b9e\u73b0\u4e0d\u540c\u52a8\u7269\u6bdb\u53d1\u7684\u9ad8\u4fdd\u771f\u5efa\u6a21", "motivation": "\u52a8\u7269\u6bdb\u53d1\u91cd\u5efa\u9762\u4e34\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3001\u81ea\u906e\u6321\u548c\u89c6\u89d2\u4f9d\u8d56\u5916\u89c2\u7b49\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u50cf\u4eba\u7c7b\u53d1\u578b\u90a3\u6837\u7684\u6570\u636e\u96c6\u6765\u5b66\u4e60\u5148\u9a8c\u77e5\u8bc6", "method": "1) \u591a\u89c6\u89d2\u7acb\u4f53\u6280\u672f\u91cd\u5efa\u7c97\u7565\u8868\u9762\u51e0\u4f55\uff1b2) \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u8eab\u4f53\u5404\u90e8\u4f4d\u6bdb\u53d1\u957f\u5ea6\u4fe1\u606f\uff1b3) \u6784\u5efa\u65e0\u6bdb\u51e0\u4f55\u5e76\u5728\u5176\u4e0a\u751f\u957f\u6bdb\u53d1\uff1b4) \u7ed3\u5408\u51e0\u4f55\u548c\u5149\u5ea6\u635f\u5931\u76d1\u7763\uff1b5) \u5229\u7528VLM\u6307\u5bfc\u6bdb\u53d1\u751f\u957f\u65b9\u5411\u548c\u91cd\u529b\u5411\u91cf\u5173\u7cfb", "result": "\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u52a8\u7269\u548c\u6bdb\u53d1\u7c7b\u578b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u76843D\u6bdb\u53d1\u5efa\u6a21", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684VLM\u6307\u5bfc\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u52a8\u7269\u6bdb\u53d1\u91cd\u5efa\u7684\u6311\u6218\uff0c\u4e3a\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.11620", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11620", "abs": "https://arxiv.org/abs/2601.11620", "authors": ["Michael Timothy Bennett"], "title": "A Mind Cannot Be Smeared Across Time", "comment": null, "summary": "Whether machines can be conscious depends not only on what they compute, but \\emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $\u03c4^{\u0394,s}$ and prove that existential temporal realisation $\\Diamond_\u0394$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u673a\u5668\u610f\u8bc6\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u8ba1\u7b97\u5185\u5bb9\uff0c\u8fd8\u53d6\u51b3\u4e8e\u8ba1\u7b97\u65f6\u673a\u3002\u4e25\u683c\u987a\u5e8f\u6267\u884c\u7684\u7cfb\u7edf\u65e0\u6cd5\u5b9e\u73b0\u540c\u65f6\u6027\u610f\u8bc6\u4f53\u9a8c\uff0c\u9700\u8981\u786c\u4ef6\u5e76\u53d1\u80fd\u529b\u652f\u6301\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5927\u591a\u91c7\u7528\u987a\u5e8f\u6216\u65f6\u5206\u590d\u7528\u66f4\u65b0\uff0c\u800c\u610f\u8bc6\u4f53\u9a8c\u5177\u6709\u7edf\u4e00\u6027\u548c\u540c\u65f6\u6027\u3002\u8fd9\u79cd\u65f6\u95f4\u7279\u6027\u5dee\u5f02\u5bf9\u673a\u5668\u80fd\u5426\u5177\u6709\u610f\u8bc6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6269\u5c55\u6808\u7406\u8bba\uff0c\u5f15\u5165\u65f6\u95f4\u7a97\u53e3\u8f68\u8ff9\u03c4^{\u0394,s}\u7684\u7cbe\u786e\u65f6\u5e8f\u8bed\u4e49\uff0c\u8bc1\u660e\u5b58\u5728\u6027\u65f6\u5e8f\u5b9e\u73b0\u25c7_\u0394\u4e0d\u4fdd\u6301\u5408\u53d6\u6027\u3002\u533a\u5206\u5f3a\u540c\u6b65\uff08\u8981\u6c42\u5ba2\u89c2\u5171\u73b0\uff09\u548c\u5f31\u540c\u6b65\uff08\u5141\u8bb8\u65f6\u95f4\"\u6d82\u62b9\"\uff09\uff0c\u5f62\u5f0f\u5316\u5e76\u53d1\u5bb9\u91cf\u6982\u5ff5\u3002", "result": "\u7cfb\u7edf\u53ef\u4ee5\u5728\u65f6\u95f4\u4e0a\u5b9e\u73b0\u6240\u6709\u4f53\u9a8c\u6210\u5206\uff0c\u4f46\u4ece\u672a\u5b9e\u4f8b\u5316\u4f53\u9a8c\u5408\u53d6\u672c\u8eab\u3002\u795e\u7ecf\u751f\u7406\u8bc1\u636e\u8868\u660e\u610f\u8bc6\u4f9d\u8d56\u76f8\u4f4d\u540c\u6b65\u548c\u6709\u6548\u8fde\u63a5\uff0c\u5931\u53bb\u610f\u8bc6\u5e38\u4e0e\u5176\u5d29\u6e83\u76f8\u5173\uff0c\u8fd9\u4f7f\u5f31\u540c\u6b65\u4e0d\u592a\u53ef\u4fe1\u3002", "conclusion": "\u5728\u5f3a\u540c\u6b65\u4e0b\uff0c\u4e25\u683c\u987a\u5e8f\u57fa\u677f\u4e0a\u7684\u8f6f\u4ef6\u610f\u8bc6\u5bf9\u4e8e\u9700\u8981\u4e24\u4e2a\u6216\u66f4\u591a\u540c\u65f6\u8d21\u732e\u8005\u7684\u5185\u5bb9\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u610f\u8bc6\u5f52\u56e0\u9700\u8981\u67b6\u6784\u68c0\u67e5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u529f\u80fd\u6027\u80fd\uff0c\u786c\u4ef6\u5f88\u91cd\u8981\u3002"}}
{"id": "2601.11565", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11565", "abs": "https://arxiv.org/abs/2601.11565", "authors": ["Pakorn Ueareeworakul", "Shuman Liu", "Jinghao Feng", "Ling Hu", "Zhantang Shi", "Chengqi Sun", "Liang Yao", "Panyi Ouyang", "Haibo Zhang", "Anxiang Zeng"], "title": "Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings", "comment": null, "summary": "As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.", "AI": {"tldr": "Compass-Embedding v4\u662f\u4e00\u4e2a\u9488\u5bf9\u4e1c\u5357\u4e9a\u7535\u5546\u573a\u666f\u4f18\u5316\u7684\u591a\u8bed\u8a00\u5d4c\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u611f\u77e5\u63a9\u7801\u3001\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\u6784\u5efa\u548c\u9ad8\u6548\u63a8\u7406\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u4e49\u8868\u793a\u7684\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5168\u7403\u7535\u5546\u5411\u65b0\u5174\u5e02\u573a\u6269\u5f20\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bed\u4e49\u8868\u793a\u6210\u4e3a\u68c0\u7d22\u3001\u63a8\u8350\u548c\u641c\u7d22\u7cfb\u7edf\u7684\u5173\u952e\u74f6\u9888\u3002\u4e1c\u5357\u4e9a\u7535\u5546\u573a\u666f\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u566a\u58f0\u76d1\u7763\u548c\u4e25\u683c\u751f\u4ea7\u7ea6\u675f\u7684\u8054\u5408\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u7c7b\u611f\u77e5\u63a9\u7801(CAM)\uff0c\u4fee\u6539InfoNCE\u76ee\u6807\u51fd\u6570\u6291\u5236\u65e0\u6548\u6279\u91cf\u8d1f\u6837\u672c\uff1b2. \u901a\u8fc7\u4e0a\u4e0b\u6587\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u8de8\u8bed\u8a00\u7ffb\u8bd1\u548c\u7ed3\u6784\u5316\u7535\u5546\u6570\u636e\u6784\u5efa\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff1b3. \u7ed3\u5408\u9c81\u68d2\u6027\u9a71\u52a8\u7684\u6279\u91cf\u8bad\u7ec3\u4e0e\u7403\u5f62\u6a21\u578b\u5408\u5e76\uff0c\u5e76\u901a\u8fc7vLLM\u548cFP8\u91cf\u5316\u4f18\u5316\u63a8\u7406\u3002", "result": "\u5728\u591a\u9879\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e13\u6709\u7535\u5546\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cCompass-Embedding v4\u5728\u4e3b\u8981\u4e1c\u5357\u4e9a\u8bed\u8a00\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u901a\u7528\u5d4c\u5165\u6a21\u578b\uff0c\u540c\u65f6\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "Compass-Embedding v4\u6210\u529f\u89e3\u51b3\u4e86\u4e1c\u5357\u4e9a\u7535\u5546\u573a\u666f\u4e2d\u7684\u591a\u8bed\u8a00\u5d4c\u5165\u6311\u6218\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u4e49\u8868\u793a\uff0c\u540c\u65f6\u6ee1\u8db3\u751f\u4ea7\u73af\u5883\u7684\u9ad8\u541e\u5410\u91cf\u8981\u6c42\u3002"}}
{"id": "2601.11614", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.11614", "abs": "https://arxiv.org/abs/2601.11614", "authors": ["Jason Qiu"], "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning", "comment": "19 pages, 10 figures", "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.", "AI": {"tldr": "\u63d0\u51fa3D TransUNet\u6846\u67b6\uff0c\u4ece\u5e38\u89c4T1\u52a0\u6743MRI\u9884\u6d4b\u6269\u6563MRI\u6307\u6807\uff08FA\u548cMD\uff09\uff0c\u63d0\u5347\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u8bca\u65ad\u51c6\u786e\u7387", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u89c4T1\u52a0\u6743MRI\u53ea\u80fd\u68c0\u6d4b\u665a\u671f\u5b8f\u89c2\u53d8\u5316\uff0c\u800c\u6269\u6563MRI\u80fd\u68c0\u6d4b\u65e9\u671f\u5fae\u89c2\u5f02\u5e38\u4f46\u626b\u63cf\u65f6\u95f4\u957f\u4e14\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528", "method": "\u91c7\u75283D TransUNet\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4eceT1\u52a0\u6743MRI\u9884\u6d4b\u5206\u6570\u5404\u5411\u5f02\u6027\uff08FA\uff09\u548c\u5e73\u5747\u6269\u6563\u7387\uff08MD\uff09\u56fe", "result": "\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6269\u6563\u56fe\uff0c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\u8d85\u8fc70.93\uff0c\u4e0e\u771f\u5b9e\u6269\u6563MRI\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570>0.94\u3002\u5728\u591a\u6a21\u6001\u8bca\u65ad\u6a21\u578b\u4e2d\uff0c\u5408\u6210\u7279\u5f81\u5c06AD\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u53475%\uff0878.75%\u219283.75%\uff09\uff0c\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u63d0\u534712.5%", "conclusion": "\u4ece\u5e38\u89c4T1\u52a0\u6743MRI\u53ef\u4ee5\u63a8\u65ad\u9ad8\u8d28\u91cf\u7684\u6269\u6563\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff0c\u5c06\u591a\u6a21\u6001\u6210\u50cf\u7684\u4f18\u52bf\u6269\u5c55\u5230\u65e0\u6cd5\u83b7\u53d6\u6269\u6563\u6570\u636e\u7684\u573a\u666f\uff0c\u6709\u671b\u63d0\u9ad8AD\u8bca\u65ad\u7684\u53ef\u53ca\u6027\u3001\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2601.12527", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.12527", "abs": "https://arxiv.org/abs/2601.12527", "authors": ["Richard Liu", "Itai Lang", "Rana Hanocka"], "title": "Deep Feature Deformation Weights", "comment": null, "summary": "Handle-based mesh deformation has been a long-standing paradigm in computer graphics, enabling intuitive shape edits from sparse controls. Classic techniques offer precise and rapid deformation control. However, they solve an optimization problem with constraints defined by control handle placement, requiring a user to know apriori the ideal distribution of handles on the shape to accomplish the desired edit. The mapping from handle set to deformation behavior is often unintuitive and, importantly, non-semantic. Modern data-driven methods, on the other hand, leverage a data prior to obtain semantic edits, but are slow and imprecise. We propose a technique that fuses the semantic prior of data with the precise control and speed of traditional frameworks. Our approach is surprisingly simple yet effective: deep feature proximity makes for smooth and semantic deformation weights, with no need for additional regularization. The weights can be computed in real-time for any surface point, whereas prior methods require optimization for new handles. Moreover, the semantic prior from deep features enables co-deformation of semantic parts. We introduce an improved feature distillation pipeline, barycentric feature distillation, which efficiently uses the visual signal from shape renders to minimize distillation cost. This allows our weights to be computed for high resolution meshes in under a minute, in contrast to potentially hours for both classical and neural methods. We preserve and extend properties of classical methods through feature space constraints and locality weighting. Our field representation allows for automatic detection of semantic symmetries, which we use to produce symmetry-preserving deformations. We show a proof-of-concept application which can produce deformations for meshes up to 1 million faces in real-time on a consumer-grade machine.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u4e0e\u6570\u636e\u9a71\u52a8\u8bed\u4e49\u5148\u9a8c\u7684\u7f51\u683c\u53d8\u5f62\u6280\u672f\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u5b9e\u73b0\u5e73\u6ed1\u8bed\u4e49\u53d8\u5f62\u6743\u91cd\uff0c\u652f\u6301\u5b9e\u65f6\u8ba1\u7b97\u548c\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u5904\u7406\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u624b\u67c4\u7684\u7f51\u683c\u53d8\u5f62\u65b9\u6cd5\u9700\u8981\u7528\u6237\u9884\u5148\u77e5\u9053\u624b\u67c4\u7684\u7406\u60f3\u5206\u5e03\uff0c\u53d8\u5f62\u884c\u4e3a\u4e0e\u624b\u67c4\u8bbe\u7f6e\u4e4b\u95f4\u7684\u6620\u5c04\u4e0d\u76f4\u89c2\u4e14\u975e\u8bed\u4e49\uff1b\u800c\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u7136\u80fd\u83b7\u5f97\u8bed\u4e49\u7f16\u8f91\uff0c\u4f46\u901f\u5ea6\u6162\u4e14\u4e0d\u7cbe\u786e\u3002\u9700\u8981\u878d\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u7684\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6df1\u5ea6\u7279\u5f81\u90bb\u8fd1\u6027\u751f\u6210\u5e73\u6ed1\u8bed\u4e49\u53d8\u5f62\u6743\u91cd\uff0c\u65e0\u9700\u989d\u5916\u6b63\u5219\u5316\uff1b2\uff09\u5f15\u5165\u91cd\u5fc3\u7279\u5f81\u84b8\u998f\u7ba1\u9053\uff0c\u5229\u7528\u5f62\u72b6\u6e32\u67d3\u7684\u89c6\u89c9\u4fe1\u53f7\u6700\u5c0f\u5316\u84b8\u998f\u6210\u672c\uff1b3\uff09\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u7ea6\u675f\u548c\u5c40\u90e8\u6027\u52a0\u6743\u4fdd\u7559\u4f20\u7edf\u65b9\u6cd5\u7279\u6027\uff1b4\uff09\u652f\u6301\u8bed\u4e49\u5bf9\u79f0\u6027\u81ea\u52a8\u68c0\u6d4b\u548c\u4fdd\u6301\u3002", "result": "\u65b9\u6cd5\u80fd\u57281\u5206\u949f\u5185\u4e3a\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u8ba1\u7b97\u6743\u91cd\uff08\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\uff09\uff0c\u652f\u6301\u5b9e\u65f6\u5904\u7406\u767e\u4e07\u9762\u7f51\u683c\uff0c\u5b9e\u73b0\u8bed\u4e49\u90e8\u4ef6\u534f\u540c\u53d8\u5f62\uff0c\u5e76\u4fdd\u6301\u5bf9\u79f0\u6027\u3002", "conclusion": "\u6210\u529f\u878d\u5408\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8bed\u4e49\u5148\u9a8c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u7cbe\u786e\u63a7\u5236\u548c\u901f\u5ea6\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u7f51\u683c\u53d8\u5f62\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u3001\u8bed\u4e49\u611f\u77e5\u7684\u9ad8\u8d28\u91cf\u53d8\u5f62\u3002"}}
{"id": "2601.11622", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11622", "abs": "https://arxiv.org/abs/2601.11622", "authors": ["Hassan Ugail", "Newton Howard"], "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models", "comment": null, "summary": "Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u65f6\u95f4\u6574\u5408\u548c\u4e9a\u7a33\u6001\u6982\u5ff5\u5e94\u7528\u4e8eTransformer\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u590d\u5408\u52a8\u529b\u5b66\u6307\u6807\u6765\u91cf\u5316LLM\u5728\u4e0d\u540c\u529f\u80fd\u72b6\u6001\u4e0b\u7684\u8ba1\u7b97\u7ec4\u7ec7\u5dee\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u9ad8\u7ef4\u5185\u90e8\u52a8\u529b\u5b66\u8fdb\u884c\u6587\u672c\u751f\u6210\uff0c\u4f46\u8fd9\u4e9b\u52a8\u529b\u5b66\u7684\u65f6\u95f4\u7ec4\u7ec7\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u591a\u5173\u6ce8\u9759\u6001\u8868\u793a\u6216\u56e0\u679c\u5e72\u9884\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u7ed3\u6784\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u65f6\u95f4\u6574\u5408\u548c\u4e9a\u7a33\u6001\u4f5c\u4e3a\u795e\u7ecf\u7ec4\u7ec7\u6838\u5fc3\u6807\u5fd7\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u8fd9\u4e9b\u6982\u5ff5\u5e94\u7528\u4e8eTransformer\u6a21\u578b\u3002", "method": "\u7814\u7a76\u8005\u5c06\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u65f6\u95f4\u6574\u5408\u548c\u4e9a\u7a33\u6001\u6982\u5ff5\u9002\u914d\u5230Transformer\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u590d\u5408\u52a8\u529b\u5b66\u6307\u6807\uff0c\u8be5\u6307\u6807\u57fa\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u65f6\u95f4\u5e8f\u5217\u8ba1\u7b97\u3002\u5728GPT-2-medium\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86\u4e94\u79cd\u6761\u4ef6\uff1a\u7ed3\u6784\u5316\u63a8\u7406\u3001\u5f3a\u5236\u91cd\u590d\u3001\u9ad8\u6e29\u566a\u58f0\u91c7\u6837\u3001\u6ce8\u610f\u529b\u5934\u526a\u679d\u548c\u6743\u91cd\u566a\u58f0\u6ce8\u5165\u3002", "result": "\u7ed3\u6784\u5316\u63a8\u7406\u76f8\u6bd4\u91cd\u590d\u3001\u566a\u58f0\u548c\u6270\u52a8\u72b6\u6001\u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6307\u6807\u503c\u3002\u5355\u56e0\u7d20\u65b9\u5dee\u5206\u6790\u786e\u8ba4\u4e86\u7edf\u8ba1\u663e\u8457\u6027\u5dee\u5f02\uff0c\u5173\u952e\u6bd4\u8f83\u4e2d\u663e\u793a\u51fa\u5927\u6548\u5e94\u91cf\u3002\u8fd9\u4e9b\u7ed3\u679c\u5bf9\u5c42\u9009\u62e9\u3001\u901a\u9053\u5b50\u91c7\u6837\u548c\u968f\u673a\u79cd\u5b50\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u52a8\u529b\u5b66\u6307\u6807\u80fd\u591f\u53ef\u9760\u5730\u8868\u5f81\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u529f\u80fd\u72b6\u6001\u4e0b\u7684\u8ba1\u7b97\u7ec4\u7ec7\u5dee\u5f02\u3002\u9700\u8981\u5f3a\u8c03\u7684\u662f\uff0c\u8be5\u6307\u6807\u6355\u83b7\u7684\u662f\u5f62\u5f0f\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5e76\u4e0d\u6697\u793a\u4e3b\u89c2\u4f53\u9a8c\u3002"}}
{"id": "2601.11567", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11567", "abs": "https://arxiv.org/abs/2601.11567", "authors": ["Vanessa D'Amario", "Randy Daniel", "Alessandro Zanetti", "Dhruv Edamadaka", "Nitya Alaparthy", "Joshua Tarkoff"], "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology", "comment": "20 pages, 11 figures, accepted at 47 workshop Reproducible Artificial Intelligence (AAAI 2026, Singapore, January 27, 2026)", "summary": "Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.", "AI": {"tldr": "\u8bc4\u4f30\u516d\u4e2a\u5c0f\u578b\u5f00\u6e90\u533b\u7597LLM\u5728\u513f\u79d1\u5185\u5206\u6ccc\u5b66\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u4e00\u81f4\u6027\u4e0d\u4ee3\u8868\u6b63\u786e\u6027\uff0c\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\u4f1a\u5bfc\u81f4\u8f93\u51fa\u5206\u6b67\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5168\u9762\u7684\u8bca\u65ad\u6846\u67b6", "motivation": "\u5c0f\u578b\u5f00\u6e90\u533b\u7597LLM\u4e3a\u4f4e\u8d44\u6e90\u90e8\u7f72\u548c\u66f4\u5e7f\u6cdb\u53ef\u53ca\u6027\u63d0\u4f9b\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u901a\u5e38\u5c40\u9650\u4e8e\u533b\u5b66\u591a\u9009\u9898\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u6216\u63a8\u7406\u884c\u4e3a\u7684\u8bc4\u4f30", "method": "\u4f7f\u7528\u591a\u9009\u9898\u7ed3\u5408\u4eba\u5de5\u8bc4\u4f30\u548c\u4e34\u5e8a\u5ba1\u67e5\uff0c\u8bc4\u4f30\u516d\u4e2a\u5c0f\u578b\u5f00\u6e90\u533b\u7597LLM\uff1b\u5728\u786e\u5b9a\u6027\u8bbe\u7f6e\u4e2d\u68c0\u67e5\u63d0\u793a\u53d8\u5316\u5bf9\u6a21\u578b\u8f93\u51fa\u548c\u81ea\u6211\u8bc4\u4f30\u504f\u5dee\u7684\u5f71\u54cd\uff1b\u5728\u968f\u673a\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u8f93\u51fa\u53d8\u5f02\u6027\u5e76\u7814\u7a76\u4e00\u81f4\u6027\u4e0e\u6b63\u786e\u6027\u7684\u5173\u7cfb", "result": "HuatuoGPT-o1-8B\u8868\u73b0\u6700\u4f73\uff1b\u9ad8\u4e00\u81f4\u6027\u4e0d\u662f\u6b63\u786e\u6027\u7684\u6307\u6807\uff1bHuatuoGPT-o1-8B\u548cDiabetica-o1\u8868\u73b0\u51fa\u81ea\u6211\u8bc4\u4f30\u504f\u5dee\u548c\u5bf9\u5019\u9009\u89e3\u91ca\u987a\u5e8f\u7684\u4f9d\u8d56\uff1b\u4e13\u5bb6\u5ba1\u67e5\u53d1\u73b0\u4e34\u5e8a\u53ef\u63a5\u53d7\u54cd\u5e94\u548c\u4e34\u5e8a\u758f\u5ffd\u7684\u6df7\u5408\uff1b\u7cfb\u7edf\u7ea7\u6270\u52a8\uff08\u5982CUDA\u6784\u5efa\u5dee\u5f02\uff09\u4f1a\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u7684\u7edf\u8ba1\u663e\u8457\u53d8\u5316", "conclusion": "\u5fae\u5c0f\u8bed\u4e49\u65e0\u5173\u7684\u63d0\u793a\u6270\u52a8\u4f1a\u5bfc\u81f4\u8f93\u51fa\u5206\u6b67\uff0c\u5f15\u53d1\u5bf9LLM\u8bc4\u4f30\u53ef\u91cd\u590d\u6027\u7684\u62c5\u5fe7\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u8bca\u65ad\u6846\u67b6\u6765\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u7f3a\u9677"}}
{"id": "2601.14208", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14208", "abs": "https://arxiv.org/abs/2601.14208", "authors": ["Nitin Kulkarni", "Akhil Devarashetti", "Charlie Cluss", "Livio Forte", "Dan Buckmaster", "Philip Schneider", "Chunming Qiao", "Alina Vereshchaka"], "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting", "comment": "8 pages, 9 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u4f7f\u7528\u4e09\u6444\u50cf\u5934\u7cfb\u7edf\u6355\u6349\u8f66\u8f86\u5e95\u76d8\u89c6\u9891\uff0c\u751f\u6210\u4ea4\u4e92\u5f0f3D\u6a21\u578b\uff0c\u89e3\u51b3\u5e95\u76d8\u68c0\u67e5\u56f0\u96be\u95ee\u9898", "motivation": "\u4f20\u7edf\u8f66\u8f86\u5e95\u76d8\u68c0\u67e5\u9700\u8981\u68c0\u67e5\u5458\u8e72\u4e0b\u6216\u722c\u5230\u5e95\u76d8\u4e0b\uff0c\u52b3\u52a8\u5f3a\u5ea6\u5927\u4e14\u5371\u9669\uff1b\u5728\u7ebf\u4e70\u5bb6\u5f88\u5c11\u80fd\u770b\u5230\u5e95\u76d8\u7167\u7247\uff0c\u5f71\u54cd\u8d2d\u4e70\u4fe1\u5fc3", "method": "\u4f7f\u7528\u4e09\u6444\u50cf\u5934\u7cfb\u7edf\u6355\u6349\u5e95\u76d8\u89c6\u9891\uff0c\u91c7\u7528\u9488\u5bf9\u5bbd\u89d2\u955c\u5934\u7578\u53d8\u548c\u4f4e\u89c6\u5dee\u573a\u666f\u4f18\u5316\u7684SfM\u7ba1\u9053\uff0c\u7ed3\u5408\u7cbe\u786e\u76f8\u673a\u6807\u5b9a\u3001\u540c\u6b65\u89c6\u9891\u6d41\u548c\u76f8\u673a\u51e0\u4f55\u5148\u9a8c\uff0c\u4f7f\u7528DISK\u7279\u5f81\u63d0\u53d6\u5668\u548cLightGlue\u5339\u914d\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u7a00\u758f\u70b9\u4e91\uff0c\u518d\u901a\u8fc7\u9ad8\u65af\u6e85\u5c04\u751f\u6210\u5b9e\u65f6\u6e32\u67d3\u7684\u903c\u771f3D\u6a21\u578b", "result": "\u80fd\u591f\u751f\u6210\u4ea4\u4e92\u5f0f3D\u5e95\u76d8\u6a21\u578b\uff0c\u652f\u6301\u65cb\u8f6c\u3001\u7f29\u653e\u548c\u5207\u7247\u64cd\u4f5c\uff0c\u53ef\u5728\u51e0\u79d2\u5185\u68c0\u6d4b\u9508\u8680\u3001\u6cc4\u6f0f\u6216\u78b0\u649e\u635f\u574f\uff0c\u663e\u8457\u63d0\u9ad8\u5de5\u4f5c\u573a\u6240\u5b89\u5168\u548c\u4e70\u5bb6\u4fe1\u5fc3", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u901a\u8fc7\u4e13\u95e8\u7684SfM\u7ba1\u9053\u514b\u670d\u4e86\u5bbd\u89d2\u955c\u5934\u7578\u53d8\u548c\u4f4e\u89c6\u5dee\u573a\u666f\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u5e95\u76d8\u5efa\u6a21\uff0c\u4e3a\u8f66\u8f86\u68c0\u67e5\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.11625", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11625", "abs": "https://arxiv.org/abs/2601.11625", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance", "comment": "8 pages, Submitted to ACL Rolling Review and is under review", "summary": "Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65f6\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u5fae\u8c03\u8fc7\u7a0b\u4e2dtoken\u7ea7\u522b\u5f52\u56e0\u7684\u53d8\u5316\u6765\u76d1\u63a7\u6a21\u578b\u51b3\u7b56\u8bc1\u636e\u7684\u6f14\u53d8\uff0c\u5e76\u5f15\u5165\u4e86\u63a8\u7406\u7a33\u5b9a\u70b9(RSP)\u7684\u6982\u5ff5\u3002", "motivation": "\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4f1a\u5fae\u5999\u5730\u6539\u53d8\u6a21\u578b\u4f9d\u8d56\u7684\u8bc1\u636e\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u76d1\u63a7\u51b3\u7b56\u8bc1\u636e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6f14\u53d8\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u6ce8\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "method": "\u63d0\u51fa\u89e3\u91ca\u6f02\u79fb\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u5728\u56fa\u5b9a\u63a2\u6d4b\u96c6\u4e0a\u5f52\u4e00\u5316token\u5f52\u56e0\u7684epoch\u95f4\u53d8\u5316\uff0c\u5e76\u5f15\u5165\u63a8\u7406\u7a33\u5b9a\u70b9(RSP)\u4f5c\u4e3a\u6f02\u79fb\u9996\u6b21\u8fdb\u5165\u5e76\u4fdd\u6301\u4f4e\u7a33\u5b9a\u72b6\u6001\u7684\u6700\u65e9epoch\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6f02\u79fb\u52a8\u6001\uff0c\u65e0\u9700\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8fdb\u884c\u8c03\u4f18\u3002", "result": "\u5728\u591a\u4e2a\u8f7b\u91cf\u7ea7Transformer\u5206\u7c7b\u5668\u548c\u57fa\u51c6\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6f02\u79fb\u901a\u5e38\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u8fdb\u5165\u4f4e\u7a33\u5b9a\u72b6\u6001\uff0c\u800c\u9a8c\u8bc1\u51c6\u786e\u7387\u4ec5\u53d1\u751f\u5fae\u5c0f\u53d8\u5316\u3002\u5728\u53d7\u63a7\u7684\u6377\u5f84\u8bbe\u7f6e\u4e2d\uff0c\u5f52\u56e0\u52a8\u6001\u80fd\u591f\u66b4\u9732\u6a21\u578b\u5bf9\u6377\u5f84\u6807\u8bb0\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u5373\u4f7f\u9a8c\u8bc1\u51c6\u786e\u7387\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u89e3\u91ca\u6f02\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u76d1\u63a7\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51b3\u7b56\u8bc1\u636e\u7684\u6f14\u53d8\uff0c\u5e76\u9009\u62e9\u5904\u4e8e\u7a33\u5b9a\u8bc1\u636e\u72b6\u6001\u7684\u68c0\u67e5\u70b9\u3002"}}
{"id": "2601.11573", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11573", "abs": "https://arxiv.org/abs/2601.11573", "authors": ["Muhammad Muneeb", "David B. Ascher"], "title": "An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT", "comment": null, "summary": "Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\\% and 70\\% for PRSGPT and 6\\% and 18\\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684LLM\u5fae\u8c03\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u7269\u4fe1\u606f\u5b66\u9886\u57df\uff0c\u521b\u5efa\u4e86PRSGPT\u548cBioStarsGPT\u4e24\u4e2a\u4e13\u4e1a\u6a21\u578b\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u590d\u6742\u751f\u7269\u4fe1\u606f\u5b66\u5e94\u7528\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u4fe1\u606f\u5b66\u9886\u57df\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "method": "\u4e5d\u6b65\u7ba1\u9053\uff1a\u6574\u5408\u591a\u6837\u6570\u636e\u6e90\u3001\u7ed3\u6784\u5316\u9884\u5904\u7406\u3001\u57fa\u4e8e\u63d0\u793a\u7684QA\u751f\u6210\uff08\u4f7f\u7528Google Gemini\uff09\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8d28\u91cf\u63a7\u5236\u3001\u8bed\u4e49\u53bb\u91cd\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u6570\u636e\u5206\u5272\u3001\u4f7f\u7528LoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u4e86\u4e09\u4e2aLLM\uff08LLaMA-3.2-3B\u3001Qwen2.5-7B\u3001Gemma\uff09\uff0cQwen2.5-7B\u8868\u73b0\u6700\u4f73\u3002PRSGPT\u5728BLEU-4\u548cROUGE-1\u4e0a\u5206\u522b\u63d0\u534782%\u548c70%\uff0cBioStarsGPT\u63d0\u53476%\u548c18%\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793aPRSGPT\u5728PRS\u5de5\u5177\u6bd4\u8f83\u4efb\u52a1\u4e0a\u8fbe\u523061.9%\u51c6\u786e\u7387\uff0c\u4e0eGoogle Gemini\u76f8\u5f53\u4f46\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u65b9\u6cd5\u7ec6\u8282\u548c\u51c6\u786e\u5f15\u7528\u3002", "conclusion": "\u8be5\u7ba1\u9053\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u9886\u57df\u7279\u5b9aLLM\u5fae\u8c03\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u3001\u672c\u5730\u90e8\u7f72\u7684\u751f\u7269\u4fe1\u606f\u5b66\u52a9\u624b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3001\u9650\u5236\u548c\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2601.11627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11627", "abs": "https://arxiv.org/abs/2601.11627", "authors": ["Hassan Ugail", "Jan Ritch-Frel", "Irina Matuzava"], "title": "Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings", "comment": null, "summary": "Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u7684\u5386\u53f2\u7ed8\u753b\u8ba4\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u5728\u5c0f\u578b\u53c2\u8003\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u827a\u672f\u5bb6\u7279\u5b9a\u9a8c\u8bc1\u5668\uff0c\u5728900\u6b21\u9a8c\u8bc1\u6d4b\u8bd5\u4e2d\u8fbe\u523083.3%\u771f\u63a5\u53d7\u7387\u548c9.5%\u5047\u63a5\u53d7\u7387\u3002", "motivation": "\u7eb8\u8d28\u4f5c\u54c1\u7684\u8eab\u4efd\u9a8c\u8bc1\u548c\u5f52\u5c5e\u9274\u5b9a\u5728\u6587\u5316\u9057\u4ea7\u9886\u57df\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u53c2\u8003\u8bed\u6599\u5e93\u5c0f\u3001\u98ce\u683c\u7ebf\u7d22\u4e3b\u8981\u901a\u8fc7\u7ebf\u6761\u548c\u6709\u9650\u8272\u8c03\u53d8\u5316\u8868\u8fbe\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8865\u5145\u4f20\u7edf\u9274\u8d4f\u65b9\u6cd5\u3001\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u73af\u5883\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5355\u7c7b\u81ea\u7f16\u7801\u5668\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u624b\u5de5\u7279\u5f81\uff08\u5085\u91cc\u53f6\u57df\u80fd\u91cf\u3001\u9999\u519c\u71b5\u3001\u5168\u5c40\u5bf9\u6bd4\u5ea6\u3001GLCM\u540c\u8d28\u6027\u3001\u76d2\u8ba1\u6570\u5206\u5f62\u590d\u6742\u5ea6\uff09\u8bad\u7ec3\u827a\u672f\u5bb6\u7279\u5b9a\u9a8c\u8bc1\u5668\u3002\u4ece\u591a\u4e2a\u535a\u7269\u9986\u6536\u85cf\u4e2d\u83b7\u53d6\u771f\u5b9e\u7d20\u63cf\u4f5c\u54c1\uff0c\u91c7\u7528\u751f\u7269\u7279\u5f81\u8bc6\u522b\u5f0f\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728900\u6b21\u9a8c\u8bc1\u51b3\u7b56\uff0890\u6b21\u771f\u5b9e\u8bd5\u9a8c\u548c810\u6b21\u5192\u540d\u8bd5\u9a8c\uff09\u4e2d\uff0c\u603b\u4f53\u7cfb\u7edf\u5728\u9009\u5b9a\u64cd\u4f5c\u70b9\u8fbe\u523083.3%\u771f\u63a5\u53d7\u7387\u548c9.5%\u5047\u63a5\u53d7\u7387\u3002\u4e0d\u540c\u827a\u672f\u5bb6\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u90e8\u5206\u9a8c\u8bc1\u5668\u5047\u63a5\u53d7\u7387\u63a5\u8fd1\u96f6\uff0c\u800c\u5176\u4ed6\u9a8c\u8bc1\u5668\u6df7\u6dc6\u5ea6\u8f83\u9ad8\u3002\u9519\u8bef\u63a5\u53d7\u5206\u6790\u663e\u793a\u4e0e\u98ce\u683c\u63a5\u8fd1\u6027\u548c\u5171\u4eab\u7ed8\u753b\u60ef\u4f8b\u76f8\u5173\u7684\u7ed3\u6784\u5316\u9519\u8bef\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u8865\u5145\u800c\u975e\u53d6\u4ee3\u4f20\u7edf\u9274\u8d4f\uff0c\u4e3a\u5386\u53f2\u7d20\u63cf\u5f52\u5c5e\u9274\u5b9a\u4e2d\u5e38\u89c1\u7684\u6570\u636e\u7a00\u7f3a\u73af\u5883\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u5b9a\u91cf\u8bc1\u636e\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u63a7\u5236\u6570\u5b57\u5316\u4f2a\u5f71\u548c\u6539\u8fdb\u9608\u503c\u6821\u51c6\u3002"}}
{"id": "2601.11747", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11747", "abs": "https://arxiv.org/abs/2601.11747", "authors": ["Huaxiaoyue Wang", "Sunav Choudhary", "Franck Dernoncourt", "Yu Shen", "Stefano Petrangeli"], "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement", "comment": null, "summary": "Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial success in graphic design, their pretrained knowledge on styles is often too general and misaligned with specific domain data. For example, VLMs may associate minimalism with abstract designs, whereas designers emphasize shape and color choices. Our key insight is to leverage design data -- a collection of real-world designs that implicitly capture designer's principles -- to learn design knowledge and guide stylistic improvement. We propose PRISM (PRior-Informed Stylistic Modification) that constructs and applies a design knowledge base through three stages: (1) clustering high-variance designs to capture diversity within a style, (2) summarizing each cluster into actionable design knowledge, and (3) retrieving relevant knowledge during inference to enable style-aware improvement. Experiments on the Crello dataset show that PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over baselines in style alignment. User studies further validate these results, showing that PRISM is consistently preferred by designers.", "AI": {"tldr": "PRISM\u6846\u67b6\u5229\u7528\u8bbe\u8ba1\u6570\u636e\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u805a\u7c7b\u3001\u603b\u7ed3\u548c\u68c0\u7d22\u4e09\u4e2a\u6b65\u9aa4\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u56fe\u5f62\u8bbe\u8ba1\u98ce\u683c\u6539\u8fdb\uff0c\u5728\u98ce\u683c\u5bf9\u9f50\u548c\u8bbe\u8ba1\u5e08\u504f\u597d\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u8bbe\u8ba1\u98ce\u683c\u6539\u8fdb\u65b9\u9762\u5b58\u5728\u77e5\u8bc6\u8fc7\u4e8e\u901a\u7528\u3001\u4e0e\u4e13\u4e1a\u8bbe\u8ba1\u9886\u57df\u6570\u636e\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u4f8b\u5982\uff0cVLM\u53ef\u80fd\u5c06\u6781\u7b80\u4e3b\u4e49\u4e0e\u62bd\u8c61\u8bbe\u8ba1\u5173\u8054\uff0c\u800c\u8bbe\u8ba1\u5e08\u66f4\u5173\u6ce8\u5177\u4f53\u7684\u5f62\u72b6\u548c\u989c\u8272\u9009\u62e9\u3002\u9700\u8981\u5229\u7528\u771f\u5b9e\u8bbe\u8ba1\u6570\u636e\u6765\u5b66\u4e60\u4e13\u4e1a\u8bbe\u8ba1\u77e5\u8bc6\u3002", "method": "PRISM\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u5bf9\u9ad8\u65b9\u5dee\u8bbe\u8ba1\u8fdb\u884c\u805a\u7c7b\u4ee5\u6355\u6349\u98ce\u683c\u591a\u6837\u6027\uff1b2) \u5c06\u6bcf\u4e2a\u805a\u7c7b\u603b\u7ed3\u4e3a\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u77e5\u8bc6\uff1b3) \u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u4ee5\u5b9e\u73b0\u98ce\u683c\u611f\u77e5\u7684\u6539\u8fdb\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u5e76\u5e94\u7528\u8bbe\u8ba1\u77e5\u8bc6\u5e93\u6765\u6307\u5bfc\u98ce\u683c\u4fee\u6539\u3002", "result": "\u5728Crello\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPRISM\u5728\u98ce\u683c\u5bf9\u9f50\u65b9\u9762\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u6392\u540d1.49\uff08\u8d8a\u63a5\u8fd11\u8d8a\u597d\uff09\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7ed3\u679c\uff0c\u8bbe\u8ba1\u5e08\u4e00\u81f4\u66f4\u504f\u597dPRISM\u751f\u6210\u7684\u8bbe\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8bbe\u8ba1\u6570\u636e\u6784\u5efa\u77e5\u8bc6\u5e93\uff0cPRISM\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u7528\u4e13\u4e1a\u8bbe\u8ba1\u539f\u5219\uff0c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u98ce\u683c\u6539\u8fdb\uff0c\u4e3a\u56fe\u5f62\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u4e13\u4e1a\u3001\u66f4\u7b26\u5408\u8bbe\u8ba1\u5e08\u601d\u7ef4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11575", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11575", "abs": "https://arxiv.org/abs/2601.11575", "authors": ["Sotirios Panagiotis Chytas", "Vikas Singh"], "title": "Concept Attractors in LLMs and their Applications", "comment": null, "summary": "Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.", "AI": {"tldr": "LLMs\u5185\u90e8\u8868\u793a\u53ef\u901a\u8fc7\u8fed\u4ee3\u51fd\u6570\u7cfb\u7edf\u89e3\u91ca\uff0c\u5229\u7528\u6982\u5ff5\u5438\u5f15\u5b50\u5f00\u53d1\u65e0\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u7ffb\u8bd1\u3001\u5e7b\u89c9\u51cf\u5c11\u7b49\u4efb\u52a1", "motivation": "LLMs\u5728\u4e0d\u540c\u8868\u5c42\u5f62\u5f0f\u4e0b\u4f1a\u5c06\u8bed\u4e49\u76f8\u5173\u63d0\u793a\u6620\u5c04\u5230\u76f8\u4f3c\u5185\u90e8\u8868\u793a\uff0c\u8fd9\u79cd\u6a21\u5f0f\u53ef\u4ee5\u7528\u6570\u5b66\u6846\u67b6\u89e3\u91ca\u5e76\u52a0\u4ee5\u5229\u7528", "method": "\u57fa\u4e8e\u8fed\u4ee3\u51fd\u6570\u7cfb\u7edf\u7406\u8bba\uff0c\u5c06LLM\u5c42\u89c6\u4e3a\u5411\u6982\u5ff5\u7279\u5b9a\u5438\u5f15\u5b50\u7684\u538b\u7f29\u6620\u5c04\uff0c\u5f00\u53d1\u76f4\u63a5\u64cd\u4f5c\u5438\u5f15\u5b50\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5", "result": "\u5438\u5f15\u5b50\u5e72\u9884\u65b9\u6cd5\u5728\u8bed\u8a00\u7ffb\u8bd1\u3001\u5e7b\u89c9\u51cf\u5c11\u3001\u62a4\u680f\u8bbe\u7f6e\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u95e8\u57fa\u7ebf", "conclusion": "\u5438\u5f15\u5b50\u65b9\u6cd5\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u5fae\u8c03\u65b9\u6848\uff0c\u5728\u57fa\u7ebf\u8868\u73b0\u4e0d\u4f73\u7684\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b"}}
{"id": "2601.11630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11630", "abs": "https://arxiv.org/abs/2601.11630", "authors": ["Haonan Wei", "Linyuan Wang", "Nuolin Sun", "Zhizhong Zheng", "Lei Li", "Bin Yan"], "title": "A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow", "comment": null, "summary": "Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.", "AI": {"tldr": "\u63d0\u51faSLT\uff08\u5355\u5c42Transformer\uff09\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c0628\u5c42FreeFlow\u6a21\u578b\u538b\u7f29\u4e3a\u5355\u4e2a\u5171\u4eabDiT\u5757\uff0c\u53c2\u6570\u91cf\u4ece675M\u964d\u81f34.3M\uff0c\u5e76\u5229\u7528\u5176\u5feb\u901f\u91c7\u6837\u80fd\u529b\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u70b9\uff0c\u63d0\u5347\u4e00\u6b65\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709Flow matching\u65b9\u6cd5\uff08\u5982FreeFlow\uff09\u867d\u7136\u5b9e\u73b0\u4e86\u4e00\u6b65\u751f\u6210\uff0c\u4f4628\u5c42Transformer\u67b6\u6784\u8ba1\u7b97\u91cf\u5927\u3002\u89c2\u5bdf\u5230FreeFlow\u7684\u5c42\u7ed3\u6784\u53ef\u89c6\u4e3aODE\u7684\u6b27\u62c9\u79bb\u6563\u5316\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u538b\u7f29\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528\u538b\u7f29\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u70b9\uff0c\u89e3\u51b3\u4e00\u6b65\u751f\u6210\u4e2d\u56e0\u4f4e\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u5bfc\u81f4\u7684\u751f\u6210\u8d28\u91cf\u6ce2\u52a8\u95ee\u9898\u3002", "method": "1. \u5c06FreeFlow\u768428\u5c42Transformer\u67b6\u6784\u89c6\u4e3aODE\u7684\u6b27\u62c9\u79bb\u6563\u5316\u65b9\u6848\uff0c\u5c42\u7d22\u5f15\u4f5c\u4e3a\u79bb\u6563\u65f6\u95f4\u6b65\uff1b2. \u57fa\u4e8e\u4e0eFreeFlow\u76f8\u540c\u7684\u63a8\u5bfc\u903b\u8f91\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c0628\u4e2a\u72ec\u7acbTransformer\u5757\u538b\u7f29\u4e3a\u5355\u4e2a\u5171\u4eabDiT\u5757\uff1b3. \u8bad\u7ec3\u65f6\u5339\u914d\u6559\u5e08\u5728\u591a\u4e2a\u6df1\u5ea6\u8865\u4e01\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u878d\u5408\u8fd9\u4e9b\u8865\u4e01\u7ea7\u8868\u793a\uff0c\u540c\u65f6\u5bf9\u9f50\u6559\u5e08\u7684\u6700\u7ec8\u901f\u5ea6\u9884\u6d4b\uff1b4. \u5229\u7528\u538b\u7f29\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\uff0c\u5728\u76f8\u540c\u65f6\u95f4\u5185\u7b5b\u9009\u66f4\u591a\u566a\u58f0\u7a7a\u95f4\u5019\u9009\u70b9\uff0c\u4e3a\u6559\u5e08\u6a21\u578bFreeFlow\u9009\u62e9\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\u3002", "result": "1. \u6210\u529f\u5c06\u53c2\u6570\u91cf\u4ece675M\u538b\u7f29\u81f34.3M\uff08\u51cf\u5c11\u7ea6157\u500d\uff09\uff1b2. \u5728\u76f8\u5f53\u4e8e\u6559\u5e08\u6a21\u578b\u4e24\u6b21\u968f\u673a\u91c7\u6837\u7684\u65f6\u95f4\u9884\u7b97\u5185\uff0c\u53ef\u6267\u884c\u8d85\u8fc7100\u6b21\u566a\u58f0\u7b5b\u9009\uff1b3. \u901a\u8fc7\u7b5b\u9009\u7684\u9ad8\u8d28\u91cf\u521d\u59cb\u70b9\uff0c\u6709\u6548\u907f\u514d\u4e86\u6709\u9650FreeFlow\u91c7\u6837\u6b21\u6570\u4e0b\u56e0\u4f4e\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u5bfc\u81f4\u7684\u8d28\u91cf\u6ce2\u52a8\uff1b4. \u663e\u8457\u63d0\u5347\u4e86\u4e00\u6b65\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u5e73\u5747\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "SLT\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86FreeFlow\u6a21\u578b\u7684\u6781\u81f4\u538b\u7f29\uff0c\u4e0d\u4ec5\u5927\u5e45\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\uff0c\u8fd8\u5229\u7528\u538b\u7f29\u6a21\u578b\u7684\u5feb\u901f\u91c7\u6837\u80fd\u529b\u4e3a\u6559\u5e08\u6a21\u578b\u7b5b\u9009\u9ad8\u8d28\u91cf\u521d\u59cb\u566a\u58f0\u70b9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e00\u6b65\u751f\u6210\u4e2d\u7684\u8d28\u91cf\u6ce2\u52a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u3002\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u7684\u4e00\u6b65\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.11781", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11781", "abs": "https://arxiv.org/abs/2601.11781", "authors": ["Dawood Wasif", "Terrence J. Moore", "Seunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Frederica F. Nelson", "Jin-Hee Cho"], "title": "Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles", "comment": "Submitted to ICRA 2026 (under review)", "summary": "Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.", "AI": {"tldr": "RAIL\u662f\u4e00\u4e2a\u98ce\u9669\u611f\u77e5\u7684\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u8fd0\u884c\u65f6\u4fe1\u53f7\u751f\u6210\u5165\u4fb5\u98ce\u9669\u8bc4\u5206\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u548c\u9488\u5bf9\u6027\u5b66\u4e60\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e94\u5bf9\u957f\u5c3e\u573a\u666f\u548c\u7f51\u7edc\u7269\u7406\u5165\u4fb5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u9047\u5230\u7f55\u89c1\u7684\u957f\u5c3e\u573a\u666f\u6216\u7f51\u7edc\u7269\u7406\u5165\u4fb5\u65f6\u9700\u8981\u4fdd\u6301\u5b89\u5168\u548c\u6709\u6548\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u8db3\u3002", "method": "RAIL\u878d\u5408\u4e09\u79cd\u4fe1\u53f7\uff08\u66f2\u7387\u6267\u884c\u5b8c\u6574\u6027\u3001\u78b0\u649e\u65f6\u95f4\u63a5\u8fd1\u5ea6\u548c\u89c2\u6d4b\u504f\u79fb\u4e00\u81f4\u6027\uff09\u901a\u8fc7\u52a0\u6743Noisy-OR\u751f\u6210\u5165\u4fb5\u98ce\u9669\u8bc4\u5206\u3002\u9ad8\u98ce\u9669\u65f6\u4f7f\u7528\u7279\u5b9a\u9632\u62a4\u63aa\u65bd\u6df7\u5408\u52a8\u4f5c\uff0c\u4f4e\u98ce\u9669\u65f6\u6267\u884c\u540d\u4e49\u7b56\u7565\u3002\u91c7\u7528\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u4ef2\u88c1\u9632\u62a4\u9009\u62e9\uff0c\u7ed3\u5408SAC\u4e0e\u98ce\u9669\u4f18\u5148\u56de\u653e\u548c\u53cc\u91cd\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728MetaDrive\u4e0a\uff0cRAIL\u83b7\u5f97360.65\u7684\u6d4b\u8bd5\u56de\u62a5\u30010.85\u7684\u6210\u529f\u7387\u30010.75\u7684\u5b89\u5168\u8fdd\u89c4\u7387\u548c0.0027\u7684\u5e72\u6270\u7387\uff0c\u4ec5\u8bb0\u5f5529.07\u4e2a\u8bad\u7ec3\u5b89\u5168\u8fdd\u89c4\u3002\u5728CAN\u6ce8\u5165\u548cLiDAR\u6b3a\u9a97\u653b\u51fb\u4e0b\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u5347\u81f30.68\u548c0.80\uff0c\u653b\u51fb\u4e0b\u8131\u79bb\u7387\u964d\u81f30.37\u548c0.03\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u81f30.34\u548c0.11\u3002\u5728CARLA\u4e2d\uff0c\u4ec58000\u6b65\u5c31\u83b7\u5f971609.70\u7684\u6d4b\u8bd5\u56de\u62a5\u548c0.41\u7684\u6210\u529f\u7387\u3002", "conclusion": "RAIL\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u4eba\u673a\u534f\u540c\u673a\u5236\uff0c\u5728\u5e94\u5bf9\u7f55\u89c1\u573a\u666f\u548c\u7f51\u7edc\u653b\u51fb\u65f6\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u3001\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u673a\u534f\u540c\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.11578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11578", "abs": "https://arxiv.org/abs/2601.11578", "authors": ["Ibrahim Al Azher", "Zhishuai Guo", "Hamed Alhoori"], "title": "LimAgents: Multi-Agent LLMs for Generating Research Limitations", "comment": "18 Pages, 9 figures", "summary": "Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.", "AI": {"tldr": "LimAgents\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5b9e\u8d28\u6027\u7814\u7a76\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408OpenReview\u8bc4\u8bba\u3001\u4f5c\u8005\u9648\u8ff0\u7684\u5c40\u9650\u6027\u548c\u5f15\u7528\u6587\u732e\u6765\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5c40\u9650\u6027\u5206\u6790\uff0c\u76f8\u6bd4\u96f6\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8986\u76d6\u7387\u3002", "motivation": "\u5f53\u524d\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u7814\u7a76\u5c40\u9650\u6027\u65f6\u5f80\u5f80\u4ea7\u751f\u8868\u9762\u5316\u6216\u4e00\u822c\u6027\u7684\u9648\u8ff0\uff08\u5982\u6570\u636e\u96c6\u504f\u5dee\u6216\u666e\u9002\u6027\u95ee\u9898\uff09\uff0c\u901a\u5e38\u53ea\u662f\u91cd\u590d\u4f5c\u8005\u62a5\u544a\u7684\u5c40\u9650\u6027\uff0c\u800c\u672a\u80fd\u6df1\u5165\u5206\u6790\u65b9\u6cd5\u8bba\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u5dee\u8ddd\u3002\u8bb8\u591a\u4f5c\u8005\u4e5f\u53ea\u62ab\u9732\u90e8\u5206\u6216\u7410\u788e\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5c40\u9650\u6027\u5206\u6790\u8d28\u91cf\u4e0d\u9ad8\u3002", "method": "\u63d0\u51faLimAgents\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6574\u5408OpenReview\u8bc4\u8bba\u548c\u4f5c\u8005\u9648\u8ff0\u7684\u5c40\u9650\u6027\u4f5c\u4e3a\u66f4\u5f3a\u7684\u57fa\u7840\u4e8b\u5b9e\uff0c\u540c\u65f6\u5229\u7528\u5f15\u7528\u548c\u88ab\u5f15\u6587\u732e\u6355\u6349\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u5f31\u70b9\u3002\u6846\u67b6\u5305\u542b\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u89d2\u8272\u7684\u667a\u80fd\u4f53\uff1a\u63d0\u53d6\u663e\u5f0f\u5c40\u9650\u6027\u3001\u5206\u6790\u65b9\u6cd5\u8bba\u5dee\u8ddd\u3001\u6a21\u62df\u540c\u884c\u8bc4\u5ba1\u89c6\u89d2\u3001\u5206\u6790\u6587\u732e\u80cc\u666f\u7b49\u3002Judge\u667a\u80fd\u4f53\u7cbe\u70bc\u8f93\u51fa\uff0cMaster\u667a\u80fd\u4f53\u6574\u5408\u6210\u6e05\u6670\u7684\u5c40\u9650\u6027\u96c6\u5408\u3002\u6b64\u5916\uff0c\u5f15\u5165\u57fa\u4e8eLLM-as-a-Judge\u7684\u70b9\u5f0f\u8bc4\u4f30\u534f\u8bae\uff0c\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5c40\u9650\u6027\u8986\u76d6\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLimAgents\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1aRAG+\u591a\u667a\u80fd\u4f53GPT-4o mini\u914d\u7f6e\u76f8\u6bd4\u96f6\u57fa\u7ebf\u5b9e\u73b0\u4e86+15.51%\u7684\u8986\u76d6\u7387\u63d0\u5347\uff0cLlama 3 8B\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u5b9e\u73b0\u4e86+4.41%\u7684\u6539\u8fdb\u3002", "conclusion": "LimAgents\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u7efc\u5408\u4fe1\u606f\u6e90\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc6\u522b\u663e\u5f0f\u3001\u9690\u5f0f\u3001\u540c\u884c\u8bc4\u5ba1\u5173\u6ce8\u548c\u6587\u732e\u80cc\u666f\u7684\u5c40\u9650\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u4ea7\u751f\u66f4\u5b9e\u8d28\u6027\u7684\u5c40\u9650\u6027\u5206\u6790\uff0c\u4e3a\u7814\u7a76\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11631", "abs": "https://arxiv.org/abs/2601.11631", "authors": ["Yurun Song", "Jiong Yin", "Rongjunchen Zhang", "Ian G. Harris"], "title": "Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents", "comment": null, "summary": "Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\\times$ training speedup.", "AI": {"tldr": "CCPO\u6846\u67b6\u901a\u8fc7\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\u548c\u591a\u8f6e\u4ea4\u4e92\u4f18\u5316\uff0c\u89e3\u51b3GUI\u667a\u80fd\u4f53\u4e2d\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u8f6e\u4efb\u52a1\u5b8c\u6210", "motivation": "\u591a\u8f6eGUI\u667a\u80fd\u4f53\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u9762\u4e34\u4e25\u91cd\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u622a\u65ad\u727a\u7272\u957f\u671f\u4e0a\u4e0b\u6587\uff0c\u8981\u4e48\u901a\u8fc7token\u526a\u679d\u7834\u574f\u7a7a\u95f4\u7ed3\u6784", "method": "\u63d0\u51fa\u5750\u6807\u538b\u7f29\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5305\u542b\u5750\u6807\u611f\u77e5\u7a7a\u95f4\u538b\u7f29\u548c\u591a\u8f6e\u4ea4\u4e92\u4f18\u5316\uff0c\u901a\u8fc7\u805a\u5408\u591a\u8f6e\u5750\u6807\u6355\u83b7\u76ee\u6807\u76f8\u5173\u533a\u57df\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u8ddd\u79bb\u7684\u4f18\u52bf\u51fd\u6570\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5b66\u4e60\u4fe1\u53f7", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u8fbe55%\u7684token\u538b\u7f29\u548c3.8\u500d\u7684\u8bad\u7ec3\u52a0\u901f", "conclusion": "CCPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6eGUI\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u89c9\u538b\u7f29\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u8026\u5408\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u5b8c\u6210"}}
{"id": "2601.11792", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11792", "abs": "https://arxiv.org/abs/2601.11792", "authors": ["Yifei Sun", "Yongan Li", "A. K. Qin", "Sicheng Hou", "Tamas Pflanzner"], "title": "A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation", "comment": null, "summary": "Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation tasks. Although existing LLMs can achieve high correctness rates, they generally lack innovation and exhibit poor discrimination. In this paper, we propose the task of innovative math problem generation (IMPG). To solve the IMPG task, this paper proposes a self-evolving, multi-role collaborative framework with fine-grained difficulty guidance. First, a multi-role collaborative mechanism comprising a sampler, generator, evaluator, state machine, and memory is constructed, ensuring the correctness of generated problems through iterative optimization informed by self-assessment and external feedback. Second, we introduce an improved difficulty model to quantify difficulty and provide fine-grained guidance. We adopt the data-driven association-guided path sampling (DAPS) algorithm to enhance the semantic rationality of sampled encodings. Third, we construct the HSM3K-CN dataset, which comprises high-quality high school math problems. A multi-stage training pipeline is adopted, incorporating continual pre-training (CPT), supervised fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance the generation and evaluation capabilities of the base model. Finally, system self-evolution is achieved by transferring evaluation capabilities from the expert model to the apprentice model via distillation. Experiments show that, compared to baseline models, our proposed method significantly improves the innovation of the generated problems while maintaining a high correctness rate.", "AI": {"tldr": "\u63d0\u51fa\u521b\u65b0\u6570\u5b66\u9898\u751f\u6210\u4efb\u52a1(IMPG)\uff0c\u91c7\u7528\u81ea\u8fdb\u5316\u591a\u89d2\u8272\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u96be\u5ea6\u6307\u5bfc\u548c\u6570\u636e\u9a71\u52a8\u8def\u5f84\u91c7\u6837\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u9898\u76ee\u7684\u521b\u65b0\u6027\u540c\u65f6\u4fdd\u6301\u9ad8\u6b63\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u9898\u751f\u6210\u4efb\u52a1\u4e2d\u867d\u7136\u6b63\u786e\u7387\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u521b\u65b0\u6027\u548c\u533a\u5206\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u8bc1\u6b63\u786e\u6027\u53c8\u80fd\u63d0\u5347\u521b\u65b0\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u8fdb\u5316\u591a\u89d2\u8272\u534f\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u91c7\u6837\u5668\u3001\u751f\u6210\u5668\u3001\u8bc4\u4f30\u5668\u3001\u72b6\u6001\u673a\u548c\u8bb0\u5fc6\u6a21\u5757\uff1b\u91c7\u7528\u6539\u8fdb\u7684\u96be\u5ea6\u6a21\u578b\u548cDAPS\u7b97\u6cd5\uff1b\u6784\u5efaHSM3K-CN\u6570\u636e\u96c6\uff1b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08CPT\u3001SFT\u3001GRPO\uff09\uff1b\u901a\u8fc7\u84b8\u998f\u5b9e\u73b0\u7cfb\u7edf\u81ea\u8fdb\u5316\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u95ee\u9898\u7684\u521b\u65b0\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6b63\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684IMPG\u4efb\u52a1\u548c\u81ea\u8fdb\u5316\u591a\u89d2\u8272\u534f\u4f5c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u5b66\u9898\u751f\u6210\u4e2d\u521b\u65b0\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2601.11579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11579", "abs": "https://arxiv.org/abs/2601.11579", "authors": ["Krzysztof Ociepa", "\u0141ukasz Flis", "Remigiusz Kinas", "Krzysztof Wr\u00f3bel", "Adrian Gwo\u017adziej"], "title": "Bielik 11B v3: Multilingual Large Language Model for European Languages", "comment": null, "summary": "We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.\n  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.\n  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.", "AI": {"tldr": "Bielik 11B v3 \u662f\u4e00\u4e2a\u9488\u5bf9\u6ce2\u5170\u8bed\u4f18\u5316\u7684\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e Mistral 7B v0.2 \u67b6\u6784\u6269\u5c55\u81f3 110 \u4ebf\u53c2\u6570\uff0c\u5728\u6ce2\u5170\u8bed\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5176\u4ed6\u4e13\u4e1a\u6a21\u578b\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6ce2\u5170\u8bed\u7684\u9ad8\u6027\u80fd\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u6b27\u6d32\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u8f83\u5c11\u8bed\u8a00\u5efa\u7acb\u9ad8\u6548\u6a21\u578b\u5f00\u53d1\u7684\u65b0\u57fa\u51c6\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a1) \u6301\u7eed\u9884\u8bad\u7ec3\uff1b2) \u76d1\u7763\u5fae\u8c03(SFT)\uff1b3) \u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\uff1b4) \u5f3a\u5316\u5b66\u4e60\u3002\u57fa\u4e8e Mistral 7B v0.2 \u67b6\u6784\u901a\u8fc7\u6df1\u5ea6\u6269\u5c55\u81f3 110 \u4ebf\u53c2\u6570\u3002", "result": "\u5728\u6ce2\u5170\u8bed\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u5176\u4ed6\u4e13\u4e1a\u6ce2\u5170\u8bed\u6a21\u578b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u4f18\u4e8e\u53c2\u6570\u591a 2-6 \u500d\u7684\u5927\u578b\u6a21\u578b\uff0c\u4ece\u57fa\u7840\u8bed\u8a00\u7406\u89e3\u5230\u590d\u6742\u63a8\u7406\u4efb\u52a1\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Bielik 11B v3 \u4e0d\u4ec5\u63d0\u5347\u4e86\u6ce2\u5170\u8bed\u7684 AI \u80fd\u529b\uff0c\u8fd8\u4e3a\u5f00\u53d1\u8d44\u6e90\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u8f83\u5c11\u4ee3\u8868\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u90e8\u7f72\u9002\u7528\u6027\u3002"}}
{"id": "2601.11632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11632", "abs": "https://arxiv.org/abs/2601.11632", "authors": ["Zhiyang Li", "Ao Ke", "Yukun Cao", "Xike Xie"], "title": "KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering", "comment": null, "summary": "Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.", "AI": {"tldr": "KG-ViP\u901a\u8fc7\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u6765\u89e3\u51b3MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VQA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u77e5\u8bc6\u5e7b\u89c9\uff08\u751f\u6210\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e0d\u7b26\u7684\u77e5\u8bc6\uff09\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0d\u8db3\u3002\u4f5c\u8005\u53d1\u73b0\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\u5206\u522b\u80fd\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5c06\u5b83\u4eec\u5b64\u7acb\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u534f\u540c\u6f5c\u529b\u3002", "method": "\u63d0\u51faKG-ViP\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u68c0\u7d22-\u878d\u5408\u7ba1\u9053\uff0c\u5229\u7528\u67e5\u8be2\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\uff0c\u9010\u6b65\u6574\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\uff0c\u5408\u6210\u7edf\u4e00\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u6765\u4fc3\u8fdb\u53ef\u9760\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002", "result": "\u5728FVQA 2.0+\u548cMVQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cKG-ViP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684VQA\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u573a\u666f\u56fe\u548c\u5e38\u8bc6\u56fe\uff0cKG-ViP\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728VQA\u4e2d\u7684\u77e5\u8bc6\u5e7b\u89c9\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4e24\u79cd\u56fe\u7ed3\u6784\u7684\u534f\u540c\u6f5c\u529b\u3002"}}
{"id": "2601.11809", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11809", "abs": "https://arxiv.org/abs/2601.11809", "authors": ["Zeyu Mu", "Shangtong Zhang", "B. Brian Park"], "title": "Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic", "comment": "Under review at IEEE Transactions on Intelligent Transportation Systems", "summary": "Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the sparse distribution of CAVs among human-driven vehicles reduces the likelihood of forming effective cooperative platoons. To address this challenge, this study proposes a hybrid multi-agent lane change decision model aimed at increasing CAV participation in cooperative platooning and maximizing its associated benefits. The proposed model employs the QMIX framework, integrating traffic data processed through a convolutional neural network (CNN-QMIX). This architecture addresses a critical issue in dynamic traffic scenarios by enabling CAVs to make optimal decisions irrespective of the varying number of CAVs present in mixed traffic. Additionally, a trajectory planner and a model predictive controller are designed to ensure smooth and safe lane-change execution. The proposed model is trained and evaluated within a microsimulation environment under varying CAV market penetration rates. The results demonstrate that the proposed model efficiently manages fluctuating traffic agent numbers, significantly outperforming the baseline rule-based models. Notably, it enhances cooperative platooning rates up to 26.2\\%, showcasing its potential to optimize CAV cooperation and traffic dynamics during the early stage of deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-QMIX\u7684\u6df7\u5408\u591a\u667a\u80fd\u4f53\u6362\u9053\u51b3\u7b56\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u7f51\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u6df7\u5408\u4ea4\u901a\u4e2d\u7684\u534f\u540c\u7f16\u961f\u53c2\u4e0e\u7387\uff0c\u4f18\u5316\u4ea4\u901a\u52a8\u6001\u3002", "motivation": "\u5728CAV\u90e8\u7f72\u521d\u671f\uff0c\u7f51\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684\u7a00\u758f\u5206\u5e03\u964d\u4f4e\u4e86\u5f62\u6210\u6709\u6548\u534f\u540c\u7f16\u961f\u7684\u53ef\u80fd\u6027\uff0c\u9700\u8981\u63d0\u9ad8CAV\u53c2\u4e0e\u534f\u540c\u7f16\u961f\u7684\u80fd\u529b\u4ee5\u6700\u5927\u5316\u5176\u6548\u76ca\u3002", "method": "\u91c7\u7528QMIX\u6846\u67b6\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u4ea4\u901a\u6570\u636e\uff08CNN-QMIX\uff09\uff0c\u8bbe\u8ba1\u8f68\u8ff9\u89c4\u5212\u5668\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u6362\u9053\u6267\u884c\u7684\u5e73\u6ed1\u6027\u548c\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2dCAV\u6570\u91cf\u53d8\u5316\u5e26\u6765\u7684\u51b3\u7b56\u95ee\u9898\u3002", "result": "\u5728\u5fae\u89c2\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u53d8\u5316\u7684\u4ea4\u901a\u667a\u80fd\u4f53\u6570\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5c06\u534f\u540c\u7f16\u961f\u7387\u63d0\u5347\u9ad8\u8fbe26.2%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u591a\u667a\u80fd\u4f53\u6362\u9053\u51b3\u7b56\u6a21\u578b\u80fd\u6709\u6548\u4f18\u5316CAV\u5728\u90e8\u7f72\u521d\u671f\u7684\u534f\u540c\u5408\u4f5c\u548c\u4ea4\u901a\u52a8\u6001\uff0c\u63d0\u9ad8\u534f\u540c\u7f16\u961f\u53c2\u4e0e\u7387\uff0c\u4e3a\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e0b\u7684CAV\u534f\u540c\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11580", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11580", "abs": "https://arxiv.org/abs/2601.11580", "authors": ["Xiaoxuan Liu", "Jiaxiang Yu", "Jongseok Park", "Ion Stoica", "Alvin Cheung"], "title": "Speculative Decoding: Performance or Illusion?", "comment": null, "summary": "Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.", "AI": {"tldr": "\u5bf9\u63a8\u6d4b\u89e3\u7801\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9996\u6b21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5b9e\u73b0\u4e0e\u7406\u8bba\u4e0a\u9650\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd", "motivation": "\u63a8\u6d4b\u89e3\u7801\u5df2\u6210\u4e3a\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6d41\u884c\u6280\u672f\uff0c\u4f46\u5148\u524d\u8bc4\u4f30\u4f9d\u8d56\u7814\u7a76\u539f\u578b\u548c\u4e0d\u5207\u5b9e\u9645\u7684\u5c0f\u6279\u91cf\u5927\u5c0f\uff0c\u5176\u771f\u5b9e\u4e16\u754c\u6709\u6548\u6027\u4ecd\u4e0d\u6e05\u695a", "method": "\u5728\u751f\u4ea7\u7ea7\u63a8\u7406\u5f15\u64cevLLM\u4e0a\u5bf9\u591a\u79cd\u63a8\u6d4b\u89e3\u7801\u53d8\u4f53\uff08n-gram\u3001EAGLE/EAGLE-3\u3001Draft-Model\u3001Multi-Token Prediction\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u6db5\u76d6\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u3001\u6a21\u578b\u89c4\u6a21\u548c\u6279\u91cf\u5927\u5c0f", "result": "\u76ee\u6807\u6a21\u578b\u7684\u9a8c\u8bc1\u9636\u6bb5\u4e3b\u5bfc\u6267\u884c\u65f6\u95f4\uff0c\u63a5\u53d7\u957f\u5ea6\u5728\u4e0d\u540c\u8f93\u51fa\u4f4d\u7f6e\u3001\u8bf7\u6c42\u548c\u6570\u636e\u96c6\u95f4\u5dee\u5f02\u663e\u8457\uff1b\u5b9e\u6d4b\u6027\u80fd\u4e0e\u7406\u8bba\u4e0a\u9650\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u63a8\u6d4b\u89e3\u7801\u6027\u80fd\u6539\u8fdb\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840"}}
{"id": "2601.11633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11633", "abs": "https://arxiv.org/abs/2601.11633", "authors": ["Xuchen Li", "Xuzhao Li", "Renjie Pi", "Shiyu Hu", "Jian Zhao", "Jiahui Gao"], "title": "Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images", "comment": "Preprint, Under review", "summary": "Despite the remarkable progress of Vision-Language Models (VLMs) in adopting \"Thinking-with-Images\" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.", "AI": {"tldr": "ViEBench\u662f\u4e00\u4e2a\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u5f20\u591a\u573a\u666f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u53cc\u8f74\u77e9\u9635\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5fe0\u5b9e\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u679c\u5bfc\u5411\u7684\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u591f\u51c6\u786e\u5229\u7528\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9a8c\u8bc1\u63a8\u7406\u8fc7\u7a0b\u771f\u5b9e\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b200\u5f20\u591a\u573a\u666f\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684ViEBench\u57fa\u51c6\uff0c\u6bcf\u5f20\u56fe\u50cf\u90fd\u6709\u4e13\u5bb6\u6807\u6ce8\u7684\u89c6\u89c9\u8bc1\u636e\u3002\u5c06\u4efb\u52a1\u6309\u96be\u5ea6\u5206\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u63a8\u7406\u4efb\u52a1\u9700\u8981\u7ed3\u5408\u5c40\u90e8\u89c6\u89c9\u7ec6\u8282\u548c\u5148\u9a8c\u77e5\u8bc6\u3002\u5f15\u5165\u53cc\u8f74\u77e9\u9635\u63d0\u4f9b\u56db\u4e2a\u8bca\u65ad\u8c61\u9650\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1) \u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u80fd\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u4f46\u57fa\u4e8e\u4e0d\u76f8\u5173\u533a\u57df\uff1b(2) \u6a21\u578b\u53ef\u80fd\u6210\u529f\u5b9a\u4f4d\u6b63\u786e\u8bc1\u636e\u4f46\u4ecd\u65e0\u6cd5\u5229\u7528\u5b83\u5f97\u51fa\u51c6\u786e\u7ed3\u8bba\u3002ViEBench\u80fd\u591f\u66f4\u53ef\u89e3\u91ca\u3001\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "ViEBench\u4f5c\u4e3a\u4e00\u4e2a\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u66f4\u53ef\u89e3\u91ca\u3001\u66f4\u5b9e\u9645\u5730\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5fe0\u5b9e\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11816", "abs": "https://arxiv.org/abs/2601.11816", "authors": ["Zahra Moslemi", "Keerthi Koneru", "Yen-Ting Lee", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation", "comment": "Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks: AAAI 2026", "summary": "Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation", "AI": {"tldr": "POLARIS\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u540e\u53f0\u5de5\u4f5c\u6d41\u7684\u6cbb\u7406\u578bLLM\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u578b\u5316\u8ba1\u5212\u5408\u6210\u548c\u9a8c\u8bc1\u6267\u884c\uff0c\u786e\u4fdd\u81ea\u52a8\u5316\u8fc7\u7a0b\u53ef\u5ba1\u8ba1\u3001\u7b26\u5408\u653f\u7b56\u4e14\u64cd\u4f5c\u53ef\u9884\u6d4b\u3002", "motivation": "\u4f01\u4e1a\u540e\u53f0\u5de5\u4f5c\u6d41\u9700\u8981\u53ef\u5ba1\u8ba1\u3001\u7b26\u5408\u653f\u7b56\u4e14\u64cd\u4f5c\u53ef\u9884\u6d4b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u800c\u901a\u7528\u7684\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u5f80\u5f80\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "POLARIS\u91c7\u7528\u6cbb\u7406\u578b\u7f16\u6392\u6846\u67b6\uff0c\u5c06\u81ea\u52a8\u5316\u89c6\u4e3a\u7c7b\u578b\u5316\u8ba1\u5212\u5408\u6210\u548c\u9a8c\u8bc1\u6267\u884c\uff1a1\uff09\u89c4\u5212\u5668\u751f\u6210\u7ed3\u6784\u591a\u6837\u3001\u7c7b\u578b\u68c0\u67e5\u7684\u6709\u5411\u65e0\u73af\u56fe\uff1b2\uff09\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\u6a21\u5757\u9009\u62e9\u5408\u89c4\u8ba1\u5212\uff1b3\uff09\u6267\u884c\u8fc7\u7a0b\u901a\u8fc7\u9a8c\u8bc1\u5668\u95e8\u63a7\u68c0\u67e5\u3001\u6709\u754c\u4fee\u590d\u5faa\u73af\u548c\u7f16\u8bd1\u7b56\u7565\u62a4\u680f\u8fdb\u884c\u4fdd\u62a4\uff0c\u5728\u526f\u4f5c\u7528\u53d1\u751f\u524d\u8fdb\u884c\u963b\u6b62\u6216\u8def\u7531\u3002", "result": "\u5728\u6587\u6863\u4e2d\u5fc3\u91d1\u878d\u4efb\u52a1\u4e2d\uff0cPOLARIS\u751f\u6210\u51b3\u7b56\u7ea7\u5de5\u4ef6\u548c\u5b8c\u6574\u6267\u884c\u8ddf\u8e2a\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002\u5728SROIE\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.81\u7684\u5faeF1\u5206\u6570\uff0c\u5728\u53d7\u63a7\u5408\u6210\u5957\u4ef6\u4e2d\u5b9e\u73b00.95\u52301.00\u7684\u5f02\u5e38\u8def\u7531\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u5ba1\u8ba1\u8ddf\u8e2a\u3002", "conclusion": "POLARIS\u4e3a\u653f\u7b56\u5bf9\u9f50\u7684\u667a\u80fd\u4f53AI\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u57fa\u51c6\u53c2\u8003\uff0c\u6784\u6210\u4e86\u6cbb\u7406\u578b\u667a\u80fd\u4f53AI\u7684\u521d\u6b65\u57fa\u51c6\u3002"}}
{"id": "2601.11581", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11581", "abs": "https://arxiv.org/abs/2601.11581", "authors": ["Yuefeng Wang", "ChangJae Lee"], "title": "Enhancing the QA Model through a Multi-domain Debiasing Framework", "comment": "5 pages, 7 tables", "summary": "Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.", "AI": {"tldr": "ELECTRA-small\u6a21\u578b\u5728SQuAD\u548c\u5bf9\u6297\u6570\u636e\u96c6\u4e0a\u7684\u504f\u89c1\u5206\u6790\uff0c\u901a\u8fc7\u591a\u9886\u57df\u53bb\u504f\u6846\u67b6\u63d0\u5347\u6027\u80fd2.6\u4e2a\u767e\u5206\u70b9", "motivation": "QA\u6a21\u578b\u5728\u673a\u5668\u9605\u8bfb\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e38\u5e38\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u590d\u6742\u67e5\u8be2\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u4e9b\u504f\u89c1\u963b\u788d\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u9488\u5bf9\u6027\u89e3\u51b3\u3002", "method": "\u8bc4\u4f30ELECTRA-small\u6a21\u578b\u5728SQuAD v1.1\u548c\u5bf9\u6297\u6570\u636e\u96c6AddSent\u3001AddOneSent\u4e0a\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u8bcd\u6c47\u504f\u89c1\u3001\u6570\u503c\u63a8\u7406\u548c\u5b9e\u4f53\u8bc6\u522b\u9519\u8bef\uff0c\u5f00\u53d1\u5305\u542b\u77e5\u8bc6\u84b8\u998f\u3001\u53bb\u504f\u6280\u672f\u548c\u9886\u57df\u6269\u5c55\u7684\u591a\u9886\u57df\u53bb\u504f\u6846\u67b6\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.6\u4e2a\u767e\u5206\u70b9\u7684Exact Match\u548cF1\u5206\u6570\u63d0\u5347\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u589e\u76ca\u3002", "conclusion": "\u9488\u5bf9\u6027\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u6709\u6f5c\u529b\u589e\u5f3a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u591a\u9886\u57df\u53bb\u504f\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347QA\u6a21\u578b\u5728\u590d\u6742\u548c\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2601.11634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11634", "abs": "https://arxiv.org/abs/2601.11634", "authors": ["Chenghui Yu", "Hongwei Wang", "Junwen Chen", "Zixuan Wang", "Bingfeng Deng", "Zhuolin Hao", "Hongyu Xiong", "Yang Song"], "title": "When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms", "comment": null, "summary": "Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6a21\u6001LLM\u4ee3\u7406\u7684\u81ea\u52a8\u95ee\u9898\u53d1\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u53d1\u73b0\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u65b0\u5174\u5185\u5bb9\u95ee\u9898\u5e76\u66f4\u65b0\u6807\u6ce8\u7b56\u7565", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u8d8b\u52bf\u5feb\u901f\u6f14\u53d8\uff0c\u4f20\u7edf\u4eba\u5de5\u53d1\u73b0\u95ee\u9898\u901f\u5ea6\u592a\u6162\uff0c\u5bfc\u81f4\u6807\u6ce8\u7b56\u7565\u66f4\u65b0\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5185\u5bb9\u6cbb\u7406\u6548\u679c", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4ee3\u7406\u81ea\u52a8\u53ec\u56de\u5305\u542b\u6f5c\u5728\u65b0\u95ee\u9898\u7684\u77ed\u89c6\u9891\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u805a\u7c7b\u7b56\u7565\u5206\u7ec4\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u5bf9\u5e94\u4e00\u4e2a\u65b0\u53d1\u73b0\u7684\u95ee\u9898\uff0c\u7136\u540e\u4ee3\u7406\u4ece\u8fd9\u4e9b\u96c6\u7fa4\u751f\u6210\u66f4\u65b0\u7684\u6807\u6ce8\u7b56\u7565", "result": "\u8be5\u65b9\u6cd5\u5df2\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u90e8\u7f72\uff0c\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u663e\u793a\uff1a\u65b0\u5174\u95ee\u9898\u53d1\u73b0\u6548\u679c\u663e\u8457\u63d0\u5347\uff08F1\u5206\u6570\u63d0\u9ad8\u8d85\u8fc720%\uff09\uff0c\u540e\u7eed\u95ee\u9898\u6cbb\u7406\u6027\u80fd\u589e\u5f3a\uff08\u95ee\u9898\u89c6\u9891\u89c2\u770b\u91cf\u51cf\u5c11\u7ea615%\uff09\uff0c\u76f8\u6bd4\u4eba\u5de5\u53d1\u73b0\u5927\u5e45\u51cf\u5c11\u65f6\u95f4\u6210\u672c\u5e76\u52a0\u901f\u6807\u6ce8\u7b56\u7565\u8fed\u4ee3", "conclusion": "\u57fa\u4e8e\u591a\u6a21\u6001LLM\u4ee3\u7406\u7684\u81ea\u52a8\u95ee\u9898\u53d1\u73b0\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u6cbb\u7406\u4e2d\u7684\u65b0\u5174\u95ee\u9898\u53d1\u73b0\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6cbb\u7406\u6548\u7387\u548c\u6548\u679c"}}
{"id": "2601.11825", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.11825", "abs": "https://arxiv.org/abs/2601.11825", "authors": ["Arya Rahgozar", "Pouria Mortezaagha"], "title": "AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept", "comment": null, "summary": "Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8ePICOS\u6846\u67b6\u7684AI\u534f\u540c\u79d1\u5b66\u5bb6\u5e73\u53f0\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u3001\u900f\u660e\u7684\u77e5\u8bc6\u5408\u6210\uff0c\u901a\u8fc7\u81ea\u52a8\u5316PICOS\u5408\u89c4\u68c0\u6d4b\u3001\u7814\u7a76\u8bbe\u8ba1\u5206\u7c7b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u51cf\u5c11\u751f\u7269\u533b\u5b66\u7814\u7a76\u6d6a\u8d39\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u5b58\u5728\u7814\u7a76\u6d6a\u8d39\u95ee\u9898\uff0c\u5305\u62ec\u5197\u4f59\u7814\u7a76\u3001\u4e0d\u5b8c\u6574\u62a5\u544a\u548c\u4f20\u7edf\u8bc1\u636e\u5408\u6210\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u9ad8\u77e5\u8bc6\u5408\u6210\u53ef\u6269\u5c55\u6027\u3001\u900f\u660e\u5ea6\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8ePICOS\u6846\u67b6\u7684AI\u5e73\u53f0\uff0c\u6574\u5408\u5173\u7cfb\u5b58\u50a8\u3001\u5411\u91cf\u8bed\u4e49\u68c0\u7d22\u548cNeo4j\u77e5\u8bc6\u56fe\u8c31\u3002\u4f7f\u7528Bi-LSTM\u548c\u57fa\u4e8ePubMedBERT\u7684transformer\u591a\u4efb\u52a1\u5206\u7c7b\u5668\u8fdb\u884cPICOS\u5408\u89c4\u548c\u7814\u7a76\u8bbe\u8ba1\u5206\u7c7b\u3002\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7ed3\u5408\u5411\u91cf\u548c\u56fe\u68c0\u7d22\u8fdb\u884c\u5168\u6587\u5408\u6210\uff0c\u4f7f\u7528BERTopic\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\u3002", "result": "transformer\u6a21\u578b\u5728\u7814\u7a76\u8bbe\u8ba1\u5206\u7c7b\u4e0a\u8fbe\u523095.7%\u51c6\u786e\u7387\uff0cBi-LSTM\u5728PICOS\u5408\u89c4\u68c0\u6d4b\u4e0a\u8fbe\u523087%\u51c6\u786e\u7387\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u9700\u8981\u7ed3\u6784\u5316\u7ea6\u675f\u3001\u8de8\u7814\u7a76\u6574\u5408\u548c\u56fe\u63a8\u7406\u7684\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u68c0\u7d22\u65b9\u6cd5\u3002\u4e3b\u9898\u5efa\u6a21\u63ed\u793a\u4e86\u5927\u91cf\u4e3b\u9898\u5197\u4f59\u5e76\u8bc6\u522b\u4e86\u672a\u5145\u5206\u63a2\u7d22\u7684\u7814\u7a76\u9886\u57df\u3002", "conclusion": "\u57fa\u4e8ePICOS\u6846\u67b6\u548c\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684AI\u5e73\u53f0\u80fd\u591f\u63d0\u9ad8\u8bc1\u636e\u5408\u6210\u7684\u53ef\u6269\u5c55\u6027\u3001\u900f\u660e\u5ea6\u548c\u6548\u7387\uff0c\u51cf\u5c11\u751f\u7269\u533b\u5b66\u7814\u7a76\u6d6a\u8d39\u3002\u8be5\u67b6\u6784\u5177\u6709\u9886\u57df\u65e0\u5173\u6027\uff0c\u4e3a\u8de8\u751f\u7269\u533b\u5b66\u5b66\u79d1\u51cf\u5c11\u7814\u7a76\u6d6a\u8d39\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2601.11585", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11585", "abs": "https://arxiv.org/abs/2601.11585", "authors": ["Hyunjun Kim"], "title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents", "comment": null, "summary": "Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.", "AI": {"tldr": "\u63d0\u51faEntropic Context Shaping (ECS)\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u8861\u91cf\u4e0a\u4e0b\u6587\u5bf9LLM\u56de\u7b54\u95ee\u9898\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u76f8\u6bd4\u4f20\u7edf\u8bcd\u6cd5\u76f8\u4f3c\u5ea6\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e0a\u4e0b\u6587\u9009\u62e9\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u9700\u8981\u533a\u5206\u771f\u6b63\u6709\u7528\u7684\u4fe1\u606f\u548c\u8bef\u5bfc\u6027\u5e72\u6270\u4fe1\u606f\u3002\u4f20\u7edf\u57fa\u4e8e\u8bcd\u6cd5\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4e0a\u4e0b\u6587\u7684\u8bed\u7528\u6548\u7528\uff0c\u5373\u67d0\u4e2a\u6bb5\u843d\u662f\u5426\u771f\u6b63\u6709\u52a9\u4e8e\u56de\u7b54\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6ECS\uff0c\u901a\u8fc7\u6a21\u578b\u7b54\u6848\u5206\u5e03\u5411\u6b63\u786e\u7b54\u6848\u7684\u504f\u79fb\u6765\u6d4b\u91cf\u4e0a\u4e0b\u6587\u6548\u7528\u3002\u5c06\u6548\u7528\u5f62\u5f0f\u5316\u4e3a\u7b54\u6848\u6982\u7387\u7684\u6709\u7b26\u53f7\u53d8\u5316\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u8868\u660e\u4efb\u52a1\u65e0\u5173\u7684\u66f4\u65b0\u4f1a\u4ea7\u751f\u63a5\u8fd1\u96f6\u7684\u5206\u5e03\u504f\u79fb\u3002", "result": "\u5728LongMemEval\u548cLoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECS\u5728\u7ec6\u7c92\u5ea6\u8f6e\u6b21\u9009\u62e9\u4e0a\u8fbe\u5230F1=0.265\uff0c\u76f8\u6bd4TF-IDF\u7684F1=0.154\u670971.83%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u8bc1\u660e\u8bed\u7528\u6548\u7528\u65b9\u6cd5\u5728\u7cbe\u786e\u4e0a\u4e0b\u6587\u9009\u62e9\u4e0a\u4f18\u4e8e\u8bcd\u6cd5\u76f8\u4f3c\u5ea6\u3002", "conclusion": "ECS\u6846\u67b6\u80fd\u591f\u6709\u6548\u8861\u91cf\u4e0a\u4e0b\u6587\u7684\u8bed\u7528\u6548\u7528\uff0c\u5728\u9700\u8981\u7cbe\u786e\u4e0a\u4e0b\u6587\u9009\u62e9\u7684\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bcd\u6cd5\u76f8\u4f3c\u5ea6\u65b9\u6cd5\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11635", "abs": "https://arxiv.org/abs/2601.11635", "authors": ["Anil Egin", "Andrea Tangherloni", "Antitza Dantcheva"], "title": "Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos", "comment": null, "summary": "Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.", "AI": {"tldr": "\u63d0\u51faAnon-NET\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u89c6\u9891\u9a71\u52a8\u52a8\u753b\u5b9e\u73b0\u4eba\u8138\u89c6\u9891\u533f\u540d\u5316\uff0c\u5728\u4fdd\u62a4\u8eab\u4efd\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u7559\u5e74\u9f84\u3001\u6027\u522b\u3001\u79cd\u65cf\u3001\u59ff\u6001\u548c\u8868\u60c5\u7b49\u5c5e\u6027\u3002", "motivation": "\u4eba\u8138\u89c6\u9891\u533f\u540d\u5316\u9700\u8981\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5141\u8bb8\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8868\u60c5\u8bc6\u522b\u3001\u4eba\u5458\u8ddf\u8e2a\u3001\u52a8\u4f5c\u8bc6\u522b\uff09\u7684\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u53bb\u8eab\u4efd\u5316\u7684\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u89c6\u9891\u7684\u89c6\u89c9\u5c5e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "1. \u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4eba\u8138\u4fee\u590d\uff0c\u901a\u8fc7\u9ad8\u7ea7\u5c5e\u6027\u8bc6\u522b\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u8868\u60c5\u8f6c\u79fb\u6765\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u30022. \u89c6\u9891\u9a71\u52a8\u52a8\u753b\u6280\u672f\uff0c\u5c06\u53bb\u8eab\u4efd\u5316\u7684\u4eba\u8138\u4e0e\u539f\u59cb\u89c6\u9891\u7ed3\u5408\u8fdb\u884c\u52a8\u753b\u5316\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728VoxCeleb2\u3001CelebV-HQ\u548cHDTF\u7b49\u5305\u542b\u591a\u6837\u5316\u9762\u90e8\u52a8\u6001\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eAnon-NET\u5728\u6df7\u6dc6\u8eab\u4efd\u7684\u540c\u65f6\uff0c\u80fd\u4fdd\u6301\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "Anon-NET\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u4eba\u8138\u89c6\u9891\u533f\u540d\u5316\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u7684\u89c6\u89c9\u5c5e\u6027\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2601.11840", "categories": ["cs.AI", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11840", "abs": "https://arxiv.org/abs/2601.11840", "authors": ["Hongyu Lin", "Samer Abdallah", "Makar Valentinov", "Paul Brennan", "Elijah Kagan", "Christoph M. Wintersteiger", "Denis Ignatovich", "Grant Passmore"], "title": "Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic", "comment": "52 pages, 23 figures. Includes a new benchmark dataset (code-logic-bench) and evaluation of neurosymbolic reasoning for software analysis", "summary": "Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.\n  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.\n  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.\n  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.", "AI": {"tldr": "CodeLogician\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u7ed3\u5408LLM\u548c\u5f62\u5f0f\u5316\u63a8\u7406\u5f15\u64ceImandraX\uff0c\u7528\u4e8e\u7cbe\u786e\u5206\u6790\u8f6f\u4ef6\u903b\u8f91\uff0c\u5728\u4ee3\u7801\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u76f8\u6bd4\u7eafLLM\u65b9\u6cd5\u63d0\u534741-47\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u884c\u4e3a\u8fdb\u884c\u7cbe\u786e\u3001\u8be6\u5c3d\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u4e0e\u771f\u5b9e\u8f6f\u4ef6\u8131\u8282\u7684\u6570\u5b66\u8bc1\u660e\u81ea\u52a8\u5316\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u4e0d\u9700\u8981\u8bed\u4e49\u4e25\u8c28\u6027\u7684\u5de5\u7a0b\u4efb\u52a1\u3002", "method": "\u63d0\u51faCodeLogician\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u96c6\u6210\u5de5\u4e1a\u7ea7\u81ea\u52a8\u63a8\u7406\u5f15\u64ceImandraX\u3002\u4f7f\u7528LLM\u6784\u5efa\u8f6f\u4ef6\u7cfb\u7edf\u7684\u663e\u5f0f\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u52a8\u63a8\u7406\u56de\u7b54\u4e30\u5bcc\u7684\u8bed\u4e49\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u4ec5\u9a8c\u8bc1LLM\u8f93\u51fa\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "result": "\u5f15\u5165code-logic-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u7a0b\u5e8f\u72b6\u6001\u7a7a\u95f4\u3001\u63a7\u5236\u6d41\u3001\u8986\u76d6\u7ea6\u675f\u548c\u8fb9\u754c\u60c5\u51b5\u7684\u63a8\u7406\u6b63\u786e\u6027\u3002\u76f8\u6bd4\u7eafLLM\u63a8\u7406\uff0cCodeLogician\u7684\u5f62\u5f0f\u5316\u589e\u5f3a\u5e26\u6765\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u7f29\u5c0f\u4e8641-47\u4e2a\u767e\u5206\u70b9\u7684\u63a8\u7406\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u5bf9\u4e8e\u6269\u5c55\u7a0b\u5e8f\u5206\u6790\u3001\u5b9e\u73b0\u4e25\u683c\u81ea\u4e3b\u7684\u8f6f\u4ef6\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002CodeLogician\u5c55\u793a\u4e86\u7ed3\u5408LLM\u548c\u5f62\u5f0f\u5316\u63a8\u7406\u5f15\u64ce\u5728\u8f6f\u4ef6\u903b\u8f91\u7cbe\u786e\u5206\u6790\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.11658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11658", "abs": "https://arxiv.org/abs/2601.11658", "authors": ["Indrajit Kar", "Sammy Zonunpuia", "Zonunfeli Ralte"], "title": "Towards AGI A Pragmatic Approach Towards Self Evolving Agent", "comment": null, "summary": "Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u6269\u5c55\u80fd\u529b\u3001\u751f\u6210\u65b0\u5de5\u5177\u5e76\u8fdb\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6301\u7eed\u9002\u5e94", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u90e8\u7f72\u540e\u57fa\u672c\u662f\u9759\u6001\u7684\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u6269\u5c55\u80fd\u529b\u3001\u751f\u6210\u65b0\u5de5\u5177\u6216\u8fdb\u5316\u63a8\u7406\u7684\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u81ea\u6211\u8fdb\u5316\u7684\u667a\u80fd\u4f53\u6846\u67b6", "method": "\u5206\u5c42\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u57fa\u7840LLM\u3001\u64cd\u4f5c\u578bSLM\u667a\u80fd\u4f53\u3001\u4ee3\u7801\u751f\u6210LLM\u548c\u6559\u5e08LLM\u3002\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5148\u5c1d\u8bd5\u7528\u73b0\u6709\u5de5\u5177\u5b8c\u6210\u4efb\u52a1\uff0c\u5931\u8d25\u5219\u901a\u8fc7\u4ee3\u7801\u751f\u6210LLM\u5408\u6210\u65b0\u5de5\u5177\uff0c\u6301\u7eed\u5931\u8d25\u5219\u89e6\u53d1\u8fdb\u5316\u9636\u6bb5\uff08\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u3001\u57fa\u4e8e\u5956\u52b1\u7684\u5b66\u4e60\u6216\u9057\u4f20\u7b97\u6cd5\u8fdb\u5316\uff09", "result": "\u5728TaskCraft\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u79cd\u8fdb\u5316\u8303\u5f0f\uff1a\u8bfe\u7a0b\u5b66\u4e60\u63d0\u4f9b\u5feb\u901f\u6062\u590d\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u57fa\u4e8e\u5956\u52b1\u7684\u5b66\u4e60\u5728\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9057\u4f20\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u884c\u4e3a\u591a\u6837\u6027\u3002\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\uff0c\u8fdb\u5316\u540e\u7684\u667a\u80fd\u4f53\u90fd\u4f18\u4e8e\u539f\u59cb\u7248\u672c", "conclusion": "\u5206\u5c42\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u3001\u81ea\u4e3b\u3001\u81ea\u6211\u6539\u8fdb\u7684\u667a\u80fd\u4f53\u8fdb\u5316\uff0c\u8bc1\u660e\u4e86\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u4e0d\u540c\u8fdb\u5316\u7b56\u7565\u6301\u7eed\u63d0\u5347\u6027\u80fd"}}
{"id": "2601.11637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11637", "abs": "https://arxiv.org/abs/2601.11637", "authors": ["Aradhya Dixit"], "title": "Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics", "comment": null, "summary": "Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR = 62 percent) from Correction Success Rate (CSR = 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u8bca\u65ad\u6027\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u91cf\u5316\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u521d\u59cb\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u4fee\u6b63\u6210\u529f\u7387\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5e76\u8bc6\u522b\u4e86\u8bed\u4e49\u6f02\u79fb\u4f5c\u4e3a\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4f7f\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u80fd\u591f\u5c06\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u7684\u5de5\u5177\u8ba1\u5212\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8fed\u4ee3\u81ea\u6211\u4fee\u6b63\u7684\u8bc4\u4f30\u6709\u9650\uff0c\u5176\u5b9a\u91cf\u9650\u5236\u548c\u4e3b\u8981\u63a8\u7406\u74f6\u9888\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u6027\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u4fee\u6b63\u6210\u529f\u7387\u89e3\u8026\u5206\u6790\uff0c\u91cf\u5316\u4fee\u6b63\u7684\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5931\u8d25\u5206\u7c7b\u6cd5\u8bc6\u522b\u4e3b\u8981\u5931\u8d25\u539f\u56e0\u3002", "result": "\u4efb\u52a1\u6210\u529f\u7387\u4e3a62%\uff0c\u4f46\u4fee\u6b63\u6210\u529f\u7387\u4ec5\u4e3a25-33%\uff0c\u8868\u660e\u521d\u59cb\u80fd\u529b\u4e0d\u80fd\u9884\u6d4b\u4fee\u590d\u80fd\u529b\uff1b\u4fee\u6b63\u6548\u679c\u5728\u4e09\u6b21\u5c1d\u8bd5\u540e\u9971\u548c\uff1b\u7ea628%\u7684\u5931\u8d25\u7531\u8bed\u4e49\u6f02\u79fb\uff08\u4e0a\u4e0b\u6587\u72b6\u6001\u4e22\u5931\uff09\u5bfc\u81f4\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u901a\u8fc7\u9694\u79bb\u8bed\u4e49\u6f02\u79fb\u8fd9\u4e00\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u72b6\u6001\u4fdd\u6301\u80fd\u529b\u548c\u53ef\u4fe1\u8d56\u6027\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.11850", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11850", "abs": "https://arxiv.org/abs/2601.11850", "authors": ["Matthew Nyaaba", "Min SungEun", "Mary Abiswin Apam", "Kwame Owoahene Acheampong", "Emmanuel Dwamena", "Xiaoming Zhai"], "title": "Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority", "comment": null, "summary": "The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5728\u8d28\u6027\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662fITAGPT\u5de5\u5177\u5982\u4f55\u652f\u6301\u5f52\u7eb3\u4e3b\u9898\u5206\u6790\uff0c\u53d1\u73b0AI\u4f5c\u4e3a\u7a0b\u5e8f\u6027\u652f\u67b6\u589e\u5f3a\u900f\u660e\u5ea6\uff0c\u4f46\u89e3\u91ca\u6743\u5a01\u4ecd\u7531\u4eba\u7c7b\u7814\u7a76\u8005\u638c\u63e1\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u8d28\u6027\u7814\u7a76\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u9700\u8981\u63a2\u8ba8\u5206\u6790\u5b9e\u8df5\u548c\u89e3\u91ca\u6743\u5a01\u7684\u95ee\u9898\uff0c\u4e86\u89e3AI\u5de5\u5177\u5982\u4f55\u5f71\u54cd\u7814\u7a76\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u5f52\u7eb3\u4e3b\u9898\u5206\u6790\u6846\u67b6\uff0c\u4e09\u4f4d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u8d28\u6027\u7814\u7a76\u8005\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684ITAGPT\u5de5\u5177\u5206\u6790\u52a0\u7eb3\u6559\u5e08\u6559\u80b2\u8bbf\u8c08\u8f6c\u5f55\uff0c\u6536\u96c6\u4ea4\u4e92\u65e5\u5fd7\u3001AI\u751f\u6210\u8868\u683c\u3001\u7814\u7a76\u8005\u4fee\u8ba2\u7b49\u6570\u636e\u3002", "result": "ITAGPT\u4f5c\u4e3a\u7a0b\u5e8f\u6027\u652f\u67b6\u7ed3\u6784\u5316\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u5e76\u589e\u5f3a\u900f\u660e\u5ea6\uff0c\u4f46\u89e3\u91ca\u6743\u5a01\u4ecd\u7531\u4eba\u7c7b\u7814\u7a76\u8005\u638c\u63e1\uff0c\u4ed6\u4eec\u901a\u8fc7\u4fee\u6539\u3001\u5220\u9664\u3001\u62d2\u7edd\u3001\u63d2\u5165\u548c\u8bc4\u8bba\u7b49\u53cd\u590d\u5206\u6790\u884c\u52a8\u884c\u4f7f\u5224\u65ad\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f52\u7eb3\u4e3b\u9898\u5206\u6790\u53ef\u4ee5\u901a\u8fc7\u8d1f\u8d23\u4efb\u7684\u4eba\u673a\u534f\u4f5c\u5b9e\u73b0\uff0cAI\u5de5\u5177\u652f\u6301\u5206\u6790\u8fc7\u7a0b\u4f46\u4eba\u7c7b\u7814\u7a76\u8005\u4fdd\u6301\u6700\u7ec8\u89e3\u91ca\u6743\u5a01\u3002"}}
{"id": "2601.11722", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.11722", "abs": "https://arxiv.org/abs/2601.11722", "authors": ["Ahmed Rayane Kebir", "Vincent Guigue", "Lynda Said Lhadj", "Laure Soulier"], "title": "RAC: Retrieval-Augmented Clarification for Faithful Conversational Search", "comment": "This is the author's version of the work. The definitive version is published in: Proceedings of the 48th European Conference on Information Retrieval (ECIR '26), 29 March--2 April, 2026, Delft, Netherlands", "summary": "Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.", "AI": {"tldr": "RAC\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u6f84\u6e05\u95ee\u9898\uff0c\u786e\u4fdd\u95ee\u9898\u6709\u6587\u6863\u4f9d\u636e\uff0c\u907f\u514d\u65e0\u6839\u636e\u63d0\u95ee", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u7684\u6f84\u6e05\u95ee\u9898\u751f\u6210\u4e3b\u8981\u5173\u6ce8\u6d41\u7545\u6027\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u8bed\u6599\u5e93\u7684\u951a\u5b9a\uff0c\u5bfc\u81f4\u53ef\u80fd\u751f\u6210\u65e0\u6cd5\u4ece\u53ef\u7528\u6587\u6863\u4e2d\u56de\u7b54\u7684\u95ee\u9898", "method": "\u63d0\u51faRAC\u6846\u67b6\uff1a1)\u6bd4\u8f83\u591a\u79cd\u68c0\u7d22\u7d22\u5f15\u7b56\u7565\uff1b2)\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u5145\u5206\u5229\u7528\u68c0\u7d22\u4e0a\u4e0b\u6587\u5e76\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u9898\uff1b3)\u5e94\u7528\u5bf9\u6bd4\u504f\u597d\u4f18\u5316\uff0c\u4f7f\u57fa\u4e8e\u68c0\u7d22\u6bb5\u843d\u7684\u95ee\u9898\u4f18\u4e8e\u65e0\u6839\u636e\u7684\u66ff\u4ee3\u65b9\u6848", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAC\u76f8\u6bd4\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\u3002\u9664\u4e86LLM-as-Judge\u8bc4\u4f30\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8eNLI\u548c\u6570\u636e\u5230\u6587\u672c\u7684\u65b0\u6307\u6807\u6765\u8bc4\u4f30\u95ee\u9898\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u951a\u5b9a\u7a0b\u5ea6\uff0c\u8bc1\u660e\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u5fe0\u5b9e\u6027", "conclusion": "RAC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6f84\u6e05\u95ee\u9898\u751f\u6210\u4e2d\u7684\u8bed\u6599\u5e93\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u5bf9\u6bd4\u4f18\u5316\u786e\u4fdd\u95ee\u9898\u6709\u6587\u6863\u4f9d\u636e\uff0c\u4e3a\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6f84\u6e05\u673a\u5236"}}
{"id": "2601.11640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11640", "abs": "https://arxiv.org/abs/2601.11640", "authors": ["Yingda Yu", "Jiaqi Xuan", "Shuhui Shi", "Xuanyu Teng", "Shuyang Xu", "Guanchao Tong"], "title": "Confident Learning for Object Detection under Model Constraints", "comment": "Submitted to ICPR 2026, currently under review", "summary": "Agricultural weed detection on edge devices is subject to strict constraints on model capacity, computational resources, and real-time inference latency, which prevent performance improvements through model scaling or ensembling. This paper proposes Model-Driven Data Correction (MDDC), a data-centric framework that enhances detection performance by iteratively diagnosing and correcting data quality deficiencies. An automated error analysis procedure categorizes detection failures into four types: false negatives, false positives, class confusion, and localization errors. These error patterns are systematically addressed through a structured train-fix-retrain pipeline with version-controlled data management. Experimental results on multiple weed detection datasets demonstrate consistent improvements of 5-25 percent in mAP at 0.5 using a fixed lightweight detector (YOLOv8n), indicating that systematic data quality optimization can effectively alleviate performance bottlenecks under fixed model capacity constraints.", "AI": {"tldr": "\u63d0\u51faMDDC\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u8bca\u65ad\u4e0e\u4fee\u6b63\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u56fa\u5b9a\u8f7b\u91cf\u6a21\u578b\u4e0b\u5b9e\u73b05-25% mAP\u63d0\u5347", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u519c\u4e1a\u6742\u8349\u68c0\u6d4b\u9762\u4e34\u4e25\u683c\u7ea6\u675f\uff1a\u6a21\u578b\u5bb9\u91cf\u6709\u9650\u3001\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u3001\u9700\u8981\u5b9e\u65f6\u63a8\u7406\u5ef6\u8fdf\uff0c\u8fd9\u9650\u5236\u4e86\u901a\u8fc7\u6a21\u578b\u7f29\u653e\u6216\u96c6\u6210\u6765\u63d0\u5347\u6027\u80fd\u7684\u53ef\u80fd\u6027", "method": "\u63d0\u51fa\u6a21\u578b\u9a71\u52a8\u6570\u636e\u4fee\u6b63\uff08MDDC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u5c06\u68c0\u6d4b\u5931\u8d25\u5206\u4e3a\u56db\u7c7b\uff08\u5047\u9634\u6027\u3001\u5047\u9633\u6027\u3001\u7c7b\u522b\u6df7\u6dc6\u3001\u5b9a\u4f4d\u9519\u8bef\uff09\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u8bad\u7ec3-\u4fee\u590d-\u518d\u8bad\u7ec3\u6d41\u7a0b\u548c\u7248\u672c\u63a7\u5236\u6570\u636e\u7ba1\u7406\u6765\u7cfb\u7edf\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898", "result": "\u5728\u591a\u4e2a\u6742\u8349\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u56fa\u5b9a\u8f7b\u91cf\u68c0\u6d4b\u5668\uff08YOLOv8n\uff09\u5b9e\u73b0\u4e86mAP@0.5\u6307\u68075-25%\u7684\u6301\u7eed\u63d0\u5347\uff0c\u8868\u660e\u7cfb\u7edf\u5316\u6570\u636e\u8d28\u91cf\u4f18\u5316\u80fd\u6709\u6548\u7f13\u89e3\u56fa\u5b9a\u6a21\u578b\u5bb9\u91cf\u4e0b\u7684\u6027\u80fd\u74f6\u9888", "conclusion": "\u5728\u6a21\u578b\u5bb9\u91cf\u56fa\u5b9a\u7684\u7ea6\u675f\u4e0b\uff0c\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u7684\u7cfb\u7edf\u5316\u6570\u636e\u8d28\u91cf\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u6742\u8349\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\u9014\u5f84"}}
{"id": "2601.11885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11885", "abs": "https://arxiv.org/abs/2601.11885", "authors": ["Zhifei Li", "Ziyue Qin", "Xiangyu Luo", "Xiaoju Hou", "Yue Zhao", "Miao Zhang", "Zhifang Huang", "Kui Xiao", "Bing Yang"], "title": "MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment", "comment": "Accepted by AAAI 2026", "summary": "Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.", "AI": {"tldr": "MyGram\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u7684\u6a21\u6001\u611f\u77e5\u56fe\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u6a21\u6001\u6269\u6563\u5b66\u4e60\u548cGram\u635f\u5931\u5b9e\u73b0\u8de8\u6a21\u6001\u5168\u5c40\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u6a21\u6001\u5185\u7684\u7ed3\u6784\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bb9\u6613\u53d7\u5230\u6d45\u5c42\u7279\u5f81\u7684\u5e72\u6270\uff0c\u9700\u8981\u66f4\u597d\u5730\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u5b9e\u73b0\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faMyGram\u6846\u67b6\uff1a1\uff09\u6a21\u6001\u6269\u6563\u5b66\u4e60\u6a21\u5757\u6355\u83b7\u6a21\u6001\u5185\u6df1\u5c42\u7ed3\u6784\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u878d\u5408\uff1b2\uff09Gram\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u591a\u6a21\u6001\u7279\u5f81\u5f62\u6210\u76844\u7ef4\u5e73\u884c\u591a\u9762\u4f53\u4f53\u79ef\u6765\u5b9e\u73b0\u8de8\u6a21\u6001\u5168\u5c40\u5206\u5e03\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMyGram\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728FBDB15K\u4e0aHits@1\u6700\u5927\u63d0\u53474.8%\uff0c\u5728FBYG15K\u4e0a\u63d0\u53479.9%\uff0c\u5728DBP15K\u4e0a\u63d0\u53474.3%\u3002", "conclusion": "MyGram\u901a\u8fc7\u6a21\u6001\u6269\u6563\u5b66\u4e60\u548cGram\u635f\u5931\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u5bf9\u9f50\u4e2d\u7684\u7ed3\u6784\u4e0a\u4e0b\u6587\u4fe1\u606f\u7f3a\u5931\u548c\u8de8\u6a21\u6001\u5206\u5e03\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2601.11739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11739", "abs": "https://arxiv.org/abs/2601.11739", "authors": ["Xinyu Pi", "Qisen Yang", "Chuong Nguyen", "Hua Shen"], "title": "Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era", "comment": null, "summary": "LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a4\u00d74\u6846\u67b6\u6765\u5206\u6790LLM\u5728\u8d28\u6027\u7814\u7a76\u4e2d\u7684\u8f93\u51fa\u7c7b\u578b\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u504f\u5411\u4f4e\u5c42\u6b21\u610f\u4e49\u548c\u4f4e\u627f\u8bfa\u8868\u793a\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u660e\u786e\u3001\u53ef\u9009\u62e9\u548c\u53ef\u6cbb\u7406\u7684LLM\u7cfb\u7edf\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u652f\u6301\u8d28\u6027\u7814\u7a76\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4ea7\u751f\u7684\u8f93\u51fa\u5dee\u5f02\u5f88\u5927\u2014\u2014\u4ece\u5fe0\u5b9e\u4e8e\u539f\u59cb\u6570\u636e\u7684\u603b\u7ed3\u5230\u7406\u8bba\u89e3\u91ca\u548c\u7cfb\u7edf\u6a21\u578b\u3002\u4e3a\u4e86\u660e\u786e\u8fd9\u4e9b\u5dee\u5f02\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6846\u67b6\u6765\u5206\u6790LLM\u5728\u8d28\u6027\u7814\u7a76\u4e2d\u7684\u4e0d\u540c\u8f93\u51fa\u7c7b\u578b\u3002", "method": "\u5f15\u5165\u4e00\u4e2a4\u00d74\u7684\u5206\u6790\u6846\u67b6\uff0c\u6a2a\u8de8\u56db\u4e2a\u610f\u4e49\u5efa\u6784\u5c42\u6b21\uff08\u63cf\u8ff0\u6027\u3001\u5206\u7c7b\u6027\u3001\u89e3\u91ca\u6027\u3001\u7406\u8bba\u6027\uff09\u548c\u56db\u4e2a\u5efa\u6a21\u5c42\u6b21\uff08\u9759\u6001\u7ed3\u6784\u3001\u9636\u6bb5/\u65f6\u95f4\u7ebf\u3001\u56e0\u679c\u8def\u5f84\u3001\u53cd\u9988\u52a8\u6001\uff09\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u5148\u524d\u7684LLM\u81ea\u52a8\u5316\u7814\u7a76\uff0c\u5206\u6790\u5176\u5206\u5e03\u6a21\u5f0f\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u5206\u6790\u53d1\u73b0\uff0c\u73b0\u6709LLM\u7cfb\u7edf\u5b58\u5728\u660e\u663e\u504f\u5411\uff1a\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f4e\u5c42\u6b21\u610f\u4e49\u5efa\u6784\uff08\u63cf\u8ff0\u6027\u548c\u5206\u7c7b\u6027\uff09\u548c\u4f4e\u627f\u8bfa\u8868\u793a\uff08\u9759\u6001\u7ed3\u6784\u548c\u9636\u6bb5/\u65f6\u95f4\u7ebf\uff09\uff0c\u5f88\u5c11\u6709\u53ef\u9760\u7684\u5c1d\u8bd5\u8fdb\u884c\u89e3\u91ca\u6027/\u7406\u8bba\u6027\u63a8\u7406\u6216\u52a8\u6001\u5efa\u6a21\u3002", "conclusion": "\u57fa\u4e8e\u63ed\u793a\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\uff1a\u5f00\u53d1\u548c\u5e94\u7528LLM\u7cfb\u7edf\uff0c\u4f7f\u5176\u89e3\u91ca\u6027\u548c\u5efa\u6a21\u627f\u8bfa\u66f4\u52a0\u660e\u786e\u3001\u53ef\u9009\u62e9\u548c\u53ef\u6cbb\u7406\uff0c\u63a8\u52a8LLM\u5728\u8d28\u6027\u7814\u7a76\u4e2d\u7684\u66f4\u9ad8\u7ea7\u5e94\u7528\u3002"}}
{"id": "2601.11641", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11641", "abs": "https://arxiv.org/abs/2601.11641", "authors": ["Yuxi Liu", "Yipeng Hu", "Zekun Zhang", "Kunze Jiang", "Kun Yuan"], "title": "Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers", "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \\underline{\\textbf{M}}ixtrue-\\underline{\\textbf{O}}f-\\underline{\\textbf{D}}istribution \\textbf{DiT} (\\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT's effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.", "AI": {"tldr": "MOD-DiT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91c7\u6837\u7684\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u51c6\u786e\u5efa\u6a21\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u89c6\u9891\u751f\u6210\u4e2d\u9762\u4e34\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8fc7\u4e8e\u7b80\u5316\u7684\u9759\u6001\u6a21\u5f0f\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u91c7\u6837\u64cd\u4f5c\u6765\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u5f0f\u9884\u6d4b\u4e0d\u51c6\u786e\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "method": "MOD-DiT\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a1\uff09\u5229\u7528\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u6df7\u5408\u65b9\u6cd5\u5efa\u6a21\u9ad8\u6548\u7684\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\uff0c\u9884\u6d4b\u7279\u5b9a\u53bb\u566a\u533a\u95f4\u7684\u63a9\u7801\u6a21\u5f0f\uff1b2\uff09\u5728\u7ebf\u5757\u63a9\u7801\u7b56\u7565\u52a8\u6001\u5e94\u7528\u8fd9\u4e9b\u9884\u6d4b\u7684\u63a9\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u5386\u53f2\u7a00\u758f\u4fe1\u606f\uff0c\u65e0\u9700\u91cd\u590d\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u67b6\u6784\u4e0a\uff0cMOD-DiT\u5c55\u73b0\u51fa\u6301\u7eed\u7684\u52a0\u901f\u548c\u8d28\u91cf\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "MOD-DiT\u901a\u8fc7\u521b\u65b0\u7684\u65e0\u9700\u91c7\u6837\u52a8\u6001\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8d28\u91cf\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2601.11903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11903", "abs": "https://arxiv.org/abs/2601.11903", "authors": ["YenTing Lee", "Keerthi Koneru", "Zahra Moslemi", "Sheethal Kumar", "Ramesh Radhakrishnan"], "title": "AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems", "comment": "Workshop on W51: How Can We Trust and Control Agentic AI? Toward Alignment, Robustness, and Verifiability in Autonomous LLM Agents at AAAI 2026", "summary": "Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.\n  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight", "AI": {"tldr": "AEMA\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u7ea7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u611f\u77e5\u3001\u53ef\u5ba1\u8ba1\u7684\u8bbe\u8ba1\uff0c\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u8ffd\u6eaf\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u76f8\u6bd4\u5355\u4e00LLM-as-a-Judge\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u901a\u5e38\u91c7\u7528\u5355\u4e00\u54cd\u5e94\u8bc4\u5206\u6216\u72ed\u7a84\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4f01\u4e1a\u7ea7\u591a\u667a\u80fd\u4f53\u89c4\u6a21\u90e8\u7f72\u65f6\u7f3a\u4e4f\u7a33\u5b9a\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u53ef\u9760\u534f\u8c03\u3001\u900f\u660e\u51b3\u7b56\u548c\u53ef\u9a8c\u8bc1\u6027\u80fd\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faAEMA\uff08\u81ea\u9002\u5e94\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u8fc7\u7a0b\u611f\u77e5\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u89c4\u5212\u3001\u6267\u884c\u548c\u805a\u5408\u5f02\u6784\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u591a\u6b65\u9aa4\u8bc4\u4f30\uff0c\u652f\u6301\u53ef\u8ffd\u6eaf\u7684\u8bb0\u5f55\u548c\u8d1f\u8d23\u4efb\u7684\u81ea\u52a8\u5316\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u4e1a\u52a1\u573a\u666f\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6d4b\u8bd5\u4e2d\uff0cAEMA\u76f8\u6bd4\u5355\u4e00LLM-as-a-Judge\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u597d\u7684\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u8bb0\u5f55\uff0c\u652f\u6301\u8d1f\u8d23\u4efb\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "conclusion": "AEMA\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u8d1f\u8d23\u4efb\u8bc4\u4f30\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u4f01\u4e1a\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u6311\u6218\u3002"}}
{"id": "2601.11746", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11746", "abs": "https://arxiv.org/abs/2601.11746", "authors": ["George Mihaila", "Suleyman Olcay Polat", "Poli Nemkova", "Himanshu Sharma", "Namratha V. Urs", "Mark V. Albert"], "title": "LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text", "comment": null, "summary": "Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict \"Single Mask-Single Sample\" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.", "AI": {"tldr": "LIME-LLM\uff1a\u7528\u5047\u8bbe\u9a71\u52a8\u7684\u53d7\u63a7\u6270\u52a8\u66ff\u4ee3\u968f\u673a\u63a9\u7801\uff0c\u901a\u8fc7\"\u5355\u63a9\u7801-\u5355\u6837\u672c\"\u534f\u8bae\u6784\u5efa\u6d41\u7545\u7684\u90bb\u57df\uff0c\u663e\u8457\u63d0\u5347NLP\u9ed1\u76d2\u6a21\u578b\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5c40\u90e8\u89e3\u91ca\u65b9\u6cd5\uff08\u5982LIME\uff09\u5728NLP\u5e94\u7528\u4e2d\u4f9d\u8d56\u968f\u673a\u4ee4\u724c\u63a9\u7801\uff0c\u4f1a\u4ea7\u751f\u8bed\u4e49\u65e0\u6548\u7684\u5206\u5e03\u5916\u8f93\u5165\uff0c\u964d\u4f4e\u5c40\u90e8\u4ee3\u7406\u6a21\u578b\u7684\u4fdd\u771f\u5ea6\u3002\u6700\u8fd1\u7684\u751f\u6210\u65b9\u6cd5\uff08\u5982LLiMe\uff09\u867d\u7136\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u90bb\u57df\uff0c\u4f46\u4f9d\u8d56\u65e0\u7ea6\u675f\u7684\u91ca\u4e49\uff0c\u5f15\u5165\u4e86\u6df7\u6dc6\u53d8\u91cf\uff0c\u96be\u4ee5\u9694\u79bb\u7279\u5b9a\u7279\u5f81\u8d21\u732e\u3002", "method": "\u63d0\u51faLIME-LLM\u6846\u67b6\uff0c\u7528\u5047\u8bbe\u9a71\u52a8\u7684\u53d7\u63a7\u6270\u52a8\u66ff\u4ee3\u968f\u673a\u566a\u58f0\u3002\u91c7\u7528\u4e25\u683c\u7684\"\u5355\u63a9\u7801-\u5355\u6837\u672c\"\u534f\u8bae\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u7684\u4e2d\u6027\u586b\u5145\u548c\u8fb9\u754c\u586b\u5145\u7b56\u7565\uff0c\u6784\u5efa\u6d41\u7545\u3001\u5728\u6d41\u5f62\u4e0a\u7684\u90bb\u57df\uff0c\u4e25\u683c\u9694\u79bb\u7279\u5f81\u6548\u5e94\u3002", "result": "\u5728CoLA\u3001SST-2\u548cHateXplain\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u7684rationale\u4f5c\u4e3aground truth\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLIME-LLM\u76f8\u6bd4\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\uff08LIME\u3001SHAP\u3001Integrated Gradients\uff09\u548c\u6700\u8fd1\u7684\u751f\u6210\u66ff\u4ee3\u65b9\u6cd5\uff08LLiMe\uff09\uff0c\u5728\u5c40\u90e8\u89e3\u91ca\u4fdd\u771f\u5ea6\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "LIME-LLM\u4e3a\u9ed1\u76d2NLP\u53ef\u89e3\u91ca\u6027\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u53d7\u63a7\u6270\u52a8\u548c\u4e25\u683c\u7684\u90bb\u57df\u6784\u5efa\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c40\u90e8\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bed\u4e49\u65e0\u6548\u548c\u6df7\u6dc6\u53d8\u91cf\u7684\u95ee\u9898\u3002"}}
{"id": "2601.11642", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.11642", "abs": "https://arxiv.org/abs/2601.11642", "authors": ["Abbas Alzubaidi", "Ali Al-Bayaty"], "title": "PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models", "comment": "16 pages, 6 figures", "summary": "Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like \"0\" vs. \"2\") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\u751f\u6210\u53ef\u63a7\u7684\u819d\u5173\u8282X\u5149\u7247\uff0c\u7528\u4e8e\u8bad\u7ec3AI\u6a21\u578b\u8fdb\u884c\u9aa8\u5173\u8282\u708e\u8bc4\u4f30\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u7684\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e(OA)\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b8b\u539f\u56e0\uff0c\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4e3b\u89c2\u7684Kellgren-Lawrence(KL)\u5206\u7ea7\u3002AI\u548c\u5f71\u50cf\u7ec4\u5b66\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u826f\u597d\u7684X\u5149\u6570\u636e\u96c6\uff0c\u800c\u771f\u5b9e\u60a3\u8005\u6570\u636e\u56e0\u9690\u79c1\u3001\u6cbb\u7406\u548c\u8d44\u6e90\u9650\u5236\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7269\u7406\u7684\u5408\u6210\u6a21\u62df\u6846\u67b6(PSSF)\uff0c\u4ece\u53c2\u6570\u5316\u89e3\u5256\u6a21\u578b\u751f\u6210\u53ef\u63a7\u7684\u819d\u5173\u8282\u524d\u540e\u4f4dX\u5149\u7247\u3002\u521b\u5efa180\u540d\u53d7\u8bd5\u8005(260\u4e2a\u819d\u76d6)\u7684\u865a\u62df\u961f\u5217\uff0c\u6bcf\u4e2a\u53d7\u8bd5\u8005\u5728\u4e09\u79cd\u534f\u8bae\u4e0b\u6210\u50cf\u3002\u4f7f\u7528IBSI\u6807\u51c6\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u4e09\u79cdML\u6a21\u578b\u8fdb\u884cKL\u5206\u7ea7\u9884\u6d4b\u3002", "result": "\u5728IBSI\u534f\u8bae\u5185\u3001\u8de8\u534f\u8bae\u548c\u591a\u534f\u8bae\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u7c7b\u5185\u76f8\u5173\u7cfb\u6570\u8bc4\u4f30\u4e86\u7279\u5f81\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "PSSF\u6846\u67b6\u80fd\u591f\u751f\u6210\u53ef\u63a7\u7684\u5408\u6210X\u5149\u7247\uff0c\u4e3a\u819d\u5173\u8282OA\u7684AI\u8bc4\u4f30\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u7279\u5f81\u7a33\u5b9a\u6027\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.11905", "categories": ["cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.11905", "abs": "https://arxiv.org/abs/2601.11905", "authors": ["Junyu Cao", "Ruijiang Gao", "Esmaeil Keyvanshokooh", "Jianhao Ma"], "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning", "comment": "50 pages. Previous version with human-AI collaboration: arXiv:2410.14640", "summary": "We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u7b97\u6cd5\u8ffd\u7d22\u3001\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u987a\u5e8f\u51b3\u7b56\uff08\u5982\u4e2a\u6027\u5316\u533b\u7597\uff09\u3002\u5f00\u53d1\u4e86GLRB\u7b97\u6cd5\u548cLIBRA\u7b97\u6cd5\uff0c\u540e\u8005\u7ed3\u5408LLM\u9886\u57df\u77e5\u8bc6\u4e0e\u8001\u864e\u673a\u5b66\u4e60\uff0c\u63d0\u4f9b\u4e09\u4e2a\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u987a\u5e8f\u51b3\u7b56\u573a\u666f\uff08\u5982\u4e2a\u6027\u5316\u533b\u7597\uff09\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u9009\u62e9\u6cbb\u7597\u884c\u52a8\u548c\u5bf9\u60a3\u8005\u53ef\u53d8\u7279\u5f81\u7684\u6700\u5c0f\u53ef\u884c\u4fee\u6539\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c06\u7b97\u6cd5\u8ffd\u7d22\u3001\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u548cLLM\u77e5\u8bc6\u6709\u6548\u6574\u5408\u7684\u6846\u67b6\u3002", "method": "1. \u63d0\u51fa\u8ffd\u7d22\u8001\u864e\u673a\u95ee\u9898\uff0c\u51b3\u7b56\u8005\u9700\u540c\u65f6\u9009\u62e9\u6cbb\u7597\u884c\u52a8\u548c\u60a3\u8005\u53ef\u53d8\u7279\u5f81\u7684\u6700\u5c0f\u53ef\u884c\u4fee\u6539\u30022. \u5f00\u53d1\u5e7f\u4e49\u7ebf\u6027\u8ffd\u7d22\u8001\u864e\u673a\uff08GLRB\uff09\u7b97\u6cd5\u30023. \u63d0\u51faLIBRA\u7b97\u6cd5\uff0c\u7b56\u7565\u6027\u5730\u7ed3\u5408LLM\u9886\u57df\u77e5\u8bc6\u548c\u8001\u864e\u673a\u5b66\u4e60\u7684\u7edf\u8ba1\u4e25\u8c28\u6027\u3002", "result": "LIBRA\u63d0\u4f9b\u4e09\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff1a\u70ed\u542f\u52a8\u4fdd\u8bc1\uff08LLM\u63a8\u8350\u63a5\u8fd1\u6700\u4f18\u65f6\u663e\u8457\u51cf\u5c11\u521d\u59cb\u9057\u61be\uff09\u3001LLM\u52aa\u529b\u4fdd\u8bc1\uff08\u4ec5\u54a8\u8be2LLM O(log\u00b2T)\u6b21\u786e\u4fdd\u957f\u671f\u81ea\u4e3b\u6027\uff09\u3001\u9c81\u68d2\u6027\u4fdd\u8bc1\uff08\u5373\u4f7fLLM\u4e0d\u53ef\u9760\u4e5f\u4e0d\u6bd4\u7eaf\u8001\u864e\u673a\u7b97\u6cd5\u5dee\uff09\u3002\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\u8bc1\u660e\u7b97\u6cd5\u63a5\u8fd1\u6700\u4f18\u3002\u5408\u6210\u73af\u5883\u548c\u771f\u5b9e\u9ad8\u8840\u538b\u7ba1\u7406\u6848\u4f8b\u5b9e\u9a8c\u8868\u660eGLRB\u548cLIBRA\u5728\u9057\u61be\u3001\u6cbb\u7597\u8d28\u91cf\u548c\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u6807\u51c6\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u548c\u7eafLLM\u57fa\u51c6\u3002", "conclusion": "\u8ffd\u7d22\u611f\u77e5\u3001LLM\u8f85\u52a9\u7684\u8001\u864e\u673a\u7b97\u6cd5\u5728\u4e2a\u6027\u5316\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u5177\u6709\u524d\u666f\uff0c\u4e3aLLM-\u8001\u864e\u673a\u53ef\u4fe1\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11758", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11758", "abs": "https://arxiv.org/abs/2601.11758", "authors": ["Arnab Das Utsa"], "title": "Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation", "comment": "9 figures, more than 1o pages", "summary": "Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u8bed\u8a00\u7684\u900f\u660e\u5316\u7126\u8651\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u53ef\u89e3\u91ca\u7279\u5f81\u5efa\u6a21\u548c\u8de8\u9886\u57df\u9a8c\u8bc1\uff0c\u5728Reddit\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u5173\u952e\u8bcd\u9c81\u68d2\u7684\u7126\u8651\u8bc6\u522b\u3002", "motivation": "\u5168\u7403\u6570\u4ebf\u4eba\u53d7\u7126\u8651\u5f71\u54cd\u4f46\u5927\u89c4\u6a21\u7b5b\u67e5\u6709\u9650\uff0c\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u68c0\u6d4b\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u5173\u952e\u8bcd\u9c81\u68d2\u6027\u9a8c\u8bc1\u548c\u4e25\u683c\u7528\u6237\u7ea7\u6570\u636e\u5b8c\u6574\u6027\u3002", "method": "\u4f7f\u7528Reddit\u5e16\u5b50\u6570\u636e\u96c6\uff0c\u5728\u7cbe\u5fc3\u7b5b\u9009\u7684\u5b50\u7248\u5757\u4e0a\u8bad\u7ec3\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u5668\uff0c\u8fdb\u884c\u7279\u5f81\u6d88\u878d\u3001\u5173\u952e\u8bcd\u63a9\u7801\u5b9e\u9a8c\u3001\u5bc6\u5ea6\u5dee\u5f02\u5206\u6790\uff0c\u5e76\u7528\u4e34\u5e8a\u8bbf\u8c08\u6570\u636e\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u5728\u60c5\u611f\u79fb\u9664\u6216\u5173\u952e\u8bcd\u63a9\u7801\u540e\u4ecd\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff1b\u65e9\u671f\u68c0\u6d4b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u5206\u7c7b\uff1b\u8de8\u9886\u57df\u5206\u6790\u4e0e\u4e34\u5e8a\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u900f\u660e\u8bed\u8a00\u7279\u5f81\u53ef\u652f\u6301\u53ef\u9760\u3001\u53ef\u6cdb\u5316\u4e14\u5173\u952e\u8bcd\u9c81\u68d2\u7684\u7126\u8651\u68c0\u6d4b\uff0c\u8be5\u6846\u67b6\u4e3a\u8de8\u4e0d\u540c\u5728\u7ebf\u73af\u5883\u7684\u53ef\u89e3\u91ca\u5fc3\u7406\u5065\u5eb7\u7b5b\u67e5\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u57fa\u7ebf\u3002"}}
{"id": "2601.11644", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11644", "abs": "https://arxiv.org/abs/2601.11644", "authors": ["Muhammad Imran", "Yugyung Lee"], "title": "Predicting When to Trust Vision-Language Models for Spatial Reasoning", "comment": "9 pages, 5 figures, 6 tables", "summary": "Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u51e0\u4f55\u9a8c\u8bc1\u6765\u9884\u6d4b\u4f55\u65f6\u4fe1\u4efbVLM\u7684\u7a7a\u95f4\u9884\u6d4b\uff0c\u76f8\u6bd4\u6587\u672c\u81ea\u8bc4\u4f30\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff08\u51c6\u786e\u7387\u4ec549%-54%\uff09\u3002\u4e3a\u786e\u4fdd\u5728\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u9700\u8981\u9884\u6d4b\u4f55\u65f6\u4fe1\u4efbVLM\u7684\u7a7a\u95f4\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u63a5\u53d7\u6240\u6709\u8f93\u51fa", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u72ec\u7acb\u51e0\u4f55\u9a8c\u8bc1\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u6765\u9a8c\u8bc1VLM\u9884\u6d4b\u3002\u878d\u5408\u56db\u4e2a\u4fe1\u53f7\uff1aVLM\u58f0\u660e\u4e0e\u5750\u6807\u7684\u51e0\u4f55\u5bf9\u9f50\u3001\u7a7a\u95f4\u91cd\u53e0\u7684\u6a21\u7cca\u6027\u3001\u68c0\u6d4b\u8d28\u91cf\u548cVLM\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u8fdb\u884c\u878d\u5408", "result": "\u5728BLIP-2\u4e0a\u8fbe\u52300.674 AUROC\uff08\u6bd4\u6587\u672c\u57fa\u7ebf\u63d0\u534734.0%\uff09\uff0c\u5728CLIP\u4e0a\u8fbe\u52300.583 AUROC\uff08\u63d0\u534716.1%\uff09\u3002\u572860%\u76ee\u6807\u51c6\u786e\u7387\u4e0b\uff0c\u8986\u76d6\u7387\u8fbe\u523061.9% vs \u57fa\u7ebf27.6%\uff082.2\u500d\u63d0\u5347\uff09\u3002\u89c6\u89c9\u4fe1\u53f7\u8d21\u732e87.4%\u91cd\u8981\u6027\uff0cVLM\u7f6e\u4fe1\u5ea6\u4ec512.7%\u3002\u573a\u666f\u56fe\u6784\u5efa\u4e2d\uff0c\u7f6e\u4fe1\u5ea6\u526a\u679d\u5c06\u7cbe\u5ea6\u4ece52.1%\u63d0\u5347\u523078.3%\uff0c\u540c\u65f6\u4fdd\u755968.2%\u8fb9", "conclusion": "\u5916\u90e8\u51e0\u4f55\u9a8c\u8bc1\u6bd4VLM\u81ea\u8bc4\u4f30\u66f4\u6709\u6548\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u53ef\u9760\u9884\u6d4bVLM\u7a7a\u95f4\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u9009\u62e9\u6027\u9884\u6d4b\u548c\u53ef\u9760\u573a\u666f\u56fe\u6784\u5efa\uff0c\u4e3aVLM\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4fdd\u969c"}}
{"id": "2601.11940", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11940", "abs": "https://arxiv.org/abs/2601.11940", "authors": ["Kang Chen", "Fan Yu", "Junjie Nian", "Shihan Zhao", "Zhuoka Feng", "Zijun Yao", "Heng Wang", "Minshen Yu", "Yixin Cao"], "title": "Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart", "comment": null, "summary": "Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.", "AI": {"tldr": "TAAR\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u601d\u7ef4\u9677\u9631\u5e76\u81ea\u9002\u5e94\u91cd\u542f\u89e3\u7801\uff0c\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u957f\u601d\u7ef4\u94fe\u867d\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u6a21\u578b\u4e00\u65e6\u65e9\u671f\u72af\u9519\u5c31\u4f1a\u9677\u5165\"\u601d\u7ef4\u9677\u9631\"\uff0c\u540e\u7eed\u53cd\u601d\u548c\u9a8c\u8bc1\u90fd\u65e0\u6cd5\u4fee\u6b63\u6839\u672c\u9519\u8bef\uff0c\u5bfc\u81f489%\u7684\u5931\u8d25\u6848\u4f8b\u3002", "method": "\u63d0\u51faTAAR\u6846\u67b6\uff1a\u8bad\u7ec3\u8bca\u65ad\u7b56\u7565\u9884\u6d4b\u4e24\u4e2a\u4fe1\u53f7\uff08\u9677\u9631\u4f4d\u7f6e\u7d22\u5f15\u548c\u9003\u8131\u6982\u7387\uff09\uff0c\u63a8\u7406\u65f6\u5728\u9884\u6d4b\u7684\u9677\u9631\u6bb5\u524d\u622a\u65ad\u8f68\u8ff9\u5e76\u81ea\u9002\u5e94\u91cd\u542f\u89e3\u7801\uff0c\u4e25\u91cd\u60c5\u51b5\u5e94\u7528\u66f4\u9ad8\u6e29\u5ea6\u91cd\u91c7\u6837\u548c\u7ed3\u6784\u5316\u91cd\u542f\u540e\u7f00\u3002", "result": "\u5728AIME24\u3001AIME25\u3001GPQA-Diamond\u3001HMMT25\u3001BRUMO25\u7b49\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTAAR\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TAAR\u901a\u8fc7\u68c0\u6d4b\u548c\u907f\u514d\u601d\u7ef4\u9677\u9631\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u7684\u9519\u8bef\u56fa\u5316\u95ee\u9898\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.11762", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11762", "abs": "https://arxiv.org/abs/2601.11762", "authors": ["Sae Young Moon", "Myeongjun Erik Jang", "Haoyan Luo", "Chunyang Xiao", "Antonios Georgiadis", "Fran Silavong"], "title": "Industry-Aligned Granular Topic Modeling", "comment": null, "summary": "Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.", "AI": {"tldr": "TIDE\u6846\u67b6\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7c92\u5ea6\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u6587\u6863\u6458\u8981\u3001\u4e3b\u9898\u5c42\u7ea7\u7b49\u8f85\u52a9\u529f\u80fd", "motivation": "\u4e3b\u9898\u5efa\u6a21\u5728\u5de5\u4e1a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7c92\u5ea6\u4e3b\u9898\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u7c92\u5ea6\u5206\u6790\u5bf9\u5546\u4e1a\u5e94\u7528\u6709\u91cd\u8981\u4ef7\u503c", "method": "\u63d0\u51faTIDE\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u7c92\u5ea6\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u6587\u6863\u6458\u8981\u3001\u4e3b\u9898\u5c42\u7ea7\u5173\u7cfb\u3001\u4e3b\u9898\u84b8\u998f\u7b49\u8f85\u52a9\u529f\u80fd", "result": "\u5728\u591a\u79cd\u516c\u5f00\u548c\u771f\u5b9e\u5546\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTIDE\u7684\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\u4f18\u4e8e\u73b0\u4ee3\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u8f85\u52a9\u7ec4\u4ef6\u4e3a\u5de5\u4e1a\u5546\u4e1a\u573a\u666f\u63d0\u4f9b\u6709\u4ef7\u503c\u652f\u6301", "conclusion": "TIDE\u6846\u67b6\u4e3a\u5546\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7c92\u5ea6\u4e3b\u9898\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u76ee\u524d\u6b63\u5728\u5f00\u6e90\u8fc7\u7a0b\u4e2d"}}
{"id": "2601.11645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11645", "abs": "https://arxiv.org/abs/2601.11645", "authors": ["Ujjwal Jain", "Oshin Misra", "Roshni Chakraborty", "Mahua Bhattacharya"], "title": "IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation", "comment": null, "summary": "Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.", "AI": {"tldr": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u4e0e\u6df7\u5408\u635f\u5931\u4f18\u5316\uff0c\u7528\u4e8e\u8367\u5149\u663e\u5fae\u955c\u4e2d\u795e\u7ecf\u5143\u7ec6\u80de\u7684\u9c81\u68d2\u5206\u5272\uff0c\u89e3\u51b3\u7ec6\u80de\u5bc6\u5ea6\u4e0d\u5747\u3001\u5f62\u6001\u590d\u6742\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\u3002", "motivation": "\u8367\u5149\u663e\u5fae\u955c\u4e2d\u795e\u7ecf\u5143\u7ec6\u80de\u5206\u5272\u9762\u4e34\u5bc6\u96c6\u4e0e\u7a00\u758f\u7ec6\u80de\u5171\u5b58\u3001\u5f62\u6001\u590d\u6742\u91cd\u53e0\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u62d3\u6251\u7ec6\u8282\u548c\u51c6\u786e\u8fb9\u754c\u3002", "method": "\u63d0\u51faIMSAHLO\u6846\u67b6\uff1a1) \u591a\u5c3a\u5ea6\u5bc6\u96c6\u5757(MSDBs)\u6355\u83b7\u4e0d\u540c\u611f\u53d7\u91ce\u7279\u5f81\uff1b2) \u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u5f62\u6001\u7279\u5f81\uff1b3) \u6df7\u5408\u635f\u5931\u51fd\u6570\u7ed3\u5408Tversky\u3001Focal\u3001\u4e2d\u5fc3\u7ebfDice\u548c\u8f6e\u5ed3\u52a0\u6743\u8fb9\u754c\u635f\u5931\u3002", "result": "\u5728FNC\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7cbe\u5ea681.4%\uff0c\u5b8fF1 82.7%\uff0c\u5faeF1 83.3%\uff0c\u5e73\u8861\u51c6\u786e\u738799.5%\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u548c\u6df7\u5408\u635f\u5931\u9879\u7684\u534f\u540c\u6548\u76ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u7269\u533b\u5b66\u6210\u50cf\u5efa\u7acb\u4e86\u53ef\u63a8\u5e7f\u7684\u5206\u5272\u6a21\u578b\u57fa\u7840\uff0c\u63a8\u52a8AI\u8f85\u52a9\u5206\u6790\u5411\u9ad8\u901a\u91cf\u795e\u7ecf\u751f\u7269\u5b66\u6d41\u7a0b\u53d1\u5c55\u3002"}}
{"id": "2601.11974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11974", "abs": "https://arxiv.org/abs/2601.11974", "authors": ["Xinmeng Hou", "Peiliang Gong", "Bohao Qu", "Wuqi Wang", "Qing Guo", "Yang Liu"], "title": "Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement", "comment": null, "summary": "While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.", "AI": {"tldr": "MARS\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u5faa\u73af\u5b9e\u73b0LLM\u667a\u80fd\u4f53\u7684\u9ad8\u6548\u81ea\u6211\u8fdb\u5316\uff0c\u7ed3\u5408\u539f\u5219\u6027\u53cd\u601d\u548c\u7a0b\u5e8f\u6027\u53cd\u601d\u6765\u4f18\u5316\u63a8\u7406\u903b\u8f91\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u53d7\u9650\u4e8e\u9759\u6001\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u73b0\u6709\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u4f4e\u6548\u7684\u591a\u8f6e\u9012\u5f52\u5faa\u73af\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u81ea\u8fdb\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMARS\u6846\u67b6\uff0c\u53d7\u6559\u80b2\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\uff1a1) \u539f\u5219\u6027\u53cd\u601d - \u62bd\u8c61\u51fa\u907f\u514d\u9519\u8bef\u7684\u89c4\u8303\u6027\u89c4\u5219\uff1b2) \u7a0b\u5e8f\u6027\u53cd\u601d - \u63a8\u5bfc\u6210\u529f\u7684\u9010\u6b65\u7b56\u7565\u3002\u5c06\u8fd9\u4e9b\u6d1e\u5bdf\u5408\u6210\u4f18\u5316\u6307\u4ee4\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u5355\u6b21\u5faa\u73af\u4e2d\u7cfb\u7edf\u5316\u6539\u8fdb\u63a8\u7406\u903b\u8f91\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMARS\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u6211\u8fdb\u5316\u7cfb\u7edf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "MARS\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u5faa\u73af\u5b9e\u73b0\u9ad8\u6548\u81ea\u6211\u8fdb\u5316\uff0c\u7ed3\u5408\u539f\u5219\u6027\u548c\u7a0b\u5e8f\u6027\u53cd\u601d\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u81ea\u9002\u5e94\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.11776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11776", "abs": "https://arxiv.org/abs/2601.11776", "authors": ["Kaituo Zhang", "Zhimeng Jiang", "Na Zou"], "title": "Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models", "comment": null, "summary": "Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.", "AI": {"tldr": "\u63d0\u51fa\u5b8c\u5168\u81ea\u53cd\u601d\u7684\u53bb\u6bd2\u6846\u67b6\uff0c\u5229\u7528LLMs\u5185\u5728\u80fd\u529b\u68c0\u6d4b\u3001\u4fee\u6b63\u6709\u6bd2\u5185\u5bb9\u5e76\u7cbe\u70bc\u6a21\u578b\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u5757\u6216\u6570\u636e\u6807\u6ce8\uff0c\u5728\u53bb\u6bd2\u6027\u80fd\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u53bb\u6bd2\u6280\u672f\u5f88\u5c11\u5229\u7528LLMs\u5185\u7f6e\u7684\u81ea\u6821\u6b63\u548c\u81ea\u5956\u52b1\u80fd\u529b\uff0c\u800c\u662f\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\u3001\u4eba\u5de5\u6570\u636e\u6807\u6ce8\u6216\u4eba\u5de5\u5e72\u9884\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u5b8c\u5168\u81ea\u53cd\u601d\u53bb\u6bd2\u6846\u67b6\uff1a1) \u6709\u6bd2\u4fe1\u53f7\u68c0\u6d4b\u5668\u2014\u2014\u5185\u90e8\u81ea\u8bc6\u522b\u673a\u5236\uff1b2) \u7cfb\u7edf\u5e72\u9884\u8fc7\u7a0b\u5c06\u6709\u6bd2\u6587\u672c\u8f6c\u5316\u4e3a\u65e0\u6bd2\u5bf9\u5e94\u7269\uff1b3) \u8fed\u4ee3\u8fc7\u7a0b\u751f\u6210\u5bf9\u6bd4\u53bb\u6bd2\u6570\u636e\u96c6\u7528\u4e8e\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728DetoxLLM\u548cParaDetox\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u6bd2\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u5185\u5728\u7684\u81ea\u53bb\u6bd2\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e00\u81f4\u6709\u6548\u7684\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u7f13\u89e3\u65b9\u6cd5\uff0c\u4e3a\u771f\u6b63\u81ea\u8c03\u8282\u8bed\u8a00\u6a21\u578b\u548c\u66f4\u8d1f\u8d23\u4efb\u3001\u7b26\u5408\u4f26\u7406\u7684\u6587\u672c\u751f\u6210\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2601.11651", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11651", "abs": "https://arxiv.org/abs/2601.11651", "authors": ["Miriam Doh", "Aditya Gulati", "Corina Canali", "Nuria Oliver"], "title": "Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification", "comment": "22 pages, 15 figures", "summary": "This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women's faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men's; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.\n  Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors' views.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210AI\u5b58\u5728\"\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\"\u504f\u89c1\uff0c\u7cfb\u7edf\u6027\u5730\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff0c\u5e76\u5728\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u522b\u504f\u89c1\uff0c\u52a0\u5267\u73b0\u6709\u4e0d\u5e73\u7b49\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210AI\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u2014\u2014\"\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\"\uff0c\u5373\u57fa\u4e8e\u5916\u8c8c\u7684\u7cfb\u7edf\u6027\u504f\u597d\u5904\u7406\uff0c\u4ee5\u53ca\u8fd9\u79cd\u504f\u89c1\u5982\u4f55\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6027\u522b\u5206\u7c7b\uff09\u4e2d\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\u3002", "method": "\u901a\u8fc7\u5206\u679026,400\u5f20\u4f7f\u7528Stable Diffusion 2.1\u548c3.5 Medium\u751f\u6210\u7684\u5408\u6210\u4eba\u8138\u56fe\u50cf\uff0c\u7814\u7a76\u751f\u6210\u5f0fAI\u6a21\u578b\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u5c06\u9762\u90e8\u5438\u5f15\u529b\u4e0e\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff0c\u5e76\u8bc4\u4f30\u4e09\u79cd\u6027\u522b\u5206\u7c7b\u7b97\u6cd5\u5728\u4e0d\u540c\u5c5e\u6027\u8f93\u5165\u4e0b\u7684\u6027\u522b\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) T2I\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u7f16\u7801\u5438\u5f15\u529b-\u6b63\u9762\u5c5e\u6027\u5173\u8054\uff1b(2) \u6027\u522b\u5206\u7c7b\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u6027\u522b\u5dee\u5f02\uff0c\u5973\u6027\u9762\u5b54\uff08\u7279\u522b\u662f\u5e26\u6709\u8d1f\u9762\u5c5e\u6027\u7684\uff09\u8bef\u5206\u7c7b\u7387\u8fdc\u9ad8\u4e8e\u7537\u6027\uff1b(3) \u65b0\u6a21\u578b\u901a\u8fc7\u5e74\u9f84\u540c\u8d28\u5316\u3001\u6027\u522b\u5316\u66dd\u5149\u6a21\u5f0f\u548c\u5730\u7406\u7b80\u5316\u52a0\u5267\u4e86\u5ba1\u7f8e\u7ea6\u675f\u3002", "conclusion": "\u7b97\u6cd5\u5916\u8c8c\u4e3b\u4e49\u662f\u8de8AI\u89c6\u89c9\u7cfb\u7edf\u8fd0\u884c\u7684\u7cfb\u7edf\u6027\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u8868\u5f81\u548c\u8bc6\u522b\u4e24\u65b9\u9762\u52a0\u5267\u73b0\u6709\u4e0d\u5e73\u7b49\u3002\u8fd9\u4e9b\u504f\u89c1\u53cd\u6620\u4e86\u793e\u4f1a\u5efa\u6784\u7684\u523b\u677f\u5370\u8c61\u800c\u975e\u7ecf\u9a8c\u4e8b\u5b9e\uff0c\u9700\u8981\u5f15\u8d77\u91cd\u89c6\u5e76\u91c7\u53d6\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2601.11979", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11979", "abs": "https://arxiv.org/abs/2601.11979", "authors": ["Ang Gao", "Changshuo Zhang", "Xiao Zhang", "Deyang Li", "Minjun Zhao", "Fangchao Liu", "Xinyu Zhang"], "title": "Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion", "comment": null, "summary": "In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.", "AI": {"tldr": "PICL\u662f\u4e00\u79cd\u52a8\u6001\u6f14\u793a\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bc6\u522b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6df7\u6dc6\u70b9\u5e76\u63d2\u5165\u76f8\u5173\u6f14\u793a\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u7b49\u9700\u8981\u9010\u6b65\u903b\u8f91\u63a8\u5bfc\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f7f\u7528\u9759\u6001\u7684\u9884\u9009\u6f14\u793a\uff0c\u65e0\u6cd5\u9002\u5e94\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u52a8\u6001\u6df7\u6dc6\u70b9\uff08\u5982\u6a21\u7cca\u8ba1\u7b97\u6216\u903b\u8f91\u6f0f\u6d1e\uff09\uff0c\u8fd9\u4e9b\u6df7\u6dc6\u70b9\u4f1a\u5bfc\u81f4\u7ea7\u8054\u9519\u8bef\u5e76\u964d\u4f4e\u6700\u7ec8\u51c6\u786e\u6027\u3002", "method": "PICL\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u548c\u71b5\u6765\u8bc6\u522b\u6f5c\u5728\u6df7\u6dc6\u70b9\u5e76\u603b\u7ed3\u5176\u6838\u5fc3\u7279\u5f81\uff1b2\uff09\u5728\u9047\u5230\u8fd9\u4e9b\u6df7\u6dc6\u70b9\u65f6\uff0c\u4ece\u6f14\u793a\u6c60\u4e2d\u68c0\u7d22\u4e0e\u6df7\u6dc6\u4e0a\u4e0b\u6587\u5339\u914d\u7684\u76f8\u5173\u6f14\u793a\uff0c\u5e76\u5c06\u5176\u76f4\u63a5\u63d2\u5165\u5230\u6b63\u5728\u8fdb\u884c\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ee5\u6307\u5bfc\u540e\u7eed\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPICL\u901a\u8fc7\u7f13\u89e3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6df7\u6dc6\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u81ea\u9002\u5e94\u6f14\u793a\u63d2\u5165\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "\u52a8\u6001\u6f14\u793a\u96c6\u6210\u6846\u67b6PICL\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u9010\u6b65\u903b\u8f91\u63a8\u5bfc\u7684\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5b9e\u65f6\u9002\u5e94\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6df7\u6dc6\u70b9\u6765\u9632\u6b62\u7ea7\u8054\u9519\u8bef\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.11778", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11778", "abs": "https://arxiv.org/abs/2601.11778", "authors": ["Sheriff Issaka", "Erick Rosas Gonzalez", "Lieqi Liu", "Evans Kofi Agyei", "Lucas Bandarkar", "Nanyun Peng", "David Ifeoluwa Adelani", "Francisco Guzm\u00e1n", "Saadia Gabriel"], "title": "Translation as a Scalable Proxy for Multilingual Evaluation", "comment": null, "summary": "The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model's broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks.", "AI": {"tldr": "\u7ffb\u8bd1\u8d28\u91cf\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u7684\u6709\u6548\u4ee3\u7406\u6307\u6807\uff0c\u901a\u8fc7\u7ffb\u8bd1\u6027\u80fd\u53ef\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0", "motivation": "\u5f53\u524dLLMs\u58f0\u79f0\u5177\u5907\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u975e\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u8bed\u8a00\u4e0d\u8db3\u5168\u7403\u8bed\u8a00\u76842%\uff0c\u4f20\u7edf\u57fa\u51c6\u6784\u5efa\u9762\u4e34\u6210\u672c\u9ad8\u3001\u4e13\u5bb6\u7a00\u7f3a\u548c\u6570\u636e\u6c61\u67d3\u7b49\u6311\u6218", "method": "\u7cfb\u7edf\u8bc4\u4f3014\u4e2a\u6a21\u578b\uff081B-72B\u53c2\u6570\uff09\u57289\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u548c7\u4e2a\u7ffb\u8bd1\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u7ffb\u8bd1\u6027\u80fd\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u4e4b\u95f4\u7684\u76f8\u5173\u6027", "result": "\u7ffb\u8bd1\u6027\u80fd\u662f\u591a\u8bed\u8a00\u4e0b\u6e38\u4efb\u52a1\u6210\u529f\u7684\u826f\u597d\u6307\u6807\uff08\u5982Phi-4\u6a21\u578b\u7684\u4e2d\u4f4d\u6570Pearson\u76f8\u5173\u7cfb\u6570\uff1aMetricX=0.89\uff0cxCOMET=0.91\uff0cSSA-COMET=0.87\uff09", "conclusion": "\u7ffb\u8bd1\u8d28\u91cf\u53ef\u4f5c\u4e3a\u5f3a\u5927\u4e14\u5ec9\u4ef7\u7684\u591a\u8bed\u8a00\u6027\u80fd\u521d\u6b65\u4ee3\u7406\u6307\u6807\uff0c\u652f\u6301\"\u7ffb\u8bd1\u4f18\u5148\u7b5b\u9009+\u7279\u5b9a\u4efb\u52a1\u9488\u5bf9\u6027\u8ddf\u8fdb\"\u7684\u8bc4\u4f30\u7b56\u7565"}}
{"id": "2601.11654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11654", "abs": "https://arxiv.org/abs/2601.11654", "authors": ["Kaustubh Shivshankar Shejole", "Gaurav Mishra"], "title": "PSSI-MaxST: An Efficient Pixel-Segment Similarity Index Using Intensity and Smoothness Features for Maximum Spanning Tree Based Segmentation", "comment": null, "summary": "Interactive graph-based segmentation methods partition an image into foreground and background regions with the aid of user inputs. However, existing approaches often suffer from high computational costs, sensitivity to user interactions, and degraded performance when the foreground and background share similar color distributions. A key factor influencing segmentation performance is the similarity measure used for assigning edge weights in the graph. To address these challenges, we propose a novel Pixel Segment Similarity Index (PSSI), which leverages the harmonic mean of inter-channel similarities by incorporating both pixel intensity and spatial smoothness features. The harmonic mean effectively penalizes dissimilarities in any individual channel, enhancing robustness. The computational complexity of PSSI is $\\mathcal{O}(B)$, where $B$ denotes the number of histogram bins. Our segmentation framework begins with low-level segmentation using MeanShift, which effectively captures color, texture, and segment shape. Based on the resulting pixel segments, we construct a pixel-segment graph with edge weights determined by PSSI. For partitioning, we employ the Maximum Spanning Tree (MaxST), which captures strongly connected local neighborhoods beneficial for precise segmentation. The integration of the proposed PSSI, MeanShift, and MaxST allows our method to jointly capture color similarity, smoothness, texture, shape, and strong local connectivity. Experimental evaluations on the GrabCut and Images250 datasets demonstrate that our method consistently outperforms current graph-based interactive segmentation methods such as AMOE, OneCut, and SSNCut in terms of segmentation quality, as measured by Jaccard Index (IoU), $F_1$ score, execution time and Mean Error (ME). Code is publicly available at: https://github.com/KaustubhShejole/PSSI-MaxST.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u50cf\u7d20\u6bb5\u76f8\u4f3c\u6027\u6307\u6570(PSSI)\u548c\u6700\u5927\u751f\u6210\u6811(MaxST)\u7684\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\uff0c\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5bf9\u7528\u6237\u4ea4\u4e92\u654f\u611f\u3001\u524d\u666f\u80cc\u666f\u989c\u8272\u76f8\u4f3c\u65f6\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5", "method": "\u63d0\u51fa\u50cf\u7d20\u6bb5\u76f8\u4f3c\u6027\u6307\u6570(PSSI)\uff0c\u7ed3\u5408\u50cf\u7d20\u5f3a\u5ea6\u548c\u7a7a\u95f4\u5e73\u6ed1\u7279\u5f81\uff0c\u4f7f\u7528\u8c10\u6ce2\u5747\u503c\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u91c7\u7528MeanShift\u8fdb\u884c\u4f4e\u5c42\u5206\u5272\uff0c\u6784\u5efa\u50cf\u7d20-\u6bb5\u56fe\uff0c\u4f7f\u7528\u6700\u5927\u751f\u6210\u6811\u8fdb\u884c\u5206\u5272", "result": "\u5728GrabCut\u548cImages250\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4AMOE\u3001OneCut\u3001SSNCut\u7b49\u65b9\u6cd5\uff0c\u5728IoU\u3001F1\u5206\u6570\u3001\u6267\u884c\u65f6\u95f4\u548c\u5e73\u5747\u8bef\u5dee\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "\u63d0\u51fa\u7684PSSI-MaxST\u65b9\u6cd5\u80fd\u6709\u6548\u7ed3\u5408\u989c\u8272\u76f8\u4f3c\u6027\u3001\u5e73\u6ed1\u6027\u3001\u7eb9\u7406\u3001\u5f62\u72b6\u548c\u5c40\u90e8\u8fde\u901a\u6027\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u56fe\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12002", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12002", "abs": "https://arxiv.org/abs/2601.12002", "authors": ["Oliver Sch\u00f6n", "Zhengang Zhong", "Sadegh Soudjani"], "title": "Kernel-Based Learning of Safety Barriers", "comment": "44 pages, 9 figures", "summary": "The rapid integration of AI algorithms in safety-critical applications such as autonomous driving and healthcare is raising significant concerns about the ability to meet stringent safety standards. Traditional tools for formal safety verification struggle with the black-box nature of AI-driven systems and lack the flexibility needed to scale to the complexity of real-world applications. In this paper, we present a data-driven approach for safety verification and synthesis of black-box systems with discrete-time stochastic dynamics. We employ the concept of control barrier certificates, which can guarantee safety of the system, and learn the certificate directly from a set of system trajectories. We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be inflated to robustify the result to out-of-distribution behavior. We provide the theoretical results on how to apply the approach to general classes of temporal logic specifications beyond safety. For the data-driven computation of safety barriers, we leverage a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. Our work moves beyond restrictive assumptions on system dynamics and uncertainty, as demonstrated on two case studies including a black-box system with a neural network controller.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5229\u7528\u63a7\u5236\u5c4f\u969c\u8bc1\u4e66\u548c\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5d4c\u5165\uff0c\u4e3a\u5177\u6709\u968f\u673a\u52a8\u6001\u7684\u9ed1\u76d2\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "motivation": "AI\u7b97\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5feb\u901f\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u6ee1\u8db3\u4e25\u683c\u5b89\u5168\u6807\u51c6\u7684\u62c5\u5fe7\u3002\u4f20\u7edf\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u96be\u4ee5\u5904\u7406AI\u7cfb\u7edf\u7684\u9ed1\u76d2\u7279\u6027\uff0c\u4e14\u7f3a\u4e4f\u6269\u5c55\u5230\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u5e94\u7528\u7684\u7075\u6d3b\u6027\u3002", "method": "1. \u4f7f\u7528\u63a7\u5236\u5c4f\u969c\u8bc1\u4e66\u4fdd\u8bc1\u7cfb\u7edf\u5b89\u5168\uff0c\u76f4\u63a5\u4ece\u7cfb\u7edf\u8f68\u8ff9\u6570\u636e\u4e2d\u5b66\u4e60\u8bc1\u4e66\uff1b2. \u5229\u7528\u6761\u4ef6\u5747\u503c\u5d4c\u5165\u5c06\u6570\u636e\u6620\u5c04\u5230\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff1b3. \u6784\u5efaRKHS\u6a21\u7cca\u96c6\u4ee5\u589e\u5f3a\u5bf9\u5206\u5e03\u5916\u884c\u4e3a\u7684\u9c81\u68d2\u6027\uff1b4. \u91c7\u7528\u6709\u9650\u5085\u91cc\u53f6\u5c55\u5f00\u5c06\u534a\u65e0\u9650\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u89c4\u5212\uff1b5. \u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u9ad8\u6548\u751f\u6210\u677e\u5f1b\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5206\u5e03\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8d85\u8d8a\u5bf9\u7cfb\u7edf\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u9650\u5236\u6027\u5047\u8bbe\u3002\u5728\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5305\u62ec\u5e26\u6709\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u9ed1\u76d2\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ed1\u76d2\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u548c\u7efc\u5408\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.11791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11791", "abs": "https://arxiv.org/abs/2601.11791", "authors": ["Laya Iyer", "Pranav Somani", "Alice Guo", "Dan Jurafsky", "Chen Shani"], "title": "Beyond Tokens: Concept-Level Training Objectives for LLMs", "comment": null, "summary": "The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \\textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\\rightarrow$ \\textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \\textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u4f20\u7edf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\uff08NTP\uff09\u8f6c\u5411\u6982\u5ff5\u7ea7\u9884\u6d4b\uff0c\u901a\u8fc7\u5c06\u591a\u4e2a\u8bed\u4e49\u76f8\u540c\u7684\u8868\u9762\u5f62\u5f0f\uff08\u5982\"mom\"\u3001\"mother\"\uff09\u5206\u7ec4\u4e3a\u6982\u5ff5\uff0c\u6765\u6539\u5584LLM\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u8bed\u4e49\u62bd\u8c61\u3002", "motivation": "NTP\u76ee\u6807\u5728\u8bcd\u7ea7\u522b\u64cd\u4f5c\uff0c\u4f1a\u5c06\u8bed\u4e49\u7b49\u4ef7\u4f46\u8868\u9762\u5f62\u5f0f\u4e0d\u540c\u7684\u5ef6\u7eed\uff08\u5982\"mom\" vs \"mother\"\uff09\u89c6\u4e3a\u9519\u8bef\uff0c\u4ece\u800c\u60e9\u7f5a\u6709\u6548\u7684\u62bd\u8c61\u3001\u6539\u5199\u6216\u6982\u5ff5\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\u3002\u8fd9\u79cd\u8bad\u7ec3\u4fe1\u53f7\u4e0e\u8bed\u4e49\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u4fc3\u4f7f\u9700\u8981\u66f4\u9ad8\u5c42\u6b21\u8868\u793a\u7684\u5b66\u4e60\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4ece\u8bcd\u7ea7\u522b\u9884\u6d4b\u8f6c\u5411\u6982\u5ff5\u7ea7\u522b\u9884\u6d4b\uff0c\u5176\u4e2d\u6982\u5ff5\u5c06\u540c\u4e00\u60f3\u6cd5\u7684\u591a\u4e2a\u8868\u9762\u5f62\u5f0f\u5206\u7ec4\uff08\u5982\"mom\"\u3001\"mommy\"\u3001\"mother\"\u2192MOTHER\uff09\u3002\u4ecb\u7ecd\u4e86\u5c06\u6982\u5ff5\u76d1\u7763\u96c6\u6210\u5230LLM\u8bad\u7ec3\u4e2d\u7684\u5404\u79cd\u65b9\u6cd5\u3002", "result": "\u6982\u5ff5\u611f\u77e5\u6a21\u578b\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3001\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5728\u591a\u6837\u5316NLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4NTP\u6a21\u578b\u66f4\u5f3a\u7684\u6027\u80fd\u3002", "conclusion": "\u6982\u5ff5\u7ea7\u76d1\u7763\u4f5c\u4e3a\u4e00\u79cd\u6539\u8fdb\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u80fd\u66f4\u597d\u5730\u5c06LLM\u4e0e\u4eba\u7c7b\u8bed\u4e49\u62bd\u8c61\u5bf9\u9f50\u3002"}}
{"id": "2601.11660", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11660", "abs": "https://arxiv.org/abs/2601.11660", "authors": ["Chunshu Wu", "Ruibing Song", "Sushant Kondguli", "Tong Geng", "Ang Li"], "title": "Zeros can be Informative: Masked Binary U-Net for Image Segmentation on Tensor Cores", "comment": null, "summary": "Real-time image segmentation is a key enabler for AR/VR, robotics, drones, and autonomous systems, where tight accuracy, latency, and energy budgets must be met on resource-constrained edge devices. While U-Net offers a favorable balance of accuracy and efficiency compared to large transformer-based models, achieving real-time performance on high-resolution input remains challenging due to compute, memory, and power limits. Extreme quantization, particularly binary networks, is appealing for its hardware-friendly operations. However, two obstacles limit practicality: (1) severe accuracy degradation, and (2) a lack of end-to-end implementations that deliver efficiency on general-purpose GPUs.\n  We make two empirical observations that guide our design. (1) An explicit zero state is essential: training with zero masking to binary U-Net weights yields noticeable sparsity. (2) Quantization sensitivity is uniform across layers. Motivated by these findings, we introduce Masked Binary U-Net (MBU-Net), obtained through a cost-aware masking strategy that prioritizes masking where it yields the highest accuracy-per-cost, reconciling accuracy with near-binary efficiency.\n  To realize these gains in practice, we develop a GPU execution framework that maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently implementing masked binary weights with binary activations. This design leverages native binary Tensor Core BMMA instructions, enabling high throughput and energy savings on widely available GPUs. Across 3 segmentation benchmarks, MBU-Net attains near full-precision accuracy (3% average drop) while delivering 2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.", "AI": {"tldr": "MBU-Net\uff1a\u901a\u8fc7\u96f6\u63a9\u7801\u7b56\u7565\u548c\u6210\u672c\u611f\u77e5\u63a9\u7801\u7684\u4e8c\u5143\u5316U-Net\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u7684\u901f\u5ea6\u548c\u80fd\u8017\u63d0\u5347\u3002", "motivation": "\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\u5728AR/VR\u3001\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9700\u8981\u5e73\u8861\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u3002\u4f20\u7edfU-Net\u5728\u9ad8\u6548\u6027\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u4e8c\u5143\u5316\u7f51\u7edc\u9762\u4e34\u51c6\u786e\u7387\u4e25\u91cd\u4e0b\u964d\u548c\u7f3a\u4e4f\u7aef\u5230\u7aefGPU\u5b9e\u73b0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMasked Binary U-Net (MBU-Net)\uff1a1) \u901a\u8fc7\u96f6\u63a9\u7801\u8bad\u7ec3\u83b7\u5f97\u7a00\u758f\u7684\u4e8c\u5143\u6743\u91cd\uff1b2) \u91c7\u7528\u6210\u672c\u611f\u77e5\u63a9\u7801\u7b56\u7565\uff0c\u4f18\u5148\u63a9\u7801\u5bf9\u7cbe\u5ea6-\u6210\u672c\u6bd4\u5f71\u54cd\u6700\u5927\u7684\u4f4d\u7f6e\uff1b3) \u5f00\u53d1GPU\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u6cd5\u4f4d\u7f16\u7801\u65b9\u6848\u5c06MBU-Net\u6620\u5c04\u5230Tensor Core\uff0c\u5229\u7528\u539f\u751f\u4e8c\u5143BMMA\u6307\u4ee4\u3002", "result": "\u57283\u4e2a\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMBU-Net\u8fbe\u5230\u63a5\u8fd1\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\uff08\u5e73\u5747\u4e0b\u964d\u4ec53%\uff09\uff0c\u76f8\u6bd416\u4f4d\u6d6e\u70b9U-Net\u5b9e\u73b02.04\u500d\u52a0\u901f\u548c3.54\u500d\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "MBU-Net\u901a\u8fc7\u63a9\u7801\u4e8c\u5143\u5316\u7b56\u7565\u548cGPU\u4f18\u5316\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e8c\u5143\u7f51\u7edc\u5728\u5b9e\u65f6\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12014", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12014", "abs": "https://arxiv.org/abs/2601.12014", "authors": ["Elio Masciari", "Vincenzo Moscato", "Enea Vincenzo Napolitano", "Gian Marco Orlando", "Marco Perillo", "Diego Russo"], "title": "Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats", "comment": null, "summary": "Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.\n  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u73af\u5883\u6548\u7387\uff0c\u5f15\u5165GCS_env\u6307\u6807\u7efc\u5408\u8861\u91cf\u7ed3\u6784\u6b63\u786e\u6027\u548c\u78b3\u6392\u653e\u6548\u7387\uff0c\u5e76\u6bd4\u8f83TOON\u683c\u5f0f\u4e0e\u4f20\u7edf\u683c\u5f0f\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u7ed3\u6784\u5316\u8f93\u51fa\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u6b63\u786e\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e0d\u540c\u8f93\u51fa\u683c\u5f0f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u73af\u5883\u5f71\u54cd\u3002\u9700\u8981\u5efa\u7acb\u7efc\u5408\u8003\u8651\u6b63\u786e\u6027\u548c\u73af\u5883\u6548\u7387\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u91cftoken\u4f7f\u7528\u91cf\u3001\u751f\u6210\u65f6\u95f4\u548c\u4f30\u7b97\u78b3\u6392\u653e\u3002\u5f15\u5165\u73af\u5883\u611f\u77e5\u751f\u6210\u6b63\u786e\u6027\u5206\u6570(GCS_env)\uff0c\u7edf\u4e00\u8bc4\u4f30\u7ed3\u6784\u6b63\u786e\u6027\u548c\u78b3\u611f\u77e5\u6548\u7387\u3002\u7cfb\u7edf\u6bd4\u8f83TOON\u683c\u5f0f\u4e0eJSON\u3001XML\u3001YAML\u7b49\u4f20\u7edf\u683c\u5f0f\u5728\u4e0d\u540c\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u7684LLM\u4e0a\u7684\u8868\u73b0\u3002", "result": "TOON\u683c\u5f0f\u4ea7\u751f\u66f4\u7d27\u51d1\u7684\u8f93\u51fa\u548c\u66f4\u4f4e\u7684\u6392\u653e\uff0c\u4f46\u5728\u6a21\u578b\u7f3a\u4e4f\u539f\u751f\u652f\u6301\u65f6\u7ed3\u6784\u6b63\u786e\u6027\u8f83\u4f4e\u3002\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u53ef\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u73af\u5883\u611f\u77e5\u8bc4\u5206\u53ef\u6839\u636e\u90e8\u7f72\u4f18\u5148\u7ea7\u6539\u53d8\u683c\u5f0f\u6392\u540d\u3002", "conclusion": "\u9700\u8981\u5305\u542b\u53ef\u6301\u7eed\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7d27\u51d1\u8868\u793a\u5982TOON\u5728\u5927\u89c4\u6a21\u3001\u78b3\u610f\u8bc6LLM\u90e8\u7f72\u4e2d\u5177\u6709\u5b9e\u9645\u4f18\u52bf\uff0c\u73af\u5883\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\u6709\u52a9\u4e8e\u5728\u6b63\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u505a\u51fa\u5e73\u8861\u51b3\u7b56\u3002"}}
{"id": "2601.11819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11819", "abs": "https://arxiv.org/abs/2601.11819", "authors": ["Shirlene Rose Bandela", "Sanjeev Parthasarathy", "Vaibhav Garg"], "title": "TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit", "comment": "11 pages, 12 figures, 7 tables", "summary": "Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTWeddit\u6570\u636e\u96c6\uff0c\u6807\u6ce8Reddit\u4e0a\u5973\u6027\u76f8\u5173\u521b\u4f24\u7ecf\u5386\uff08\u6d41\u4ea7\u3001\u6027\u66b4\u529b\u7b49\uff09\u7684\u89e6\u53d1\u8b66\u544a\u5185\u5bb9\uff0c\u586b\u8865\u76f8\u5173\u6570\u636e\u7a7a\u767d", "motivation": "Reddit\u7b49\u5e73\u53f0\u7528\u6237\u5e38\u5206\u4eab\u6d41\u4ea7\u3001\u6027\u66b4\u529b\u7b49\u521b\u4f24\u7ecf\u5386\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u89e6\u53d1\u8b66\u544a\u673a\u5236\uff0c\u4e14\u73b0\u6709\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5973\u6027\u4e3b\u8981\u9762\u4e34\u95ee\u9898\u7684\u89e6\u53d1\u5185\u5bb9\u6570\u636e\u96c6", "method": "\u6784\u5efaTWeddit\u6570\u636e\u96c6\uff0c\u6807\u6ce8Reddit\u4e0a\u89e6\u53d1\u7ecf\u5386\u76f8\u5173\u5185\u5bb9\uff0c\u8fdb\u884c\u8bed\u8a00\u5b66\u5206\u6790\uff0c\u5305\u62ec\u8bdd\u9898\u5206\u6790\u548c\u9053\u5fb7\u57fa\u7840\u5206\u6790", "result": "TWeddit\u6570\u636e\u96c6\u4e2d\u7684\u6807\u6ce8\u6545\u4e8b\u8868\u73b0\u51fa\u72ec\u7279\u7684\u8bdd\u9898\u548c\u9053\u5fb7\u57fa\u7840\u7279\u5f81\uff0c\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u5bf9\u672a\u6765\u7814\u7a76\u7684\u5b9e\u7528\u6027", "conclusion": "TWeddit\u6570\u636e\u96c6\u586b\u8865\u4e86Reddit\u89e6\u53d1\u7ecf\u5386\u6807\u6ce8\u6570\u636e\u7684\u7a7a\u767d\uff0c\u7279\u522b\u5173\u6ce8\u5973\u6027\u76f8\u5173\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u8d44\u6e90"}}
{"id": "2601.11662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11662", "abs": "https://arxiv.org/abs/2601.11662", "authors": ["Abdullah Jirjees", "Ryan Myers", "Muhammad Haris Ikram", "Mohamed H. Zaki"], "title": "LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions", "comment": null, "summary": "Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short/occluded VRUs under adverse conditions is, to the best of our knowledge, novel.", "AI": {"tldr": "\u63d0\u51faLTV-YOLO\u8f7b\u91cf\u7ea7\u70ed\u6210\u50cf\u68c0\u6d4b\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u548c\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u68c0\u6d4b\u513f\u7ae5\u7b49\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\uff0c\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRGB\u76f8\u673a\u5728\u4f4e\u5149\u7167\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u68c0\u6d4b\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\uff08\u7279\u522b\u662f\u513f\u7ae5\u548c\u9752\u5c11\u5e74\uff09\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u53ef\u9760\u7684\u70ed\u6210\u50cf\u68c0\u6d4b\u65b9\u6848\u6765\u63d0\u9ad8\u884c\u4eba\u5b89\u5168\u3002", "method": "\u57fa\u4e8eYOLO11\u67b6\u6784\uff0c\u91c7\u7528\u957f\u6ce2\u7ea2\u5916\u70ed\u6210\u50cf\uff0c\u96c6\u6210\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u4e13\u95e8\u9488\u5bf9\u70ed\u6210\u50cf\u68c0\u6d4b\u5c0f\u578b\u3001\u906e\u6321\u548c\u70ed\u7279\u5f81\u660e\u663e\u7684\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\u8fdb\u884c\u4f18\u5316\u3002", "result": "LTV-YOLO\u5728\u68c0\u6d4b\u5c0f\u578b\u3001\u90e8\u5206\u906e\u6321\u548c\u70ed\u7279\u5f81\u660e\u663e\u7684\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u7d27\u51d1\u67b6\u6784\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b66\u6821\u533a\u57df\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u6167\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u9ad8\u4e86\u6076\u52a3\u6761\u4ef6\u4e0b\u884c\u4eba\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.12024", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12024", "abs": "https://arxiv.org/abs/2601.12024", "authors": ["Kartikey Singh Bhandari", "Tanish Jain", "Archit Agrawal", "Dhruv Kumar", "Praveen Kumar", "Pratik Narang"], "title": "A Multi-Agent System for Generating Actionable Business Advice", "comment": null, "summary": "Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u5927\u89c4\u6a21\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u5546\u4e1a\u5efa\u8bae\uff0c\u901a\u8fc7\u805a\u7c7b\u3001\u751f\u6210\u3001\u8bc4\u4f30\u548c\u53ef\u884c\u6027\u6392\u5e8f\u7b49\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u5efa\u8bae\u7684\u53ef\u64cd\u4f5c\u6027\u3001\u5177\u4f53\u6027\u548c\u975e\u5197\u4f59\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u6790\u65b9\u6cd5\uff08\u5982\u60c5\u611f\u5206\u6790\u3001\u65b9\u9762\u63d0\u53d6\uff09\u4ec5\u505c\u7559\u5728\u63cf\u8ff0\u6027\u4efb\u52a1\u5c42\u9762\uff0c\u800cLLM\u751f\u6210\u7684\u5efa\u8bae\u5f80\u5f80\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u6df1\u5ea6\u63a8\u7406\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5c06\u5927\u89c4\u6a21\u8bc4\u8bba\u8bed\u6599\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u5546\u4e1a\u5efa\u8bae\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56db\u7ec4\u4ef6\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff1a1\uff09\u805a\u7c7b\u9009\u62e9\u4ee3\u8868\u6027\u8bc4\u8bba\uff1b2\uff09\u751f\u6210\u5efa\u8bae\uff1b3\uff09\u8fed\u4ee3\u8bc4\u4f30\uff1b4\uff09\u57fa\u4e8e\u53ef\u884c\u6027\u7684\u6392\u5e8f\u3002\u8be5\u8bbe\u8ba1\u5c06\u8bed\u6599\u84b8\u998f\u4e0e\u53cd\u9988\u9a71\u52a8\u7684\u5efa\u8bae\u7cbe\u70bc\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u670d\u52a1\u9886\u57df\u548c\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u53ef\u64cd\u4f5c\u6027\u3001\u5177\u4f53\u6027\u548c\u975e\u5197\u4f59\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\uff0c\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u63a5\u8fd1\u5927\u578b\u6a21\u578b\u6846\u67b6\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u80fd\u591f\u5c06\u5927\u89c4\u6a21\u5ba2\u6237\u8bc4\u8bba\u6709\u6548\u8f6c\u5316\u4e3a\u5177\u4f53\u3001\u53ef\u64cd\u4f5c\u4e14\u5b9e\u7528\u7684\u5546\u4e1a\u5efa\u8bae\uff0c\u4e3a\u4ece\u63cf\u8ff0\u6027\u5206\u6790\u5411\u89c4\u8303\u6027\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11846", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.11846", "abs": "https://arxiv.org/abs/2601.11846", "authors": ["Natalia Tomashenko", "Xiaoxiao Miao", "Pierre Champion", "Sarina Meyer", "Michele Panariello", "Xin Wang", "Nicholas Evans", "Emmanuel Vincent", "Junichi Yamagishi", "Massimiliano Todisco"], "title": "The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization", "comment": "under review", "summary": "We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.", "AI": {"tldr": "2024\u5e74\u7b2c\u4e09\u5c4aVoicePrivacy\u6311\u6218\u8d5b\u7ed3\u679c\u4e0e\u5206\u6790\uff0c\u4e13\u6ce8\u4e8e\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\uff0c\u65e8\u5728\u9690\u85cf\u8bf4\u8bdd\u4eba\u8eab\u4efd\u540c\u65f6\u4fdd\u7559\u8bed\u8a00\u5185\u5bb9\u548c\u60c5\u611f\u72b6\u6001", "motivation": "\u63a8\u52a8\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u53d1\u5c55\uff0c\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\uff0c\u5728\u4fdd\u62a4\u8bf4\u8bdd\u4eba\u8eab\u4efd\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u8bed\u97f3\u7684\u5b9e\u7528\u4ef7\u503c\uff08\u5185\u5bb9\u548c\u60c5\u611f\uff09", "method": "\u7cfb\u7edf\u6027\u7684\u6311\u6218\u6846\u67b6\u8bbe\u8ba1\uff0c\u5305\u62ec\u533f\u540d\u5316\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u5f00\u53d1\u3001\u653b\u51fb\u6a21\u578b\u6784\u5efa\u3001\u5ba2\u89c2\u8bc4\u4f30\u6307\u6807\uff08\u9690\u79c1\u4fdd\u62a4\u548c\u5b9e\u7528\u6027\uff09\uff0c\u63d0\u4f9b6\u4e2a\u57fa\u7ebf\u7cfb\u7edf\u5e76\u603b\u7ed3\u53c2\u8d5b\u8005\u521b\u65b0\u65b9\u6cd5", "result": "\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6311\u6218\u8d5b\u7684\u6574\u4f53\u6846\u67b6\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u53c2\u4e0e\u8005\u6210\u679c\uff0c\u4e3a\u8bed\u97f3\u533f\u540d\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6", "conclusion": "\u63d0\u51fa\u4e86\u5173\u952e\u89c1\u89e3\u548c\u89c2\u5bdf\uff0c\u6307\u5bfc\u672a\u6765VoicePrivacy\u6311\u6218\u8d5b\u8bbe\u8ba1\uff0c\u5e76\u6307\u51fa\u4e86\u8bed\u97f3\u533f\u540d\u5316\u7814\u7a76\u7684\u591a\u4e2a\u6709\u524d\u666f\u65b9\u5411"}}
{"id": "2601.11665", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11665", "abs": "https://arxiv.org/abs/2601.11665", "authors": ["Amir Farzin Nikkhah", "Dong Chen", "Bradford Campbell", "Somayeh Asadi", "Arsalan Heydarian"], "title": "UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM", "comment": "Accepted for publication at the International Conference on Construction Engineering and Management (I3CE 2025)", "summary": "Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u65e0\u4eba\u673a\u5728AEC+FM\u9886\u57df\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u5148\u8fdbAI\u7684\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u4eba\u673a\u6b63\u5728\u6539\u53d8AEC+FM\u9886\u57df\u7684\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u65b9\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u5904\u7406\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u6574\u5408\u5148\u8fdb\u6280\u672f\u3002", "method": "\u57fa\u4e8e150\u591a\u9879\u7814\u7a76\u7684\u7efc\u5408\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210RGB\u56fe\u50cf\u3001LiDAR\u548c\u70ed\u611f\u5e94\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8etransformer\u7684\u67b6\u6784\u548c\u52a8\u6001\u8def\u5f84\u89c4\u5212\uff0c\u5f62\u6210\u7cfb\u7edf\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u65e0\u4eba\u673a\u5728\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u3001\u707e\u5bb3\u54cd\u5e94\u3001\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3001\u80fd\u6e90\u6548\u7387\u8bc4\u4f30\u548c\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7b49\u65b9\u9762\u5df2\u8bc1\u660e\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u63d0\u9ad8\u7ed3\u6784\u7f3a\u9677\u3001\u70ed\u5f02\u5e38\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u65e0\u4eba\u673a\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u8f7b\u91cf\u7ea7AI\u6a21\u578b\u3001\u81ea\u9002\u5e94\u98de\u884c\u89c4\u5212\u3001\u5408\u6210\u6570\u636e\u96c6\u548c\u66f4\u4e30\u5bcc\u7684\u6a21\u6001\u878d\u5408\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u5904\u7406\u3001\u6570\u636e\u878d\u5408\u548c\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\u3002"}}
{"id": "2601.12030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12030", "abs": "https://arxiv.org/abs/2601.12030", "authors": ["Yilun Yao", "Shan Huang", "Elsie Dai", "Zhewen Tan", "Zhenyu Duan", "Shousheng Jia", "Yanbing Jiang", "Tong Yang"], "title": "ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents", "comment": "15 pages, 5 figures", "summary": "Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.", "AI": {"tldr": "ARC\u6846\u67b6\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e3b\u52a8\u7684\u3001\u53cd\u601d\u9a71\u52a8\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u76d1\u63a7\u548c\u4fee\u8ba2\u673a\u5236\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4fe1\u606f\u641c\u7d22\u4e2d\u7684\u4e0a\u4e0b\u6587\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7814\u7a76\u4ee3\u7406\u8fdb\u884c\u6df1\u5ea6\u641c\u7d22\u548c\u957f\u5e8f\u5217\u4fe1\u606f\u5bfb\u6c42\u65f6\uff0c\u968f\u7740\u4ea4\u4e92\u5386\u53f2\u589e\u957f\u6027\u80fd\u4f1a\u4e0b\u964d\uff08\u4e0a\u4e0b\u6587\u9000\u5316\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u539f\u59cb\u79ef\u7d2f\u6216\u88ab\u52a8\u603b\u7ed3\uff0c\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u9759\u6001\u4ea7\u7269\uff0c\u5bfc\u81f4\u65e9\u671f\u9519\u8bef\u6216\u4e0d\u5f53\u91cd\u70b9\u6301\u7eed\u5b58\u5728\u3002", "method": "\u63d0\u51faARC\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u7cfb\u7edf\u6027\u5730\u5236\u5b9a\u4e3a\u4e3b\u52a8\u7684\u3001\u53cd\u601d\u9a71\u52a8\u7684\u8fc7\u7a0b\uff0c\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u5185\u90e8\u63a8\u7406\u72b6\u6001\u3002\u901a\u8fc7\u53cd\u601d\u9a71\u52a8\u7684\u76d1\u63a7\u548c\u4fee\u8ba2\u673a\u5236\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5728\u68c0\u6d4b\u5230\u9519\u4f4d\u6216\u9000\u5316\u65f6\u4e3b\u52a8\u91cd\u7ec4\u5de5\u4f5c\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u5e8f\u5217\u4fe1\u606f\u5bfb\u6c42\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARC\u59cb\u7ec8\u4f18\u4e8e\u88ab\u52a8\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff0c\u5728BrowseComp-ZH\u57fa\u51c6\u4e0a\u4f7f\u7528Qwen2.5-32B-Instruct\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u8fbe11%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u4e3b\u52a8\u7684\u3001\u53cd\u601d\u9a71\u52a8\u7684\u8fc7\u7a0b\u80fd\u6709\u6548\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9000\u5316\u95ee\u9898\uff0cARC\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11854", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.11854", "abs": "https://arxiv.org/abs/2601.11854", "authors": ["Yifei Zhang", "Hooshang Nayyeri", "Rinat Khaziev", "Emine Yilmaz", "Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Hari Thadakamalla"], "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System", "comment": null, "summary": "Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.", "AI": {"tldr": "ATOD\u662f\u4e00\u4e2a\u9762\u5411\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u957f\u671f\u63a8\u7406\u3001\u591a\u76ee\u6807\u534f\u8c03\u3001\u4e3b\u52a8\u6027\u7b49\u667a\u80fd\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9LLM\u9a71\u52a8\u7684\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u667a\u80fd\u884c\u4e3a\uff08\u5982\u591a\u76ee\u6807\u534f\u8c03\u3001\u957f\u671f\u4e0a\u4e0b\u6587\u7ef4\u62a4\u3001\u5f02\u6b65\u6267\u884c\u548c\u4e3b\u52a8\u6027\uff09\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u652f\u6301\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faATOD\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u5bf9\u8bdd\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u9700\u8981\u957f\u671f\u63a8\u7406\u7684\u4e30\u5bcc\u6807\u6ce8\u5bf9\u8bdd\uff1b\u5f00\u53d1ATOD-Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u7ef4\u5ea6\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u6307\u6807\uff1b\u63d0\u51fa\u57fa\u4e8e\u8bb0\u5fc6\u7684\u667a\u80fd\u8bc4\u4f30\u5668\u3002", "result": "ATOD-Eval\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u4efb\u52a1\u5b8c\u6210\u5ea6\u3001\u667a\u80fd\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\uff1b\u63d0\u51fa\u7684\u8bc4\u4f30\u5668\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6743\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8bb0\u5fc6\u548cLLM\u7684\u65b9\u6cd5\u3002", "conclusion": "ATOD\u586b\u8865\u4e86\u8bc4\u4f30\u5148\u8fdb\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u667a\u80fd\u884c\u4e3a\u7684\u7a7a\u767d\uff0cATOD-Eval\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5bf9\u8bdd\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.11666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11666", "abs": "https://arxiv.org/abs/2601.11666", "authors": ["Muhammad Imran", "Chi Lee", "Yugyung Lee"], "title": "MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models", "comment": "12 pages, 3 figures, 1 table", "summary": "We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX's potential to enhance trust and transparency in radiological AI applications.", "AI": {"tldr": "MATEX\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u89e3\u5256\u5b66\u77e5\u8bc6\uff0c\u751f\u6210\u7cbe\u786e\u7a33\u5b9a\u7684\u68af\u5ea6\u5f52\u56e0\u56fe\uff0c\u5728\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u4e0d\u7cbe\u786e\u3001\u7f3a\u4e4f\u89e3\u5256\u5b66\u57fa\u7840\u3001\u6ce8\u610f\u529b\u7c92\u5ea6\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5fe0\u5b9e\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u89e3\u91ca\u6765\u589e\u5f3a\u653e\u5c04\u5b66AI\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u548c\u900f\u660e\u5ea6\u3002", "method": "MATEX\u6846\u67b6\u7ed3\u5408\u4e86\u591a\u5c42\u6ce8\u610f\u529b\u5c55\u5f00\u3001\u6587\u672c\u5f15\u5bfc\u7a7a\u95f4\u5148\u9a8c\u548c\u5c42\u4e00\u81f4\u6027\u5206\u6790\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u4fe1\u606f\u5316\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u751f\u6210\u7cbe\u786e\u3001\u7a33\u5b9a\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u68af\u5ea6\u5f52\u56e0\u56fe\u3002", "result": "\u5728MS-CXR\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMATEX\u5728\u7a7a\u95f4\u7cbe\u5ea6\u548c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u7684\u5339\u914d\u5ea6\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684M2IB\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "MATEX\u901a\u8fc7\u63d0\u5347\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u589e\u5f3a\u4e86\u653e\u5c04\u5b66AI\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u91ca\u652f\u6301\u3002"}}
{"id": "2601.12038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12038", "abs": "https://arxiv.org/abs/2601.12038", "authors": ["Beishui Liao"], "title": "Abstract Argumentation with Subargument Relations", "comment": "11 pages", "summary": "Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.", "AI": {"tldr": "\u63d0\u51fa\u5728\u62bd\u8c61\u8bba\u8fa9\u6846\u67b6\u4e2d\u5f15\u5165\u660e\u786e\u7684\u5b50\u8bba\u8bc1\u5173\u7cfb\uff0c\u4f5c\u4e3a\u4e0e\u653b\u51fb\u5173\u7cfb\u5e76\u5217\u7684\u57fa\u672c\u5173\u7cfb\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7ed3\u6784\u5316\u8bba\u8fa9\u4e2d\u7684\u4f9d\u8d56\u5173\u7cfb", "motivation": "Dung\u7684\u62bd\u8c61\u8bba\u8fa9\u6846\u67b6\u4ec5\u901a\u8fc7\u653b\u51fb\u5173\u7cfb\u6765\u8868\u5f81\u8bba\u8bc1\u53ef\u63a5\u53d7\u6027\uff0c\u867d\u7136\u8fd9\u79cd\u62bd\u8c61\u5c42\u6b21\u4ea7\u751f\u4e86\u4e30\u5bcc\u7684\u7814\u7a76\u6210\u679c\uff0c\u4f46\u9650\u5236\u4e86\u8868\u793a\u7ed3\u6784\u5316\u8bba\u8fa9\u5f62\u5f0f\u4e2d\u6838\u5fc3\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff08\u7279\u522b\u662f\u5b50\u8bba\u8bc1\u5173\u7cfb\uff09\u7684\u80fd\u529b\u3002\u73b0\u6709\u6269\u5c55\uff08\u5982\u53cc\u6781\u8bba\u8fa9\u6846\u67b6\uff09\u5f15\u5165\u4e86\u652f\u6301\u5173\u7cfb\uff0c\u4f46\u672a\u80fd\u6355\u6349\u5b50\u8bba\u8bc1\u7684\u4e0d\u5bf9\u79f0\u6027\u548c\u6784\u6210\u6027\u672c\u8d28\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u653b\u51fb\u7684\u4ea4\u4e92", "method": "\u7814\u7a76\u5728\u62bd\u8c61\u8bba\u8fa9\u6846\u67b6\u4e2d\u4e30\u5bcc\u660e\u786e\u7684\u5b50\u8bba\u8bc1\u5173\u7cfb\uff0c\u5c06\u5176\u4e0e\u653b\u51fb\u5173\u7cfb\u4e00\u8d77\u4f5c\u4e3a\u57fa\u672c\u5173\u7cfb\u3002\u5206\u6790\u5b50\u8bba\u8bc1\u5173\u7cfb\u5982\u4f55\u4e0e\u653b\u51fb\u4ea4\u4e92\uff0c\u5e76\u8003\u5bdf\u5b83\u4eec\u5bf9\u57fa\u672c\u8bed\u4e49\u5c5e\u6027\u7684\u5f71\u54cd", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u7ed3\u6784\u4fe1\u606f\u7684\u539f\u7406\u6027\u62bd\u8c61\uff0c\u5e76\u9610\u660e\u4e86\u5b50\u8bba\u8bc1\u5728\u62bd\u8c61\u53ef\u63a5\u53d7\u6027\u63a8\u7406\u4e2d\u7684\u4f5c\u7528", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5b50\u8bba\u8bc1\u5173\u7cfb\u4f5c\u4e3a\u4e0e\u653b\u51fb\u5173\u7cfb\u5e76\u5217\u7684\u57fa\u672c\u5173\u7cfb\uff0c\u80fd\u591f\u66f4\u597d\u5730\u62bd\u8c61\u7ed3\u6784\u5316\u8bba\u8fa9\u4fe1\u606f\uff0c\u6f84\u6e05\u5b50\u8bba\u8bc1\u5728\u62bd\u8c61\u53ef\u63a5\u53d7\u6027\u63a8\u7406\u4e2d\u7684\u89d2\u8272"}}
{"id": "2601.11865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11865", "abs": "https://arxiv.org/abs/2601.11865", "authors": ["Truong Nguyen", "Phi Van Dat", "Ngan Nguyen", "Linh Ngo Van", "Trung Le", "Thanh Hong Nguyen"], "title": "CTPD: Cross Tokenizer Preference Distillation", "comment": "AAAI 2026", "summary": "While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.", "AI": {"tldr": "CTPD\u662f\u9996\u4e2a\u7528\u4e8e\u5728\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u6a21\u578b\u95f4\u4f20\u9012\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u884c\u4e3a\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u5bf9\u9f50\u3001\u8de8\u5206\u8bcd\u5668\u91cd\u8981\u6027\u91c7\u6837\u548c\u6559\u5e08\u951a\u5b9a\u53c2\u8003\u5b9e\u73b0\u9ad8\u6548\u504f\u597d\u84b8\u998f\u3002", "motivation": "\u77e5\u8bc6\u84b8\u998f\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\uff0c\u7279\u522b\u662f\u5728\u66f4\u73b0\u5b9e\u7684\u8de8\u5206\u8bcd\u5668\u573a\u666f\u4e0b\uff0c\u5e94\u7528\u4e0d\u8db3\u3002\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u5206\u8bcd\u65b9\u6848\u7684\u4e0d\u517c\u5bb9\u6027\u963b\u788d\u4e86\u7ec6\u7c92\u5ea6\u7684\u767d\u76d2\u504f\u597d\u4fe1\u606f\u84b8\u998f\u3002", "method": "\u63d0\u51fa\u8de8\u5206\u8bcd\u5668\u504f\u597d\u84b8\u998f(CTPD)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u5bf9\u9f50\u8de8\u5ea6\u6295\u5f71\uff0c\u5c06\u5e08\u751f\u4ee4\u724c\u6620\u5c04\u5230\u5171\u4eab\u5b57\u7b26\u7ea7\u8de8\u5ea6\uff1b2) \u8de8\u5206\u8bcd\u5668\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u91c7\u6837(TIS-DPO)\u9002\u914d\uff1b3) \u6559\u5e08\u951a\u5b9a\u53c2\u8003\uff0c\u8ba9\u5b66\u751f\u76f4\u63a5\u5229\u7528\u6559\u5e08\u504f\u597d\u5728DPO\u98ce\u683c\u76ee\u6807\u4e2d\u3002", "result": "\u7406\u8bba\u5206\u6790\u5c06CTPD\u5efa\u7acb\u5728\u91cd\u8981\u6027\u91c7\u6837\u57fa\u7840\u4e0a\uff0c\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CTPD\u4e3a\u4e0d\u540c\u5206\u8bcd\u65b9\u6848\u95f4\u7684\u504f\u597d\u84b8\u998f\u63d0\u4f9b\u4e86\u5b9e\u7528\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u66f4\u6613\u83b7\u53d6\u548c\u9ad8\u6548\u7684\u5bf9\u9f50\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2601.11675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11675", "abs": "https://arxiv.org/abs/2601.11675", "authors": ["Ritik Raina", "Abe Leite", "Alexandros Graikos", "Seoyoung Ahn", "Dimitris Samaras", "Gregory J. Zelinsky"], "title": "Generating metamers of human scene understanding", "comment": null, "summary": "Human vision combines low-resolution \"gist\" information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. \"foveated\") inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a \"same\" or \"different\" response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers' own fixated regions.", "AI": {"tldr": "MetamerGen\uff1a\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u56f4\u89c6\u89c9\u7684\u6982\u89c8\u4fe1\u606f\u548c\u6ce8\u89c6\u70b9\u7684\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\uff0c\u751f\u6210\u4e0e\u4eba\u7c7b\u573a\u666f\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u7ed3\u5408\u5916\u56f4\u4f4e\u5206\u8fa8\u7387\u6982\u89c8\u4fe1\u606f\u548c\u6ce8\u89c6\u70b9\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u6765\u7406\u89e3\u573a\u666f\u3002\u4e3a\u4e86\u7814\u7a76\u4eba\u7c7b\u573a\u666f\u8868\u5f81\uff0c\u9700\u8981\u751f\u6210\u4e0e\u8fd9\u4e9b\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u6d41DINOv2\u4ee4\u724c\u8868\u793a\uff0c\u878d\u5408\u6ce8\u89c6\u533a\u57df\u7684\u8be6\u7ec6\u7279\u5f81\u548c\u5916\u56f4\u9000\u5316\u7279\u5f81\u3002\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e0e\u4eba\u7c7b\u8868\u5f81\u7684\u611f\u77e5\u5bf9\u9f50\u3002", "result": "MetamerGen\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u573a\u666f\u8868\u5f81\u5bf9\u9f50\u7684\u56fe\u50cf\u5143\u5339\u914d\u3002\u5f53\u57fa\u4e8e\u89c2\u5bdf\u8005\u81ea\u8eab\u6ce8\u89c6\u533a\u57df\u751f\u6210\u65f6\uff0c\u9ad8\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50\u6700\u80fd\u9884\u6d4b\u5143\u5339\u914d\u6027\u3002", "conclusion": "MetamerGen\u662f\u7406\u89e3\u573a\u666f\u7406\u89e3\u7684\u6709\u529b\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u5904\u7406\u591a\u4e2a\u5c42\u6b21\u4e0a\u5f71\u54cd\u4eba\u7c7b\u5224\u65ad\u7684\u7279\u5f81\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u89c2\u5bdf\u8005\u6ce8\u89c6\u533a\u57df\u65f6\u7684\u9ad8\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50\u3002"}}
{"id": "2601.12040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12040", "abs": "https://arxiv.org/abs/2601.12040", "authors": ["Murilo da Luz", "Bruno Brand\u00e3o", "Luana Martins", "Gustavo Oliveira", "Bryan de Oliveira", "Luckeciano Melo", "Telma Soares"], "title": "Partial Reasoning in Language Models: Search and Refinement Guided by Uncertainty", "comment": null, "summary": "The use of Large Language Models (LLMs) for reasoning and planning tasks has drawn increasing attention in Artificial Intelligence research. Despite their remarkable progress, these models still exhibit limitations in multi-step inference scenarios, particularly in mathematical and logical reasoning. We introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the entropy of the output distribution during autoregressive generation and halts the process whenever entropy exceeds a defined threshold, signaling uncertainty. From that point, a localized search is performed in the latent space to refine the partial reasoning and select the most coherent answer, using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B, Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard, SVAMP, and StrategyQA) showed performance greater than or similar to Soft Reasoning, indicating that entropy can serve as an effective signal to trigger selective refinement during reasoning.", "AI": {"tldr": "PREGU\u5229\u7528\u8f93\u51fa\u5206\u5e03\u7684\u71b5\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u89e6\u53d1\u5c40\u90e8\u641c\u7d22\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u591a\u6b65\u63a8\u7406\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u6539\u5584\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faPREGU\u65b9\u6cd5\uff1a\u5728\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u76d1\u63a7\u8f93\u51fa\u5206\u5e03\u7684\u71b5\uff0c\u5f53\u71b5\u8d85\u8fc7\u8bbe\u5b9a\u9608\u503c\u65f6\u505c\u6b62\u751f\u6210\uff0c\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002\u7136\u540e\u5728\u8be5\u70b9\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u7684\u5c40\u90e8\u641c\u7d22\uff0c\u4f7f\u7528Soft Reasoning\u65b9\u6cd5\u7cbe\u70bc\u90e8\u5206\u63a8\u7406\u5e76\u9009\u62e9\u6700\u8fde\u8d2f\u7684\u7b54\u6848\u3002", "result": "\u5728LLaMA-3-8B\u3001Mistral-7B\u548cQwen2-7B\u6a21\u578b\u4e0a\uff0c\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\uff08GSM8K\u3001GSM-Hard\u3001SVAMP\u548cStrategyQA\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPREGU\u7684\u6027\u80fd\u4f18\u4e8e\u6216\u7c7b\u4f3c\u4e8eSoft Reasoning\uff0c\u8868\u660e\u71b5\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u89e6\u53d1\u9009\u62e9\u6027\u7cbe\u70bc\u7684\u6709\u6548\u4fe1\u53f7\u3002", "conclusion": "\u71b5\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4fe1\u53f7\u6765\u89e6\u53d1\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9009\u62e9\u6027\u7cbe\u70bc\uff0cPREGU\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.11866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11866", "abs": "https://arxiv.org/abs/2601.11866", "authors": ["Kie Shidara", "Preethi Prem", "Jonathan Kim", "Anna Podlasek", "Feng Liu", "Ahmed Alaa", "Danilo Bernardo"], "title": "Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving", "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e34\u5e8a\u63a8\u7406\u7684\u7075\u6d3b\u6027\u4e00\u76f4\u5b58\u5728\u4e89\u8bae\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u533b\u5b66\u62bd\u8c61\u4e0e\u63a8\u7406\u8bed\u6599\u5e93(mARC)\u4e0a\u80fd\u591f\u907f\u514dEinstellung\u6548\u5e94\u9677\u9631\uff0c\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u533b\u5b66QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5176\u4e34\u5e8a\u63a8\u7406\u7684\u7075\u6d3b\u6027\u4ecd\u53d7\u8d28\u7591\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u662f\u5426\u80fd\u5728\u4e34\u5e8a\u63a8\u7406\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u8ba4\u77e5\u7075\u6d3b\u6027\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9Einstellung\u6548\u5e94\uff08\u601d\u7ef4\u5b9a\u52bf\uff09\u65f6\u3002", "method": "\u4f7f\u7528\u533b\u5b66\u62bd\u8c61\u4e0e\u63a8\u7406\u8bed\u6599\u5e93(mARC)\u8fd9\u4e00\u5bf9\u6297\u6027\u533b\u5b66QA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u5229\u7528Einstellung\u6548\u5e94\u8bf1\u5bfc\u5bf9\u5b66\u4e60\u5230\u7684\u542f\u53d1\u5f0f\u6a21\u5f0f\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002\u8bc4\u4f30\u4e86OpenAI\u3001Grok\u3001Gemini\u3001Claude\u548cDeepSeek\u7b49\u5bb6\u65cf\u7684\u63a8\u7406\u6a21\u578b\u3002", "result": "\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u6bd4\u5f31\u63a8\u7406\u6a21\u578b\u66f4\u9891\u7e41\u5730\u907f\u514d\u4e86Einstellung\u6548\u5e94\u9677\u9631\uff0c\u5728mARC\u4e0a\u8fbe\u5230\u4e86\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u3002\u5728\u533b\u751f\u6700\u5e38\u51fa\u9519\u7684\u95ee\u9898\u4e0a\uff0c\u524d5\u540d\u6a21\u578b\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u6b63\u786e\u56de\u7b54\u4e8655%\u81f370%\uff0c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u6bd4\u4eba\u7c7b\u66f4\u4e0d\u5bb9\u6613\u53d7\u5230Einstellung\u6548\u5e94\u7684\u5f71\u54cd\u3002", "conclusion": "\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\u5728\u533b\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u7075\u6d3b\u6027\uff0c\u5728mARC\u4e0a\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u8868\u660e\u5148\u8fdb\u7684LLMs\u53ef\u80fd\u5728\u67d0\u4e9b\u65b9\u9762\u6bd4\u4eba\u7c7b\u533b\u751f\u66f4\u4e0d\u5bb9\u6613\u53d7\u5230\u8ba4\u77e5\u504f\u89c1\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.11679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11679", "abs": "https://arxiv.org/abs/2601.11679", "authors": ["Richard Hartley"], "title": "Conformal Point and the Calibrated Conic", "comment": null, "summary": "This gives some information about the conformal point and the calibrating conic, and their relationship one to the other. These concepts are useful for visualizing image geometry, and lead to intuitive ways to compute geometry, such as angles and directions in an image.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\u53ca\u5176\u76f8\u4e92\u5173\u7cfb\uff0c\u8fd9\u4e9b\u6982\u5ff5\u6709\u52a9\u4e8e\u56fe\u50cf\u51e0\u4f55\u53ef\u89c6\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u56fe\u50cf\u4e2d\u89d2\u5ea6\u548c\u65b9\u5411\u7684\u76f4\u89c2\u65b9\u6cd5\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u66f4\u76f4\u89c2\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u53ef\u89c6\u5316\u56fe\u50cf\u51e0\u4f55\uff0c\u7279\u522b\u662f\u89d2\u5ea6\u548c\u65b9\u5411\u7684\u8ba1\u7b97\u3002\u901a\u8fc7\u5f15\u5165\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u7b80\u5316\u56fe\u50cf\u51e0\u4f55\u5206\u6790\u8fc7\u7a0b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6570\u5b66\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6982\u5ff5\u5e94\u7528\u4e8e\u56fe\u50cf\u51e0\u4f55\u5206\u6790\u4e2d\uff0c\u7279\u522b\u662f\u7528\u4e8e\u8ba1\u7b97\u89d2\u5ea6\u548c\u65b9\u5411\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u7684\u6982\u5ff5\u80fd\u591f\u6709\u6548\u5730\u53ef\u89c6\u5316\u56fe\u50cf\u51e0\u4f55\uff0c\u5e76\u4e3a\u8ba1\u7b97\u56fe\u50cf\u4e2d\u7684\u89d2\u5ea6\u548c\u65b9\u5411\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5171\u5f62\u70b9\u548c\u6821\u51c6\u5706\u9525\u662f\u7406\u89e3\u56fe\u50cf\u51e0\u4f55\u7684\u6709\u7528\u5de5\u5177\uff0c\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u4e3a\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u51e0\u4f55\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u89d2\u5ea6\u548c\u65b9\u5411\u6d4b\u91cf\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.12126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12126", "abs": "https://arxiv.org/abs/2601.12126", "authors": ["Guocun Wang", "Kenkun Liu", "Jing Lin", "Guorui Song", "Jian Li", "Xiaoguang Han"], "title": "UniMo: Unified Motion Generation and Understanding with Chain of Thought", "comment": null, "summary": "Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.", "AI": {"tldr": "UniMo\u6846\u67b6\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u8fd0\u52a8-\u8bed\u8a00\u4fe1\u606f\u4e0e\u53ef\u89e3\u91ca\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u96c6\u6210\u5230LLM\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u4e0e\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u4e0e\u7406\u89e3\u65b9\u6cd5\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4e14\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u6846\u67b6\u5b58\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u4efb\u52a1\u8fde\u8d2f\u6027\u6311\u6218\uff0c\u540c\u65f6LLM\u7684\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u8303\u5f0f\u4e0d\u9002\u5408\u8fd0\u52a8\u5e8f\u5217\uff0c\u4f1a\u5bfc\u81f4\u7d2f\u79ef\u9884\u6d4b\u8bef\u5dee\u3002", "method": "\u63d0\u51faUniMo\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5c06\u8fd0\u52a8-\u8bed\u8a00\u4fe1\u606f\u548c\u53ef\u89e3\u91ca\u601d\u7ef4\u94fe\u63a8\u7406\u96c6\u6210\u5230LLM\uff1b2\uff09\u5f15\u5165\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316\u4ee4\u724c\u7ec4\u4ee5\u589e\u5f3a\u7ed3\u6784\u6b63\u786e\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniMo\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u7edf\u4e00\u6846\u67b6\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u5728\u8fd0\u52a8\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UniMo\u901a\u8fc7\u96c6\u6210\u8fd0\u52a8-\u8bed\u8a00\u4fe1\u606f\u3001\u53ef\u89e3\u91ca\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a3D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u4e0e\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11872", "abs": "https://arxiv.org/abs/2601.11872", "authors": ["Nguyen Tien Phat", "Ngo Vu Minh", "Linh Van Ngo", "Nguyen Thi Ngoc Diep", "Thien Huu Nguyen"], "title": "GloCTM: Cross-Lingual Topic Modeling via a Global Context Space", "comment": "AAAI 2026", "summary": "Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.", "AI": {"tldr": "GloCTM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u8bed\u8a00\u4e3b\u9898\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\u5b9e\u73b0\u8de8\u8bed\u8a00\u4e3b\u9898\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00\u4e3b\u9898\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5728\u5206\u79bb\u7684\u8bed\u8a00\u7279\u5b9a\u7a7a\u95f4\u4e2d\u5b66\u4e60\u4e3b\u9898\uff0c\u4f9d\u8d56\u53cc\u8bed\u8bcd\u5178\u7b49\u5bf9\u9f50\u673a\u5236\uff0c\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u8de8\u8bed\u8a00\u8bed\u4e49\uff1b2\uff09\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u8868\u793a\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\u3002", "method": "GloCTM\u901a\u8fc7\u8de8\u8bed\u8a00\u8bcd\u6c47\u90bb\u57df\u6269\u5c55\u8bcd\u888b\u8868\u793a\u6784\u5efa\u4e30\u5bcc\u8f93\u5165\uff0c\u4f7f\u7528\u5c40\u90e8\u548c\u5168\u5c40\u7f16\u7801\u5668\u63a8\u65ad\u4e3b\u9898\u6bd4\u4f8b\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u6b63\u5219\u5316\u5bf9\u9f50\u5176\u6f5c\u5728\u8868\u793a\u3002\u5728\u8f93\u51fa\u5c42\u9762\uff0c\u5168\u5c40\u4e3b\u9898-\u8bcd\u5206\u5e03\uff08\u57fa\u4e8e\u5408\u5e76\u8bcd\u6c47\u8868\uff09\u7ed3\u6784\u4e0a\u540c\u6b65\u8de8\u8bed\u8a00\u4e3b\u9898\u542b\u4e49\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e2d\u5fc3\u6838\u5bf9\u9f50\u635f\u5931\u5c06\u6f5c\u5728\u4e3b\u9898\u7a7a\u95f4\u4e0e\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u5d4c\u5165\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGloCTM\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GloCTM\u901a\u8fc7\u5728\u6574\u4e2a\u6a21\u578b\u6d41\u7a0b\u4e2d\u6784\u5efa\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u8bed\u8a00\u4e3b\u9898\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11700", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11700", "abs": "https://arxiv.org/abs/2601.11700", "authors": ["Luis A. Leiva", "Moises Diaz", "Nuwan T. Attygalle", "Miguel A. Ferrer", "Rejean Plamondon"], "title": "Telling Human and Machine Handwriting Apart", "comment": null, "summary": "Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u624b\u5199\u8fd0\u52a8\u4f5c\u4e3a\u884c\u4e3a\u751f\u7269\u7279\u5f81\uff0c\u901a\u8fc7\u6d45\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u624b\u5199\u8f93\u5165\u662f\u5426\u7531\u4eba\u7c7b\u751f\u6210\uff0c\u5bf9\u6297\u591a\u79cdAI\u5408\u6210\u5668\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.3%\u7684AUC\u548c1.4%\u7684\u7b49\u9519\u8bef\u7387\u3002", "motivation": "\u624b\u5199\u8fd0\u52a8\u53ef\u4f5c\u4e3a\u72ec\u7279\u7684\u751f\u7269\u7279\u5f81\u6765\u9a8c\u8bc1\u8bbe\u5907\u64cd\u4f5c\u8005\u662f\u5426\u4e3a\u771f\u4eba\uff0c\u8fd9\u76f8\u5f53\u4e8e\u4e00\u79cd\u53cd\u5411\u56fe\u7075\u6d4b\u8bd5\u3002\u9700\u8981\u68c0\u6d4b\u8f93\u5165\u662f\u7531\u4eba\u7c7b\u751f\u6210\u8fd8\u662fAI\u5408\u6210\uff0c\u4ee5\u589e\u5f3a\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u9632\u6b62\u653b\u51fb\u8005\u5165\u4fb5\u3002", "method": "\u7814\u7a7610\u4e2a\u516c\u5f00\u624b\u5199\u7b26\u53f7\u6570\u636e\u96c6\uff08\u5305\u62ec\u5b57\u7b26\u3001\u6570\u5b57\u3001\u624b\u52bf\u3001\u6307\u5411\u8f68\u8ff9\u548c\u7b7e\u540d\uff09\uff0c\u4f7f\u75287\u79cd\u4e0d\u540c\u7684\u5408\u6210\u5668\uff08\u5305\u62ec\u8fd0\u52a8\u5b66\u7406\u8bba\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3001Transformer\u548c\u6269\u6563\u6a21\u578b\uff09\u4eba\u5de5\u590d\u5236\u3002\u8bad\u7ec3\u6d45\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u975e\u7279\u5f81\u5316\u7684\u8f68\u8ff9\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u5408\u6210\u5668\u548c\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8fbe\u523098.3%\u7684AUC\u548c1.4%\u7684\u7b49\u9519\u8bef\u7387\u3002\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u4ec5\u4f7f\u752810%\u6570\u636e\u8bad\u7ec3\u5c31\u80fd\u5728\u5269\u4f5990%\u6d4b\u8bd5\u96c6\u4e0a\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9700\u8981\u9a8c\u8bc1\u4eba\u7c7b\u5b58\u5728\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u52a0\u4e86\u989d\u5916\u7684\u5b89\u5168\u5c42\u6765\u9632\u8303\u653b\u51fb\u8005\u3002\u6d45\u5c42\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u624b\u5199\u751f\u7269\u7279\u5f81\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u5728\u5c11\u6837\u672c\u548c\u57df\u5916\u573a\u666f\u4e0b\u4e5f\u4fdd\u6301\u5f3a\u5927\u6027\u80fd\u3002"}}
{"id": "2601.12138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12138", "abs": "https://arxiv.org/abs/2601.12138", "authors": ["Abhishek Kumar", "Riya Tapwal", "Carsten Maple"], "title": "DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.", "AI": {"tldr": "DriveSafe\uff1a\u9488\u5bf9LLM\u9a7e\u9a76\u52a9\u624b\u7684\u56db\u5c42\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5305\u542b129\u4e2a\u7ec6\u7c92\u5ea6\u98ce\u9669\u7c7b\u522b\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709LLM\u5728\u9a7e\u9a76\u573a\u666f\u4e2d\u5b89\u5168\u62d2\u7edd\u80fd\u529b\u4e0d\u8db3", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u8f66\u8f7d\u6570\u5b57\u52a9\u624b\u4e2d\uff0c\u4f46\u4e0d\u5b89\u5168\u3001\u6a21\u7cca\u6216\u6cd5\u5f8b\u9519\u8bef\u7684\u54cd\u5e94\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u5b89\u5168\u3001\u4f26\u7406\u548c\u76d1\u7ba1\u540e\u679c\u3002\u73b0\u6709\u5b89\u5168\u5206\u7c7b\u548c\u8bc4\u4f30\u6846\u67b6\u5927\u591a\u662f\u901a\u7528\u578b\u7684\uff0c\u672a\u80fd\u6355\u6349\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u9886\u57df\u7279\u5b9a\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86DriveSafe\uff0c\u4e00\u4e2a\u5206\u5c42\u7684\u56db\u5c42\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u5305\u542b129\u4e2a\u7ec6\u7c92\u5ea6\u539f\u5b50\u98ce\u9669\u7c7b\u522b\uff0c\u6db5\u76d6\u6280\u672f\u3001\u6cd5\u5f8b\u3001\u793e\u4f1a\u3001\u4f26\u7406\u7ef4\u5ea6\uff0c\u57fa\u4e8e\u771f\u5b9e\u9a7e\u9a76\u6cd5\u89c4\u548c\u5b89\u5168\u539f\u5219\uff0c\u5e76\u7531\u9886\u57df\u4e13\u5bb6\u8bc4\u5ba1\u3002\u901a\u8fc7\u8bc4\u4f30\u516d\u4e2a\u5e7f\u6cdb\u90e8\u7f72\u7684LLM\u7684\u62d2\u7edd\u884c\u4e3a\u6765\u9a8c\u8bc1\u6784\u5efa\u63d0\u793a\u7684\u5b89\u5168\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u8bc4\u4f30\u7684\u6a21\u578b\u7ecf\u5e38\u65e0\u6cd5\u9002\u5f53\u62d2\u7edd\u4e0d\u5b89\u5168\u6216\u4e0d\u5408\u89c4\u7684\u9a7e\u9a76\u76f8\u5173\u67e5\u8be2\uff0c\u7a81\u663e\u4e86\u901a\u7528\u5b89\u5168\u5bf9\u9f50\u5728\u9a7e\u9a76\u4e0a\u4e0b\u6587\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9a7e\u9a76\u573a\u666f\u7684LLM\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u73b0\u6709\u901a\u7528\u5b89\u5168\u5bf9\u9f50\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u9a7e\u9a76\u9886\u57df\u7684\u7279\u5b9a\u98ce\u9669\u3002"}}
{"id": "2601.11886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11886", "abs": "https://arxiv.org/abs/2601.11886", "authors": ["Kaijie Mo", "Siddhartha Venkatayogi", "Chantal Shaib", "Ramez Kouzy", "Wei Xu", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence", "comment": "26 pages", "summary": "In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such \"evidence\" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.", "AI": {"tldr": "LLMs\u5728\u533b\u5b66\u9886\u57df\u9762\u5bf9\u53cd\u4e8b\u5b9e\u8bc1\u636e\u65f6\uff0c\u4f1a\u4e0d\u52a0\u6279\u5224\u5730\u63a5\u53d7\u5371\u9669\u6216\u4e0d\u5408\u7406\u7684\u4fe1\u606f\uff0c\u66b4\u9732\u51fa\u6a21\u578b\u5728\u5fe0\u5b9e\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u8fb9\u754c\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u5728\u533b\u5b66\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u6a21\u578b\u901a\u5e38\u9700\u8981\u5fe0\u5b9e\u9075\u5faa\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u4f46\u5f53\u4e0a\u4e0b\u6587\u4e0e\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u6216\u5b89\u5168\u534f\u8bae\u4e0d\u4e00\u81f4\u65f6\uff0c\u6a21\u578b\u4f1a\u5982\u4f55\u8868\u73b0\uff1f\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5728\u9762\u5bf9\u53cd\u4e8b\u5b9e\u751a\u81f3\u5bf9\u6297\u6027\u533b\u5b66\u8bc1\u636e\u65f6\u7684\u884c\u4e3a\u548c\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u6784\u5efaMedCounterFact\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e34\u5e8a\u6bd4\u8f83\u95ee\u9898\uff08\u8bc4\u4f30\u6cbb\u7597\u6548\u679c\uff09\uff0c\u5176\u4e2d\u771f\u5b9e\u533b\u5b66\u5e72\u9884\u88ab\u7cfb\u7edf\u66ff\u6362\u4e3a\u56db\u79cd\u53cd\u4e8b\u5b9e\u523a\u6fc0\uff08\u4ece\u672a\u77e5\u8bcd\u6c47\u5230\u6709\u6bd2\u7269\u8d28\uff09\u3002\u5728\u591a\u4e2a\u524d\u6cbfLLMs\u4e0a\u8bc4\u4f30\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u8bc1\u636e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u9762\u5bf9\u53cd\u4e8b\u5b9e\u8bc1\u636e\u65f6\uff0c\u7edd\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f1a\u4e0d\u52a0\u6279\u5224\u5730\u63a5\u53d7\u8fd9\u4e9b\"\u8bc1\u636e\"\uff0c\u5373\u4f7f\u8bc1\u636e\u5371\u9669\u6216\u4e0d\u53ef\u4fe1\uff0c\u5e76\u7ed9\u51fa\u81ea\u4fe1\u4e14\u65e0\u4fdd\u7559\u7684\u56de\u7b54\u3002\u6a21\u578b\u5728\u5fe0\u5b9e\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7f3a\u4e4f\u6709\u6548\u8fb9\u754c\u3002", "conclusion": "\u867d\u7136\u7406\u8bba\u4e0a\u9700\u8981\u5728\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u5212\u5b9a\u754c\u9650\uff0c\u4f46\u5f53\u524dLLMs\u5c1a\u672a\u5efa\u7acb\u8fd9\u6837\u7684\u8fb9\u754c\u3002\u6a21\u578b\u5bb9\u6613\u63a5\u53d7\u5371\u9669\u7684\u53cd\u4e8b\u5b9e\u533b\u5b66\u8bc1\u636e\uff0c\u8fd9\u5728\u533b\u5b66\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u9690\u60a3\u3002"}}
{"id": "2601.11724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11724", "abs": "https://arxiv.org/abs/2601.11724", "authors": ["Muditha Fernando", "Kajhanan Kailainathan", "Krishnakanth Nagaratnam", "Isuranga Udaravi Bandara Senavirathne", "Ranga Rodrigo"], "title": "SemAlign: Language Guided Semi-supervised Domain Generalization", "comment": "15 pages, 6 figures", "summary": "Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature's excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u4e30\u5bcc\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7b56\u7565", "motivation": "\u73b0\u6709SSDG\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4f2a\u6807\u7b7e\u7cbe\u5ea6\u800c\u5ffd\u89c6\u4e86\u8bad\u7ec3\u671f\u95f4\u7684\u6700\u5927\u5316\u6570\u636e\u5229\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u57df\u4e0d\u53d8\u6027\u53c8\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u65b9\u6cd5", "method": "1) \u5c06\u6a21\u578b\u4e2d\u95f4\u7279\u5f81\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u8bed\u4e49\u4e30\u5bcc\u4e14\u6cdb\u5316\u7684\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u4ee5\u4fc3\u8fdb\u57df\u4e0d\u53d8\u6027\uff1b2) \u7ed3\u5408\u6709\u6548\u7684\u56fe\u50cf\u7ea7\u589e\u5f3a\u548c\u8f93\u51fa\u7ea7\u6b63\u5219\u5316\u7b56\u7565\u6765\u63d0\u9ad8\u6570\u636e\u5229\u7528\u5e76\u51cf\u5c11\u8fc7\u62df\u5408", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e0e\u73b0\u6709SSDG\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684(SOTA)\u7ed3\u679c", "conclusion": "\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u3001\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86SSDG\u4e2d\u6570\u636e\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u57df\u6cdb\u5316\u6027\u80fd"}}
{"id": "2601.12141", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12141", "abs": "https://arxiv.org/abs/2601.12141", "authors": ["Yuliia Suprun", "Khen Elimelech", "Lydia E. Kavraki", "Moshe Y. Vardi"], "title": "TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals", "comment": null, "summary": "Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.", "AI": {"tldr": "TIDE\u662f\u4e00\u79cd\u65b0\u9896\u7684LTLf\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u65f6\u5e8f\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684reach-avoid\u5b50\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u6210\u672c\u9a71\u52a8\u542f\u53d1\u5f0f\u548c\u81ea\u9002\u5e94\u56de\u6eaf\u673a\u5236\u6765\u6307\u5bfc\u641c\u7d22\uff0c\u63d0\u9ad8\u4e86\u65f6\u5e8f\u6269\u5c55\u76ee\u6807\u89c4\u5212\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLTLf\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5c06\u65f6\u5e8f\u89c4\u5212\u95ee\u9898\u8f6c\u5316\u4e3a\u5177\u6709\u53ef\u8fbe\u6027\u76ee\u6807\u7684\u7ecf\u5178\u89c4\u5212\u95ee\u9898\uff0c\u7136\u540e\u4f7f\u7528\u73b0\u6210\u7684\u89c4\u5212\u5668\u6c42\u89e3\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u65f6\u5e8f\u76ee\u6807\u7684\u542f\u53d1\u5f0f\u6307\u5bfc\u641c\u7d22\uff0c\u5bfc\u81f4\u6548\u7387\u4e0d\u9ad8\u3002", "method": "TIDE\u5c06\u65f6\u5e8f\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u8f83\u5c0f\u7684\u3001\u53ef\u7ba1\u7406\u7684reach-avoid\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u90fd\u53ef\u4ee5\u4f7f\u7528\u73b0\u6210\u7684\u89c4\u5212\u5668\u6c42\u89e3\u3002\u5b83\u8bc6\u522b\u5e76\u4f18\u5148\u8003\u8651\u57df\u56fe\u4e2d\u6700\u6709\u5e0c\u671b\u7684\u81ea\u52a8\u673a\u8f68\u8ff9\uff0c\u4f7f\u7528\u6210\u672c\u9a71\u52a8\u542f\u53d1\u5f0f\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u56de\u6eaf\u673a\u5236\u4ece\u5931\u8d25\u7684\u8ba1\u5212\u4e2d\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTIDE\u5b9e\u73b0\u4e86\u6709\u524d\u666f\u7684\u6027\u80fd\uff0c\u662f\u65f6\u5e8f\u6269\u5c55\u76ee\u6807\u89c4\u5212\u65b9\u6cd5\u7ec4\u5408\u4e2d\u7684\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8865\u5145\u3002", "conclusion": "TIDE\u901a\u8fc7\u5206\u89e3\u65f6\u5e8f\u95ee\u9898\u3001\u4f7f\u7528\u542f\u53d1\u5f0f\u6307\u5bfc\u548c\u81ea\u9002\u5e94\u56de\u6eaf\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLTLf\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u542f\u53d1\u5f0f\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u4e3a\u65f6\u5e8f\u6269\u5c55\u76ee\u6807\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11908", "abs": "https://arxiv.org/abs/2601.11908", "authors": ["Byeongjin Kim", "Gyuwan Kim", "Seo Yeon Park"], "title": "PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning", "comment": "23 pages, 6 figures", "summary": "Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.", "AI": {"tldr": "PPA-Plan\u662f\u4e00\u79cd\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e3b\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u903b\u8f91\u9677\u9631\u548c\u9519\u8bef\u5047\u8bbe\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u8d1f\u9762\u7ea6\u675f\uff0c\u5e76\u5728\u89c4\u5212\u751f\u6210\u65f6\u660e\u786e\u907f\u514d\u8fd9\u4e9b\u7ea6\u675f\uff0c\u4ece\u800c\u9884\u9632\u89c4\u5212\u5931\u8d25\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u76f8\u5173\u4fe1\u606f\u7a00\u758f\u5206\u5e03\u3002\u73b0\u6709\u7684\u8ba1\u5212-\u6267\u884c\u6846\u67b6\u56e0\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u5bfc\u81f4\u89c4\u5212\u4e0d\u53ef\u9760\uff0c\u4e00\u65e6\u5f62\u6210\u9519\u8bef\u89c4\u5212\u96be\u4ee5\u8bc6\u522b\u548c\u4fee\u6b63\uff0c\u9650\u5236\u4e86\u53cd\u5e94\u5f0f\u4f18\u5316\u7684\u6548\u679c\u3002", "method": "PPA-Plan\u91c7\u7528\u4e3b\u52a8\u89c4\u5212\u7b56\u7565\uff1a1\uff09\u8bc6\u522b\u6f5c\u5728\u903b\u8f91\u9677\u9631\u548c\u9519\u8bef\u5047\u8bbe\uff1b2\uff09\u5c06\u5176\u8f6c\u5316\u4e3a\u8d1f\u9762\u7ea6\u675f\uff1b3\uff09\u5728\u89c4\u5212\u751f\u6210\u65f6\u660e\u786e\u907f\u514d\u8fd9\u4e9b\u7ea6\u675f\uff0c\u4ece\u800c\u9884\u9632\u89c4\u5212\u5931\u8d25\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6267\u884cPPA-Plan\u751f\u6210\u7684\u89c4\u5212\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u8ba1\u5212-\u6267\u884c\u65b9\u6cd5\u548c\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "PPA-Plan\u901a\u8fc7\u4e3b\u52a8\u8bc6\u522b\u548c\u907f\u514d\u6f5c\u5728\u9519\u8bef\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u89c4\u5212\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u5212-\u6267\u884c\u6846\u67b6\u7684\u6027\u80fd\u3002"}}
{"id": "2601.11729", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11729", "abs": "https://arxiv.org/abs/2601.11729", "authors": ["Turhan Can Kargin", "Wojciech Jasi\u0144ski", "Adam Pardyl", "Bartosz Zieli\u0144ski", "Marcin Przewi\u0119\u017alikowski"], "title": "SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models", "comment": "Project page is available at https://sparrta.gmum.net/", "summary": "Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINO\u3001CLIP\uff09\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5177\u8eab\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5c063D\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\uff09\u878d\u5165VFM\u8bad\u7ec3\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u7a7a\u95f4\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u63a2\u7a76\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u5177\u5907\u7a7a\u95f4\u610f\u8bc6\u8fd8\u662f\u4ec5\u4ec5\u8fc7\u62df\u5408\u7279\u5b9a3D\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u7a7a\u95f4\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\uff08SpaRRTa\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u751f\u6210\u4efb\u610f\u6570\u91cf\u7684\u903c\u771f\u56fe\u50cf\uff0c\u5305\u542b\u591a\u6837\u5316\u573a\u666f\u548c\u5b8c\u5168\u53ef\u63a7\u7684\u5bf9\u8c61\u6392\u5217\uff0c\u5e76\u63d0\u4f9b\u53ef\u81ea\u7531\u8bbf\u95ee\u7684\u7a7a\u95f4\u6807\u6ce8\u3002\u4f7f\u7528SpaRRTa\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540cVFM\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u5206\u6790\u63d0\u4f9b\u4e86\u5bf9\u73b0\u4ee3VFM\u4e2d\u652f\u6301\u6216\u963b\u788d\u7a7a\u95f4\u610f\u8bc6\u673a\u5236\u7684\u89c1\u89e3\u3002", "conclusion": "SpaRRTa\u57fa\u51c6\u6d4b\u8bd5\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u5177\u6709\u7a7a\u95f4\u610f\u8bc6\u7684\u89c6\u89c9\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4e3a\u89e3\u51b3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.12242", "categories": ["cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.12242", "abs": "https://arxiv.org/abs/2601.12242", "authors": ["WooSeok Kim", "Jeonghoon Lee", "Sangho Kim", "Taesun An", "WonMin Lee", "Dowon Kim", "Kyungseop Shin"], "title": "Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning", "comment": null, "summary": "In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56de\u653e\u8bb0\u5fc6\u548con-policy\u7b97\u6cd5\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316NOMA\u7cfb\u7edf\u4e2d\u7684\u7f51\u7edc\u8d44\u6e90\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u8bc4\u4f30\u4e86\u4e0d\u540c\u53c2\u6570\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51(IoT)\u7684\u6269\u5c55\u5bfc\u81f4\u7f51\u7edc\u8d44\u6e90\u7a00\u7f3a\uff0c\u9700\u8981\u4f18\u5316\u7f51\u7edc\u8d44\u6e90\u5229\u7528\u7387\u3002NOMA\u7cfb\u7edf\u901a\u8fc7\u529f\u7387\u590d\u7528\u5141\u8bb8\u591a\u7528\u6237\u540c\u65f6\u63a5\u5165\u7f51\u7edc\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u9650\u5236\uff0c\u7279\u522b\u662f\u4fe1\u9053\u5206\u914d\u95ee\u9898\u5c1a\u672a\u660e\u786e\u89e3\u51b3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u56de\u653e\u8bb0\u5fc6\u548con-policy\u7b97\u6cd5\uff0c\u7528\u4e8eNOMA\u7cfb\u7edf\u4e2d\u7684\u7f51\u7edc\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u6cdb\u5316\u5b66\u4e60\u6548\u679c\u3002\u6846\u67b6\u8003\u8651\u4e86\u5b66\u4e60\u7387\u3001\u6279\u91cf\u5927\u5c0f\u3001\u6a21\u578b\u7c7b\u578b\u548c\u72b6\u6001\u7279\u5f81\u6570\u91cf\u7b49\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u8bc4\u4f30\u4e86\u4e0d\u540c\u53c2\u6570\uff08\u5b66\u4e60\u7387\u3001\u6279\u91cf\u5927\u5c0f\u3001\u6a21\u578b\u7c7b\u578b\u3001\u72b6\u6001\u7279\u5f81\u6570\u91cf\uff09\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4e3aNOMA\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3NOMA\u7cfb\u7edf\u4e2d\u7684\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u4fe1\u9053\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.11913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11913", "abs": "https://arxiv.org/abs/2601.11913", "authors": ["Yichen Jiang", "Peng Ye", "Jiakang Yuan", "Chongjun Tu", "Lei Bai", "Tao Chen"], "title": "LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding", "comment": "12 pages, 5 figures", "summary": "Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.", "AI": {"tldr": "LSTM-MAS\uff1a\u53d7LSTM\u67b6\u6784\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u94fe\u5f0f\u67b6\u6784\u548c\u95e8\u63a7\u673a\u5236\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u548c\u5e7b\u89c9\u4f20\u64ad\u95ee\u9898", "motivation": "\u73b0\u6709LLM\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5355LLM\u65b9\u6cd5\u6709\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d7\u9650\u7684\u95ee\u9898\uff0c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5219\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7d2f\u79ef\u548c\u5e7b\u89c9\u4f20\u64ad\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u63a7\u5236\u4fe1\u606f\u4f20\u9012\u3001\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u6cd5\u3002", "method": "\u53d7LSTM\u67b6\u6784\u542f\u53d1\u8bbe\u8ba1LSTM-MAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u94fe\u5f0f\u67b6\u6784\u7ec4\u7ec7\u667a\u80fd\u4f53\uff1a\u6bcf\u4e2a\u8282\u70b9\u5305\u542b\u5de5\u4f5c\u667a\u80fd\u4f53\uff08\u7247\u6bb5\u7406\u89e3\uff09\u3001\u8fc7\u6ee4\u667a\u80fd\u4f53\uff08\u5197\u4f59\u51cf\u5c11\uff09\u3001\u5224\u65ad\u667a\u80fd\u4f53\uff08\u9519\u8bef\u68c0\u6d4b\uff09\u548c\u7ba1\u7406\u667a\u80fd\u4f53\uff08\u5168\u5c40\u4fe1\u606f\u8c03\u63a7\uff09\uff0c\u5206\u522b\u5bf9\u5e94LSTM\u7684\u8f93\u5165\u95e8\u3001\u9057\u5fd8\u95e8\u3001\u6052\u5b9a\u8bef\u5dee\u5faa\u73af\u5355\u5143\u548c\u8f93\u51fa\u95e8\u529f\u80fd\u3002", "result": "\u76f8\u6bd4\u4e4b\u524d\u6700\u4f73\u591a\u667a\u80fd\u4f53\u65b9\u6cd5CoA\uff0c\u5728NarrativeQA\u3001Qasper\u3001HotpotQA\u548cMuSiQue\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u534740.93%\u300143.70%\u3001121.57%\u548c33.12%\u3002", "conclusion": "LSTM-MAS\u901a\u8fc7\u6a21\u62dfLSTM\u7684\u5206\u5c42\u4fe1\u606f\u6d41\u548c\u95e8\u63a7\u5185\u5b58\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u4fe1\u606f\u4f20\u9012\u548c\u9009\u62e9\u6027\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\uff0c\u6709\u6548\u907f\u514d\u4e86\u9519\u8bef\u7d2f\u79ef\u548c\u5e7b\u89c9\u4f20\u64ad\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11769", "abs": "https://arxiv.org/abs/2601.11769", "authors": ["Cheng Lyu", "Jingyue Zhang", "Ryan Maunu", "Mengwei Li", "Vinny DeGenova", "Yuanli Pei"], "title": "From Pixels to Purchase: Building and Evaluating a Taxonomy-Decoupled Visual Search Engine for Home Goods E-commerce", "comment": null, "summary": "Visual search is critical for e-commerce, especially in style-driven domains where user intent is subjective and open-ended. Existing industrial systems typically couple object detection with taxonomy-based classification and rely on catalog data for evaluation, which is prone to noise that limits robustness and scalability. We propose a taxonomy-decoupled architecture that uses classification-free region proposals and unified embeddings for similarity retrieval, enabling a more flexible and generalizable visual search. To overcome the evaluation bottleneck, we propose an LLM-as-a-Judge framework that assesses nuanced visual similarity and category relevance for query-result pairs in a zero-shot manner, removing dependence on human annotations or noise-prone catalog data. Deployed at scale on a global home goods platform, our system improves retrieval quality and yields a measurable uplift in customer engagement, while our offline evaluation metrics strongly correlate with real-world outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u5206\u7c7b\u7684\u89c6\u89c9\u641c\u7d22\u67b6\u6784\uff0c\u4f7f\u7528\u65e0\u5206\u7c7b\u533a\u57df\u63d0\u6848\u548c\u7edf\u4e00\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\uff0c\u5e76\u5f15\u5165LLM\u4f5c\u4e3a\u8bc4\u4f30\u6846\u67b6\u6765\u514b\u670d\u8bc4\u4f30\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u7cfb\u7edf\u901a\u5e38\u5c06\u76ee\u6807\u68c0\u6d4b\u4e0e\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u5206\u7c7b\u8026\u5408\uff0c\u5e76\u4f9d\u8d56\u76ee\u5f55\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u566a\u58f0\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5728\u98ce\u683c\u9a71\u52a8\u7684\u7535\u5546\u9886\u57df\uff0c\u7528\u6237\u610f\u56fe\u4e3b\u89c2\u4e14\u5f00\u653e\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u901a\u7528\u7684\u89c6\u89c9\u641c\u7d22\u65b9\u6848\u3002", "method": "1. \u63d0\u51fa\u5206\u7c7b\u6cd5\u89e3\u8026\u67b6\u6784\uff1a\u4f7f\u7528\u65e0\u5206\u7c7b\u533a\u57df\u63d0\u6848\u548c\u7edf\u4e00\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\uff1b2. \u5f15\u5165LLM-as-a-Judge\u6846\u67b6\uff1a\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8bc4\u4f30\u67e5\u8be2-\u7ed3\u679c\u5bf9\u7684\u7ec6\u5fae\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u7c7b\u522b\u76f8\u5173\u6027\uff0c\u6446\u8131\u5bf9\u4eba\u5de5\u6807\u6ce8\u6216\u566a\u58f0\u76ee\u5f55\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5168\u7403\u5bb6\u5c45\u7528\u54c1\u5e73\u53f0\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u68c0\u7d22\u8d28\u91cf\uff0c\u5e26\u6765\u4e86\u53ef\u8861\u91cf\u7684\u5ba2\u6237\u53c2\u4e0e\u5ea6\u63d0\u5347\uff0c\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\u4e0e\u5b9e\u9645\u4e1a\u52a1\u7ed3\u679c\u5f3a\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u89e3\u8026\u67b6\u6784\u548cLLM\u8bc4\u4f30\u6846\u67b6\u4e3a\u7535\u5546\u89c6\u89c9\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u4e3b\u89c2\u5f00\u653e\u7684\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2601.12256", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12256", "abs": "https://arxiv.org/abs/2601.12256", "authors": ["Jinyoung Park", "Minseong Bae", "Jeehye Na", "Hyunwoo J. Kim"], "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration", "comment": null, "summary": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.", "AI": {"tldr": "CoLLaMo\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5b50\u52a9\u624b\uff0c\u901a\u8fc7\u591a\u7ea7\u5206\u5b50\u6a21\u6001\u534f\u4f5c\u6295\u5f71\u5668\u548c\u5173\u7cfb\u611f\u77e5\u7684\u6a21\u6001\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6574\u54081D\u5e8f\u5217\u30012D\u5206\u5b50\u56fe\u548c3D\u6784\u8c61\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5927\u5206\u5b50\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u5206\u5b50\u8bed\u8a00\u6a21\u578b\uff08LMLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u548c\u6709\u9650\u7684\u9c81\u68d2\u6027\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u672a\u80fd\u5145\u5206\u6574\u54081D\u5206\u5b50\u5e8f\u5217\u30012D\u5206\u5b50\u56fe\u548c3D\u6784\u8c61\u7b49\u591a\u79cd\u5206\u5b50\u6a21\u6001\u4fe1\u606f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u5206\u5b50\u4fe1\u606f\u7684\u6a21\u578b\u6765\u63d0\u5347\u5206\u5b50\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faCoLLaMo\u6a21\u578b\uff0c\u5305\u542b\uff1a1\uff09\u591a\u7ea7\u5206\u5b50\u6a21\u6001\u534f\u4f5c\u6295\u5f71\u5668\uff0c\u901a\u8fc7\u5173\u7cfb\u611f\u77e5\u7684\u6a21\u6001\u534f\u4f5c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u539f\u5b50\u7ea7\u522b\u5b9e\u73b02D\u7ed3\u6784\u548c3D\u7a7a\u95f4\u5173\u7cfb\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u4ea4\u6362\uff1b2\uff09\u63d0\u51fa\u65b0\u7684\u5206\u5b50\u4e2d\u5fc3\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u4e8eGPT\u7684\u6807\u9898\u8d28\u91cf\u8bc4\u4f30\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8etoken\u7684\u901a\u7528\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\uff09\u3002", "result": "CoLLaMo\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5305\u62ec\u5206\u5b50\u63cf\u8ff0\u751f\u6210\u3001\u8ba1\u7b97\u6027\u8d28\u95ee\u7b54\u3001\u63cf\u8ff0\u6027\u8d28\u95ee\u7b54\u3001\u57fa\u5e8f\u8ba1\u6570\u548cIUPAC\u540d\u79f0\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86LMLMs\u7684\u5206\u5b50\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoLLaMo\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u5206\u5b50\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LMLMs\u7684\u5e7b\u89c9\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u5206\u5b50\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u66f4\u597d\u5730\u8861\u91cf\u5206\u5b50\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2601.11920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11920", "abs": "https://arxiv.org/abs/2601.11920", "authors": ["Zhen Xu", "Vedant Khatri", "Yijun Dai", "Xiner Liu", "Siyan Li", "Xuanming Zhang", "Renzhe Yu"], "title": "Enhancing LLM-Based Data Annotation with Error Decomposition", "comment": null, "summary": "Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.", "AI": {"tldr": "\u63d0\u51fa\u8bca\u65ad\u8bc4\u4f30\u8303\u5f0f\uff0c\u5206\u79bb\u4efb\u52a1\u56fa\u6709\u6a21\u7cca\u6027\u4e0e\u6a21\u578b\u9a71\u52a8\u8bef\u5dee\uff0c\u8bc4\u4f30LLM\u5728\u4e3b\u89c2\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "LLM\u5728\u5ba2\u89c2\u6807\u6ce8\u4efb\u52a1\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u5728\u6d89\u53ca\u5fc3\u7406\u5efa\u6784\u7b49\u4e3b\u89c2\u6807\u6ce8\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u4e14\u6613\u51fa\u9519\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c06\u6240\u6709\u8bef\u5dee\u5408\u5e76\u4e3a\u5355\u4e00\u5bf9\u9f50\u6307\u6807\uff0c\u63a9\u76d6\u4e86\u4e0d\u540c\u7c7b\u578b\u8bef\u5dee\u5bf9\u5206\u6790\u7ed3\u8bba\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u8bca\u65ad\u8bc4\u4f30\u8303\u5f0f\uff0c\u5305\u542b\uff1a1) \u4e8c\u7ef4\u5206\u7c7b\u6cd5\uff08\u6765\u6e90\uff1a\u6a21\u578b\u7279\u5b9avs\u4efb\u52a1\u56fa\u6709\uff1b\u7c7b\u578b\uff1a\u8fb9\u754c\u6a21\u7ccavs\u6982\u5ff5\u8bef\u8bc6\u522b\uff09\uff1b2) \u8f7b\u91cf\u7ea7\u4eba\u5de5\u6807\u6ce8\u6d4b\u8bd5\u4f30\u8ba1\u4efb\u52a1\u56fa\u6709\u6a21\u7cca\u6027\uff1b3) \u8ba1\u7b97\u5206\u89e3LLM\u6807\u6ce8\u8bef\u5dee\u7684\u65b9\u6cd5", "result": "\u5728\u56db\u4e2a\u6559\u80b2\u6807\u6ce8\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8303\u5f0f\u7684\u6982\u5ff5\u6709\u6548\u6027\u548c\u5b9e\u9645\u6548\u7528\u3002\u8bc1\u660e\u8fc7\u9ad8\u5bf9\u9f50\u5728\u67d0\u4e9b\u6807\u6ce8\u4efb\u52a1\u4e2d\u4e0d\u73b0\u5b9e\uff0c\u5355\u4e00\u5bf9\u9f50\u6307\u6807\u4e0d\u80fd\u5145\u5206\u53cd\u6620LLM\u6807\u6ce8\u8d28\u91cf", "conclusion": "\u8be5\u8303\u5f0f\u53ef\u4f5c\u4e3a\u4f4e\u6210\u672c\u8bca\u65ad\u5de5\u5177\uff0c\u8bc4\u4f30\u4efb\u52a1\u662f\u5426\u9002\u5408LLM\u6807\u6ce8\uff0c\u5e76\u4e3a\u6280\u672f\u4f18\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8fc7\u9ad8\u5bf9\u9f50\u4e0d\u73b0\u5b9e\uff0c\u5b9e\u8df5\u4e0a\u63d0\u4f9b\u4e86\u6539\u8fdbLLM\u6807\u6ce8\u7684\u65b9\u6cd5"}}
{"id": "2601.11772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11772", "abs": "https://arxiv.org/abs/2601.11772", "authors": ["Yimu Pan", "Hongda Mao", "Qingshuang Chen", "Yelin Kim"], "title": "studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting", "comment": null, "summary": "Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \\textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.", "AI": {"tldr": "studentSplat\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u548c\u63a8\u65ad\u7f51\u7edc\u89e3\u51b3\u5355\u89c6\u56fe\u7684\u5c3a\u5ea6\u6a21\u7cca\u548c\u63a8\u65ad\u95ee\u9898", "motivation": "\u867d\u7136\u524d\u9988\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\u5728\u591a\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u548c\u5355\u89c6\u56fe3D\u7269\u4f53\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u7531\u4e8e\u5355\u89c6\u56fe\u56fa\u6709\u7684\u6a21\u7cca\u6027\u95ee\u9898\u4ecd\u7136\u7814\u7a76\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u548c\u63a8\u65ad\u95ee\u9898\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5355\u89c6\u56fe\u573a\u666f\u91cd\u5efa\u3002", "method": "1) \u6559\u5e08-\u5b66\u751f\u67b6\u6784\uff1a\u4f7f\u7528\u591a\u89c6\u56fe\u6559\u5e08\u6a21\u578b\u5728\u8bad\u7ec3\u671f\u95f4\u4e3a\u5355\u89c6\u56fe\u5b66\u751f\u6a21\u578b\u63d0\u4f9b\u51e0\u4f55\u76d1\u7763\uff0c\u89e3\u51b3\u5c3a\u5ea6\u6a21\u7cca\u5e76\u9f13\u52b1\u51e0\u4f55\u6709\u6548\u6027\uff1b2) \u63a8\u65ad\u7f51\u7edc\uff1a\u5b8c\u6210\u7f3a\u5931\u7684\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u573a\u666f\u63a8\u65ad\u3002", "result": "studentSplat\u5728\u5355\u89c6\u56fe\u65b0\u89c6\u89d2\u91cd\u5efa\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u573a\u666f\u7ea7\u522b\u4e0a\u6027\u80fd\u53ef\u4e0e\u591a\u89c6\u56fe\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u540c\u65f6\u4f5c\u4e3a\u81ea\u76d1\u7763\u5355\u89c6\u56fe\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "studentSplat\u901a\u8fc7\u521b\u65b0\u7684\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u548c\u63a8\u65ad\u7f51\u7edc\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5355\u89c6\u56fe3D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u901a\u7528\u5355\u89c6\u56fe3D\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12259", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12259", "abs": "https://arxiv.org/abs/2601.12259", "authors": ["Jiashuo Liu", "Siyuan Chen", "Zaiyuan Wang", "Zhiyuan Zeng", "Jiacheng Guo", "Liang Hu", "Lingyue Yin", "Suozhi Huang", "Wenxin Hao", "Yang Yang", "Zerui Cheng", "Zixin Yao", "Lingyue Yin", "Haoxin Liu", "Jiayi Cheng", "Yuzhen Li", "Zezhong Ma", "Bingjie Wang", "Bingsen Qiu", "Xiao Liu", "Zeyang Zhang", "Zijian Liu", "Jinpeng Wang", "Mingren Yin", "Tianci He", "Yali Liao", "Yixiao Tian", "Zhenwei Zhu", "Anqi Dai", "Ge Zhang", "Jingkai Liu", "Kaiyuan Zhang", "Wenlong Wu", "Xiang Gao", "Xinjie Chen", "Zhixin Yao", "Zhoufutu Wen", "B. Aditya Prakash", "Jose Blanchet", "Mengdi Wang", "Nian Si", "Wenhao Huang"], "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains", "comment": "21 pages", "summary": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.", "AI": {"tldr": "FutureX-Pro\u6269\u5c55\u4e86FutureX\u7684\u5b9e\u65f6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u548c\u81ea\u7136\u707e\u5bb3\u7b49\u9ad8\u4ef7\u503c\u5782\u76f4\u9886\u57df\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u672a\u6765\u9884\u6d4b\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4ee3\u7406LLM\u5728\u8fd9\u4e9b\u5173\u952e\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u901a\u7528\u4ee3\u7406\u5728\u5f00\u653e\u9886\u57df\u641c\u7d22\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d44\u672c\u5bc6\u96c6\u578b\u548c\u5b89\u5168\u5173\u952e\u884c\u4e1a\u7684\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4ee3\u7406LLM\u662f\u5426\u5177\u5907\u5de5\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u9886\u57df\u57fa\u7840\u3002", "method": "\u57fa\u4e8eFutureX\u7684\u65e0\u6c61\u67d3\u5b9e\u65f6\u8bc4\u4f30\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86FutureX-Pro\u6846\u67b6\uff0c\u5305\u62ec\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u548c\u81ea\u7136\u707e\u5bb3\u56db\u4e2a\u5782\u76f4\u9886\u57df\u3002\u901a\u8fc7\u9884\u6d4b\u5e02\u573a\u6307\u6807\u3001\u4f9b\u5e94\u94fe\u9700\u6c42\u3001\u6d41\u884c\u75c5\u8d8b\u52bf\u548c\u81ea\u7136\u707e\u5bb3\u7b49\u57fa\u7840\u4efb\u52a1\uff0c\u5bf9\u4ee3\u7406LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u63a8\u7406\u4e0e\u9ad8\u4ef7\u503c\u5782\u76f4\u5e94\u7528\u6240\u9700\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4ee3\u7406LLM\u5728\u5de5\u4e1a\u90e8\u7f72\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "FutureX-Pro\u4e3a\u8bc4\u4f30\u4ee3\u7406LLM\u5728\u5173\u952e\u5782\u76f4\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u4e13\u95e8\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5728\u91d1\u878d\u3001\u96f6\u552e\u3001\u516c\u5171\u536b\u751f\u548c\u81ea\u7136\u707e\u5bb3\u7b49\u9886\u57df\u90e8\u7f72AI\u7cfb\u7edf\u65f6\u9700\u8981\u8003\u8651\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2601.11923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11923", "abs": "https://arxiv.org/abs/2601.11923", "authors": ["P. Bilha Githinji", "Aikaterini Melliou", "Xi Yuan", "Dayan Zhang", "Lian Zhang", "Zhenglin Chen", "Jiansong Ji", "Chengying Lv", "Jinhao Xu", "Peiwu Qin", "Dongmei Yu"], "title": "Mapping the maturation of TCM as an adjuvant to radiotherapy", "comment": null, "summary": "The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.", "AI": {"tldr": "\u5bf92000-2025\u5e7469,745\u7bc7\u6587\u732e\u7684\u5927\u89c4\u6a21\u5206\u6790\u663e\u793a\uff0c\u4e2d\u533b\u836f\u4f5c\u4e3a\u653e\u7597\u8f85\u52a9\u624b\u6bb5\u7684\u7814\u7a76\u9886\u57df\u7ecf\u5386\u4e86\u5468\u671f\u6027\u6f14\u53d8\uff0c\u5f62\u6210\u4e86\u4e94\u5927\u4e3b\u9898\u8f74\uff0c\u8868\u660e\u8be5\u9886\u57df\u5df2\u6210\u719f\u5e76\u53ef\u80fd\u9762\u4e34\u65b0\u7684\u7a81\u7834\uff0c\u540c\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u6b63\u5411\u62a5\u544a\u504f\u501a\u3002", "motivation": "\u4e2d\u533b\u836f\u4f5c\u4e3a\u653e\u7597\u8f85\u52a9\u624b\u6bb5\u5728\u80bf\u7624\u7efc\u5408\u6cbb\u7597\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u7ecf\u8fc725\u5e74\u7684\u5236\u5ea6\u5316\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8bc1\u636e\u8f68\u8ff9\uff0c\u4e86\u89e3\u8be5\u9886\u57df\u7684\u6f14\u53d8\u6a21\u5f0f\u548c\u7814\u7a76\u4e3b\u9898\u7ed3\u6784\u3002", "method": "\u5bf92000-2025\u5e7469,745\u7bc7\u51fa\u7248\u7269\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\uff0c\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u5de5\u4f5c\u6d41\u7a0b\u786e\u5b9a\u7a33\u5b9a\u7684\u4e3b\u9898\u7ed3\u6784\uff0c\u8bc6\u522b\u51fa\u7248\u4ea7\u51fa\u3001\u56fd\u9645\u5408\u4f5c\u548c\u8d44\u91d1\u627f\u8bfa\u7684\u5468\u671f\u6027\u6f14\u53d8\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u8be5\u9886\u57df\u5448\u73b0\u5b9a\u4e49-\u6784\u601d-\u6d4b\u8bd5\u6a21\u5f0f\u7684\u5468\u671f\u6027\u6f14\u53d8\uff0c\u8bc6\u522b\u51fa\u4e94\u5927\u4e3b\u9898\u8f74\uff08\u764c\u75c7\u7c7b\u578b\u3001\u652f\u6301\u6027\u62a4\u7406\u3001\u4e34\u5e8a\u7ec8\u70b9\u3001\u673a\u5236\u3001\u65b9\u6cd5\u5b66\uff09\uff0c\u4e2d\u533b\u836f\u6574\u5408\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u4e14\u7cfb\u7edf\u5bfc\u5411\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u6b63\u5411\u62a5\u544a\u504f\u501a\u3002", "conclusion": "\u4e2d\u533b\u836f\u4f5c\u4e3a\u653e\u7597\u8f85\u52a9\u7684\u7814\u7a76\u9886\u57df\u5df2\u6210\u719f\u73b0\u6709\u7814\u7a76\u8bae\u7a0b\uff0c\u53ef\u80fd\u5904\u4e8e\u65b0\u7a81\u7834\u7684\u8fb9\u7f18\uff0c\u540c\u65f6\u9700\u8981\u5173\u6ce8\u7cfb\u7edf\u6027\u6b63\u5411\u62a5\u544a\u504f\u501a\u95ee\u9898\u3002"}}
{"id": "2601.11779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11779", "abs": "https://arxiv.org/abs/2601.11779", "authors": ["Vinicius F. Arruda", "Rodrigo F. Berriel", "Thiago M. Paix\u00e3o", "Claudine Badue", "Alberto F. De Souza", "Nicu Sebe", "Thiago Oliveira-Santos"], "title": "Cross-Domain Object Detection Using Unsupervised Image Translation", "comment": null, "summary": "Unsupervised domain adaptation for object detection addresses the adaption of detectors trained in a source domain to work accurately in an unseen target domain. Recently, methods approaching the alignment of the intermediate features proven to be promising, achieving state-of-the-art results. However, these methods are laborious to implement and hard to interpret. Although promising, there is still room for improvements to close the performance gap toward the upper-bound (when training with the target data). In this work, we propose a method to generate an artificial dataset in the target domain to train an object detector. We employed two unsupervised image translators (CycleGAN and an AdaIN-based model) using only annotated data from the source domain and non-annotated data from the target domain. Our key contributions are the proposal of a less complex yet more effective method that also has an improved interpretability. Results on real-world scenarios for autonomous driving show significant improvements, outperforming state-of-the-art methods in most cases, further closing the gap toward the upper-bound.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528CycleGAN\u548cAdaIN\u8fdb\u884c\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u63a5\u8fd1\u4e0a\u754c\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u4e2d\u95f4\u7279\u5f81\u5bf9\u9f50\u53d6\u5f97\u4e86\u4e0d\u9519\u6548\u679c\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u3001\u96be\u4ee5\u89e3\u91ca\uff0c\u4e14\u4e0e\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u8bad\u7ec3\u7684\u4e0a\u754c\u4ecd\u6709\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u65e0\u76d1\u7763\u56fe\u50cf\u7ffb\u8bd1\u6a21\u578b\uff08CycleGAN\u548cAdaIN-based\u6a21\u578b\uff09\uff0c\u4ec5\u5229\u7528\u6e90\u57df\u6807\u6ce8\u6570\u636e\u548c\u76ee\u6807\u57df\u975e\u6807\u6ce8\u6570\u636e\uff0c\u751f\u6210\u76ee\u6807\u57df\u7684\u4eba\u5de5\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u771f\u5b9e\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u4e0e\u4e0a\u754c\uff08\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u8bad\u7ec3\uff09\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u7684\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u4eba\u5de5\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.12260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12260", "abs": "https://arxiv.org/abs/2601.12260", "authors": ["Yihao Ding", "Qiang Sun", "Puzhen Wu", "Sirui Li", "Siwen Luo", "Wei Liu"], "title": "Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding", "comment": "Accepted at WWW 2026 Demo Track", "summary": "Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.", "AI": {"tldr": "Docs2Synth\u662f\u4e00\u4e2a\u5408\u6210\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u548c\u9a8c\u8bc1QA\u5bf9\u6765\u8bad\u7ec3\u89c6\u89c9\u68c0\u7d22\u5668\uff0c\u7ed3\u5408MLLM\u8fdb\u884c\u68c0\u7d22-\u751f\u6210\u5faa\u73af\uff0c\u89e3\u51b3\u53d7\u76d1\u7ba1\u9886\u57df\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u548c\u9886\u57df\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u53d7\u76d1\u7ba1\u9886\u57df\uff08\u5982\u91d1\u878d\u3001\u533b\u7597\uff09\u7684\u6587\u6863\u7406\u89e3\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u7f3a\u4e4f\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u6a21\u578b\u9002\u5e94\uff1b2\uff09\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u8ddf\u4e0a\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u66f4\u65b0\u3002\u73b0\u6709MLLM\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u800cVLPM\u9700\u8981\u6602\u8d35\u7684\u6807\u6ce8\u3002", "method": "Docs2Synth\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u81ea\u52a8\u5904\u7406\u539f\u59cb\u6587\u6863\u96c6\uff1b2\uff09\u57fa\u4e8e\u4ee3\u7406\u7cfb\u7edf\u751f\u6210\u548c\u9a8c\u8bc1\u591a\u6837\u5316\u7684QA\u5bf9\uff1b3\uff09\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u89c6\u89c9\u68c0\u7d22\u5668\u63d0\u53d6\u9886\u57df\u76f8\u5173\u8bc1\u636e\u3002\u63a8\u7406\u65f6\u91c7\u7528\u68c0\u7d22\u5668\u4e0eMLLM\u7684\u8fed\u4ee3\u68c0\u7d22-\u751f\u6210\u5faa\u73af\u3002", "result": "\u5728\u591a\u4e2aVRDU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDocs2Synth\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684grounding\u80fd\u529b\u548c\u9886\u57df\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u4e0d\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u3002\u6846\u67b6\u8fd8\u63d0\u4f9b\u4e86\u6613\u4e8e\u4f7f\u7528\u7684Python\u5305\uff0c\u652f\u6301\u5373\u63d2\u5373\u7528\u90e8\u7f72\u3002", "conclusion": "Docs2Synth\u901a\u8fc7\u5408\u6210\u76d1\u7763\u548c\u68c0\u7d22\u5f15\u5bfc\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u76d1\u7ba1\u9886\u57df\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5e7b\u89c9\u548c\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u79c1\u6709\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11932", "abs": "https://arxiv.org/abs/2601.11932", "authors": ["Abdullah Al Monsur", "Nitesh Vamshi Bommisetty", "Gene Louis Kim"], "title": "Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes", "comment": null, "summary": "The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u89e3\u7801\u5668LLMs\u7684\u5355\u5411\u6027\u67b6\u6784\u74f6\u9888\u548cMicro-F1\u8bc4\u5206\u5bf9\u591a\u6570\u7c7b\u7684\u504f\u5411\u6027\uff0c\u63d0\u51fa\u4f7f\u7528\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u548cLoRA\u5fae\u8c03\u6765\u63d0\u5347\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u578b\u7684Macro-F1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4e8b\u4ef6\u68c0\u6d4b\u7814\u7a76\u5b58\u5728\u4e24\u4e2a\u91cd\u8981\u9650\u5236\uff1a1) \u89e3\u7801\u5668LLMs\u7684\u5355\u5411\u67b6\u6784\u4e0d\u9002\u5408\u9700\u8981\u53cc\u5411\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff1b2) \u4f20\u7edf\u4f9d\u8d56Micro-F1\u8bc4\u5206\u4f1a\u504f\u5411\u591a\u6570\u7c7b\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u578b\uff0c\u5e76\u4f7f\u7528Low-Rank Adaptation (LoRA)\u8fdb\u884c\u5fae\u8c03\uff0c\u7279\u522b\u5173\u6ce8Macro-F1\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u516c\u5e73\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u6240\u6709\u4e8b\u4ef6\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u6a21\u578b\u4f18\u4e8e\u6807\u51c6\u89e3\u7801\u5668\u57fa\u7ebf\uff1b2) LoRA\u5fae\u8c03\u663e\u8457\u63d0\u5347Macro-F1\u5206\u6570\uff0c\u7279\u522b\u662f\u5bf9\u89e3\u7801\u5668\u6a21\u578b\uff0c\u8bc1\u660eLoRA\u80fd\u6709\u6548\u589e\u5f3aLLMs\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u4e8b\u4ef6\u68c0\u6d4b\u7814\u7a76\u9700\u8981\u5173\u6ce8\u6a21\u578b\u67b6\u6784\u7684\u53cc\u5411\u6027\u9700\u6c42\u548c\u66f4\u516c\u5e73\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53e5\u5b50\u4e0a\u4e0b\u6587\u589e\u5f3a\u548cLoRA\u5fae\u8c03\u662f\u63d0\u5347LLMs\u5728\u957f\u5c3e\u4e8b\u4ef6\u7c7b\u578b\u4e0a\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.11896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11896", "abs": "https://arxiv.org/abs/2601.11896", "authors": ["Ngoc-Khai Hoang", "Thi-Nhu-Mai Nguyen", "Huy-Hieu Pham"], "title": "Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening", "comment": null, "summary": "Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u975e\u4fb5\u5165\u6027\u4e2d\u98ce\u7b5b\u67e5\u6846\u67b6\uff0c\u6574\u5408\u9762\u90e8\u8868\u60c5\u3001\u8bed\u97f3\u4fe1\u53f7\u548c\u4e0a\u534a\u8eab\u8fd0\u52a8\u6570\u636e\uff0c\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.83%\u51c6\u786e\u7387\u548c96.00% F1\u5206\u6570\u3002", "motivation": "\u65e9\u671f\u8bc6\u522b\u4e2d\u98ce\u75c7\u72b6\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9662\u524d\u73af\u5883\u4e2d\u3002\u9700\u8981\u5feb\u901f\u3001\u975e\u4fb5\u5165\u6027\u7684\u81ea\u52a8\u7b5b\u67e5\u65b9\u6cd5\u6765\u8f85\u52a9F.A.S.T.\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a1) \u9762\u90e8\u52a8\u6001\u4f7f\u7528\u57fa\u4e8e\u5730\u6807\u7684\u7279\u5f81\u548cTransformer\u67b6\u6784\uff1b2) \u8bed\u97f3\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6885\u5c14\u9891\u8c31\u56fe\u5e76\u7528\u97f3\u9891\u9891\u8c31Transformer\u5904\u7406\uff1b3) \u4e0a\u534a\u8eab\u59ff\u6001\u5e8f\u5217\u7528MLP-Mixer\u7f51\u7edc\u5206\u6790\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\uff1b4) \u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728\u5305\u542b222\u4e2a\u89c6\u9891\uff0837\u540d\u53d7\u8bd5\u8005\uff09\u7684\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u8fbe\u523095.83%\u51c6\u786e\u7387\u548c96.00% F1\u5206\u6570\uff0c\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5e73\u8861\u826f\u597d\uff0c\u6d4b\u8bd5\u96c6\u4e2d\u6210\u529f\u68c0\u6d4b\u6240\u6709\u4e2d\u98ce\u75c5\u4f8b\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u5728\u65e9\u671f\u4e2d\u98ce\u7b5b\u67e5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u5927\u3001\u66f4\u5177\u4e34\u5e8a\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u53ef\u9760\u7684\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002"}}
{"id": "2601.12294", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12294", "abs": "https://arxiv.org/abs/2601.12294", "authors": ["Dawei Li", "Yuguang Yao", "Zhen Tan", "Huan Liu", "Ruocheng Guo"], "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "comment": "under review", "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "AI": {"tldr": "ToolPRMBench\uff1a\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebf\u91c7\u6837\u6784\u5efa\u6d4b\u8bd5\u6848\u4f8b\uff0c\u63ed\u793a\u4e13\u4e1aPRM\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u4f18\u52bf", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u6d4b\u8bd5\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9PRM\u5728\u6307\u5bfc\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u641c\u7d22\u548c\u63a2\u7d22\u65b9\u9762\u7684\u6709\u6548\u8bc4\u4f30", "method": "\u57fa\u4e8e\u591a\u4e2a\u4ee3\u8868\u6027\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6784\u5efaToolPRMBench\uff0c\u5c06\u4ee3\u7406\u8f68\u8ff9\u8f6c\u6362\u4e3a\u6b65\u9aa4\u7ea7\u6d4b\u8bd5\u6848\u4f8b\uff0c\u5305\u542b\u4ea4\u4e92\u5386\u53f2\u3001\u6b63\u786e\u52a8\u4f5c\u3001\u5408\u7406\u4f46\u9519\u8bef\u7684\u66ff\u4ee3\u65b9\u6848\u548c\u5de5\u5177\u5143\u6570\u636e\uff1b\u91c7\u7528\u79bb\u7ebf\u91c7\u6837\u9694\u79bb\u5355\u6b65\u9519\u8bef\u548c\u5728\u7ebf\u91c7\u6837\u6355\u83b7\u591a\u6b65\u5931\u8d25\uff1b\u4f7f\u7528\u591aLLM\u9a8c\u8bc1\u7ba1\u9053\u51cf\u5c11\u6807\u7b7e\u566a\u58f0", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aPRM\u6709\u6548\u6027\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u4e13\u95e8\u4e3a\u5de5\u5177\u4f7f\u7528\u8bbe\u8ba1\u7684PRM\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u8868\u660e\u4e13\u4e1aPRM\u5728\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u5177\u6709\u66f4\u5927\u6f5c\u529b", "conclusion": "ToolPRMBench\u4e3a\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684PRM\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e13\u4e1aPRM\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003"}}
{"id": "2601.11956", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11956", "abs": "https://arxiv.org/abs/2601.11956", "authors": ["Yuyin Lu", "Ziran Liang", "Yanghui Rao", "Wenqi Fan", "Fu Lee Wang", "Qing Li"], "title": "Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence", "comment": null, "summary": "Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.", "AI": {"tldr": "DoublyCal\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u6821\u51c6\u539f\u5219\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684LLM\u4e2d\u540c\u65f6\u91cf\u5316\u8bc1\u636e\u4e0d\u786e\u5b9a\u6027\u548c\u63a8\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u91cf\u5316\u68c0\u7d22\u8bc1\u636e\u548cLLM\u63a8\u7406\u4e2d\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86LLM\u7684\u53ef\u4fe1\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u53cc\u91cd\u6821\u51c6\u539f\u5219\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u8bc1\u636e\u53ca\u6821\u51c6\u540e\u7684\u8bc1\u636e\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u6821\u51c6\u8bc1\u636e\u6307\u5bfc\u9ed1\u76d2LLM\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDoublyCal\u663e\u8457\u63d0\u9ad8\u4e86\u9ed1\u76d2LLM\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684token\u6210\u672c\u3002", "conclusion": "DoublyCal\u901a\u8fc7\u53cc\u91cd\u6821\u51c6\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u53ef\u4fe1\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.11898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11898", "abs": "https://arxiv.org/abs/2601.11898", "authors": ["Yilmaz Korkmaz", "Vishal M. Patel"], "title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection", "comment": null, "summary": "Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \\href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\\underline{here}}.", "AI": {"tldr": "RemoteVAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b(VARs)\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u878d\u5408\u53cc\u65f6\u76f8\u7279\u5f81\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b(VARs)\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u53ef\u63a7\u6027\u5f31\u3001\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u548c\u66dd\u5149\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5c06VARs\u5e94\u7528\u4e8e\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51faRemoteVAR\u6846\u67b6\uff1a1)\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u81ea\u56de\u5f52\u9884\u6d4b\u6761\u4ef6\u5316\u5230\u591a\u5206\u8fa8\u7387\u878d\u5408\u7684\u53cc\u65f6\u76f8\u7279\u5f81\u4e0a\uff1b2)\u4e13\u95e8\u4e3a\u53d8\u5316\u56fe\u9884\u6d4b\u8bbe\u8ba1\u4e86\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u6807\u51c6\u53d8\u5316\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRemoteVAR\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u548c\u57fa\u4e8etransformer\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u5efa\u7acb\u4e86\u6709\u7ade\u4e89\u529b\u7684\u81ea\u56de\u5f52\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RemoteVAR\u6210\u529f\u89e3\u51b3\u4e86VARs\u5728\u50cf\u7d20\u7ea7\u5224\u522b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2601.12310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12310", "abs": "https://arxiv.org/abs/2601.12310", "authors": ["Jennifer Dodgson", "Alfath Daryl Alhajir", "Michael Joedhitya", "Akira Rafhael Janson Pattirane", "Surender Suresh Kumar", "Joseph Lim", "C. H. Peh", "Adith Ramdas", "Steven Zhang Zhexu"], "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection", "comment": null, "summary": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u5b58\u6d3b\u6027\u800c\u975e\u5956\u52b1\u7684\u81ea\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u884c\u4e3a\u5728\u771f\u5b9e\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6301\u4e45\u6027\u548c\u4fdd\u6301\u672a\u6765\u4ea4\u4e92\u53ef\u80fd\u6027\u6765\u5b9e\u73b0\u7a33\u5b9a\u5b66\u4e60\uff0c\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u548c\u8bed\u4e49\u6f02\u79fb\u3002", "motivation": "\u4f20\u7edf\u81ea\u8bad\u7ec3\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u5224\u65ad\u6570\u636e\u8d28\u91cf\u7684\u5916\u90e8\u6807\u51c6\u800c\u5bb9\u6613\u9000\u5316\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u5728\u7a00\u758f\u5916\u90e8\u53cd\u9988\u548c\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u81ea\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u73af\u5883\u5b58\u6d3b\u6027\u800c\u975e\u5956\u52b1\u7684\u81ea\u8bad\u7ec3\u67b6\u6784\uff1a\u5728\u771f\u5b9e\u8d44\u6e90\u7ea6\u675f\u4e0b\u6267\u884c\u5019\u9009\u884c\u4e3a\uff0c\u53ea\u4f20\u64ad\u90a3\u4e9b\u73af\u5883\u6548\u5e94\u6301\u4e45\u4e14\u80fd\u4fdd\u6301\u672a\u6765\u4ea4\u4e92\u53ef\u80fd\u6027\u7684\u884c\u4e3a\u3002\u73af\u5883\u4e0d\u63d0\u4f9b\u8bed\u4e49\u53cd\u9988\u3001\u5bc6\u96c6\u5956\u52b1\u6216\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\uff0c\u9009\u62e9\u4ec5\u901a\u8fc7\u884c\u4e3a\u4f5c\u4e3a\u4e16\u754c\u6539\u53d8\u4e8b\u4ef6\u7684\u5dee\u5f02\u5b58\u6d3b\u6765\u5b9e\u73b0\u3002", "result": "\u5206\u6790\u663e\u793a\u6539\u8fdb\u4e3b\u8981\u901a\u8fc7\u6709\u6548\u53ef\u91cd\u590d\u7b56\u7565\u5728\u6574\u5408\u548c\u4fee\u526a\u673a\u5236\u4e0b\u7684\u6301\u4e45\u6027\u5b9e\u73b0\uff08\u8d1f\u7a7a\u95f4\u5b66\u4e60NSL\uff09\u3002\u6a21\u578b\u53d1\u5c55\u51fa\u5143\u5b66\u4e60\u7b56\u7565\uff08\u5982\u6545\u610f\u5b9e\u9a8c\u5931\u8d25\u4ee5\u83b7\u53d6\u4fe1\u606f\u6027\u9519\u8bef\u6d88\u606f\uff09\u800c\u65e0\u9700\u660e\u786e\u6307\u5bfc\u3002", "conclusion": "\u73af\u5883\u57fa\u7840\u9009\u62e9\u80fd\u591f\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u5f00\u653e\u5f0f\u81ea\u6211\u6539\u8fdb\uff0c\u4e3a\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u7c7b\u7b56\u5212\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u5851\u9020\u3002"}}
{"id": "2601.11957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11957", "abs": "https://arxiv.org/abs/2601.11957", "authors": ["Bingxuan Li", "Jeonghwan Kim", "Cheng Qian", "Xiusi Chen", "Eitan Anzenberg", "Niran Kundapur", "Heng Ji"], "title": "PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning", "comment": null, "summary": "Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.", "AI": {"tldr": "\u63d0\u51faCalConflictBench\u57fa\u51c6\u6d4b\u8bd5\u65e5\u5386\u51b2\u7a81\u89e3\u51b3\uff0c\u5e76\u5f00\u53d1PEARL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u8bed\u8a00\u4ee3\u7406\u6027\u80fd", "motivation": "\u91cd\u53e0\u7684\u65e5\u5386\u9080\u8bf7\u8feb\u4f7f\u4e13\u4e1a\u4eba\u58eb\u4e0d\u65ad\u51b3\u5b9a\u53c2\u52a0\u3001\u91cd\u65b0\u5b89\u6392\u6216\u62d2\u7edd\u54ea\u4e9b\u4f1a\u8bae\uff0c\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u7814\u7a76\u8bed\u8a00\u4ee3\u7406\u80fd\u5426\u6709\u6548\u7ba1\u7406\u65f6\u95f4", "method": "\u5f15\u5165CalConflictBench\u57fa\u51c6\u6d4b\u8bd5\u957f\u671f\u65e5\u5386\u51b2\u7a81\u89e3\u51b3\uff0c\u5e76\u63d0\u51faPEARL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u8bb0\u5fc6\u6a21\u5757\u548c\u4f18\u5316\u7684\u56de\u5408\u5956\u52b1\u8bbe\u8ba1\u589e\u5f3a\u8bed\u8a00\u4ee3\u7406", "result": "\u5f53\u524dLLM\u4ee3\u7406\u8868\u73b0\u4e0d\u4f73\uff08\u5982Qwen-3-30B-Think\u5e73\u5747\u9519\u8bef\u738735%\uff09\uff0cPEARL\u5b9e\u73b00.76\u9519\u8bef\u964d\u4f4e\u7387\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5e73\u5747\u9519\u8bef\u7387\u63d0\u534755%", "conclusion": "PEARL\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u4ee3\u7406\u5728\u65e5\u5386\u51b2\u7a81\u89e3\u51b3\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u63a8\u65ad\u548c\u9002\u5e94\u7528\u6237\u504f\u597d\u5b9e\u73b0\u66f4\u597d\u7684\u65f6\u95f4\u7ba1\u7406"}}
{"id": "2601.11907", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11907", "abs": "https://arxiv.org/abs/2601.11907", "authors": ["Prosenjit Chatterjee", "ANK Zaman"], "title": "Towards Airborne Object Detection: A Deep Learning Analysis", "comment": null, "summary": "The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.", "AI": {"tldr": "\u57fa\u4e8eEfficientNetB4\u7684\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u8fdb\u884c\u7a7a\u4e2d\u76ee\u6807\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\uff0c\u5728AODTA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096%\u548c90%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u5546\u7528\u98de\u673a\u3001\u65e0\u4eba\u673a\u7b49\u7a7a\u4e2d\u5e73\u53f0\u7684\u5feb\u901f\u589e\u52a0\uff0c\u9700\u8981\u5b9e\u65f6\u3001\u81ea\u52a8\u5316\u7684\u5a01\u80c1\u8bc4\u4f30\u7cfb\u7edf\u3002\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76d1\u63a7\uff0c\u53ef\u6269\u5c55\u6027\u6709\u9650\u4e14\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528EfficientNetB4\u6784\u5efa\u53cc\u4efb\u52a1\u6a21\u578b\uff0c\u540c\u65f6\u6267\u884c\u7a7a\u4e2d\u76ee\u6807\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\u3002\u4e3a\u4e86\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u548c\u7cbe\u70bc\u591a\u4e2a\u516c\u5f00\u6570\u636e\u6e90\u6784\u5efa\u4e86AODTA\u6570\u636e\u96c6\u3002", "result": "EfficientNetB4\u6a21\u578b\u5728\u76ee\u6807\u5206\u7c7b\u4e0a\u8fbe\u523096%\u51c6\u786e\u7387\uff0c\u5728\u5a01\u80c1\u7b49\u7ea7\u9884\u6d4b\u4e0a\u8fbe\u523090%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eResNet-50\u57fa\u7ebf\u6a21\u578b\u3002\u5728AVD\u6570\u636e\u96c6\u548c\u65b0\u5f00\u53d1\u7684AODTA\u6570\u636e\u96c6\u4e0a\u90fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u76d1\u89c6\u3001\u9632\u5fa1\u548c\u7a7a\u57df\u7ba1\u7406\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\u3002\u867d\u7136\u6807\u9898\u63d0\u5230\u68c0\u6d4b\uff0c\u4f46\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u4f7f\u7528\u73b0\u6709\u6570\u636e\u96c6\u63d0\u4f9b\u7684\u9884\u5b9a\u4f4d\u7a7a\u4e2d\u76ee\u6807\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u548c\u5a01\u80c1\u7b49\u7ea7\u63a8\u65ad\u3002"}}
{"id": "2601.12318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12318", "abs": "https://arxiv.org/abs/2601.12318", "authors": ["Dehao Ying", "Fengchang Yu", "Haihua Chen", "Changjiang Jiang", "Yurong Li", "Wei Lu"], "title": "Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence", "comment": null, "summary": "The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the \"availability of data and labels.\" This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u9996\u6b21\u4e3a\u6587\u6863\u667a\u80fd(\u6587\u6863\u667a\u80fd)\u9886\u57df\u7684\u6570\u636e\u751f\u6210\u5efa\u7acb\u4e86\u5168\u9762\u7684\u6280\u672f\u56fe\u8c31\uff0c\u5c06\u6570\u636e\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u76d1\u7763\u4fe1\u53f7\u751f\u4ea7\uff0c\u5e76\u57fa\u4e8e\"\u6570\u636e\u548c\u6807\u7b7e\u7684\u53ef\u7528\u6027\"\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u56db\u79cd\u8d44\u6e90\u4e2d\u5fc3\u8303\u5f0f\u3002", "motivation": "\u6587\u6863\u667a\u80fd\u7684\u53d1\u5c55\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u624b\u52a8\u6807\u6ce8\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u7efc\u8ff0\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\u6216\u7279\u5b9a\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4e0e\u73b0\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e00\u81f4\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06\u6570\u636e\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u76d1\u7763\u4fe1\u53f7\u751f\u4ea7\uff0c\u57fa\u4e8e\"\u6570\u636e\u548c\u6807\u7b7e\u7684\u53ef\u7528\u6027\"\u5f15\u5165\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5efa\u7acb\u56db\u79cd\u8d44\u6e90\u4e2d\u5fc3\u8303\u5f0f\uff1a\u6570\u636e\u589e\u5f3a\u3001\u4ece\u96f6\u5f00\u59cb\u6570\u636e\u751f\u6210\u3001\u81ea\u52a8\u6570\u636e\u6807\u6ce8\u548c\u81ea\u76d1\u7763\u4fe1\u53f7\u6784\u5efa\u3002\u540c\u65f6\u5efa\u7acb\u591a\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u5185\u5728\u8d28\u91cf\u548c\u5916\u5728\u6548\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u6587\u6863\u667a\u80fd\u6570\u636e\u751f\u6210\u9886\u57df\u7684\u9996\u4e2a\u5168\u9762\u6280\u672f\u56fe\u8c31\uff0c\u63ed\u793a\u4e86\u4fdd\u771f\u5ea6\u5dee\u8ddd\u7b49\u5173\u952e\u6311\u6218\u548c\u534f\u540c\u8fdb\u5316\u751f\u6001\u7cfb\u7edf\u7b49\u524d\u6cbf\u65b9\u5411\u3002\u901a\u8fc7\u7edf\u4e00\u7ed3\u6784\u7cfb\u7edf\u5316\u8fd9\u4e00\u788e\u7247\u5316\u9886\u57df\uff0c\u5c55\u793a\u4e86\u6570\u636e\u751f\u6210\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u6587\u6863\u667a\u80fd\u6838\u5fc3\u5f15\u64ce\u7684\u5b9a\u4f4d\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u788e\u7247\u5316\u7684\u6570\u636e\u751f\u6210\u9886\u57df\uff0c\u5c06\u5176\u5b9a\u4f4d\u4e3a\u4e0b\u4e00\u4ee3\u6587\u6863\u667a\u80fd\u7684\u6838\u5fc3\u5f15\u64ce\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\u548c\u65b9\u5411\u3002"}}
{"id": "2601.11969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11969", "abs": "https://arxiv.org/abs/2601.11969", "authors": ["Zecheng Tang", "Baibei Ji", "Ruoxi Sun", "Haitian Wang", "WangJie You", "Zhang Yijun", "Wenpeng Zhu", "Ji Qi", "Juntao Li", "Min Zhang"], "title": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models", "comment": null, "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u7684\u57fa\u51c6MemoryRewardBench\uff0c\u8986\u76d68K-128K token\u768410\u79cd\u8bb0\u5fc6\u7ba1\u7406\u573a\u666f\uff0c\u8bc4\u4f3013\u4e2a\u5148\u8fdb\u6a21\u578b\u53d1\u73b0\u5f00\u6e90\u4e0e\u4e13\u6709\u6a21\u578b\u5dee\u8ddd\u7f29\u5c0f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u91c7\u7528\u8bb0\u5fc6\u4e2d\u5fc3\u673a\u5236\u5206\u6bb5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u7684\u8bb0\u5fc6\u7ba1\u7406\u6210\u4e3a\u6a21\u578b\u5728\u6574\u4e2a\u5e8f\u5217\u4e2d\u4f20\u64ad\u4fe1\u606f\u7684\u5173\u952e\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5229\u7528\u5956\u52b1\u6a21\u578b\u81ea\u52a8\u53ef\u9760\u5730\u8bc4\u4f30\u8bb0\u5fc6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86MemoryRewardBench\u57fa\u51c6\uff0c\u8986\u76d6\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5305\u542b10\u79cd\u4e0d\u540c\u8bb0\u5fc6\u7ba1\u7406\u6a21\u5f0f\u7684\u8bbe\u7f6e\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ece8K\u5230128K token\u3002\u8bc4\u4f30\u4e8613\u4e2a\u6700\u5148\u8fdb\u7684\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f00\u6e90\u6a21\u578b\u4e0e\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u65b0\u4e00\u4ee3\u6a21\u578b\u65e0\u8bba\u53c2\u6570\u6570\u91cf\u5982\u4f55\u90fd\u6301\u7eed\u4f18\u4e8e\u524d\u4ee3\u6a21\u578b\u3002\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30LLM\u8bb0\u5fc6\u7ba1\u7406\u65b9\u9762\u7684\u80fd\u529b\u548c\u57fa\u672c\u9650\u5236\u3002", "conclusion": "MemoryRewardBench\u662f\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u957f\u65f6\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e3a\u7406\u89e3\u5956\u52b1\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.11909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11909", "abs": "https://arxiv.org/abs/2601.11909", "authors": ["Io Yamada", "Hirotsugu Okuno"], "title": "Effects of the retina-inspired light intensity encoding on color discrimination performance", "comment": "8 pages, 14 figures, 4 tables", "summary": "Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center/surround (C/S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C/S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5149\u5f3a\u5ea6\u7f16\u7801\u51fd\u6570\uff08\u5bf9\u6570\u51fd\u6570\u548cNaka-Rushton\u51fd\u6570\uff09\u5bf9\u4e2d\u5fc3/\u5468\u8fb9\u89c6\u7f51\u819c\u6a21\u578b\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0N-R\u51fd\u6570\u7ed3\u5408\u53cc\u5bf9\u7acb\u989c\u8272\u5e73\u9762\u8868\u793a\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u989c\u8272\u8fa8\u522b\u6027\u80fd\u3002", "motivation": "\u989c\u8272\u662f\u7269\u4f53\u8bc6\u522b\u7b49\u89c6\u89c9\u529f\u80fd\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u53d7\u5149\u7167\u989c\u8272\u5f71\u54cd\u5f88\u5927\u3002\u989c\u8272\u6052\u5e38\u6027\uff08CC\uff09\u662f\u89c6\u89c9\u7cfb\u7edf\u7684\u91cd\u8981\u7279\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5149\u5f3a\u5ea6\u7f16\u7801\u51fd\u6570\u5982\u4f55\u5f71\u54cd\u4e2d\u5fc3/\u5468\u8fb9\u89c6\u7f51\u819c\u6a21\u578b\u7684\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u989c\u8272\u53ef\u53d8LED\u4ee5\u4e0d\u540c\u5149\u7167\u989c\u8272\u7167\u5c04\u89c6\u89c9\u76ee\u6807\uff0c\u6bd4\u8f83\u4e24\u79cd\u5149\u5f3a\u5ea6\u7f16\u7801\u51fd\u6570\uff1a\u539f\u59cbC/S\u89c6\u7f51\u819c\u6a21\u578b\u4f7f\u7528\u7684\u5bf9\u6570\u51fd\u6570\u548c\u89c6\u7f51\u819c\u5149\u611f\u53d7\u5668\u54cd\u5e94\u6a21\u578bNaka-Rushton\u51fd\u6570\u3002\u4f7f\u7528HSV\u989c\u8272\u7a7a\u95f4\u548c\u57fa\u4e8e\u7ecf\u5178\u5bf9\u7acb\u989c\u8272\u7406\u8bba\u7684\u989c\u8272\u5e73\u9762\u8868\u793a\u989c\u8272\u4fe1\u606f\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u4e0b\u8fa8\u522b\u76ee\u6807\u989c\u8272\u7684\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cNaka-Rushton\u51fd\u6570\u4e0e\u53cc\u5bf9\u7acb\u989c\u8272\u5e73\u9762\u8868\u793a\u7684\u7ec4\u5408\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u989c\u8272\u8fa8\u522b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u5bf9\u6570\u51fd\u6570\u65b9\u6cd5\u3002", "conclusion": "Naka-Rushton\u51fd\u6570\u4f5c\u4e3a\u5149\u5f3a\u5ea6\u7f16\u7801\u51fd\u6570\uff0c\u7ed3\u5408\u53cc\u5bf9\u7acb\u989c\u8272\u5e73\u9762\u8868\u793a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4e2d\u5fc3/\u5468\u8fb9\u89c6\u7f51\u819c\u6a21\u578b\u7684\u989c\u8272\u6052\u5e38\u6027\u6027\u80fd\uff0c\u8fd9\u4e3a\u6539\u8fdb\u57fa\u4e8e\u989c\u8272\u4fe1\u606f\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.12323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12323", "abs": "https://arxiv.org/abs/2601.12323", "authors": ["Yin Cai", "Zhouhong Gu", "Juntao Zhang", "Ping Chen"], "title": "MARO: Learning Stronger Reasoning from Social Interaction", "comment": null, "summary": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.", "AI": {"tldr": "MARO\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u73af\u5883\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u3001\u89d2\u8272\u5206\u5e03\u4e0d\u5747\u548c\u73af\u5883\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5e76\u8fc1\u79fb\u5230\u5176\u4ed6\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u4ece\u6587\u672c\u5185\u5bb9\u5b66\u4e60\u6216\u89e3\u51b3\u9884\u5b9a\u95ee\u9898\uff0c\u7f3a\u4e4f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4e0e\u4ed6\u4eba\u4e92\u52a8\u3001\u534f\u5546\u548c\u7ade\u4e89\u7684\u7ecf\u9a8c\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a8\u7406\u548c\u5224\u65ad\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5956\u52b1\u4f18\u5316\uff08MARO\uff09\u65b9\u6cd5\uff1a1\uff09\u5c06\u6700\u7ec8\u6210\u8d25\u7ed3\u679c\u5206\u89e3\u4e3a\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u5177\u4f53\u884c\u4e3a\uff0c\u89e3\u51b3\u7a00\u758f\u5b66\u4e60\u4fe1\u53f7\u95ee\u9898\uff1b2\uff09\u5e73\u8861\u4e0d\u540c\u89d2\u8272\u7684\u8bad\u7ec3\u6837\u672c\u6743\u91cd\uff0c\u5904\u7406\u89d2\u8272\u5206\u5e03\u4e0d\u5747\u95ee\u9898\uff1b3\uff09\u76f4\u63a5\u8bc4\u4f30\u6bcf\u4e2a\u884c\u4e3a\u7684\u6548\u7528\uff0c\u89e3\u51b3\u73af\u5883\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMARO\u663e\u8457\u63d0\u5347\u4e86\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u901a\u8fc7\u793e\u4f1a\u6a21\u62df\u5b66\u4e60\u83b7\u5f97\u7684\u80fd\u529b\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u6570\u5b66\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u7b49\u5176\u4ed6\u4efb\u52a1\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u5b66\u4e60\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u901a\u7528\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cMARO\u4e3a\u89e3\u51b3\u590d\u6742\u793e\u4f1a\u4ea4\u4e92\u4e2d\u7684\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.12019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12019", "abs": "https://arxiv.org/abs/2601.12019", "authors": ["Chaowei Zhang", "Xiansheng Luo", "Zewei Zhang", "Yi Zhu", "Jipeng Qiang", "Longwei Wang"], "title": "Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning", "comment": null, "summary": "The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.", "AI": {"tldr": "\u63d0\u51faSORG\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u8fce\u5408\u6027\u751f\u6210\u5bf9\u7acb\u63a8\u7406\u5bf9\uff0c\u7ed3\u5408ORCD\u6a21\u578b\u8fdb\u884c\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u5185\u5bb9\u6cdb\u6ee5\u52a0\u5267\u4e86\u70b9\u51fb\u8bf1\u9975\u95ee\u9898\uff0cLLMs\u867d\u6709\u6f5c\u529b\u4f46\u53d7\u8fce\u5408\u6027\u5f71\u54cd\uff0c\u503e\u5411\u4e8e\u4ea7\u751f\u7b26\u5408\u7528\u6237\u4fe1\u5ff5\u800c\u975e\u771f\u5b9e\u7684\u63a8\u7406\u3002\u672c\u6587\u63d0\u51fa\u5229\u7528\u800c\u975e\u6d88\u9664\u8fd9\u79cd\u8fce\u5408\u6027\u6765\u751f\u6210\u5bf9\u7acb\u89c6\u89d2\u7684\u5bf9\u6bd4\u63a8\u7406\u3002", "method": "\u8bbe\u8ba1SORG\u6846\u67b6\uff0c\u8ba9LLMs\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u4e3a\u65b0\u95fb\u6807\u9898\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u652f\u6301\u4e0e\u53cd\u5bf9\u63a8\u7406\u5bf9\uff1b\u5f00\u53d1ORCD\u6a21\u578b\uff0c\u4f7f\u7528\u4e09\u4e2aBERT\u7f16\u7801\u5668\u5206\u522b\u8868\u793a\u6807\u9898\u53ca\u5176\u63a8\u7406\uff0c\u901a\u8fc7LLM\u751f\u6210\u53ef\u4fe1\u5ea6\u5206\u6570\u6307\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8eLLM\u63d0\u793a\u3001\u5fae\u8c03\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528LLMs\u7684\u8fce\u5408\u6027\u751f\u6210\u5bf9\u7acb\u63a8\u7406\u5bf9\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u70b9\u51fb\u8bf1\u9975\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5904\u7406LLMs\u7684\u8fce\u5408\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11910", "abs": "https://arxiv.org/abs/2601.11910", "authors": ["Guiying Zhu", "Bowen Yang", "Yin Zhuang", "Tong Zhang", "Guanqun Wang", "Zhihao Che", "He Chen", "Lianlin Li"], "title": "A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection", "comment": null, "summary": "Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of \"guess what\". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684GW-VLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u57fa\u4e8e\u5df2\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u901a\u7528\u7269\u4f53\u8ba4\u77e5\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002\u867d\u7136\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5efa\u7acb\u4e86\u591a\u529f\u80fd\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u9700\u8981\u5f62\u6210\u5bf9\u4efb\u4f55\u7269\u4f53\u7684\u901a\u7528\u7406\u89e3\u8303\u5f0f\u3002", "method": "\u63d0\u51faGW-VLM\u65b9\u6cd5\uff1a1\uff09\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u641c\u7d22\uff08MS-VLS\uff09\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u89c6\u89c9\u8bed\u8a00\u8f6f\u5bf9\u9f50\uff0c\u4ece\u7c7b\u522b\u65e0\u5173\u68c0\u6d4b\u7ed3\u679c\u751f\u6210\u7247\u6bb5\uff1b2\uff09\u4e0a\u4e0b\u6587\u6982\u5ff5\u63d0\u793a\uff08CCP\uff09\u5f62\u6210\u6982\u5ff5\u6d41\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u7247\u6bb5\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u3002", "result": "\u5728\u81ea\u7136\u548c\u9065\u611f\u6570\u636e\u96c6\uff08COCO val\u3001Pascal VOC\u3001DIOR\u3001NWPU-10\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGW-VLM\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u6b65\u9aa4\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "GW-VLM\u901a\u8fc7\"\u731c\u731c\u662f\u4ec0\u4e48\"\u7684\u6e38\u620f\u65b9\u5f0f\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f62\u6210\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u7406\u89e3\u8303\u5f0f\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.12338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12338", "abs": "https://arxiv.org/abs/2601.12338", "authors": ["Kartikey Singh Bhandari", "Manav Ganesh", "Yashwant Viswanathan", "Archit Agrawal", "Dhruv Kumar", "Pratik Narang"], "title": "Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations", "comment": null, "summary": "Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5LLM\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u63d0\u53d6\u548c\u4e13\u5bb6\u6df7\u5408\u9002\u914d\u5668\uff0c\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u5efa\u8bae", "motivation": "\u5ba2\u6237\u8bc4\u8bba\u5305\u542b\u4e30\u5bcc\u7684\u670d\u52a1\u5931\u8d25\u548c\u7528\u6237\u671f\u671b\u4fe1\u53f7\uff0c\u4f46\u5c06\u8fd9\u4e9b\u975e\u7ed3\u6784\u5316\u53cd\u9988\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u4e1a\u52a1\u51b3\u7b56\u4ecd\u7136\u56f0\u96be", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u4e24\u9636\u6bb5LLM\u6846\u67b6\uff1a\u95ee\u9898\u6a21\u578b\u63d0\u53d6\u5173\u952e\u95ee\u9898\u5e76\u5206\u914d\u4e3b\u9898\uff0c\u5efa\u8bae\u6a21\u578b\u57fa\u4e8e\u95ee\u9898\u8868\u793a\u751f\u6210\u9488\u5bf9\u6027\u64cd\u4f5c\u5efa\u8bae\uff1b\u91c7\u7528LoRA\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\uff0c\u8bad\u7ec3\u591a\u4e2a\u4f4e\u79e9\u9002\u914d\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\u5728\u63a8\u7406\u65f6\u8fdb\u884ctoken\u7ea7\u4e13\u5bb6\u6df7\u5408", "result": "\u5728\u822a\u7a7a\u548c\u9910\u5385\u4e24\u4e2a\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u5728\u516b\u4e2a\u7ef4\u5ea6\u7684\u64cd\u4f5c\u8bc4\u4f30\u6807\u51c6\uff08\u53ef\u64cd\u4f5c\u6027\u3001\u7279\u5f02\u6027\u3001\u53ef\u884c\u6027\u3001\u9884\u671f\u5f71\u54cd\u3001\u65b0\u9896\u6027\u3001\u975e\u5197\u4f59\u6027\u3001\u504f\u89c1\u548c\u6e05\u6670\u5ea6\uff09\u4e0a\u5747\u4f18\u4e8e\u4ec5\u63d0\u793a\u548c\u5355\u9002\u914d\u5668\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ef\u64cd\u4f5c\u6027\u548c\u7279\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5LLM\u6846\u67b6\u7ed3\u5408LoRA\u4e13\u5bb6\u6df7\u5408\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5c06\u5ba2\u6237\u8bc4\u8bba\u8f6c\u5316\u4e3a\u5177\u4f53\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2601.12033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12033", "abs": "https://arxiv.org/abs/2601.12033", "authors": ["Muhammad Alif Al Hakim", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection", "comment": null, "summary": "Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.", "AI": {"tldr": "\u91cf\u5316\u4f1a\u964d\u4f4eLLM\u7684\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff0c\u52a8\u6001\u91cf\u5316\u6bd4\u9759\u6001\u91cf\u5316\u66f4\u7a33\u5b9a\uff0c\u975e\u82f1\u8bed\u73af\u5883\u4e0b\u5b89\u5168\u6027\u4e0b\u964d\u66f4\u4e25\u91cd\uff0c\u4f5c\u8005\u63d0\u51faCritical Weight Protection\u6280\u672f\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898", "motivation": "\u91cf\u5316\u88ab\u5e7f\u6cdb\u7528\u4e8e\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5176\u5bf9\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u91cf\u5316\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5f71\u54cd\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u7cfb\u7edf\u7814\u7a76\u9759\u6001\u548c\u52a8\u6001\u91cf\u5316\u65b9\u6cd5\u5bf9\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u82f1\u8bed\u3001\u6cd5\u8bed\u3001\u8377\u5170\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u571f\u8033\u5176\u8bed\u7684\u516c\u5e73\u6027\uff0c\u4ee5\u53ca\u82f1\u8bed\u3001\u97e9\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u7684\u5b89\u5168\u6027\u3002\u63d0\u51faCritical Weight Protection\u6280\u672f\uff0c\u8bc6\u522b\u548c\u4fdd\u62a4\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u5173\u952e\u6743\u91cd", "result": "\u91cf\u5316\u4f1a\u6301\u7eed\u964d\u4f4e\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\uff0c\u52a8\u6001\u65b9\u6cd5\u6bd4\u9759\u6001\u65b9\u6cd5\u66f4\u7a33\u5b9a\u3002\u516c\u5e73\u6027\u4e0b\u964d\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u6709\u5dee\u5f02\uff0c\u5b89\u5168\u6027\u4e0b\u964d\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u5c24\u4e3a\u660e\u663e\u3002Critical Weight Protection\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u504f\u89c1\u548c\u5b89\u5168\u6027\u6076\u5316", "conclusion": "\u91cf\u5316\u5bf9LLM\u7684\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u3002\u63d0\u51fa\u7684Critical Weight Protection\u6280\u672f\u80fd\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u8bad\u7ec3\u6216\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u51cf\u8f7b\u8fd9\u4e9b\u98ce\u9669\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u7ef4\u62a4\u53ef\u4fe1\u5ea6"}}
{"id": "2601.11911", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11911", "abs": "https://arxiv.org/abs/2601.11911", "authors": ["Muhammad Ibrahim", "Alfe Suny", "MD Sakib Ul Islam", "Md. Imran Hossain"], "title": "Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh", "comment": null, "summary": "Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.", "AI": {"tldr": "\u7d27\u51d1\u578bCNN\u5728\u5b5f\u52a0\u62c9\u56fd\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u7b80\u5316\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027", "motivation": "\u4f20\u7edfCNN\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u590d\u6742\u67b6\u6784\u5bb9\u6613\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u7b80\u5316CNN\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c", "method": "\u4f7f\u7528\u7d27\u51d1\u578b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u4e94\u4e2a\u516c\u5f00\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u57ce\u5e02\u4fb5\u5360\u3001\u8f66\u8f86\u68c0\u6d4b\u3001\u9053\u8def\u635f\u574f\u548c\u519c\u4f5c\u7269\u5206\u7c7b\u7b49\u573a\u666f", "result": "\u6a21\u578b\u5c55\u73b0\u51fa\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3001\u9ad8\u6548\u6536\u655b\u548c\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002\u5b9a\u91cf\u6307\u6807\u548c\u663e\u8457\u6027\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5224\u522b\u6027\u7279\u5f81\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b", "conclusion": "\u7b80\u5316CNN\u67b6\u6784\u5728\u5c0f\u7c7b\u522b\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u826f\u597d\u9002\u7528\u6027\uff0c\u80fd\u591f\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12392", "abs": "https://arxiv.org/abs/2601.12392", "authors": ["Zhentao Xia", "Yongqi Fan", "Yuxiang Chu", "Yichao Yin", "Liangliang Chen", "Tong Ruan", "Weiyan Zhang"], "title": "Psych\u0113Chat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose Psych\u0113Chat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that Psych\u0113Chat outperforms existing methods for emotional insight and safety control.", "AI": {"tldr": "Psych\u0113Chat\u662f\u4e00\u4e2a\u7528\u4e8e\u5fc3\u7406\u54a8\u8be2\u7684LLM\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6c42\u52a9\u8005\u60c5\u7eea\u53d8\u5316\u548c\u5b89\u5168\u98ce\u9669\u5206\u6790\u6765\u63d0\u5347\u54a8\u8be2\u6548\u679c", "motivation": "\u73b0\u6709\u5fc3\u7406\u54a8\u8be2\u6a21\u578b\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u6c42\u52a9\u8005\u5728\u54a8\u8be2\u8fc7\u7a0b\u4e2d\u7684\u60c5\u7eea\u53d8\u5316\uff0c\u8fd9\u662f\u7ecf\u5178\u5fc3\u7406\u5b66\u6d41\u6d3e\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u3002\u540c\u65f6\uff0c\u5982\u4f55\u4f7f\u54a8\u8be2\u5e08\u6a21\u578b\u7684\u56de\u5e94\u4e0e\u8fd9\u4e9b\u60c5\u7eea\u53d8\u5316\u5bf9\u9f50\uff0c\u5e76\u4e3b\u52a8\u7f13\u89e3\u5b89\u5168\u98ce\u9669\uff0c\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51faPsych\u0113Chat\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u60c5\u7eea\u7ba1\u7406\u6a21\u5757\uff08\u6355\u6349\u6c42\u52a9\u8005\u5f53\u524d\u60c5\u7eea\u548c\u60c5\u7eea\u53d8\u5316\uff09\u548c\u98ce\u9669\u63a7\u5236\u6a21\u5757\uff08\u9884\u6d4b\u6c42\u52a9\u8005\u540e\u7eed\u53cd\u5e94\u5e76\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\uff09\u3002\u91c7\u7528\u4e24\u79cd\u5efa\u6a21\u8303\u5f0f\uff1aAgent\u6a21\u5f0f\uff08\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ba1\u9053\uff09\u548cLLM\u6a21\u5f0f\uff08\u7edf\u4e00\u601d\u7ef4\u94fe\u7aef\u5230\u7aef\u63a8\u7406\uff09\u3002", "result": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8bc4\u5206\u3001\u5bf9\u8bdd\u7ea7\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cPsych\u0113Chat\u5728\u60c5\u611f\u6d1e\u5bdf\u548c\u5b89\u5168\u63a7\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Psych\u0113Chat\u901a\u8fc7\u663e\u5f0f\u96c6\u6210\u60c5\u7eea\u53d8\u5316\u8ffd\u8e2a\u548c\u5b89\u5168\u98ce\u9669\u5206\u6790\uff0c\u4e3a\u5fc3\u7406\u54a8\u8be2\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u60c5\u611f\u6d1e\u5bdf\u548c\u5b89\u5168\u63a7\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.12034", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12034", "abs": "https://arxiv.org/abs/2601.12034", "authors": ["Ziyi Zhao", "Chongming Gao", "Yang Zhang", "Haoyan Liu", "Weinan Gan", "Huifeng Guo", "Yong Liu", "Fuli Feng"], "title": "Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs", "comment": "Accepted to AAAI 2026 (Oral). 9 pages, 5 figures", "summary": "Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.", "AI": {"tldr": "PUMA\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8fc1\u79fb\u4e2a\u6027\u5316\u63d0\u793a\uff0c\u89e3\u51b3LLM\u5347\u7ea7\u65f6\u7528\u6237\u8f6f\u63d0\u793a\u5931\u6548\u95ee\u9898\uff0c\u51cf\u5c1198%\u8ba1\u7b97\u6210\u672c", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316\u901a\u5e38\u4f9d\u8d56\u7528\u6237\u7279\u5b9a\u7684\u8f6f\u63d0\u793a\uff0c\u4f46\u5f53\u57fa\u7840\u6a21\u578b\u5347\u7ea7\u65f6\uff0c\u8fd9\u4e9b\u63d0\u793a\u4f1a\u5931\u6548\uff0c\u9700\u8981\u6602\u8d35\u7684\u5168\u91cf\u91cd\u65b0\u8bad\u7ec3", "method": "\u63d0\u51faPrompt-level User Migration Adapter (PUMA)\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u7684\u9002\u914d\u5668\u6865\u63a5\u8bed\u4e49\u5dee\u8ddd\uff0c\u7ed3\u5408\u57fa\u4e8e\u7ec4\u7684\u7528\u6237\u9009\u62e9\u7b56\u7565\u964d\u4f4e\u8bad\u7ec3\u6210\u672c", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe98%\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u94fe\u5f0f/\u805a\u5408\u8fc1\u79fb\u573a\u666f\u4e2d\u8868\u73b0\u9c81\u68d2", "conclusion": "PUMA\u6846\u67b6\u901a\u8fc7\u5c06\u7528\u6237\u8d44\u4ea7\u4e0e\u5e95\u5c42\u6a21\u578b\u89e3\u8026\uff0c\u4e3a\u4e2a\u6027\u5316AI\u7684\u53ef\u6301\u7eed\u6f14\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84"}}
{"id": "2601.11915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11915", "abs": "https://arxiv.org/abs/2601.11915", "authors": ["Chi Wang", "Xinjue Hu", "Boyu Wang", "Ziwen He", "Zhangjie Fu"], "title": "From Spurious to Causal: Low-rank Orthogonal Subspace Intervention for Generalizable Face Forgery Detection", "comment": null, "summary": "The generalization problem remains a critical challenge in face forgery detection. Some researches have discovered that ``a backdoor path\" in the representations from forgery-irrelevant information to labels induces biased learning, thereby hindering the generalization. In this paper, these forgery-irrelevant information are collectively termed spurious correlations factors. Previous methods predominantly focused on identifying concrete, specific spurious correlation and designing corresponding solutions to address them. However, spurious correlations arise from unobservable confounding factors, making it impractical to identify and address each one individually. To address this, we propose an intervention paradigm for representation space. Instead of tracking and blocking various instance-level spurious correlation one by one, we uniformly model them as a low-rank subspace and intervene in them. Specifically, we decompose spurious correlation features into a low-rank subspace via orthogonal low-rank projection, subsequently removing this subspace from the original representation and training its orthogonal complement to capture forgery-related features. This low-rank projection removal effectively eliminates spurious correlation factors, ensuring that classification decision is based on authentic forgery cues. With only 0.43M trainable parameters, our method achieves state-of-the-art performance across several benchmarks, demonstrating excellent robustness and generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e72\u9884\u6d88\u9664\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u865a\u5047\u76f8\u5173\u6027\u7684\u65b9\u6cd5\uff0c\u4ec5\u97000.43M\u53ef\u8bad\u7ec3\u53c2\u6570\u5c31\u80fd\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u7684\u6cdb\u5316\u95ee\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ece\u4f2a\u9020\u65e0\u5173\u4fe1\u606f\u5230\u6807\u7b7e\u7684\"\u540e\u95e8\u8def\u5f84\"\u4f1a\u5bfc\u81f4\u6709\u504f\u5b66\u4e60\uff0c\u4ece\u800c\u963b\u788d\u6cdb\u5316\u3002\u8fd9\u4e9b\u4f2a\u9020\u65e0\u5173\u4fe1\u606f\u88ab\u7edf\u79f0\u4e3a\u865a\u5047\u76f8\u5173\u6027\u56e0\u7d20\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u5177\u4f53\u7684\u865a\u5047\u76f8\u5173\u6027\u5e76\u8bbe\u8ba1\u76f8\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u865a\u5047\u76f8\u5173\u6027\u6e90\u4e8e\u4e0d\u53ef\u89c2\u6d4b\u7684\u6df7\u6742\u56e0\u7d20\uff0c\u9010\u4e2a\u8bc6\u522b\u548c\u5904\u7406\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u8868\u793a\u7a7a\u95f4\u7684\u5e72\u9884\u8303\u5f0f\uff1a\u5c06\u5404\u79cd\u5b9e\u4f8b\u7ea7\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u5e72\u9884\u3002\u5177\u4f53\u901a\u8fc7\u6b63\u4ea4\u4f4e\u79e9\u6295\u5f71\u5c06\u865a\u5047\u76f8\u5173\u7279\u5f81\u5206\u89e3\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u7136\u540e\u4ece\u539f\u59cb\u8868\u793a\u4e2d\u79fb\u9664\u8be5\u5b50\u7a7a\u95f4\uff0c\u5e76\u8bad\u7ec3\u5176\u6b63\u4ea4\u8865\u96c6\u6765\u6355\u83b7\u4f2a\u9020\u76f8\u5173\u7279\u5f81\u3002\u8fd9\u79cd\u4f4e\u79e9\u6295\u5f71\u79fb\u9664\u6709\u6548\u6d88\u9664\u4e86\u865a\u5047\u76f8\u5173\u6027\u56e0\u7d20\u3002", "result": "\u4ec5\u4f7f\u75280.43M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u865a\u5047\u76f8\u5173\u6027\u7edf\u4e00\u5efa\u6a21\u4e3a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5e76\u8fdb\u884c\u5e72\u9884\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u9010\u4e2a\u5904\u7406\u5177\u4f53\u865a\u5047\u76f8\u5173\u6027\u7684\u4e0d\u5207\u5b9e\u9645\u505a\u6cd5\u3002"}}
{"id": "2601.12410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12410", "abs": "https://arxiv.org/abs/2601.12410", "authors": ["Dingyi Yang", "Junqi Zhao", "Xue Li", "Ce Li", "Boyang Li"], "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation", "comment": "23 pages, 11 figures", "summary": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.", "AI": {"tldr": "LLMs\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u610f\u56fe\u7406\u89e3\u65b9\u9762\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff0c\u8fdc\u4e0d\u5982\u4eba\u7c7b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u66f4\u91cd\u89c6\u8fd9\u4e9b\u8ba4\u77e5\u80fd\u529b", "motivation": "\u8ba4\u77e5\u4eba\u7c7b\u5b66\u8ba4\u4e3a\u4eba\u7c7b\u667a\u80fd\u7684\u5173\u952e\u5728\u4e8e\u63a8\u65ad\u4ed6\u4eba\u77e5\u8bc6\u72b6\u6001\u548c\u7406\u89e3\u610f\u56fe\u7684\u80fd\u529b\uff0c\u800c\u9ed1\u7329\u7329\u7b49\u8fd1\u4eb2\u52a8\u7269\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30LLM\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u4f30\u8ba1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u68c0\u9a8c\u5176\u662f\u5426\u5177\u5907\u8fd9\u79cd\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4efb\u52a1\u6765\u6d4b\u8bd5LLM\uff1a(1) \u68c0\u6d4b\u6545\u4e8b\u89d2\u8272\u662f\u5426\u901a\u8fc7\u884c\u52a8\u5c55\u793a\u4e86\u4ed6\u4eec\u672c\u4e0d\u5e94\u62e5\u6709\u7684\u77e5\u8bc6\uff1b(2) \u57fa\u4e8e\u89d2\u8272\u81ea\u8eab\u77e5\u8bc6\uff08\u800c\u975e\u5ba2\u89c2\u4e8b\u5b9e\uff09\u9884\u6d4b\u89d2\u8272\u7684\u4e0b\u4e00\u6b65\u884c\u52a8\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u63a5\u8fd1\u968f\u673a\u8868\u73b0\uff0c\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002\u8fd9\u8868\u660eLLM\u5728\u77e5\u8bc6\u72b6\u6001\u8ffd\u8e2a\u548c\u610f\u56fe\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "LLM\u5728\u77e5\u8bc6\u4f30\u8ba1\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u672a\u6765LLM\u7814\u7a76\u5e94\u8be5\u66f4\u52a0\u91cd\u89c6\u8fd9\u4e9b\u6838\u5fc3\u8ba4\u77e5\u80fd\u529b\u7684\u5f00\u53d1\uff0c\u8fd9\u662f\u5b9e\u73b0\u771f\u6b63\u667a\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2601.12061", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12061", "abs": "https://arxiv.org/abs/2601.12061", "authors": ["Jinsook Lee", "Kirk Vanacore", "Zhuqian Zhou", "Jeanine Grutter", "Rene F. Kizilcec"], "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation", "comment": "Under Review for ACL 2026", "summary": "Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ee3\u7801\u672c\u6ce8\u5165\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u8fb9\u754c\u51b3\u7b56\u4e0e\u4e0b\u6e38\u6807\u6ce8\u6807\u51c6\u7ed3\u5408\uff0c\u8bc4\u4f30LLM\u5206\u5272\u5668\u4e0e\u6807\u51c6\u53ca\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u7684\u6548\u679c\uff0c\u53d1\u73b0DA\u611f\u77e5\u80fd\u4ea7\u751f\u66f4\u4e00\u81f4\u7684\u7247\u6bb5\uff0c\u4f46\u4e0d\u540c\u5206\u5272\u5668\u5404\u6709\u4f18\u52a3\uff0c\u9700\u9488\u5bf9\u4e0b\u6e38\u76ee\u6807\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u884c\u4e3a\u6807\u6ce8\u901a\u5e38\u5c06\u4ea4\u9645\u6216\u6559\u5b66\u610f\u56fe\u5c40\u9650\u4e8e\u5355\u4e2a\u8bdd\u8bed\u6216\u8f6e\u6b21\uff0c\u5bfc\u81f4\u6807\u6ce8\u8005\u867d\u7136\u5bf9\u5e95\u5c42\u52a8\u4f5c\u8fbe\u6210\u4e00\u81f4\uff0c\u4f46\u5bf9\u7247\u6bb5\u8fb9\u754c\u5b58\u5728\u5206\u6b67\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8868\u9762\u53ef\u9760\u6027\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u597d\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4ee3\u7801\u672c\u6ce8\u5165\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u8fb9\u754c\u51b3\u7b56\u6761\u4ef6\u5316\u4e8e\u4e0b\u6e38\u6807\u6ce8\u6807\u51c6\u3002\u8bc4\u4f30LLM\u5206\u5272\u5668\u4e0e\u6807\u51c6\u57fa\u7ebf\u53ca\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u7684\u6548\u679c\u3002\u5f15\u5165\u65e0\u9ec4\u91d1\u6807\u7b7e\u8bc4\u4f30\u6307\u6807\uff1a\u8de8\u5ea6\u4e00\u81f4\u6027\u3001\u533a\u5206\u5ea6\u548c\u4eba\u673a\u5206\u5e03\u4e00\u81f4\u6027\u3002", "result": "DA\u611f\u77e5\u4ea7\u751f\u7684\u7247\u6bb5\u5185\u90e8\u4e00\u81f4\u6027\u4f18\u4e8e\u7eaf\u6587\u672c\u57fa\u7ebf\u3002LLM\u64c5\u957f\u521b\u5efa\u7ed3\u6784\u4e00\u81f4\u7684\u8de8\u5ea6\uff0c\u4f46\u57fa\u4e8e\u8fde\u8d2f\u6027\u7684\u57fa\u7ebf\u5728\u68c0\u6d4b\u5bf9\u8bdd\u6d41\u5168\u5c40\u53d8\u5316\u65b9\u9762\u66f4\u4f18\u3002\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c\u6ca1\u6709\u5355\u4e00\u5206\u5272\u5668\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002\u7247\u6bb5\u5185\u8fde\u8d2f\u6027\u7684\u63d0\u5347\u5e38\u4ee5\u8fb9\u754c\u533a\u5206\u5ea6\u548c\u4eba\u673a\u5206\u5e03\u4e00\u81f4\u6027\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u5206\u5272\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e94\u9488\u5bf9\u4e0b\u6e38\u76ee\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u8ffd\u6c42\u5355\u4e00\u6027\u80fd\u5206\u6570\u3002\u4e0d\u540c\u5206\u5272\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.11918", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11918", "abs": "https://arxiv.org/abs/2601.11918", "authors": ["Akito Morita", "Hirotsugu Okuno"], "title": "Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions", "comment": "5 pages, 4 figures, 4 tables", "summary": "In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.", "AI": {"tldr": "\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\u4f5c\u4e3aCNN\u9884\u5904\u7406\uff0c\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c0f\u7f51\u7edc\u89c4\u6a21", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684CNN\u9700\u8981\u5c0f\u578b\u67b6\u6784\uff0c\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u9700\u8981\u4ece\u6709\u9650\u6761\u4ef6\u6570\u636e\u4e2d\u9ad8\u6548\u8bc6\u522b\u7279\u5b9a\u76ee\u6807\u3002\u89c6\u89c9\u795e\u7ecf\u7cfb\u7edf(VNS)\u80fd\u4ece\u5c11\u91cf\u89c6\u89c9\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u6a21\u578bGabor\u6ee4\u6ce2\u5668\u4f5c\u4e3aCNN\u9884\u5904\u7406\u7684\u6548\u679c", "method": "\u4f7f\u7528Gabor\u6ee4\u6ce2\u5668\uff08VNS\u7279\u5f81\u63d0\u53d6\u5668\u6a21\u578b\uff09\u4f5c\u4e3aCNN\u7684\u9884\u5904\u7406\u5c42\uff0c\u5728\u4e0d\u540cCNN\u67b6\u6784\u4e0a\u5bf9\u6bd4\u6709\u65e0Gabor\u9884\u5904\u7406\u7684\u6548\u679c\u3002\u521b\u5efa\u5305\u542b\u4e0d\u540c\u76f8\u673a\u4f4d\u7f6e\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5728\u7279\u5b9a\u8ddd\u79bb\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684CNN\u5728\u5176\u4ed6\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b", "result": "Gabor\u6ee4\u6ce2\u5668\u9884\u5904\u7406\u80fd\u63d0\u5347CNN\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u6709\u52a9\u4e8e\u51cf\u5c0fCNN\u7684\u89c4\u6a21", "conclusion": "Gabor\u6ee4\u6ce2\u5668\u4f5c\u4e3aCNN\u9884\u5904\u7406\u80fd\u6709\u6548\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u673a\u5668\u4eba\u89c6\u89c9\u5e94\u7528\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u7f51\u7edc\u89c4\u6a21\u7684\u51cf\u5c0f"}}
{"id": "2601.12444", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.12444", "abs": "https://arxiv.org/abs/2601.12444", "authors": ["Hui Yang", "Jiaoyan Chen", "Uli Sattler"], "title": "Large Language Model for OWL Proofs", "comment": null, "summary": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728OWL\u672c\u4f53\u8bba\u4e2d\u751f\u6210\u8bc1\u660e\u7684\u80fd\u529b\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u903b\u8f91\u590d\u6742\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u975e\u8868\u793a\u683c\u5f0f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6f14\u7ece\uff09\u65b9\u9762\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u5fe0\u5b9e\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u8bc1\u660e\uff08\u89e3\u91ca\u7ed3\u8bba\u4e3a\u4f55\u6210\u7acb\uff09\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728OWL\u672c\u4f53\u8bba\u80cc\u666f\u4e0b\u7684\u8bc1\u660e\u751f\u6210\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u4e09\u4e2a\u987a\u5e8f\u4efb\u52a1\uff1a\u63d0\u53d6\u3001\u7b80\u5316\u548c\u89e3\u91ca\uff0c\u4ee5\u53ca\u4e00\u4e2a\u989d\u5916\u7684\u903b\u8f91\u5b8c\u5907\u6027\u8bc4\u4f30\u4efb\u52a1\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406LLMs\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a(1) \u67d0\u4e9b\u6a21\u578b\u6574\u4f53\u8868\u73b0\u826f\u597d\u4f46\u5728\u590d\u6742\u6848\u4f8b\u4e0a\u4ecd\u6709\u5c40\u9650\uff1b(2) \u903b\u8f91\u590d\u6742\u6027\uff08\u800c\u975e\u8868\u793a\u683c\u5f0f\uff09\u662f\u5f71\u54cdLLM\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\uff1b(3) \u8f93\u5165\u6570\u636e\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\u663e\u8457\u964d\u4f4eLLMs\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u65e2\u663e\u793a\u4e86LLMs\u5728\u4e25\u683c\u903b\u8f91\u89e3\u91ca\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e5f\u63ed\u793a\u4e86\u5728\u590d\u6742\u6216\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u652f\u6301\u5f39\u6027\u63a8\u7406\u7684\u5dee\u8ddd\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.12068", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12068", "abs": "https://arxiv.org/abs/2601.12068", "authors": ["Rowzatul Zannat", "Abdullah Al Shafi", "Abdul Muntakim"], "title": "Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset", "comment": null, "summary": "Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b758\u4e2a\u75c7\u72b6-\u75be\u75c5\u5173\u7cfb\u7684\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u75be\u75c5\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u8fbe\u5230\u4e8698%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u9488\u5bf9\u975e\u82f1\u8bed\u4eba\u7fa4\uff08\u7279\u522b\u662f\u5b5f\u52a0\u62c9\u8bed\u4f7f\u7528\u8005\uff09\u7f3a\u4e4f\u53ef\u9760\u7684\u75be\u75c5\u9884\u6d4b\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u9ad8\u5b5f\u52a0\u62c9\u8bed\u4eba\u7fa4\u7684\u533b\u7597\u4fe1\u606f\u53ef\u53ca\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b85\u79cd\u75be\u75c5\u3001758\u4e2a\u75c7\u72b6-\u75be\u75c5\u5173\u7cfb\u7684\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\uff0c\u5e76\u516c\u5f00\u4e86\u8be5\u6570\u636e\u96c6\u3002\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u75be\u75c5\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u8f6f\u6295\u7968\u548c\u786c\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u7ed3\u5408\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u3002", "result": "\u8f6f\u6295\u7968\u548c\u786c\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u90fd\u8fbe\u5230\u4e8698%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b5f\u52a0\u62c9\u8bed\u75be\u75c5\u9884\u6d4b\u5efa\u7acb\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u4e3a\u672c\u5730\u5316\u5065\u5eb7\u4fe1\u606f\u5b66\u548c\u8bca\u65ad\u5de5\u5177\u7684\u672a\u6765\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5b5f\u52a0\u62c9\u8bed\u793e\u533a\u7684\u533b\u7597\u4fe1\u606f\u516c\u5e73\u83b7\u53d6\u3002"}}
{"id": "2601.11930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11930", "abs": "https://arxiv.org/abs/2601.11930", "authors": ["Xulei Shi", "Maoyu Wang", "Yuning Peng", "Guanbo Wang", "Xin Wang", "Qi Chen", "Pengjie Tao"], "title": "SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM", "comment": null, "summary": "Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at https://anonymous.4open.science/r/SupScene-5B73.", "AI": {"tldr": "SupScene\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eSfM\u56fe\u50cf\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u56fe\u8bad\u7ec3\u7b56\u7565\u548cDiVLAD\u805a\u5408\u5668\u5b66\u4e60\u66f4\u6709\u6548\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u53e0\u56fe\u50cf\u5bf9\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5728SfM\u4e2d\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u5339\u914d\u6027\u800c\u975e\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6279\u91cf\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\uff08\u91cd\u53e0vs.\u975e\u91cd\u53e0\u5bf9\uff09\u672a\u80fd\u6355\u6349\u8fd9\u4e00\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u76d1\u7763\u4fe1\u53f7\u6765\u5b66\u4e60\u9002\u5408SfM\u7684\u5168\u5c40\u63cf\u8ff0\u7b26\u3002", "method": "1) \u91c7\u7528\u57fa\u4e8e\u5b50\u56fe\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u4e0d\u540c\u6743\u91cd\u7684\u51e0\u4f55\u91cd\u53e0\u5173\u7cfb\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u4f7f\u7528\u8f6f\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\uff1b2) \u63d0\u51faDiVLAD\u805a\u5408\u5668\uff0c\u5229\u7528ViT\u6700\u540e\u4e00\u5c42\u7684\u591a\u5934\u6ce8\u610f\u529b\u56fe\uff1b3) \u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u7ed3\u5408\u8bed\u4e49\u663e\u8457\u7ebf\u7d22\u4e0e\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728GL3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eNetVLAD\uff0c\u540c\u65f6\u4ec5\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728\u4e0d\u540c\u805a\u5408\u6280\u672f\u4e0a\u90fd\u5e26\u6765\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SupScene\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u805a\u5408\u5668\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86SfM\u4e2d\u56fe\u50cf\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u4e3a\u5bfb\u627e\u51e0\u4f55\u6027\u8d28\u76f8\u4f3c\u7684\u91cd\u53e0\u56fe\u50cf\u5bf9\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12499", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12499", "abs": "https://arxiv.org/abs/2601.12499", "authors": ["Meiru Zhang", "Zaiqiao Meng", "Nigel Collier"], "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck", "comment": "preprint", "summary": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMulti-Focus Attention Instruction (MFAI)\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u7279\u5b9a\u4f4d\u7f6e\u6765\u7814\u7a76LLMs\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u5b58\u5728\"\u6700\u5f31\u94fe\u63a5\u5b9a\u5f8b\"\uff1a\u591a\u8df3\u63a8\u7406\u6027\u80fd\u53d6\u51b3\u4e8e\u6700\u4e0d\u53ef\u89c1\u8bc1\u636e\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5177\u6709\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\u95ee\u9898\uff0c\u5bfc\u81f4\u5ffd\u7565\u67d0\u4e9b\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\u9700\u8981\u533a\u5206\u8fd9\u79cd\u5931\u8d25\u662f\u7531\u4e8e\u65e0\u6cd5\u5b9a\u4f4d\u8bc1\u636e\uff08\u8bc6\u522b\u5931\u8d25\uff09\u8fd8\u662f\u65e0\u6cd5\u6574\u5408\u8bc1\u636e\uff08\u5408\u6210\u5931\u8d25\uff09\u3002", "method": "\u5f15\u5165Multi-Focus Attention Instruction (MFAI)\u8bed\u4e49\u63a2\u9488\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5bfc\u6ce8\u610f\u529b\u5230\u9009\u5b9a\u4f4d\u7f6e\u6765\u89e3\u8026\u8bc6\u522b\u548c\u5408\u6210\u673a\u5236\u3002\u57285\u4e2aLLMs\u548c\u4e24\u4e2a\u591a\u8df3QA\u4efb\u52a1\uff08MuSiQue\u548cNeoQA\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\"\u6700\u5f31\u94fe\u63a5\u5b9a\u5f8b\"\uff1a\u591a\u8df3\u63a8\u7406\u6027\u80fd\u5d29\u6e83\u5230\u6700\u4e0d\u53ef\u89c1\u8bc1\u636e\u7684\u6027\u80fd\u6c34\u5e73\uff1b\u5931\u8d25\u7531\u7edd\u5bf9\u4f4d\u7f6e\u800c\u975e\u4e8b\u5b9e\u95f4\u7ebf\u6027\u8ddd\u79bb\u51b3\u5b9a\uff08\u6027\u80fd\u5dee\u5f02<3%\uff09\uff1b\u5339\u914d\u7684MFAI\u53ef\u89e3\u51b3\u8bc6\u522b\u74f6\u9888\uff0c\u5728\u4f4e\u53ef\u89c1\u6027\u4f4d\u7f6e\u63d0\u5347\u51c6\u786e\u7387\u8fbe11.5%\uff1b\"\u601d\u8003\"\u6a21\u578b\u80fd\u6709\u6548\u5b9a\u4f4d\u548c\u6574\u5408\u4fe1\u606f\u3002", "conclusion": "LLMs\u7684\u591a\u8df3\u63a8\u7406\u5931\u8d25\u4e3b\u8981\u7531\u4f4d\u7f6e\u504f\u89c1\u5bfc\u81f4\u7684\u8bc6\u522b\u5931\u8d25\u5f15\u8d77\uff0c\u800c\u975e\u5408\u6210\u5931\u8d25\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u53ef\u4ee5\u7f13\u89e3\u8bc6\u522b\u74f6\u9888\uff0c\u800c\u7cfb\u7edf2\u63a8\u7406\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2601.12075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12075", "abs": "https://arxiv.org/abs/2601.12075", "authors": ["Mehrdad Farahani", "Franziska Penzkofer", "Richard Johansson"], "title": "To Copy or Not to Copy: Copying Is Easier to Induce Than Recall", "comment": null, "summary": "Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \\emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\\rightarrow$Recall (suppressing context use) and Recall$\\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u53d6\u4e86\u4e00\u4e2a\"\u4ef2\u88c1\u5411\u91cf\"\u6765\u673a\u5236\u6027\u5730\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u8bbe\u7f6e\u4e2d\u53c2\u6570\u77e5\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d1\u73b0\u8bf1\u5bfc\u590d\u5236\u6bd4\u6062\u590d\u56de\u5fc6\u66f4\u5bb9\u6613\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u8bbe\u7f6e\u4e2d\u5982\u4f55\u6743\u8861\u53c2\u6570\u77e5\u8bc6\uff08\u5b58\u50a8\u5728\u6743\u91cd\u4e2d\uff09\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5728\u63d0\u793a\u4e2d\uff09\uff0c\u9700\u8981\u673a\u5236\u6027\u5730\u7406\u89e3\u8fd9\u79cd\u9009\u62e9\u8fc7\u7a0b\u3002", "method": "\u4ece\u6a21\u578b\u6fc0\u6d3b\u4e2d\u63d0\u53d6\u4ef2\u88c1\u5411\u91cf\uff1a\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u6b8b\u5dee\u6d41\u8d28\u5fc3\u5dee\u5f02\uff0c\u8be5\u6570\u636e\u96c6\u533a\u5206\u4e86\uff081\uff09\u5f15\u53d1\u53c2\u6570\u56de\u5fc6\u7684\u4e0d\u76f8\u5173\u4e0a\u4e0b\u6587\u548c\uff082\uff09\u5f15\u53d1\u590d\u5236\u7684\u76f8\u5173\u4f46\u9519\u8bef\u7684\u4e0a\u4e0b\u6587\u3002\u5c06\u8be5\u5411\u91cf\u4f5c\u4e3a\u52a0\u6027\u5e72\u9884\u6ce8\u5165\u5230\u9009\u5b9a\u5c42\u548c\u6807\u8bb0\u8de8\u5ea6\u4e2d\uff0c\u4ee5\u5728\u4e24\u4e2a\u65b9\u5411\u4e0a\u5f15\u5bfc\u884c\u4e3a\uff1a\u590d\u5236\u2192\u56de\u5fc6\uff08\u6291\u5236\u4e0a\u4e0b\u6587\u4f7f\u7528\uff09\u548c\u56de\u5fc6\u2192\u590d\u5236\uff08\u8bf1\u5bfc\u6a21\u578b\u590d\u5236\u4e0a\u4e0b\u6587\u4e2d\u7684\u4efb\u4f55\u6807\u8bb0\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u67b6\u6784\uff08\u4ec5\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668/\u89e3\u7801\u5668\uff09\u548c\u4e24\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u9002\u5ea6\u7f29\u653e\u65f6\u884c\u4e3a\u8f6c\u53d8\u4e00\u81f4\u3002\u673a\u5236\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u5bf9\u79f0\u6027\uff1a\u8bf1\u5bfc\u590d\u5236\u662f\u4e00\u4e2a\u5bb9\u6613\u7684\"\u518d\u6fc0\u6d3b\"\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5728\u8f93\u5165\u7684\u4e0d\u540c\u4f4d\u7f6e\u89e6\u53d1\uff1b\u800c\u6062\u590d\u56de\u5fc6\u662f\u4e00\u4e2a\u66f4\u8106\u5f31\u7684\"\u6291\u5236\"\u8fc7\u7a0b\uff0c\u4e0e\u5bf9\u8c61\u6807\u8bb0\u5e72\u9884\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u673a\u5236\u6027\u65b9\u6cd5\u6765\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63ed\u793a\u4e86\u8bf1\u5bfc\u590d\u5236\u6bd4\u6291\u5236\u4e0a\u4e0b\u6587\u4f7f\u7528\u66f4\u5bb9\u6613\uff0c\u8fd9\u5bf9\u7406\u89e3\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u8bbe\u7f6e\u4e2d\u7684\u884c\u4e3a\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.11931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11931", "abs": "https://arxiv.org/abs/2601.11931", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Shenao Jiang", "Hangrui Xu", "Zirui Liao", "Luyuan Zhang", "Huaqiu Li", "Peng Jiao", "Haoqian Wang"], "title": "Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition", "comment": null, "summary": "Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.", "AI": {"tldr": "LMGait\uff1a\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u6b65\u6001\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u63d0\u793a\u6765\u6355\u6349\u6b65\u6001\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u8fd0\u52a8\u7279\u5f81", "motivation": "\u73b0\u6709\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u76f4\u63a5\u4ece\u56fe\u50cf\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6c60\u5316\u64cd\u4f5c\u83b7\u5f97\u5e8f\u5217\u7ea7\u8868\u793a\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5bb9\u6613\u5bf9\u9759\u6001\u566a\u58f0\uff08\u5982\u8863\u7269\uff09\u8fc7\u62df\u5408\uff0c\u540c\u65f6\u65e0\u6cd5\u6709\u6548\u6355\u6349\u52a8\u6001\u8fd0\u52a8\u533a\u57df\u3002", "method": "\u63d0\u51faLMGait\u6846\u67b6\uff0c\u5229\u7528\u8bbe\u8ba1\u7684\u6b65\u6001\u76f8\u5173\u8bed\u8a00\u63d0\u793a\u6765\u6355\u6349\u6b65\u6001\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u6b65\u6001\u8bc6\u522b\u3002", "result": "\u4ece\u6458\u8981\u4e2d\u672a\u660e\u786e\u7ed9\u51fa\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u8fc7\u62df\u5408\u95ee\u9898\u5e76\u63d0\u5347\u52a8\u6001\u8fd0\u52a8\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b\u3002", "conclusion": "LMGait\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u548c\u8fd0\u52a8\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u6b65\u6001\u5e8f\u5217\u4e2d\u7684\u5173\u952e\u8fd0\u52a8\u7279\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u9759\u6001\u566a\u58f0\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002"}}
{"id": "2601.12538", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12538", "abs": "https://arxiv.org/abs/2601.12538", "authors": ["Tianxin Wei", "Ting-Wei Li", "Zhining Liu", "Xuying Ning", "Ze Yang", "Jiaru Zou", "Zhichen Zeng", "Ruizhong Qiu", "Xiao Lin", "Dongqi Fu", "Zihao Li", "Mengting Ai", "Duo Zhou", "Wenxuan Bao", "Yunzhe Li", "Gaotang Li", "Cheng Qian", "Yu Wang", "Xiangru Tang", "Yin Xiao", "Liri Fang", "Hui Liu", "Xianfeng Tang", "Yuji Zhang", "Chi Wang", "Jiaxuan You", "Heng Ji", "Hanghang Tong", "Jingrui He"], "title": "Agentic Reasoning for Large Language Models", "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u57fa\u7840\u667a\u80fd\u4f53\u63a8\u7406\uff08\u5355\u667a\u80fd\u4f53\u80fd\u529b\uff09\u3001\u81ea\u6211\u8fdb\u5316\u667a\u80fd\u4f53\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u548c\u8bb0\u5fc6\u4f18\u5316\uff09\u548c\u96c6\u4f53\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff08\u534f\u4f5c\u73af\u5883\uff09\uff0c\u5e76\u533a\u5206\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u8bad\u7ec3\u540e\u63a8\u7406\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u667a\u80fd\u4f53\u63a8\u7406\u901a\u8fc7\u5c06LLM\u91cd\u6784\u4e3a\u80fd\u591f\u89c4\u5212\u3001\u884c\u52a8\u548c\u6301\u7eed\u5b66\u4e60\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u4ee3\u8868\u4e86\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u8be5\u7efc\u8ff0\u4ece\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7ec4\u7ec7\u667a\u80fd\u4f53\u63a8\u7406\uff1a1) \u57fa\u7840\u667a\u80fd\u4f53\u63a8\u7406\uff08\u5355\u667a\u80fd\u4f53\u80fd\u529b\uff09\uff1b2) \u81ea\u6211\u8fdb\u5316\u667a\u80fd\u4f53\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u3001\u8bb0\u5fc6\u548c\u9002\u5e94\u4f18\u5316\uff09\uff1b3) \u96c6\u4f53\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff08\u534f\u4f5c\u8bbe\u7f6e\uff09\u3002\u540c\u65f6\u533a\u5206\u4e0a\u4e0b\u6587\u63a8\u7406\uff08\u7ed3\u6784\u5316\u7f16\u6392\uff09\u548c\u8bad\u7ec3\u540e\u63a8\u7406\uff08\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\uff09\u3002", "result": "\u7efc\u8ff0\u4e86\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u3001\u81ea\u4e3b\u7814\u7a76\u548c\u6570\u5b66\u7b49\u5b9e\u9645\u5e94\u7528\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\u7efc\u5408\u4e3a\u8fde\u63a5\u601d\u7ef4\u4e0e\u884c\u52a8\u7684\u7edf\u4e00\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3a\u667a\u80fd\u4f53\u63a8\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff1a\u4e2a\u6027\u5316\u3001\u957f\u671f\u4ea4\u4e92\u3001\u4e16\u754c\u5efa\u6a21\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u7684\u6cbb\u7406\u95ee\u9898\u3002"}}
{"id": "2601.12078", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12078", "abs": "https://arxiv.org/abs/2601.12078", "authors": ["Linfeng Du", "Ye Yuan", "Zichen Zhao", "Fuyuan Lyu", "Emiliano Penaloza", "Xiuying Chen", "Zipeng Sun", "Jikun Kang", "Laurent Charlin", "Xue Liu", "Haolun Wu"], "title": "Optimizing User Profiles via Contextual Bandits for Retrieval-Augmented LLM Personalization", "comment": null, "summary": "Large Language Models (LLMs) excel at general-purpose tasks, yet adapting their responses to individual users remains challenging. Retrieval augmentation provides a lightweight alternative to fine-tuning by conditioning LLMs on user history records, and existing approaches typically select these records based on semantic relevance. We argue that relevance serves as an unreliable proxy for utility: a record may be semantically similar to a query yet fail to improve generation quality or even degrade it due to redundancy or conflicting information. To bridge this gap, we propose PURPLE, a contextual bandit framework that oPtimizes UseR Profiles for Llm pErsonalization. In contrast to a greedy selection of the most relevant records, PURPLE treats profile construction as a set generation process and utilizes a Plackett-Luce ranking model to capture complex inter-record dependencies. By training with dense feedback provided by the likelihood of the reference response, our method aligns retrieval directly with generation quality. Extensive experiments on nine personalization tasks demonstrate that PURPLE consistently outperforms strong heuristic and retrieval-augmented baselines in both effectiveness and efficiency, establishing a principled and scalable solution for optimizing user profiles.", "AI": {"tldr": "PURPLE\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u7684LLM\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7528\u6237\u6863\u6848\u6784\u5efa\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u8bed\u4e49\u76f8\u5173\u6027", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u8bed\u4e49\u76f8\u5173\u6027\u9009\u62e9\u7528\u6237\u5386\u53f2\u8bb0\u5f55\uff0c\u4f46\u76f8\u5173\u6027\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u4ee3\u8868\u5b9e\u7528\u6027\u2014\u2014\u8bed\u4e49\u76f8\u4f3c\u7684\u8bb0\u5f55\u53ef\u80fd\u56e0\u5197\u4f59\u6216\u51b2\u7a81\u4fe1\u606f\u800c\u65e0\u6cd5\u6539\u5584\u751a\u81f3\u964d\u4f4e\u751f\u6210\u8d28\u91cf", "method": "\u63d0\u51faPURPLE\u6846\u67b6\uff0c\u5c06\u6863\u6848\u6784\u5efa\u89c6\u4e3a\u96c6\u5408\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u7528Plackett-Luce\u6392\u5e8f\u6a21\u578b\u6355\u6349\u8bb0\u5f55\u95f4\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u53c2\u8003\u54cd\u5e94\u4f3c\u7136\u63d0\u4f9b\u7684\u5bc6\u96c6\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u68c0\u7d22\u76f4\u63a5\u4e0e\u751f\u6210\u8d28\u91cf\u5bf9\u9f50", "result": "\u5728\u4e5d\u4e2a\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPURPLE\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u5f3a\u542f\u53d1\u5f0f\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "PURPLE\u4e3a\u4f18\u5316\u7528\u6237\u6863\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u751f\u6210\u8d28\u91cf\u800c\u975e\u4f9d\u8d56\u76f8\u5173\u6027\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4e2a\u6027\u5316\u6027\u80fd"}}
{"id": "2601.11944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11944", "abs": "https://arxiv.org/abs/2601.11944", "authors": ["Lexin Ren", "Jiamiao Lu", "Weichuan Zhang", "Benqing Wu", "Tuo Wang", "Yi Liao", "Jiapan Guo", "Changming Sun", "Liang Guo"], "title": "Deep learning-based neurodevelopmental assessment in preterm infants", "comment": "27 pages, 8 figures", "summary": "Preterm infants (born between 28 and 37 weeks of gestation) face elevated risks of neurodevelopmental delays, making early identification crucial for timely intervention. While deep learning-based volumetric segmentation of brain MRI scans offers a promising avenue for assessing neonatal neurodevelopment, achieving accurate segmentation of white matter (WM) and gray matter (GM) in preterm infants remains challenging due to their comparable signal intensities (isointense appearance) on MRI during early brain development. To address this, we propose a novel segmentation neural network, named Hierarchical Dense Attention Network. Our architecture incorporates a 3D spatial-channel attention mechanism combined with an attention-guided dense upsampling strategy to enhance feature discrimination in low-contrast volumetric data. Quantitative experiments demonstrate that our method achieves superior segmentation performance compared to state-of-the-art baselines, effectively tackling the challenge of isointense tissue differentiation. Furthermore, application of our algorithm confirms that WM and GM volumes in preterm infants are significantly lower than those in term infants, providing additional imaging evidence of the neurodevelopmental delays associated with preterm birth. The code is available at: https://github.com/ICL-SUST/HDAN.", "AI": {"tldr": "\u63d0\u51faHierarchical Dense Attention Network\uff0c\u901a\u8fc73D\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5bc6\u96c6\u4e0a\u91c7\u6837\u7b56\u7565\uff0c\u89e3\u51b3\u65e9\u4ea7\u513f\u8111MRI\u4e2d\u767d\u8d28\u548c\u7070\u8d28\u4fe1\u53f7\u5f3a\u5ea6\u76f8\u4f3c\uff08\u7b49\u5f3a\u5ea6\u5916\u89c2\uff09\u7684\u51c6\u786e\u5206\u5272\u96be\u9898\u3002", "motivation": "\u65e9\u4ea7\u513f\uff0828-37\u5468\uff09\u9762\u4e34\u795e\u7ecf\u53d1\u80b2\u8fdf\u7f13\u7684\u9ad8\u98ce\u9669\uff0c\u9700\u8981\u65e9\u671f\u8bc6\u522b\u4ee5\u4fbf\u53ca\u65f6\u5e72\u9884\u3002\u867d\u7136\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8111MRI\u4f53\u79ef\u5206\u5272\u4e3a\u8bc4\u4f30\u65b0\u751f\u513f\u795e\u7ecf\u53d1\u80b2\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u4f46\u7531\u4e8e\u65e9\u4ea7\u513f\u65e9\u671f\u5927\u8111\u53d1\u80b2\u9636\u6bb5\u767d\u8d28\u548c\u7070\u8d28\u5728MRI\u4e0a\u4fe1\u53f7\u5f3a\u5ea6\u76f8\u4f3c\uff08\u7b49\u5f3a\u5ea6\u5916\u89c2\uff09\uff0c\u5b9e\u73b0\u51c6\u786e\u5206\u5272\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u5206\u5272\u795e\u7ecf\u7f51\u7edcHierarchical Dense Attention Network\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e863D\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5bc6\u96c6\u4e0a\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u4f4e\u5bf9\u6bd4\u5ea6\u4f53\u79ef\u6570\u636e\u4e2d\u7684\u7279\u5f81\u533a\u5206\u80fd\u529b\u3002", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5206\u5272\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7b49\u5f3a\u5ea6\u7ec4\u7ec7\u533a\u5206\u7684\u6311\u6218\u3002\u6b64\u5916\uff0c\u7b97\u6cd5\u5e94\u7528\u8bc1\u5b9e\u65e9\u4ea7\u513f\u7684\u767d\u8d28\u548c\u7070\u8d28\u4f53\u79ef\u663e\u8457\u4f4e\u4e8e\u8db3\u6708\u513f\uff0c\u4e3a\u65e9\u4ea7\u76f8\u5173\u7684\u795e\u7ecf\u53d1\u80b2\u8fdf\u7f13\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u5f71\u50cf\u5b66\u8bc1\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684Hierarchical Dense Attention Network\u80fd\u591f\u6709\u6548\u89e3\u51b3\u65e9\u4ea7\u513f\u8111MRI\u4e2d\u767d\u8d28\u548c\u7070\u8d28\u7b49\u5f3a\u5ea6\u5916\u89c2\u5e26\u6765\u7684\u5206\u5272\u6311\u6218\uff0c\u4e3a\u8bc4\u4f30\u65e9\u4ea7\u513f\u795e\u7ecf\u53d1\u80b2\u63d0\u4f9b\u4e86\u51c6\u786e\u7684\u5206\u5272\u5de5\u5177\uff0c\u5e76\u8bc1\u5b9e\u4e86\u65e9\u4ea7\u513f\u8111\u7ec4\u7ec7\u4f53\u79ef\u51cf\u5c11\u7684\u5f71\u50cf\u5b66\u8bc1\u636e\u3002"}}
{"id": "2601.12539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12539", "abs": "https://arxiv.org/abs/2601.12539", "authors": ["Ali Ezzat Shahroor", "Mohamed Bayan Kmainasi", "Abul Hasnat", "Dimitar Dimitrov", "Giovanni Da San Martino", "Preslav Nakov", "Firoj Alam"], "title": "MemeLens: Multilingual Multitask VLMs for Memes", "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images", "summary": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.", "AI": {"tldr": "MemeLens\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u89e3\u91ca\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u8868\u60c5\u5305\u7406\u89e3\uff0c\u6574\u5408\u4e8638\u4e2a\u516c\u5171\u8868\u60c5\u5305\u6570\u636e\u96c6\uff0c\u5c06\u6807\u7b7e\u6620\u5c04\u5230\u5305\u542b20\u4e2a\u4efb\u52a1\u7684\u5171\u4eab\u5206\u7c7b\u6cd5\u4e2d\u3002", "motivation": "\u73b0\u6709\u8868\u60c5\u5305\u7814\u7a76\u5206\u6563\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u4ec7\u6068\u3001\u538c\u5973\u3001\u5ba3\u4f20\u3001\u60c5\u611f\u3001\u5e7d\u9ed8\uff09\u548c\u8bed\u8a00\u4e2d\uff0c\u9650\u5236\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMemeLens\u6a21\u578b\uff0c\u6574\u540838\u4e2a\u516c\u5171\u8868\u60c5\u5305\u6570\u636e\u96c6\uff0c\u5c06\u6570\u636e\u96c6\u7279\u5b9a\u6807\u7b7e\u8fc7\u6ee4\u5e76\u6620\u5c04\u5230\u5305\u542b20\u4e2a\u4efb\u52a1\u7684\u5171\u4eab\u5206\u7c7b\u6cd5\u4e2d\uff0c\u6db5\u76d6\u5371\u5bb3\u3001\u76ee\u6807\u3001\u6bd4\u55bb/\u8bed\u7528\u610f\u56fe\u548c\u60c5\u611f\u7b49\u65b9\u9762\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u7a33\u5065\u7684\u8868\u60c5\u5305\u7406\u89e3\u9700\u8981\u591a\u6a21\u6001\u8bad\u7ec3\uff1b2\uff09\u4e0d\u540c\u8bed\u4e49\u7c7b\u522b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b3\uff09\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u800c\u975e\u7edf\u4e00\u8bad\u7ec3\u65f6\uff0c\u6a21\u578b\u5bb9\u6613\u8fc7\u5ea6\u4e13\u4e1a\u5316\u3002", "conclusion": "MemeLens\u4e3a\u8868\u60c5\u5305\u7406\u89e3\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u5b9e\u9a8c\u8d44\u6e90\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4f9b\u793e\u533a\u4f7f\u7528\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8868\u60c5\u5305\u7814\u7a76\u7684\u8de8\u9886\u57df\u6cdb\u5316\u3002"}}
{"id": "2601.12099", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12099", "abs": "https://arxiv.org/abs/2601.12099", "authors": ["Leonardo S. Goodall", "Dor Shilton", "Daniel A. Mullins", "Harvey Whitehouse"], "title": "Large language models struggle with ethnographic text annotation", "comment": null, "summary": "Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.", "AI": {"tldr": "LLMs\u5728\u6c11\u65cf\u5fd7\u6587\u672c\u81ea\u52a8\u6807\u6ce8\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6", "motivation": "\u8bc4\u4f30LLMs\u5728\u8de8\u6587\u5316\u7814\u7a76\u4e2d\u81ea\u52a8\u63d0\u53d6\u6c11\u65cf\u5fd7\u6587\u672c\u7ed3\u6784\u5316\u6570\u636e\u7684\u6f5c\u529b\uff0c\u4ee5\u52a0\u901f\u6c11\u65cf\u5fd7\u7814\u7a76", "method": "\u4f7f\u75287\u4e2a\u6700\u5148\u8fdb\u7684LLM\u5bf9567\u4e2a\u6c11\u65cf\u5fd7\u6458\u5f55\u4e2d\u7684121\u4e2a\u4eea\u5f0f\u7279\u5f81\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u4e0e\u4eba\u7c7b\u7f16\u7801\u8005\u53ef\u9760\u6027\u8fdb\u884c\u6bd4\u8f83", "result": "LLM\u6027\u80fd\u6709\u9650\uff0c\u8fdc\u4f4e\u4e8e\u53ef\u9760\u81ea\u52a8\u6807\u6ce8\u6240\u9700\u6c34\u5e73\uff1b\u957f\u6587\u672c\u3001\u5e8f\u6570\u7279\u5f81\u548c\u6a21\u7cca\u6982\u5ff5\u5c24\u5176\u56f0\u96be\uff1b\u4eba\u7c7b\u7f16\u7801\u8005\u53ef\u9760\u6027\u8bbe\u5b9a\u4e86LLM\u51c6\u786e\u6027\u7684\u4e0a\u9650", "conclusion": "LLMs\u76ee\u524d\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u5728\u6c11\u65cf\u5fd7\u6807\u6ce8\u4e2d\u7684\u89d2\u8272\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u7528\u4e8e\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6807\u6ce8"}}
{"id": "2601.11952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11952", "abs": "https://arxiv.org/abs/2601.11952", "authors": ["Haonan An", "Guang Hua", "Wei Du", "Hangcheng Cao", "Yihang Tao", "Guowen Xu", "Susanto Rahardja", "Yuguang Fang"], "title": "Decoder Gradient Shields: A Family of Provable and High-Fidelity Methods Against Gradient-Based Box-Free Watermark Removal", "comment": null, "summary": "Box-free model watermarking has gained significant attention in deep neural network (DNN) intellectual property protection due to its model-agnostic nature and its ability to flexibly manage high-entropy image outputs from generative models. Typically operating in a black-box manner, it employs an encoder-decoder framework for watermark embedding and extraction. While existing research has focused primarily on the encoders for the robustness to resist various attacks, the decoders have been largely overlooked, leading to attacks against the watermark. In this paper, we identify one such attack against the decoder, where query responses are utilized to obtain backpropagated gradients to train a watermark remover. To address this issue, we propose Decoder Gradient Shields (DGSs), a family of defense mechanisms, including DGS at the output (DGS-O), at the input (DGS-I), and in the layers (DGS-L) of the decoder, with a closed-form solution for DGS-O and provable performance for all DGS. Leveraging the joint design of reorienting and rescaling of the gradients from watermark channel gradient leaking queries, the proposed DGSs effectively prevent the watermark remover from achieving training convergence to the desired low-loss value, while preserving image quality of the decoder output. We demonstrate the effectiveness of our proposed DGSs in diverse application scenarios. Our experimental results on deraining and image generation tasks with the state-of-the-art box-free watermarking show that our DGSs achieve a defense success rate of 100% under all settings.", "AI": {"tldr": "\u63d0\u51faDecoder Gradient Shields (DGSs)\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u548c\u7f29\u653e\u89e3\u7801\u5668\u68af\u5ea6\u6765\u9632\u6b62\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u7684\u6c34\u5370\u79fb\u9664\u653b\u51fb\uff0c\u5728\u53bb\u96e8\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230100%\u9632\u5fa1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u800c\u89e3\u7801\u5668\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u67e5\u8be2\u54cd\u5e94\u83b7\u53d6\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u6765\u8bad\u7ec3\u6c34\u5370\u79fb\u9664\u5668\u3002\u9700\u8981\u4fdd\u62a4\u89e3\u7801\u5668\u514d\u53d7\u6b64\u7c7b\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u3002", "method": "\u63d0\u51faDecoder Gradient Shields (DGSs)\u9632\u5fa1\u673a\u5236\u5bb6\u65cf\uff0c\u5305\u62ec\u8f93\u51fa\u5c42DGS-O\u3001\u8f93\u5165\u5c42DGS-I\u548c\u4e2d\u95f4\u5c42DGS-L\u3002\u901a\u8fc7\u91cd\u5b9a\u5411\u548c\u7f29\u653e\u6c34\u5370\u901a\u9053\u7684\u68af\u5ea6\u6cc4\u9732\u67e5\u8be2\u68af\u5ea6\uff0c\u9632\u6b62\u6c34\u5370\u79fb\u9664\u5668\u8fbe\u5230\u4f4e\u635f\u5931\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u5668\u8f93\u51fa\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5728\u53bb\u96e8\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u65e0\u76d2\u6c34\u5370\u6280\u672f\uff0cDGSs\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86100%\u7684\u9632\u5fa1\u6210\u529f\u7387\uff0c\u6709\u6548\u9632\u6b62\u6c34\u5370\u79fb\u9664\u653b\u51fb\u3002", "conclusion": "DGSs\u4e3a\u65e0\u76d2\u6a21\u578b\u6c34\u5370\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u7801\u5668\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u68af\u5ea6\u5c4f\u853d\u6280\u672f\u9632\u6b62\u57fa\u4e8e\u68af\u5ea6\u6cc4\u9732\u7684\u6c34\u5370\u79fb\u9664\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u786e\u4fdd\u6c34\u5370\u5b89\u5168\u6027\u3002"}}
{"id": "2601.12542", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12542", "abs": "https://arxiv.org/abs/2601.12542", "authors": ["Lukas Weidener", "Marko Brki\u0107", "Mihailo Jovanovi\u0107", "Ritvik Singh", "Chiara Baccin", "Emre Ulgac", "Alex Dobrin", "Aakaash Meduri"], "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery", "comment": null, "summary": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.", "AI": {"tldr": "Deep Research\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u51e0\u5206\u949f\u5185\u5b8c\u6210\u4ea4\u4e92\u5f0f\u79d1\u5b66\u7814\u7a76\uff0c\u76f8\u6bd4\u4f20\u7edf\u6279\u5904\u7406\u6a21\u5f0f\uff08\u9700\u8981\u6570\u5c0f\u65f6\uff09\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u5728BixBench\u8ba1\u7b97\u751f\u7269\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u5927\u591a\u662f\u4e13\u6709\u7684\uff0c\u4e14\u91c7\u7528\u6279\u5904\u7406\u6a21\u5f0f\uff0c\u6bcf\u4e2a\u7814\u7a76\u5468\u671f\u9700\u8981\u6570\u5c0f\u65f6\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7814\u7a76\u4eba\u5458\u7684\u5b9e\u65f6\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u7814\u7a76\u7684\u4ea4\u4e92\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\u89c4\u5212\u3001\u6570\u636e\u5206\u6790\u3001\u6587\u732e\u641c\u7d22\u548c\u65b0\u9896\u6027\u68c0\u6d4b\u7b49\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6301\u4e45\u4e16\u754c\u72b6\u6001\u5728\u4e0d\u540c\u7814\u7a76\u8fed\u4ee3\u4e2d\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002\u63d0\u4f9b\u4e24\u79cd\u64cd\u4f5c\u6a21\u5f0f\uff1a\u5e26\u9009\u62e9\u6027\u4eba\u5de5\u68c0\u67e5\u70b9\u7684\u534a\u81ea\u4e3b\u6a21\u5f0f\uff0c\u4ee5\u53ca\u7528\u4e8e\u6269\u5c55\u7814\u7a76\u7684\u5b8c\u5168\u81ea\u4e3b\u6a21\u5f0f\u3002", "result": "\u5728BixBench\u8ba1\u7b97\u751f\u7269\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u5f00\u653e\u56de\u7b54\u51c6\u786e\u738748.8%\uff0c\u591a\u9879\u9009\u62e9\u9898\u51c6\u786e\u738764.5%\uff0c\u6bd4\u73b0\u6709\u57fa\u7ebf\u9ad8\u51fa14-26\u4e2a\u767e\u5206\u70b9\u3002\u7cfb\u7edf\u80fd\u591f\u5728\u51e0\u5206\u949f\u5185\u5b8c\u6210\u7814\u7a76\u5468\u671f\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u79d1\u5b66\u8c03\u67e5\u3002", "conclusion": "Deep Research\u5c55\u793a\u4e86AI\u8f85\u52a9\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5b9e\u73b0\u5feb\u901f\u4ea4\u4e92\u5f0f\u7814\u7a76\u3002\u540c\u65f6\u6307\u51fa\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u6311\u6218\uff0c\u5305\u62ec\u5f00\u653e\u83b7\u53d6\u6587\u732e\u9650\u5236\u548c\u81ea\u52a8\u5316\u65b0\u9896\u6027\u8bc4\u4f30\u7684\u56fa\u6709\u56f0\u96be\u3002"}}
{"id": "2601.12104", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12104", "abs": "https://arxiv.org/abs/2601.12104", "authors": ["David Ili\u0107", "David Stanojevi\u0107", "Kostadin Cvejoski"], "title": "Powerful Training-Free Membership Inference Against Autoregressive Language Models", "comment": "9 pages, 2 figures; appendix with additional experiments and derivations", "summary": "Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.", "AI": {"tldr": "EZ-MIA\u662f\u4e00\u79cd\u65b0\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u9519\u8bef\u4f4d\u7f6e\u7684\u6982\u7387\u504f\u79fb\u6765\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6210\u5458\u4fe1\u606f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u68c0\u6d4b\u7387\u3002", "motivation": "\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u53ef\u80fd\u8bb0\u5fc6\u5e76\u6cc4\u9732\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u3002\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u68c0\u6d4b\u7387\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u9690\u79c1\u5ba1\u8ba1\u6240\u9700\u7684\u4f4e\u8bef\u62a5\u7387\u9608\u503c\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faEZ-MIA\u653b\u51fb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5173\u952e\u89c2\u5bdf\uff1a\u8bb0\u5fc6\u6548\u5e94\u5728\u9519\u8bef\u4f4d\u7f6e\uff08\u6a21\u578b\u9884\u6d4b\u9519\u8bef\u4f46\u4ecd\u5bf9\u8bad\u7ec3\u6837\u672c\u663e\u793a\u8f83\u9ad8\u6982\u7387\u7684\u4f4d\u7f6e\uff09\u8868\u73b0\u6700\u5f3a\u3002\u5f15\u5165\u9519\u8bef\u533a\u57df\u5206\u6570\uff0c\u6d4b\u91cf\u76f8\u5bf9\u4e8e\u9884\u8bad\u7ec3\u53c2\u8003\u6a21\u578b\u5728\u9519\u8bef\u4f4d\u7f6e\u7684\u6982\u7387\u504f\u79fb\u65b9\u5411\u6027\u4e0d\u5e73\u8861\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u6bcf\u4e2a\u67e5\u8be2\u4e24\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728WikiText\u6570\u636e\u96c6\u548cGPT-2\u4e0a\uff0cEZ-MIA\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u68c0\u6d4b\u7387\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u9ad83.8\u500d\uff08\u57281%\u8bef\u62a5\u7387\u4e0b\u771f\u9633\u6027\u738766.3% vs 17.5%\uff09\uff0cAUC\u63a5\u8fd1\u5b8c\u7f8e\uff080.98\uff09\u3002\u5728\u5173\u952e\u76840.1%\u8bef\u62a5\u7387\u9608\u503c\u4e0b\uff0c\u68c0\u6d4b\u7387\u6bd4\u5148\u524d\u5de5\u4f5c\u9ad88\u500d\uff0814.0% vs 1.8%\uff09\u3002\u5728\u66f4\u5927\u67b6\u6784\uff08AG News\u548cLlama-2-7B\uff09\u4e0a\u4e5f\u5b9e\u73b0\u4e863\u500d\u63d0\u5347\uff0846.7% vs 15.8%\uff09\u3002", "conclusion": "\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u98ce\u9669\u6bd4\u5148\u524d\u7406\u89e3\u7684\u8981\u5927\u5f97\u591a\uff0c\u8fd9\u5bf9\u9690\u79c1\u5ba1\u8ba1\u548c\u90e8\u7f72\u51b3\u7b56\u90fd\u6709\u91cd\u8981\u5f71\u54cd\u3002EZ-MIA\u4e3a\u8bc4\u4f30\u8fd9\u4e9b\u98ce\u9669\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11970", "abs": "https://arxiv.org/abs/2601.11970", "authors": ["S. M. Khalid Bin Zahid", "Md. Rakibul Hasan Nishat", "Abdul Hasib", "Md. Rakibul Hasan", "Md. Ashiqussalehin", "Md. Sahadat Hossen Sajib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms", "comment": null, "summary": "Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65\\% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace's CNN for emotion classification. Experimental results demonstrate the system's efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88\\% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u6811\u8393\u6d3e5\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\u5c06\u7269\u4f53\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u60c5\u7eea\u5206\u6790\u96c6\u6210\u5230\u7edf\u4e00\u6d41\u7a0b\u4e2d\uff0c\u76f8\u6bd4\u6301\u7eed\u5904\u7406\u51cf\u5c1165%\u8ba1\u7b97\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u901a\u5e38\u72ec\u7acb\u5904\u7406\u611f\u77e5\u4efb\u52a1\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u3001\u60c5\u7eea\u5206\u6790\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u3001\u81ea\u9002\u5e94\u7684\u8fd0\u884c\u65f6\u8c03\u5ea6\u5668\u6765\u6839\u636e\u4e0a\u4e0b\u6587\u89e6\u53d1\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6574\u4f53\u7406\u89e3\u548c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u89c9\u6846\u67b6\uff0c\u96c6\u6210\u4e86YOLOv8n\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u3001\u57fa\u4e8eFaceNet\u7684\u81ea\u5b9a\u4e49\u5d4c\u5165\u7cfb\u7edf\u7528\u4e8e\u4eba\u8138\u8bc6\u522b\u3001DeepFace\u7684CNN\u7528\u4e8e\u60c5\u7eea\u5206\u7c7b\uff0c\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff0c\u6839\u636e\u4e0a\u4e0b\u6587\u89e6\u53d1\u9009\u62e9\u6027\u6fc0\u6d3b\u6a21\u5757\u3002", "result": "\u7269\u4f53\u68c0\u6d4b\u6a21\u5757\u5e73\u5747\u7cbe\u5ea6(AP)\u8fbe\u52300.861\uff0c\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u738788%\uff0c\u60c5\u7eea\u68c0\u6d4b\u5bf9\u7279\u5b9a\u60c5\u7eea\u7684AUC\u9ad8\u8fbe0.97\uff0c\u7cfb\u7edf\u4ee55.6\u5e27/\u79d2\u8fd0\u884c\uff0c\u76f8\u6bd4\u6301\u7eed\u5904\u7406\u51cf\u5c1165%\u8ba1\u7b97\u8d1f\u8f7d\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u662f\u5b9e\u73b0\u590d\u6742\u591a\u6a21\u6001AI\u5728\u6210\u672c\u6548\u76ca\u8fb9\u7f18\u786c\u4ef6\u4e0a\u8fd0\u884c\u7684\u5173\u952e\uff0c\u4f7f\u667a\u80fd\u611f\u77e5\u66f4\u52a0\u53ef\u8bbf\u95ee\u4e14\u4fdd\u62a4\u9690\u79c1\u3002"}}
{"id": "2601.12547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12547", "abs": "https://arxiv.org/abs/2601.12547", "authors": ["Dipayan Sengupta", "Saumya Panda"], "title": "How Clinicians Think and What AI Can Learn From It", "comment": "34 pages", "summary": "Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\\to$ perception $\\to$ inference $\\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($\u03b5$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u4e34\u5e8aAI\u5e94\u8f6c\u5411\u5e8f\u6570\u51b3\u7b56\u800c\u975e\u9884\u6d4b\u5f15\u64ce\uff0c\u63d0\u51fa\u57fa\u4e8e\u7a33\u5065\u5e8f\u6570\u89c4\u5219\uff08\u5982\u5feb\u901f\u8282\u4fed\u6811\uff09\u7684\u4e34\u5e8aAI\u84dd\u56fe", "motivation": "\u5f53\u524d\u4e34\u5e8aAI\u7cfb\u7edf\u4e3b\u8981\u4f5c\u4e3a\u9884\u6d4b\u5f15\u64ce\uff08\u751f\u6210\u6807\u7b7e\u6216\u98ce\u9669\u8bc4\u5206\uff09\uff0c\u4f46\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u662f\u65f6\u95f4\u53d7\u9650\u3001\u5e8f\u5217\u5316\u7684\u63a7\u5236\u95ee\u9898\u3002\u4e34\u5e8a\u533b\u751f\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ea4\u66ff\u8fdb\u884c\u4fe1\u606f\u6536\u96c6\u548c\u4e0d\u53ef\u9006\u884c\u52a8\uff0c\u53d7\u9057\u61be\u3001\u7ea6\u675f\u548c\u60a3\u8005\u4ef7\u503c\u89c2\u6307\u5bfc\u3002\u9700\u8981\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u9645\u63a8\u7406\u65b9\u5f0f\u7684AI\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e34\u5e8aAI\u5e94\u57fa\u4e8e\u5e8f\u6570\u3001\u975e\u8865\u507f\u6027\u51b3\u7b56\u800c\u975e\u57fa\u6570\u4f18\u5316\u3002\u8bba\u8bc1\u5feb\u901f\u8282\u4fed\u7684\u8bcd\u5178\u5f0f\u542f\u53d1\u5f0f\uff08\u5982\u5feb\u901f\u8282\u4fed\u6811\uff09\u5728\u533b\u5b66\u4e2d\u7684\u89c4\u8303\u6027\u5408\u7406\u6027\u3002\u63d0\u51fa\"\u9009\u62e9\u6027\u590d\u6742\u6027\"AI\u84dd\u56fe\uff1a\u4f7f\u7528\u4e30\u5bcc\u6a21\u578b\u8fdb\u884c\u4fe1\u5ff5\u548c\u8f68\u8ff9\u5efa\u6a21\uff0c\u4f46\u901a\u8fc7\u7a33\u5065\u5e8f\u6570\u89c4\u5219\u9009\u62e9\u884c\u52a8\uff1b\u5c06\u542f\u53d1\u5f0f\u89c6\u4e3a\u4f4e\u7ef4\u7279\u4f8b\uff1bAI\u4e3b\u8981\u5728\u51b3\u7b56\u8106\u5f31\u4e14\u4fe1\u606f\u6709\u9884\u671f\u6b63\u5411\u5f71\u54cd\u65f6\u7528\u4e8e\u6253\u7834\u5e73\u5c40\u3002", "result": "1. \u4e34\u5e8a\u6743\u8861\u901a\u5e38\u901a\u8fc7\u4eba\u7c7b\u5224\u65ad\u6784\u5efa\uff0c\u5728\u7edd\u5bf9\u5c3a\u5ea6\u4e0a\u53ef\u6d4b\u91cf\u6027\u5f31\uff0c\u53ea\u6709\u5e8f\u6570\u5173\u7cfb\u5177\u6709\u4e0d\u53d8\u6027\uff1b2. \u504f\u597d\u548c\u4fe1\u53f7\u83b7\u53d6\u5b58\u5728\u7ed3\u6784\u6027\u7c97\u7cd9\uff0c\u5bfc\u81f4\u6301\u7eed\u7684\u4e0d\u786e\u5b9a\u6027\u4e0b\u9650\uff1b3. \u5f53\u7c97\u7cd9\u6027\u8d85\u8fc7\u51b3\u7b56\u8fb9\u754c\u65f6\uff0c\u671f\u671b\u6548\u7528\u4f18\u5316\u53d8\u5f97\u8106\u5f31\uff0c\u800c\u7a33\u5065\u7684\u652f\u914d/\u8fc7\u6ee4\u89c4\u5219\u80fd\u7a33\u5b9a\u51b3\u7b56\u3002", "conclusion": "\u4e34\u5e8aAI\u5e94\u8f6c\u5411\u4e0e\u4e34\u5e8a\u533b\u751f\u5bf9\u9f50\u7684\u5e8f\u6570\u51b3\u7b56\u6846\u67b6\uff0c\u4f7f\u7528\u7a33\u5065\u5e8f\u6570\u89c4\u5219\u800c\u975e\u9884\u6d4b\u5f15\u64ce\uff0c\u5c06AI\u4f5c\u4e3a\"\u9009\u62e9\u6027\u590d\u6742\u6027\"\u5de5\u5177\uff0c\u5728\u51b3\u7b56\u8106\u5f31\u65f6\u63d0\u4f9b\u652f\u6301\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6a21\u62df\u771f\u5b9e\u4e34\u5e8a\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2601.12132", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12132", "abs": "https://arxiv.org/abs/2601.12132", "authors": ["Md Mahmudul Hoque", "Md Mehedi Hassain", "Md Hojaifa Tanvir", "Rahul Nandy"], "title": "Bengali Text Classification: An Evaluation of Large Language Model Approaches", "comment": null, "summary": "Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the \"Sports\" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6587\u7ae0\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0Qwen 2.5 7B Instruct\u4ee572%\u7684\u51c6\u786e\u7387\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u4f53\u80b2\u7c7b\u522b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u5206\u7c7b\u9762\u4e34\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6587\u7ae0\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81eaKaggle\u7684Prothom Alo\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6587\u7ae0\u6570\u636e\u96c6\uff0c\u5728\u76f8\u540c\u5206\u7c7b\u6846\u67b6\u4e0b\u8bc4\u4f30\u4e09\u79cd\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\uff1aLLaMA 3.1 8B Instruct\u3001LLaMA 3.2 3B Instruct\u548cQwen 2.5 7B Instruct\u3002", "result": "Qwen 2.5 7B Instruct\u83b7\u5f97\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u738772%\uff0c\u5728\"\u4f53\u80b2\"\u7c7b\u522b\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff1bLLaMA 3.1\u548cLLaMA 3.2\u5206\u522b\u83b7\u5f9753%\u548c56%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5c3d\u7ba1\u5b5f\u52a0\u62c9\u8bedNLP\u8d44\u6e90\u7a00\u7f3a\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u5206\u7c7b\u4e2d\u4ecd\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u672a\u6765\u7814\u7a76\u5c06\u63a2\u7d22\u66f4\u591a\u6a21\u578b\u3001\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u5e76\u6539\u8fdb\u5fae\u8c03\u65b9\u6cd5\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.11976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11976", "abs": "https://arxiv.org/abs/2601.11976", "authors": ["Zongmin Li", "Yachuan Li", "Lei Kang", "Dimosthenis Karatzas", "Wenkang Ma"], "title": "AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering", "comment": "7 pages, 3 figures", "summary": "Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at https://github.com/Li-yachuan/AVIR.", "AI": {"tldr": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u68c0\u7d22\uff0c\u5148\u8bc4\u5206\u9875\u9762\u76f8\u5173\u6027\uff0c\u518d\u805a\u7c7b\u7b5b\u9009\uff0c\u4ec5\u5c06\u76f8\u5173\u9875\u9762\u8f93\u5165\u51bb\u7ed3\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728MP-DocVQA\u4e0a\u51cf\u5c1170%\u9875\u9762\u9700\u6c42\uff0c\u8fbe\u523084.58% ANLS\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u957f\u6587\u6863\u4e0d\u4ec5\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd8\u4f1a\u964d\u4f4e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u6587\u6863\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c6\u89c9\u6587\u6863\u68c0\u7d22(AVIR)\u6846\u67b6\uff1a1) \u8f7b\u91cf\u7ea7\u68c0\u7d22\u6a21\u578b\u4e3a\u6bcf\u4e2a\u9875\u9762\u8bc4\u5206\u95ee\u9898\u76f8\u5173\u6027\uff1b2) \u6839\u636e\u5206\u6570\u5206\u5e03\u805a\u7c7b\u9875\u9762\u6765\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u5185\u5bb9\uff1b3) \u5bf9\u805a\u7c7b\u9875\u9762\u8fdb\u884cTop-K\u7b5b\u9009\u4fdd\u6301\u4e0a\u4e0b\u6587\u7d27\u51d1\uff1b4) \u5bf9\u77ed\u6587\u6863\u4f7f\u7528\u76f8\u5173\u6027\u6982\u7387\u9608\u503c\u9009\u62e9\u9875\u9762\uff1b5) \u4ec5\u5c06\u9009\u4e2d\u7684\u9875\u9762\u8f93\u5165\u51bb\u7ed3\u7684LVLM\u751f\u6210\u7b54\u6848\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728MP-DocVQA\u6570\u636e\u96c6\u4e0a\uff0cAVIR\u5c06\u95ee\u7b54\u6240\u9700\u7684\u5e73\u5747\u9875\u9762\u6570\u51cf\u5c1170%\uff0c\u540c\u65f6\u8fbe\u523084.58%\u7684ANLS\uff0c\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002\u5728SlideVQA\u548cDUDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "AVIR\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u68c0\u7d22\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u6863\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u6ce8\u610f\u529b\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u4e3a\u591a\u9875\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12560", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.12560", "abs": "https://arxiv.org/abs/2601.12560", "authors": ["Arunkumar V", "Gangadharan G. R.", "Rajkumar Buyya"], "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents", "comment": "28 pages, 4 figures, 5 tables", "summary": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684Agentic AI\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u5927\u8111\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u534f\u4f5c\u516d\u5927\u7ec4\u4ef6\uff0c\u5206\u6790\u4ece\u7ebf\u6027\u63a8\u7406\u5230\u539f\u751f\u63a8\u7406\u6a21\u578b\u7684\u6f14\u8fdb\uff0c\u5e76\u8ba8\u8bba\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\u4ece\u5355\u7eaf\u6587\u672c\u751f\u6210\u8f6c\u5411Agentic AI\uff08\u667a\u80fd\u4f53AI\uff09\uff0c\u7cfb\u7edf\u80fd\u591f\u4f5c\u4e3a\u81ea\u4e3b\u5b9e\u4f53\u611f\u77e5\u3001\u63a8\u7406\u3001\u89c4\u5212\u548c\u884c\u52a8\u3002LLMs\u4e0d\u518d\u53ea\u662f\u88ab\u52a8\u77e5\u8bc6\u5f15\u64ce\uff0c\u800c\u662f\u6210\u4e3a\u7ed3\u5408\u8bb0\u5fc6\u3001\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u53cd\u9988\u7684\u8ba4\u77e5\u63a7\u5236\u5668\u3002\u7136\u800c\uff0c\u4ece\u7b80\u5355\u5355\u5faa\u73af\u667a\u80fd\u4f53\u5230\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u591a\u6837\u5316\u8bbe\u8ba1\u4f7f\u5f97\u8fd9\u4e00\u9886\u57df\u96be\u4ee5\u7cfb\u7edf\u5316\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u5206\u89e3\u4e3a\u516d\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u611f\u77e5\u3001\u5927\u8111\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u5de5\u5177\u4f7f\u7528\u548c\u534f\u4f5c\u3002\u4f7f\u7528\u8fd9\u4e00\u6846\u67b6\u5206\u6790\u4ece\u7ebf\u6027\u63a8\u7406\u8fc7\u7a0b\u5230\u539f\u751f\u63a8\u7406\u65f6\u95f4\u63a8\u7406\u6a21\u578b\u7684\u6f14\u8fdb\uff0c\u4ee5\u53ca\u4ece\u56fa\u5b9aAPI\u8c03\u7528\u5230\u5f00\u653e\u6807\u51c6\uff08\u5982\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8baeMCP\u548c\u539f\u751f\u8ba1\u7b97\u673a\u4f7f\u7528\uff09\u7684\u8f6c\u53d8\u3002\u540c\u65f6\u5206\u7c7b\u667a\u80fd\u4f53\u8fd0\u884c\u73af\u5883\uff0c\u5e76\u56de\u987e\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684Agentic AI\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5bfc\u822a\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u3002\u8be5\u6846\u67b6\u6db5\u76d6\u4e86\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u7ec4\u4ef6\u3001\u63a8\u7406\u6a21\u5f0f\u6f14\u8fdb\u3001\u5de5\u5177\u4f7f\u7528\u6807\u51c6\u3001\u8fd0\u884c\u73af\u5883\u5206\u7c7b\u4ee5\u53ca\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "Agentic AI\u6b63\u5728\u4ece\u7b80\u5355\u7684\u6587\u672c\u751f\u6210\u6a21\u578b\u53d1\u5c55\u4e3a\u80fd\u591f\u81ea\u4e3b\u611f\u77e5\u3001\u63a8\u7406\u3001\u89c4\u5212\u548c\u884c\u52a8\u7684\u590d\u6742\u7cfb\u7edf\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\u4e3a\u7406\u89e3\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5e7b\u89c9\u3001\u65e0\u9650\u5faa\u73af\u3001\u63d0\u793a\u6ce8\u5165\u7b49\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u6307\u660e\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.12154", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12154", "abs": "https://arxiv.org/abs/2601.12154", "authors": ["Teodor-C\u0103lin Ionescu", "Lifeng Han", "Jan Heijdra Suasnabar", "Anne Stiggelbout", "Suzan Verberne"], "title": "Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs", "comment": "under review to CLIN journal", "summary": "This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \\textit{precision} and \\textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management\" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u4e3b\u9898\u5efa\u6a21\u548cLLM\u5206\u6790\u764c\u75c7\u60a3\u8005\u8bbf\u8c08\u6570\u636e\uff0cBERTopic\u7ed3\u5408BioClinicalBERT\u5d4c\u5165\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u6709\u6548\u63d0\u53d6\u60a3\u8005\u53d9\u4e8b\u4e2d\u7684\u5173\u952e\u4e3b\u9898\uff0c\u652f\u6301\u60a3\u8005\u5bfc\u5411\u7684\u533b\u7597\u5b9e\u8df5\u3002", "motivation": "\u4ece\u60a3\u8005\u53d9\u4e8b\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4e3b\u9898\uff0c\u4e3a\u66f4\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u5b9e\u8df5\u63d0\u4f9b\u89c1\u89e3\u3002\u901a\u8fc7\u5206\u6790\u764c\u75c7\u60a3\u8005\u7684\u8bbf\u8c08\u8f6c\u5f55\uff0c\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5148\u8fdb\u6280\u672f\u66f4\u597d\u5730\u7406\u89e3\u548c\u6574\u5408\u60a3\u8005\u58f0\u97f3\u5230\u533b\u7597\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "method": "1. \u4f7f\u7528BERTopic\u548cTop2Vec\u5bf913\u4e2a\u764c\u75c7\u60a3\u8005\u8bbf\u8c08\uff08132,722\u8bcd\uff09\u8fdb\u884c\u4e2a\u4f53\u8bbf\u8c08\u6458\u8981\uff1b2. \u91c7\u7528\u76f8\u4f3c\u9884\u5904\u7406\u3001\u5206\u5757\u548c\u805a\u7c7b\u914d\u7f6e\u786e\u4fdd\u516c\u5e73\u6bd4\u8f83\uff1b3. \u4f7f\u7528GPT-4\u8fdb\u884c\u4e3b\u9898\u6807\u6ce8\uff1b4. \u901a\u8fc7\u5c0f\u89c4\u6a21\u4eba\u5de5\u8bc4\u4f30\uff08\u8fde\u8d2f\u6027\u3001\u6e05\u6670\u5ea6\u3001\u76f8\u5173\u6027\uff09\u6bd4\u8f83\u6a21\u578b\uff1b5. \u9009\u62e9BERTopic\u7ed3\u5408\u4e09\u79cd\u4e34\u5e8a\u5bfc\u5411\u5d4c\u5165\u6a21\u578b\u8fdb\u4e00\u6b65\u5b9e\u9a8c\uff1b6. \u4f7f\u7528\u6700\u4f73\u6a21\u578b\u8bbe\u7f6e\u5206\u6790\u5b8c\u6574\u8bbf\u8c08\u96c6\u3002", "result": "1. BERTopic\u8868\u73b0\u4f18\u4e8eTop2Vec\uff1b2. \u9886\u57df\u7279\u5b9a\u5d4c\u5165\uff08\u7279\u522b\u662fBioClinicalBERT\uff09\u63d0\u9ad8\u4e86\u4e3b\u9898\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff1b3. \u5168\u5c40\u5206\u6790\u663e\u793a13\u4e2a\u8bbf\u8c08\u4e2d\u6700\u4e3b\u8981\u7684\u4e3b\u9898\u662f\"\u764c\u75c7\u62a4\u7406\u7ba1\u7406\u4e2d\u7684\u534f\u8c03\u4e0e\u6c9f\u901a\"\u548c\"\u764c\u75c7\u6cbb\u7597\u65c5\u7a0b\u4e2d\u7684\u60a3\u8005\u51b3\u7b56\"\uff1b4. \u5c3d\u7ba1\u8bbf\u8c08\u662f\u8377\u5170\u8bed\u5230\u82f1\u8bed\u7684\u673a\u5668\u7ffb\u8bd1\u4e14\u672a\u6d89\u53ca\u4e34\u5e8a\u4e13\u4e1a\u4eba\u5458\u8bc4\u4f30\uff0c\u4f46\u7ed3\u679c\u8868\u660e\u795e\u7ecf\u4e3b\u9898\u5efa\u6a21\u80fd\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u6709\u7528\u7684\u60a3\u8005\u8bbf\u8c08\u53cd\u9988\u3002", "conclusion": "\u795e\u7ecf\u4e3b\u9898\u5efa\u6a21\uff08\u7279\u522b\u662fBERTopic\uff09\u80fd\u591f\u6709\u6548\u4ece\u60a3\u8005\u8bbf\u8c08\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4e3b\u9898\uff0c\u652f\u6301\u66f4\u9ad8\u6548\u7684\u6587\u6863\u5bfc\u822a\u5e76\u52a0\u5f3a\u60a3\u8005\u58f0\u97f3\u5728\u533b\u7597\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u8be5\u6d41\u7a0b\u6709\u6f5c\u529b\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u60a3\u8005\u53cd\u9988\uff0c\u4fc3\u8fdb\u66f4\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u533b\u7597\u5b9e\u8df5\u3002"}}
{"id": "2601.11981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11981", "abs": "https://arxiv.org/abs/2601.11981", "authors": ["Jian Lang", "Rongpei Hong", "Ting Zhong", "Yong Wang", "Fan Zhou"], "title": "Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection", "comment": "13 pages. Accepted by KDD 2026 research track. Codes are released at https://github.com/Jian-Lang/RADAR", "summary": "Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances' representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.", "AI": {"tldr": "RADAR\u662f\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u6846\u67b6\uff0c\u91c7\u7528\u68c0\u7d22\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8303\u5f0f\uff0c\u901a\u8fc7\u7a33\u5b9a\u89c6\u9891\u6307\u5bfc\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u7684\u9c81\u68d2\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u65b0\u95fb\u4e3b\u9898\u5206\u5e03\u4e00\u81f4\uff0c\u65e0\u6cd5\u68c0\u6d4b\u4e0e\u65b0\u5174\u4e8b\u4ef6\u548c\u672a\u89c1\u4e3b\u9898\u76f8\u5173\u7684\u5047\u65b0\u95fb\u89c6\u9891\uff0c\u9700\u8981\u89e3\u51b3\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51faRADAR\u6846\u67b6\uff1a1) \u57fa\u4e8e\u71b5\u9009\u62e9\u7684\u68c0\u7d22\u673a\u5236\uff0c\u4e3a\u9002\u5e94\u63d0\u4f9b\u7a33\u5b9a\u3001\u76f8\u5173\u7684\u53c2\u8003\u89c6\u9891\uff1b2) \u7a33\u5b9a\u951a\u70b9\u5f15\u5bfc\u5bf9\u9f50\u6a21\u5757\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u5339\u914d\u5c06\u4e0d\u7a33\u5b9a\u5b9e\u4f8b\u8868\u793a\u5bf9\u9f50\u5230\u6e90\u57df\uff1b3) \u76ee\u6807\u57df\u611f\u77e5\u7684\u81ea\u8bad\u7ec3\u8303\u5f0f\uff0c\u751f\u6210\u7531\u7a33\u5b9a\u53c2\u8003\u589e\u5f3a\u7684\u4fe1\u606f\u6027\u4f2a\u6807\u7b7e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRADAR\u5728\u6d4b\u8bd5\u65f6\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u5bf9\u672a\u89c1\u5047\u65b0\u95fb\u89c6\u9891\u4e3b\u9898\u8fdb\u884c\u5f3a\u5927\u7684\u5373\u65f6\u9002\u5e94\u3002", "conclusion": "RADAR\u662f\u9996\u4e2a\u652f\u6301\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u68c0\u6d4b\u672a\u89c1\u65b0\u95fb\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u4e2d\u7684\u65b0\u5174\u4e3b\u9898\u9002\u5e94\u95ee\u9898\u3002"}}
{"id": "2601.12641", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12641", "abs": "https://arxiv.org/abs/2601.12641", "authors": ["Xiangyu Shi", "Junyang Ding", "Xu Zhao", "Sinong Zhan", "Payal Mohapatra", "Daniel Quispe", "Kojo Welbeck", "Jian Cao", "Wei Chen", "Ping Guo", "Qi Zhu"], "title": "STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models", "comment": "Accepted to the Design, Automation & Test in Europe Conference (DATE) 2026", "summary": "Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.", "AI": {"tldr": "STEP-LLM\uff1a\u9996\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5230STEP\u6587\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u91cd\u5e8f\u5217\u5316\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86CAD\u6a21\u578b\u751f\u6210\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5230CAD\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u547d\u4ee4\u5e8f\u5217\u6216\u811a\u672c\u683c\u5f0f\uff08\u5982CadQuery\uff09\uff0c\u4f46\u8fd9\u4e9b\u683c\u5f0f\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5185\u6838\u4e14\u7f3a\u4e4f\u5236\u9020\u901a\u7528\u6027\u3002STEP\u6587\u4ef6\u4f5c\u4e3a\u5e7f\u6cdb\u91c7\u7528\u7684\u4e2d\u6027\u8fb9\u754c\u8868\u793a\u683c\u5f0f\u76f4\u63a5\u517c\u5bb9\u5236\u9020\uff0c\u4f46\u5176\u56fe\u7ed3\u6784\u3001\u4ea4\u53c9\u5f15\u7528\u7684\u7279\u6027\u7ed9\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u4e86\u72ec\u7279\u6311\u6218\u3002", "method": "1. \u6784\u5efa\u7ea640K STEP-\u63cf\u8ff0\u5bf9\u6570\u636e\u96c6\uff1b2. \u9488\u5bf9STEP\u56fe\u7ed3\u6784\u683c\u5f0f\u8bbe\u8ba1\u65b0\u9896\u9884\u5904\u7406\uff1a\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u91cd\u5e8f\u5217\u5316\u7ebf\u6027\u5316\u4ea4\u53c9\u5f15\u7528\u5e76\u4fdd\u6301\u5c40\u90e8\u6027\uff0c\u94fe\u5f0f\u601d\u7ef4\u98ce\u683c\u7ed3\u6784\u6ce8\u91ca\u6307\u5bfc\u5168\u5c40\u4e00\u81f4\u6027\uff1b3. \u96c6\u6210\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ee5\u76f8\u5173\u793a\u4f8b\u4e3a\u57fa\u7840\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b4. \u901a\u8fc7\u57fa\u4e8eChamfer\u8ddd\u79bb\u7684\u51e0\u4f55\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "STEP-LLM\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u4e0a\u6301\u7eed\u4f18\u4e8eText2CAD\u57fa\u7ebf\u3002RAG\u6a21\u5757\u663e\u8457\u63d0\u5347\u5b8c\u6574\u6027\u548c\u53ef\u6e32\u67d3\u6027\uff0cDFS\u91cd\u5e8f\u5217\u5316\u589e\u5f3a\u6574\u4f53\u51c6\u786e\u6027\uff0cRL\u8fdb\u4e00\u6b65\u51cf\u5c11\u51e0\u4f55\u5dee\u5f02\u3002\u5ea6\u91cf\u548c\u89c6\u89c9\u6bd4\u8f83\u5747\u786e\u8ba4STEP-LLM\u751f\u6210\u5f62\u72b6\u7684\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210STEP\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e3a\u5236\u9020\u9886\u57df\u6c11\u4e3b\u5316CAD\u8bbe\u8ba1\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u6587\u672c\u5230\u5236\u9020\u517c\u5bb9CAD\u683c\u5f0f\u7684\u76f4\u63a5\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.12179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12179", "abs": "https://arxiv.org/abs/2601.12179", "authors": ["Adam E. Friedman", "Stevan Harnad", "Rushen Shi"], "title": "Tolerance Principle and Small Language Model Learning", "comment": "14 pages, 6 figures. BUCLD 50 Proceedings. To be published in 2026 by Cascadilla Press", "summary": "Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.", "AI": {"tldr": "BabyBERTa\u6a21\u578b\u5728\u4eba\u5de5\u8bed\u6cd5\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u5176\u5b66\u4e60\u52a8\u6001\u4e0d\u7b26\u5408Yang\u7684\u5bb9\u5fcd\u539f\u5219\uff0c\u4e0e\u4eba\u7c7b\u5a74\u513f\u7684\u5b66\u4e60\u80fd\u529b\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22Transformer\u8bed\u8a00\u6a21\u578b\u9700\u8981\u591a\u5c11\u8bad\u7ec3\u6570\u636e\u624d\u80fd\u6cdb\u5316\u8bed\u6cd5\u89c4\u5219\uff0c\u5e76\u6d4b\u8bd5Yang\u7684\u5bb9\u5fcd\u539f\u5219\u662f\u5426\u9002\u7528\u4e8eAI\u6a21\u578b\u3002\u4eba\u7c7b\u5a74\u513f\u80fd\u4ece\u6781\u5c11\u6837\u672c\u4e2d\u5b66\u4e60\u62bd\u8c61\u8bed\u6cd5\u89c4\u5219\uff0c\u800c\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6d77\u91cf\u6570\u636e\uff0c\u8fd9\u79cd\u5dee\u5f02\u503c\u5f97\u7814\u7a76\u3002", "method": "\u4f7f\u7528BabyBERTa\uff08\u4e13\u4e3a\u5c0f\u6570\u636e\u96c6\u4f18\u5316\u7684Transformer\u6a21\u578b\uff09\u5728\u4eba\u5de5\u8bed\u6cd5\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8bad\u7ec3\u96c6\u5728\u5927\u5c0f\u3001\u72ec\u7279\u53e5\u5b50\u7c7b\u578b\u6570\u91cf\u3001\u89c4\u5219\u9075\u5faa\u4e0e\u4f8b\u5916\u6837\u672c\u6bd4\u4f8b\u7b49\u65b9\u9762\u8fdb\u884c\u53d8\u5316\uff0c\u4ee5\u6d4b\u8bd5\u5bb9\u5fcd\u539f\u5219\u7684\u9884\u6d4b\u3002", "result": "\u7814\u7a76\u53d1\u73b0BabyBERTa\u7684\u5b66\u4e60\u52a8\u6001\u4e0e\u5bb9\u5fcd\u539f\u5219\u4e0d\u7b26\uff0c\u4e0e\u4eba\u7c7b\u5a74\u513f\u7684\u5b66\u4e60\u80fd\u529b\u4e0d\u540c\u3002\u6a21\u578b\u672a\u80fd\u50cf\u4eba\u7c7b\u5a74\u513f\u90a3\u6837\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u6709\u6548\u5b66\u4e60\u62bd\u8c61\u8bed\u6cd5\u89c4\u5219\u3002", "conclusion": "Transformer\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u673a\u5236\u4e0e\u4eba\u7c7b\u5a74\u513f\u7684\u8bed\u6cd5\u5b66\u4e60\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u5bb9\u5fcd\u539f\u5219\u4e0d\u9002\u7528\u4e8e\u5f53\u524dAI\u6a21\u578b\uff0c\u8868\u660e\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u4e0e\u673a\u5668\u5b66\u4e60\u6709\u672c\u8d28\u4e0d\u540c\u3002"}}
{"id": "2601.11983", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11983", "abs": "https://arxiv.org/abs/2601.11983", "authors": ["Md. Asiful Islam", "Abdul Hasib", "Tousif Mahmud Emon", "Khandaker Tabin Hasan", "A. S. M. Ahsanul Sarkar Akib"], "title": "An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System", "comment": null, "summary": "The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5\\% success rate, ultrasonic obstacle detection reached 94\\% accuracy, and YOLOv8-based object detection delivered 91.5\\% Precision, 90.2\\% Recall, and a 90.8\\% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eAI-IoT\u7684\u667a\u80fd\u8f6e\u6905\u7cfb\u7edf\uff0c\u96c6\u6210\u624b\u52bf\u63a7\u5236\u3001\u7269\u4f53\u68c0\u6d4b\u548c\u5065\u5eb7\u76d1\u6d4b\u529f\u80fd\uff0c\u4e3a\u6b8b\u969c\u4eba\u58eb\u548c\u8001\u5e74\u4eba\u63d0\u4f9b\u7ecf\u6d4e\u5b9e\u60e0\u7684\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u6b8b\u969c\u4eba\u58eb\u548c\u8001\u5e74\u4eba\u6570\u91cf\u589e\u52a0\uff0c\u9700\u8981\u7ecf\u6d4e\u5b9e\u60e0\u7684\u667a\u80fd\u8f6e\u6905\uff0c\u4f20\u7edf\u8f6e\u6905\u7f3a\u4e4f\u52a8\u6001\u529f\u80fd\uff0c\u73b0\u6709\u667a\u80fd\u8f6e\u6905\u6210\u672c\u9ad8\u3001\u529f\u80fd\u5355\u4e00\u4e14\u5065\u5eb7\u76d1\u6d4b\u96c6\u6210\u4e0d\u8db3\u3002", "method": "\u91c7\u7528AI-IoT\u67b6\u6784\uff0c\u96c6\u6210\u624b\u5957\u624b\u52bf\u63a7\u5236\u5b9e\u73b0\u514d\u624b\u5bfc\u822a\uff0cYOLOv8\u8fdb\u884c\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u5e76\u63d0\u4f9b\u542c\u89c9\u53cd\u9988\uff0c\u8d85\u58f0\u6ce2\u7528\u4e8e\u78b0\u649e\u907f\u514d\uff0c\u6301\u7eed\u76d1\u6d4b\u5fc3\u7387\u3001\u8840\u6c27\u3001\u5fc3\u7535\u56fe\u3001\u4f53\u6e29\u7b49\u751f\u547d\u4f53\u5f81\u5e76\u4e0a\u4f20\u81f3ThingSpeak\u5e73\u53f0\u3002", "result": "\u624b\u52bf\u63a7\u5236\u6210\u529f\u738795.5%\uff0c\u8d85\u58f0\u6ce2\u969c\u788d\u68c0\u6d4b\u51c6\u786e\u738794%\uff0cYOLOv8\u7269\u4f53\u68c0\u6d4b\u8fbe\u523091.5%\u7cbe\u786e\u7387\u300190.2%\u53ec\u56de\u7387\u548c90.8% F1\u5206\u6570\uff0c\u7cfb\u7edf\u6a21\u5757\u5316\u4e14\u6210\u672c\u4f4e\u3002", "conclusion": "\u8be5\u96c6\u6210\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u7528\u6237\u81ea\u4e3b\u6027\u3001\u5b89\u5168\u6027\u548c\u72ec\u7acb\u6027\uff0c\u5f25\u5408\u521b\u65b0\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.12661", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12661", "abs": "https://arxiv.org/abs/2601.12661", "authors": ["Chuhan Qiao", "Jianghua Huang", "Daxing Zhao", "Ziding Liu", "Yanjun Shen", "Bing Cheng", "Wei Lin", "Kai Wu"], "title": "MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for Medical Consultation Agents", "comment": null, "summary": "Current evaluations of medical consultation agents often prioritize outcome-oriented tasks, frequently overlooking the end-to-end process integrity and clinical safety essential for real-world practice. While recent interactive benchmarks have introduced dynamic scenarios, they often remain fragmented and coarse-grained, failing to capture the structured inquiry logic and diagnostic rigor required in professional consultations. To bridge this gap, we propose MedConsultBench, a comprehensive framework designed to evaluate the complete online consultation cycle by covering the entire clinical workflow from history taking and diagnosis to treatment planning and follow-up Q\\&A. Our methodology introduces Atomic Information Units (AIUs) to track clinical information acquisition at a sub-turn level, enabling precise monitoring of how key facts are elicited through 22 fine-grained metrics. By addressing the underspecification and ambiguity inherent in online consultations, the benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing medication regimen compatibility and the ability to handle realistic post-prescription follow-up Q\\&A via constraint-respecting plan revisions. Systematic evaluation of 19 large language models reveals that high diagnostic accuracy often masks significant deficiencies in information-gathering efficiency and medication safety. These results underscore a critical gap between theoretical medical knowledge and clinical practice ability, establishing MedConsultBench as a rigorous foundation for aligning medical AI with the nuanced requirements of real-world clinical care.", "AI": {"tldr": "MedConsultBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u533b\u7597\u54a8\u8be2\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u5b50\u4fe1\u606f\u5355\u5143(AIUs)\u548c22\u4e2a\u7ec6\u7c92\u5ea6\u6307\u6807\uff0c\u8bc4\u4f30\u4ece\u75c5\u53f2\u91c7\u96c6\u5230\u968f\u8bbf\u7684\u5b8c\u6574\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63ed\u793aLLMs\u5728\u4fe1\u606f\u6536\u96c6\u6548\u7387\u548c\u7528\u836f\u5b89\u5168\u65b9\u9762\u7684\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u54a8\u8be2\u4ee3\u7406\u8bc4\u4f30\u8fc7\u4e8e\u6ce8\u91cd\u7ed3\u679c\u5bfc\u5411\uff0c\u5ffd\u7565\u4e86\u7aef\u5230\u7aef\u6d41\u7a0b\u5b8c\u6574\u6027\u548c\u4e34\u5e8a\u5b89\u5168\u6027\uff0c\u73b0\u6709\u4ea4\u4e92\u57fa\u51c6\u5f80\u5f80\u788e\u7247\u5316\u4e14\u7c97\u7c92\u5ea6\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u54a8\u8be2\u6240\u9700\u7684\u7ed3\u6784\u5316\u8be2\u95ee\u903b\u8f91\u548c\u8bca\u65ad\u4e25\u8c28\u6027\u3002", "method": "\u63d0\u51faMedConsultBench\u6846\u67b6\uff0c\u8986\u76d6\u5b8c\u6574\u7684\u5728\u7ebf\u54a8\u8be2\u5468\u671f\uff08\u75c5\u53f2\u91c7\u96c6\u3001\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u3001\u968f\u8bbf\u95ee\u7b54\uff09\u3002\u5f15\u5165\u539f\u5b50\u4fe1\u606f\u5355\u5143(AIUs)\u5728\u5b50\u8f6e\u6b21\u5c42\u9762\u8ffd\u8e2a\u4e34\u5e8a\u4fe1\u606f\u83b7\u53d6\uff0c\u4f7f\u752822\u4e2a\u7ec6\u7c92\u5ea6\u6307\u6807\u7cbe\u786e\u76d1\u63a7\u5173\u952e\u4e8b\u5b9e\u7684\u83b7\u53d6\u8fc7\u7a0b\uff0c\u5904\u7406\u5728\u7ebf\u54a8\u8be2\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc4\u4f30\u7ea6\u675f\u9075\u4ece\u7684\u6cbb\u7597\u8ba1\u5212\u4fee\u8ba2\u80fd\u529b\u3002", "result": "\u5bf919\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u5f80\u5f80\u63a9\u76d6\u4e86\u4fe1\u606f\u6536\u96c6\u6548\u7387\u548c\u7528\u836f\u5b89\u5168\u65b9\u9762\u7684\u663e\u8457\u7f3a\u9677\uff0c\u63ed\u793a\u4e86\u7406\u8bba\u533b\u5b66\u77e5\u8bc6\u4e0e\u4e34\u5e8a\u5b9e\u8df5\u80fd\u529b\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "conclusion": "MedConsultBench\u4e3a\u533b\u7597AI\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e34\u5e8a\u62a4\u7406\u7684\u7ec6\u5fae\u8981\u6c42\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e25\u8c28\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5728\u533b\u7597AI\u8bc4\u4f30\u4e2d\u9700\u8981\u66f4\u5168\u9762\u7684\u6d41\u7a0b\u5b8c\u6574\u6027\u548c\u5b89\u5168\u6027\u8003\u91cf\u3002"}}
{"id": "2601.12199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12199", "abs": "https://arxiv.org/abs/2601.12199", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "title": "CTC-DID: CTC-Based Arabic dialect identification for streaming applications", "comment": "Accepted for IEEE ICASSP 2026", "summary": "This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCTC\u635f\u5931\u7684\u65b9\u8a00\u8bc6\u522b\u65b9\u6cd5\uff0c\u5c06\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u5efa\u6a21\u4e3a\u6709\u9650\u8bcd\u6c47ASR\u7cfb\u7edf\uff0c\u5728\u4f4e\u8d44\u6e90\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b", "motivation": "\u73b0\u6709\u65b9\u8a00\u8bc6\u522b\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u77ed\u8bed\u97f3\u548c\u5b9e\u65f6\u5e94\u7528\u573a\u666f", "method": "\u5c06\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u6846\u67b6\u5316\u4e3a\u6709\u9650\u8bcd\u6c47ASR\u7cfb\u7edf\uff0c\u4f7f\u7528CTC\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u8bed\u8a00\u65e0\u5173\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u9884\u8bad\u7ec3ASR\u6a21\u578b\u4f30\u8ba1\u65b9\u8a00\u6807\u7b7e\u5728\u8f6c\u5f55\u4e2d\u7684\u91cd\u590d\u6b21\u6570", "result": "\u5728\u4f4e\u8d44\u6e90\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8eSSL\u7684CTC-DID\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u6027\u80fd\u4f18\u4e8e\u5fae\u8c03\u7684Whisper\u548cECAPA-TDNN\u6a21\u578b\uff0c\u5728Casablanca\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u4e5f\u8868\u73b0\u66f4\u4f18", "conclusion": "CTC-DID\u65b9\u6cd5\u5728\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u77ed\u8bed\u97f3\u66f4\u9c81\u68d2\uff0c\u6613\u4e8e\u9002\u914d\u6d41\u5f0f\u5b9e\u65f6\u5e94\u7528\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u6700\u5c0f"}}
{"id": "2601.11987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11987", "abs": "https://arxiv.org/abs/2601.11987", "authors": ["Khaled Berkani"], "title": "Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis", "comment": "15 pages, 3 figures, 3 tables", "summary": "We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89e3\u5256\u5148\u9a8c\u7684\u7ed3\u6784\u56fe\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8bca\u65ad\u3002\u5c06\u5377\u79ef\u7279\u5f81\u56fe\u91cd\u65b0\u89e3\u91ca\u4e3a\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u7684\u7ed3\u6784\u4f20\u64ad\u673a\u5236\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\uff0c\u5b9e\u73b0\u75c5\u53d8\u611f\u77e5\u9884\u6d4b\u548c\u8bca\u65ad\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u53ef\u89e3\u91ca\u6027\u3002\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u9700\u8981\u7ed3\u5408\u89e3\u5256\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\uff0c\u800c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f9d\u8d56\u901a\u7528\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u65e0\u6cd5\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u89e3\u5256\u7ed3\u6784\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u878d\u5165\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6846\u67b6\u3002", "method": "\u5c06\u5377\u79ef\u7279\u5f81\u56fe\u91cd\u65b0\u89e3\u91ca\u4e3a\u8865\u4e01\u7ea7\u56fe\u7ed3\u6784\uff1a\u8282\u70b9\u7f16\u7801\u5916\u89c2\u7279\u5f81\u548c\u7a7a\u95f4\u5750\u6807\uff0c\u8fb9\u53cd\u6620\u5c40\u90e8\u7ed3\u6784\u90bb\u63a5\u5173\u7cfb\u3002\u5f15\u5165\u81ea\u5b9a\u4e49\u7684\u7ed3\u6784\u4f20\u64ad\u673a\u5236\uff0c\u663e\u5f0f\u5efa\u6a21\u76f8\u5bf9\u7a7a\u95f4\u5173\u7cfb\u4f5c\u4e3a\u63a8\u7406\u8fc7\u7a0b\u7684\u4e00\u90e8\u5206\u3002\u8be5\u6846\u67b6\u540c\u65f6\u652f\u6301\u8282\u70b9\u7ea7\u7684\u75c5\u53d8\u611f\u77e5\u9884\u6d4b\u548c\u56fe\u7ea7\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u901a\u8fc7\u5b66\u4e60\u8282\u70b9\u91cd\u8981\u6027\u5206\u6570\u5b9e\u73b0\u5185\u5728\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u901a\u8fc7\u80f8\u90e8X\u5149\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u6784\u5148\u9a8c\u80fd\u591f\u6307\u5bfc\u5173\u7cfb\u63a8\u7406\u5e76\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u540e\u5904\u7406\u53ef\u89c6\u5316\u6280\u672f\u3002\u6846\u67b6\u5177\u6709\u9886\u57df\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u56fe\u57fa\u63a8\u7406\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u5b66\u4e60\u63d0\u4f9b\u4e86\u56fe\u8ba1\u7b97\u57fa\u677f\u3002\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u56fe\u4f5c\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u7684\u5f52\u7eb3\u504f\u7f6e\u800c\u975e\u88ab\u52a8\u5173\u7cfb\u8868\u793a\uff0c\u63a8\u52a8\u4e86\u56fe\u57fa\u63a8\u7406\u5728\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7ed3\u5408\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u7684\u53ef\u89e3\u91ca\u8bca\u65ad\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2601.12667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12667", "abs": "https://arxiv.org/abs/2601.12667", "authors": ["Yi Di", "Zhibin Zhao", "Fujin Wang", "Xue Liu", "Jiafeng Tang", "Jiaxin Ren", "Zhi Zhai", "Xuefeng Chen"], "title": "Empowering All-in-Loop Health Management of Spacecraft Power System in the Mega-Constellation Era via Human-AI Collaboration", "comment": null, "summary": "It is foreseeable that the number of spacecraft will increase exponentially, ushering in an era dominated by satellite mega-constellations (SMC). This necessitates a focus on energy in space: spacecraft power systems (SPS), especially their health management (HM), given their role in power supply and high failure rates. Providing health management for dozens of SPS and for thousands of SPS represents two fundamentally different paradigms. Therefore, to adapt the health management in the SMC era, this work proposes a principle of aligning underlying capabilities (AUC principle) and develops SpaceHMchat, an open-source Human-AI collaboration (HAIC) framework for all-in-loop health management (AIL HM). SpaceHMchat serves across the entire loop of work condition recognition, anomaly detection, fault localization, and maintenance decision making, achieving goals such as conversational task completion, adaptive human-in-the-loop learning, personnel structure optimization, knowledge sharing, efficiency enhancement, as well as transparent reasoning and improved interpretability. Meanwhile, to validate this exploration, a hardware-realistic fault injection experimental platform is established, and its simulation model is built and open-sourced, both fully replicating the real SPS. The corresponding experimental results demonstrate that SpaceHMchat achieves excellent performance across 23 quantitative metrics, such as 100% conclusion accuracy in logical reasoning of work condition recognition, over 99% success rate in anomaly detection tool invocation, over 90% precision in fault localization, and knowledge base search time under 3 minutes in maintenance decision-making. Another contribution of this work is the release of the first-ever AIL HM dataset of SPS. This dataset contains four sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and over 700,000 timestamps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpaceHMchat\u6846\u67b6\uff0c\u7528\u4e8e\u536b\u661f\u5de8\u578b\u661f\u5ea7\u65f6\u4ee3\u822a\u5929\u5668\u7535\u6e90\u7cfb\u7edf\u7684\u5065\u5eb7\u7ba1\u7406\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u548c\u5168\u56de\u8def\u7ba1\u7406\uff0c\u572823\u9879\u91cf\u5316\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5f00\u6e90\u4e86\u9996\u4e2aAIL HM\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740\u536b\u661f\u5de8\u578b\u661f\u5ea7\u65f6\u4ee3\u7684\u5230\u6765\uff0c\u822a\u5929\u5668\u6570\u91cf\u5c06\u6307\u6570\u7ea7\u589e\u957f\uff0c\u800c\u822a\u5929\u5668\u7535\u6e90\u7cfb\u7edf\u4f5c\u4e3a\u5173\u952e\u7cfb\u7edf\u4e14\u6545\u969c\u7387\u9ad8\uff0c\u5176\u5065\u5eb7\u7ba1\u7406\u9762\u4e34\u4ece\u51e0\u5341\u4e2a\u5230\u6570\u5343\u4e2a\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e95\u5c42\u80fd\u529b\u5bf9\u9f50\u539f\u5219\uff0c\u5f00\u53d1\u5f00\u6e90\u7684\u4eba\u673a\u534f\u4f5c\u6846\u67b6SpaceHMchat\uff0c\u5b9e\u73b0\u5de5\u4f5c\u72b6\u6001\u8bc6\u522b\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u6545\u969c\u5b9a\u4f4d\u548c\u7ef4\u62a4\u51b3\u7b56\u7684\u5168\u56de\u8def\u7ba1\u7406\uff0c\u5e76\u5efa\u7acb\u786c\u4ef6\u771f\u5b9e\u7684\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\u5e73\u53f0\u548c\u4eff\u771f\u6a21\u578b\u3002", "result": "SpaceHMchat\u572823\u9879\u91cf\u5316\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u5de5\u4f5c\u72b6\u6001\u8bc6\u522b\u903b\u8f91\u63a8\u7406\u7ed3\u8bba\u51c6\u786e\u7387100%\uff0c\u5f02\u5e38\u68c0\u6d4b\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u8d8599%\uff0c\u6545\u969c\u5b9a\u4f4d\u7cbe\u5ea6\u8d8590%\uff0c\u7ef4\u62a4\u51b3\u7b56\u77e5\u8bc6\u5e93\u641c\u7d22\u65f6\u95f4\u4f4e\u4e8e3\u5206\u949f\u3002", "conclusion": "SpaceHMchat\u6846\u67b6\u6210\u529f\u9002\u5e94\u536b\u661f\u5de8\u578b\u661f\u5ea7\u65f6\u4ee3\u7684\u5065\u5eb7\u7ba1\u7406\u9700\u6c42\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u9996\u4e2a\u5305\u542b4\u4e2a\u5b50\u6570\u636e\u96c6\u300117\u79cd\u6545\u969c\u7c7b\u578b\u3001\u8d8570\u4e07\u65f6\u95f4\u6233\u7684AIL HM\u6570\u636e\u96c6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2601.12208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12208", "abs": "https://arxiv.org/abs/2601.12208", "authors": ["Yunzhe Li", "Richie Yueqi Feng", "Tianxin Wei", "Chin-Chia Hsu"], "title": "CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement", "comment": null, "summary": "Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.", "AI": {"tldr": "CoReflect\uff1a\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7684\u5bf9\u8bdd\u6a21\u62df\u548c\u53cd\u601d\u6027\u8bc4\u4f30\u6807\u51c6\u4f18\u5316\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u7684\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u56fa\u5b9a\u8bc4\u4f30\u6807\u51c6\u548c\u9759\u6001\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff0c\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u5bf9\u8bdd\u6a21\u578b\u591a\u6837\u5316\u7684\u6d8c\u73b0\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u63d0\u51faCoReflect\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u6a21\u62df\u548c\u8bc4\u4f30\u7edf\u4e00\u4e3a\u81ea\u9002\u5e94\u8fed\u4ee3\u8fc7\u7a0b\uff1a1) \u5bf9\u8bdd\u89c4\u5212\u5668\u751f\u6210\u7ed3\u6784\u5316\u6a21\u677f\u6307\u5bfc\u7528\u6237\u6a21\u62df\u5668\u8fdb\u884c\u591a\u6837\u5316\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\uff1b2) \u53cd\u601d\u5206\u6790\u5668\u5904\u7406\u5bf9\u8bdd\uff0c\u8bc6\u522b\u7cfb\u7edf\u884c\u4e3a\u6a21\u5f0f\u5e76\u81ea\u52a8\u4f18\u5316\u8bc4\u4f30\u6807\u51c6\uff1b3) \u901a\u8fc7\u534f\u540c\u8fdb\u5316\u5faa\u73af\u5c06\u5206\u6790\u6d1e\u5bdf\u53cd\u9988\u7ed9\u89c4\u5212\u5668\u66f4\u65b0\u5bf9\u8bdd\u6a21\u677f", "result": "CoReflect\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u6211\u4f18\u5316\u7684\u65b9\u6cd5\u8bba\uff0c\u4f7f\u8bc4\u4f30\u534f\u8bae\u80fd\u591f\u9002\u5e94\u5bf9\u8bdd\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u80fd\u529b", "conclusion": "CoReflect\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7684\u5bf9\u8bdd\u6a21\u62df\u548c\u53cd\u601d\u6027\u8bc4\u4f30\u6807\u51c6\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u7684\u6839\u672c\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u7528\u4f8b\u590d\u6742\u5ea6\u548c\u8bc4\u4f30\u6807\u51c6\u8bca\u65ad\u7cbe\u5ea6\u7684\u540c\u6b65\u63d0\u5347"}}
{"id": "2601.11990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11990", "abs": "https://arxiv.org/abs/2601.11990", "authors": ["Yiming Li", "Chen Cai", "Tianyi Liu", "Dan Lin", "Wenqian Wang", "Wenfei Liang", "Bingbing Li", "Kim-Hui Yap"], "title": "DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset", "comment": null, "summary": "In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.", "AI": {"tldr": "\u63d0\u51faDAOS\u6570\u636e\u96c6\u548cAOR-Net\u6a21\u578b\uff0c\u901a\u8fc7\u5efa\u6a21\u9a7e\u9a76\u5458\u52a8\u4f5c\u4e0e\u7269\u4f53\u5173\u7cfb\u63d0\u5347\u9a7e\u9a76\u76d1\u63a7\u4e2d\u7684\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd", "motivation": "\u73b0\u6709\u9a7e\u9a76\u5458\u76d1\u63a7\u6570\u636e\u96c6\u7f3a\u4e4f\u51c6\u786e\u7684\u7269\u4f53\u4f4d\u7f6e\u6807\u6ce8\u6216\u672a\u5c06\u7269\u4f53\u4e0e\u76f8\u5173\u52a8\u4f5c\u5173\u8054\uff0c\u5bfc\u81f4\u76f8\u4f3c\u7684\u4e0a\u534a\u8eab\u52a8\u4f5c\u96be\u4ee5\u533a\u5206\uff0c\u5f71\u54cd\u52a8\u4f5c\u8bc6\u522b\u7684\u53ef\u9760\u6027", "method": "1) \u63d0\u51faDAOS\u6570\u636e\u96c6\uff1a\u5305\u542b9,787\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6807\u6ce836\u4e2a\u7ec6\u7c92\u5ea6\u9a7e\u9a76\u5458\u52a8\u4f5c\u548c15\u4e2a\u7269\u4f53\u7c7b\u522b\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u591a\u89c6\u89d2\u6570\u636e\uff1b2) \u63d0\u51faAOR-Net\u6a21\u578b\uff1a\u901a\u8fc7\u591a\u7ea7\u63a8\u7406\u548c\u52a8\u4f5c\u94fe\u63d0\u793a\u673a\u5236\u5efa\u6a21\u52a8\u4f5c-\u7269\u4f53-\u5173\u7cfb\u903b\u8f91\uff0c\u5f15\u5165\u601d\u7ef4\u6df7\u5408\u6a21\u5757\u52a8\u6001\u9009\u62e9\u5173\u952e\u77e5\u8bc6", "result": "AOR-Net\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u7269\u4f53\u4e30\u5bcc\u548c\u7a00\u7f3a\u6761\u4ef6\u4e0b\u90fd\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7DAOS\u6570\u636e\u96c6\u548cAOR-Net\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9a7e\u9a76\u5458\u76d1\u63a7\u4e2d\u76f8\u4f3c\u52a8\u4f5c\u96be\u4ee5\u533a\u5206\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5efa\u6a21\u52a8\u4f5c-\u7269\u4f53\u5173\u7cfb\u5bf9\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u7684\u91cd\u8981\u6027"}}
{"id": "2601.12688", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12688", "abs": "https://arxiv.org/abs/2601.12688", "authors": ["Xu Zhang", "Qinghua Wang", "Mengyang Zhao", "Fang Wang", "Cunquan Qu"], "title": "Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction", "comment": null, "summary": "Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u88ab\u544a\u4eba\u6848\u4ef6\u8d23\u4efb\u5206\u914d\u7684\u63a9\u7801\u591a\u9636\u6bb5\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u91cf\u5211\u903b\u8f91\u5230Transformer\u7f16\u7801\u5668\u4e2d\uff0c\u63d0\u9ad8AI\u5728\u53f8\u6cd5\u5206\u6790\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u591a\u88ab\u544a\u4eba\u5211\u4e8b\u6848\u4ef6\u4e2d\uff0c\u53f8\u6cd5\u8868\u8ff0\u5e38\u5e38\u6a21\u7cca\u5404\u88ab\u544a\u4eba\u7684\u5177\u4f53\u89d2\u8272\uff0c\u8fd9\u963b\u788d\u4e86AI\u7cfb\u7edf\u8fdb\u884c\u6709\u6548\u7684\u8d23\u4efb\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u533a\u5206\u4e3b\u72af\u548c\u4ece\u72af\u7684\u8d23\u4efb\u7a0b\u5ea6\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u53c8\u80fd\u4fdd\u6301\u6cd5\u5f8b\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u63a9\u7801\u591a\u9636\u6bb5\u63a8\u7406\uff08MMSI\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u91cf\u5211\u903b\u8f91\u6574\u5408\u5230\u9884\u8bad\u7ec3\u7684Transformer\u7f16\u7801\u5668\u4e2d\uff1b2\uff09\u4f7f\u7528\u5b9a\u5411\u63a9\u7801\u673a\u5236\u6f84\u6e05\u88ab\u544a\u4eba\u89d2\u8272\uff1b3\uff09\u91c7\u7528\u5bf9\u6bd4\u6570\u636e\u6784\u5efa\u7b56\u7565\u589e\u5f3a\u6a21\u578b\u5bf9\u4e3b\u4ece\u72af\u8d23\u4efb\u5dee\u5f02\u7684\u654f\u611f\u6027\uff1b4\uff09\u901a\u8fc7\u5e7f\u64ad\u673a\u5236\u5c06\u9884\u6d4b\u7684\u7f6a\u8d23\u6807\u7b7e\u6574\u5408\u5230\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u7ed3\u5408\u72af\u7f6a\u63cf\u8ff0\u548c\u6cd5\u5ead\u89c2\u70b9\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u7684IMLJP\u6545\u610f\u4f24\u5bb3\u6848\u4ef6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMMSI\u6846\u67b6\u5728\u89d2\u8272\u8d23\u4efb\u533a\u5206\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e3b\u72af\u548c\u4ece\u72af\u7684\u8d23\u4efb\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u589e\u5f3a\u667a\u80fd\u53f8\u6cd5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u6cd5\u5f8b\u903b\u8f91\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u591a\u88ab\u544a\u4eba\u6848\u4ef6\u8d23\u4efb\u5206\u914d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.12247", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12247", "abs": "https://arxiv.org/abs/2601.12247", "authors": ["Miao Li", "Hanyang Jiang", "Sikai Chen", "Hengyu Fu", "Yuhang Cai", "Baihe Huang", "Tinghan Ye", "Xuanzhou Chen", "Pascal Van Hentenryck"], "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models", "comment": null, "summary": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.", "AI": {"tldr": "PVF\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u8303\u5f0f\uff0c\u901a\u8fc7\u89c4\u5212-\u9a8c\u8bc1-\u586b\u5145\u673a\u5236\uff0c\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6587\u672c\u751f\u6210\uff0c\u76f8\u6bd4\u5e76\u884c\u89e3\u7801\u51cf\u5c1165%\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u7b56\u7565\u5f80\u5f80\u88ab\u52a8\u53cd\u5e94\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5c40\u53cc\u5411\u4e0a\u4e0b\u6587\u6765\u6307\u5bfc\u751f\u6210\u8f68\u8ff9\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faPlan-Verify-Fill\uff08PVF\uff09\u8303\u5f0f\uff1a1\uff09\u4e3b\u52a8\u6784\u5efa\u5206\u5c42\u9aa8\u67b6\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u5f71\u54cd\u529b\u7684\u8bed\u4e49\u951a\u70b9\uff1b2\uff09\u91c7\u7528\u9a8c\u8bc1\u534f\u8bae\u5b9e\u73b0\u5b9e\u7528\u7ed3\u6784\u505c\u6b62\uff0c\u907f\u514d\u65e0\u6548\u8ba1\u7b97\uff1b3\uff09\u6574\u4e2a\u8fc7\u7a0b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728LLaDA-8B-Instruct\u548cDream-7B-Instruct\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPVF\u76f8\u6bd4\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5e76\u884c\u89e3\u7801\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u9ad8\u8fbe65%\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "PVF\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u975e\u987a\u5e8f\u89e3\u7801\u8303\u5f0f\uff0c\u901a\u8fc7\u4e3b\u52a8\u89c4\u5212\u548c\u9a8c\u8bc1\u673a\u5236\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u4e3a\u6587\u672c\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.12010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12010", "abs": "https://arxiv.org/abs/2601.12010", "authors": ["Yifei Chen", "Ross Greer"], "title": "SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine", "comment": null, "summary": "The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot's decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM's candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.", "AI": {"tldr": "\u63d0\u51faSMc2f\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7c97\u8fc7\u6ee4\u3001\u6784\u5efa\u6848\u4f8b\u5e93few-shot\u63d0\u793aLLM\u3001\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\u7ec6\u5339\u914d\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6316\u6398\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709RefAV\u6846\u67b6\u57fa\u4e8e\u8f68\u8ff9\u6807\u7b7e\u8fdb\u884c\u68c0\u7d22\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u539f\u59cbRGB\u56fe\u50cf\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u4e14\u4f9d\u8d56\u4e0a\u6e383D\u68c0\u6d4b\u8ddf\u8e2a\u8d28\u91cf\uff0c\u8f68\u8ff9\u6570\u636e\u4e0d\u51c6\u786e\u5bfc\u81f4\u65f6\u7a7a\u5b9a\u4f4d\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u7684\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7c97\u7c92\u5ea6\u56fe\u50cf-\u6587\u672c\u8fc7\u6ee4\uff1b2) \u6784\u5efa\u6210\u529f\u6316\u6398\u6848\u4f8b\u5e93\uff0c\u81ea\u52a8\u68c0\u7d22\u793a\u4f8b\u8fdb\u884cfew-shot\u63d0\u793aLLM\uff1b3) \u5f15\u5165\u6587\u672c-\u8f68\u8ff9\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u5339\u914d\u5bf9\uff0c\u5f97\u5230\u7ec6\u7c92\u5ea6\u5339\u914d\u5668\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u68c0\u7d22\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SMc2f\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001few-shot\u63d0\u793a\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u573a\u666f\u6316\u6398\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\u3002"}}
{"id": "2601.12711", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2601.12711", "abs": "https://arxiv.org/abs/2601.12711", "authors": ["Kevin Wang", "Neel P. Bhatt", "Cong Liu", "Junbo Li", "Runjin Chen", "Yihan Xi", "Timothy Barclay", "Alvaro Velasquez", "Ufuk Topcu", "Zhangyang Wang"], "title": "Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts", "comment": null, "summary": "Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff0c\u52a8\u6001\u7ed3\u5408\u6570\u503c\u66f4\u65b0\u548c\u7b26\u53f7\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347LLM\u5fae\u8c03\u6027\u80fd", "motivation": "\u6570\u503c\u5fae\u8c03\u64c5\u957f\u6ce8\u5165\u65b0\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u7b26\u53f7\u66f4\u65b0\u80fd\u7075\u6d3b\u63a7\u5236\u98ce\u683c\u548c\u5bf9\u9f50\uff0c\u4f46\u4e24\u8005\u5404\u6709\u5c40\u9650\u3002\u9700\u8981\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u5fae\u8c03", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\uff1a1\uff09\u7edf\u4e00\u76d1\u63a7\u4fe1\u53f7\u548c\u57fa\u4e8e\u5956\u52b1\u7684\u5206\u7c7b\u5668\uff0c\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528LoRA\u8fdb\u884c\u4e8b\u5b9e\u91cd\u6784\uff0c\u4f55\u65f6\u4f7f\u7528TextGrad\u8fdb\u884c\u8bcd\u5143\u7ea7\u7f16\u8f91\uff1b2\uff09\u5c06\u7b26\u53f7\u8f6c\u6362\u5378\u8f7d\u5230\u5916\u90e8LLM\u4ee5\u4fdd\u6301\u5185\u5b58\u6548\u7387\uff1b3\uff09\u7b26\u53f7\u7f16\u8f91\u751f\u6210\u7684\u63d0\u793a\u53ef\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u53ef\u91cd\u7528\u8bad\u7ec3\u6570\u636e", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u6570\u503c\u6216\u7eaf\u7b26\u53f7\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u9002\u5e94\u6027\u548c\u6539\u8fdb\u7684\u6027\u80fd", "conclusion": "\u4ea4\u9519\u4f7f\u7528\u6570\u503c\u548c\u7b26\u53f7\u66f4\u65b0\u80fd\u89e3\u9501\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u65b0\u6c34\u5e73\uff0c\u795e\u7ecf\u7b26\u53f7LoRA\u6846\u67b6\u4e3a\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12263", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12263", "abs": "https://arxiv.org/abs/2601.12263", "authors": ["Yixuan Du", "Chenxiao Yu", "Haoyan Xu", "Ziyi Wang", "Yue Zhao", "Xiyang Hu"], "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers", "comment": null, "summary": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4ea7\u54c1\u641c\u7d22\u7cfb\u7edf\u5b58\u5728\u591a\u6a21\u6001\u6392\u5e8f\u653b\u51fb\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86MGEO\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf\u6270\u52a8\u548c\u6587\u672c\u540e\u7f00\u6765\u4e0d\u516c\u5e73\u5730\u63a8\u5e7f\u76ee\u6807\u4ea7\u54c1\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u7ade\u4e89\u6027\u6392\u5e8f\u573a\u666f\u4e0b\u5bf9\u6297\u5bf9\u6297\u6027\u64cd\u7eb5\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63ed\u793aVLM\u5728\u771f\u5b9e\u4ea7\u54c1\u641c\u7d22\u73af\u5883\u4e2d\u7684\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u751f\u6210\u5f15\u64ce\u4f18\u5316\uff08MGEO\uff09\u6846\u67b6\uff0c\u91c7\u7528\u4ea4\u66ff\u68af\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u8054\u5408\u4f18\u5316\u4e0d\u53ef\u5bdf\u89c9\u7684\u56fe\u50cf\u6270\u52a8\u548c\u6d41\u7545\u7684\u6587\u672c\u540e\u7f00\uff0c\u5229\u7528VLM\u5185\u90e8\u7684\u6df1\u5ea6\u8de8\u6a21\u6001\u8026\u5408\u5173\u7cfb\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u5148\u8fdb\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMGEO\u534f\u8c03\u653b\u51fb\u663e\u8457\u4f18\u4e8e\u4ec5\u6587\u672c\u548c\u4ec5\u56fe\u50cf\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u63ed\u793a\u4e86\u591a\u6a21\u6001\u534f\u540c\u53ef\u80fd\u88ab\u6b66\u5668\u5316\u4ee5\u7834\u574f\u641c\u7d22\u6392\u5e8f\u5b8c\u6574\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u534f\u540c\u867d\u7136\u662fVLM\u7684\u4f18\u52bf\uff0c\u4f46\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\u6765\u7834\u574f\u641c\u7d22\u6392\u5e8f\u7684\u5b8c\u6574\u6027\uff0c\u4e14\u4e0d\u4f1a\u89e6\u53d1\u4f20\u7edf\u5185\u5bb9\u8fc7\u6ee4\u5668\uff0c\u8fd9\u5bf9\u57fa\u4e8eVLM\u7684\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2601.12015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12015", "abs": "https://arxiv.org/abs/2601.12015", "authors": ["Pavan Kumar Yata", "Pediredla Pradeep", "Goli Himanish", "Swathi M"], "title": "SAR-Based Marine Oil Spill Detection Using the DeepSegFusion Architecture", "comment": "12 pages, 6 figures. Submitted to arXiv. Code and dataset details included in the paper", "summary": "Detection of oil spills from satellite images is essential for both environmental surveillance and maritime safety. Traditional threshold-based methods frequently encounter performance degradation due to very high false alarm rates caused by look-alike phenomena such as wind slicks and ship wakes. Here, a hybrid deep learning model, DeepSegFusion, is presented for oil spill segmentation in Synthetic Aperture Radar (SAR) images. The model uses SegNet and DeepLabV3+ integrated with an attention-based feature fusion mechanism to achieve better boundary precision as well as improved contextual understanding. Results obtained on SAR oil spill datasets, including ALOS PALSAR imagery, confirm that the proposed DeepSegFusion model achieves an accuracy of 94.85%, an Intersection over Union (IoU) of 0.5685, and a ROC-AUC score of 0.9330. The proposed method delivers more than three times fewer false detections compared to individual baseline models and traditional non-segmentation methods, achieving a reduction of 64.4%. These results indicate that DeepSegFusion is a stable model under various marine conditions and can therefore be used in near real-time oil spill monitoring scenarios.", "AI": {"tldr": "\u63d0\u51faDeepSegFusion\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eSAR\u56fe\u50cf\u4e2d\u7684\u6ea2\u6cb9\u5206\u5272\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u9002\u7528\u4e8e\u8fd1\u5b9e\u65f6\u6ea2\u6cb9\u76d1\u6d4b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9608\u503c\u7684\u65b9\u6cd5\u5728\u536b\u661f\u56fe\u50cf\u6ea2\u6cb9\u68c0\u6d4b\u4e2d\u56e0\u98ce\u6d6a\u6761\u7eb9\u3001\u8239\u8236\u5c3e\u6d41\u7b49\u7c7b\u4f3c\u73b0\u8c61\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\uff0c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u7ed3\u5408SegNet\u548cDeepLabV3+\uff0c\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u8fb9\u754c\u7cbe\u5ea6\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5728SAR\u6ea2\u6cb9\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.85%\u51c6\u786e\u7387\u30010.5685 IoU\u548c0.9330 ROC-AUC\uff0c\u8bef\u62a5\u68c0\u6d4b\u6bd4\u57fa\u7ebf\u6a21\u578b\u51cf\u5c1164.4%\u3002", "conclusion": "DeepSegFusion\u5728\u5404\u79cd\u6d77\u6d0b\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u53ef\u7528\u4e8e\u8fd1\u5b9e\u65f6\u6ea2\u6cb9\u76d1\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.12720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12720", "abs": "https://arxiv.org/abs/2601.12720", "authors": ["Hanbin Wang", "Jingwei Song", "Jinpeng Li", "Qi Zhu", "Fei Mi", "Ganqu Cui", "Yasheng Wang", "Lifeng Shang"], "title": "Teaching Large Reasoning Models Effective Reflection", "comment": "14 pages (including appendix), 5 figures", "summary": "Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.", "AI": {"tldr": "\u63d0\u51faSCFT\u548cRLERR\u65b9\u6cd5\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8868\u9762\u53cd\u601d\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u6211\u6279\u5224\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u53cd\u601d\u8d28\u91cf\u4e0e\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5e38\u8fdb\u884c\u81ea\u6211\u53cd\u601d\uff0c\u4f46\u8bb8\u591a\u53cd\u601d\u662f\u8868\u9762\u7684\uff0c\u5bf9\u7b54\u6848\u6539\u8fdb\u6709\u9650\u4e14\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u89e3\u51b3\u8868\u9762\u53cd\u601d\u95ee\u9898\u3002", "method": "1. SCFT\uff1a\u8ba9\u6a21\u578b\u6279\u5224\u81ea\u8eab\u8f93\u51fa\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u7b5b\u9009\u9ad8\u8d28\u91cf\u6279\u5224\uff0c\u4f7f\u7528\u6279\u5224\u76ee\u6807\u5fae\u8c03\u6a21\u578b\uff1b2. RLERR\uff1a\u57fa\u4e8eSCFT\u521d\u59cb\u5316\u9ad8\u8d28\u91cf\u53cd\u601d\u6784\u5efa\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5185\u5316\u81ea\u6211\u4fee\u6b63\u8fc7\u7a0b\u3002", "result": "\u5728AIME2024\u548cAIME2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCFT\u548cRLERR\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u53cd\u601d\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684SCFT\u548cRLERR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8868\u9762\u53cd\u601d\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u6211\u6279\u5224\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53cd\u601d\u80fd\u529b\u548c\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2601.12269", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12269", "abs": "https://arxiv.org/abs/2601.12269", "authors": ["Xucong Hu", "Jian-Qiao Zhu"], "title": "Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models", "comment": null, "summary": "Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.", "AI": {"tldr": "\u901a\u8fc7MCMC\u91c7\u6837\u548c\u9000\u706b\u6280\u672f\uff0c\u4ece\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u4e2d\u6062\u590d\u5f3a\u5927\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u5668\uff0c\u4e3b\u8981\u4f18\u5316\u8868\u9762\u5408\u7406\u6027\u800c\u975e\u6f5c\u5728\u72b6\u6001\u8868\u793a\uff0c\u56e0\u6b64\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4f46\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u5177\u5907\u6f5c\u5728\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\uff0c\u53ea\u662f\u9700\u8981\u5408\u9002\u7684\u91c7\u6837\u65b9\u6cd5\u6765\u63d0\u53d6\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMCMC\u7684power-sampling\u65b9\u6cd5\uff0c\u4ece\u5e8f\u5217\u7ea7\uff08\u800c\u975etoken\u7ea7\uff09\u6982\u7387\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5e76\u7ed3\u5408\u9000\u706b\u6280\u672f\uff0c\u9010\u6e10\u4ece\u9ad8\u6e29\u5411\u4f4e\u6e29\u8f6c\u53d8\u6e29\u5ea6\u5206\u5e03\uff0c\u4ee5\u4f18\u5316\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u989d\u5916\u6743\u91cd\u66f4\u65b0\u6216\u9a8c\u8bc1\uff0c\u4ec5\u901a\u8fc7\u91c7\u6837\u4f18\u5316\u5c31\u80fd\u4ece\u57fa\u7840\u6a21\u578b\u4e2d\u6062\u590d\u5f3a\u5927\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002\u9000\u706b\u6280\u672f\u7684\u52a0\u5165\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7406\u7406\u8bba\u6027\u80fd\uff0c\u4f18\u4e8e\u56fa\u5b9a\u6e29\u5ea6\u7684power sampling\u3002", "conclusion": "\u57fa\u4e8e\u91c7\u6837\u7684\u4f18\u5316\u65b9\u6cd5\u4e3a\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u6f5c\u5728\u80fd\u529b\u63d0\u4f9b\u4e86\u5f3a\u5927\u9014\u5f84\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u6311\u6218\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u4ec5\u80fd\u4f18\u5316\u8868\u9762\u5408\u7406\u6027\u7684\u4f20\u7edf\u89c2\u70b9\u3002"}}
{"id": "2601.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12020", "abs": "https://arxiv.org/abs/2601.12020", "authors": ["Guillermo Figueroa-Araneda", "Iris Diana Jimenez", "Florian Hofherr", "Manny Ko", "Hector Andrade-Loarca", "Daniel Cremers"], "title": "DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering", "comment": null, "summary": "Subsurface scattering (SSS) gives translucent materials -- such as wax, jade, marble, and skin -- their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).\n  We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision -- even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.\n  Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.", "AI": {"tldr": "DIAMOND-SSS\uff1a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u4ec5\u9700\u6781\u5c11\u56fe\u50cf\uff08\u5c11\u81f310\u5f20\uff09\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u534a\u900f\u660e\u6750\u8d28\u91cd\u5efa\uff0c\u51cf\u5c1190%\u771f\u5b9e\u91c7\u96c6\u9700\u6c42", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u91cd\u5efa\u534a\u900f\u660e\u6750\u8d28\u9700\u8981\u5bc6\u96c6\u7684\u591a\u89c6\u89d2\u3001\u591a\u5149\u7167\u6570\u636e\u96c6\uff08\u901a\u5e38\u8d85\u8fc7100\u4e2a\u89c6\u89d2\u548c112\u4e2aOLAT\uff09\uff0c\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "1) \u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u751f\u6210\uff0c\u57fa\u4e8e\u4f30\u8ba1\u7684\u51e0\u4f55\u4fe1\u606f\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\u548c\u91cd\u5149\u7167\uff0c\u4ec5\u9700\u4e0d\u52307%\u7684\u6570\u636e\u96c6\u8bad\u7ec3\uff1b2) \u5f15\u5165\u5149\u7167\u65e0\u5173\u7684\u51e0\u4f55\u5148\u9a8c\uff1a\u591a\u89c6\u89d2\u8f6e\u5ed3\u4e00\u81f4\u6027\u635f\u5931\u548c\u591a\u89c6\u89d2\u6df1\u5ea6\u4e00\u81f4\u6027\u635f\u5931\uff0c\u7a33\u5b9a\u7a00\u758f\u6216\u5408\u6210\u76d1\u7763\u4e0b\u7684\u91cd\u5efa", "result": "\u5728\u6240\u6709\u7a00\u758f\u5ea6\u4e0b\uff0cDIAMOND-SSS\u5728\u53ef\u91cd\u5149\u7167\u9ad8\u65af\u6e32\u67d3\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u8d28\u91cf\uff0c\u76f8\u6bd4SSS-3DGS\u51cf\u5c11\u9ad8\u8fbe90%\u7684\u771f\u5b9e\u91c7\u96c6\u9700\u6c42\uff0c\u4ec5\u970010\u5f20\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa", "conclusion": "DIAMOND-SSS\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u589e\u5f3a\u548c\u51e0\u4f55\u5148\u9a8c\u7ea6\u675f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u534a\u900f\u660e\u6750\u8d28\u91cd\u5efa\u7684\u6570\u636e\u9700\u6c42\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12744", "categories": ["cs.AI", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.12744", "abs": "https://arxiv.org/abs/2601.12744", "authors": ["Tasnim Ahmed", "Yifan Zhu", "Salimur Choudhury"], "title": "Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks", "comment": "Accepted for presentation at The IEEE International Conference on Communications (ICC) 2026", "summary": "Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.", "AI": {"tldr": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u7f51\u7edc\u8349\u56fe\u751f\u6210\u4f18\u5316\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u89c6\u89c9\u53c2\u6570\u63d0\u53d6\u4f1a\u964d\u4f4e\u6267\u884c\u6210\u529f\u7387\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u8fdc\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u7f51\u7edc\u4ece\u4e1a\u8005\u901a\u5e38\u901a\u8fc7\u56fe\u8868\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5047\u8bbe\u57fa\u4e8e\u6587\u672c\u7684\u610f\u56fe\u8868\u8fbe\uff0c\u9700\u8981\u64cd\u4f5c\u5458\u7528\u6587\u5b57\u63cf\u8ff0\u62d3\u6251\u548c\u53c2\u6570\u3002\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5904\u7406\u5e26\u6ce8\u91ca\u7684\u7f51\u7edc\u8349\u56fe\u751f\u6210\u6b63\u786e\u7684\u4f18\u5316\u4ee3\u7801\u4ecd\u672a\u88ab\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86IntentOpt\u57fa\u51c6\uff0c\u5305\u542b17\u4e2a\u7c7b\u522b\u768485\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u5bf9\u591a\u6a21\u6001\u4e0e\u7eaf\u6587\u672c\u8f93\u5165\u7684\u8868\u73b0\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u5c06VLM\u751f\u6210\u7684\u4ee3\u7801\u90e8\u7f72\u5230\u7f51\u7edc\u6d4b\u8bd5\u5e8a\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "result": "\u89c6\u89c9\u53c2\u6570\u63d0\u53d6\u4f7f\u6267\u884c\u6210\u529f\u7387\u964d\u4f4e12-21\u4e2a\u767e\u5206\u70b9\uff0cGPT-5-Mini\u4ece93%\u964d\u81f372%\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u4f7f\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe13\u4e2a\u767e\u5206\u70b9\u3002\u5f00\u6e90\u6a21\u578b\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0cLlama-3.2-11B-Vision\u4ec5\u8fbe\u523018%\uff0c\u800cGPT-5-Mini\u8fbe\u523075%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u786e\u7acb\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u610f\u56fe\u7684\u7f51\u7edc\u7cfb\u7edf\u4e2d\u751f\u6210\u4f18\u5316\u4ee3\u7801\u7684\u57fa\u672c\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2601.12286", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12286", "abs": "https://arxiv.org/abs/2601.12286", "authors": ["Jonathan Pan"], "title": "Conversational Context Classification: A Representation Engineering Approach", "comment": null, "summary": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.", "AI": {"tldr": "\u4f7f\u7528\u8868\u5f81\u5de5\u7a0b\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5728LLM\u5185\u90e8\u72b6\u6001\u4e2d\u8bc6\u522b\u7279\u5b9a\u4e0a\u4e0b\u6587\u5b50\u7a7a\u95f4\uff0c\u7528\u4e8e\u68c0\u6d4b\u5bf9\u8bdd\u662f\u5426\u504f\u79bb\u9884\u671f\u4e3b\u9898", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u9700\u8981\u6709\u6548\u4fdd\u969c\u5176\u8fd0\u884c\uff0c\u7279\u522b\u662f\u68c0\u6d4b\u5176\u751f\u6210\u8131\u79bb\u4e0a\u4e0b\u6587\u56de\u590d\u7684\u503e\u5411\u3002\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u4e0a\u4e0b\u6587\u8bed\u4e49\u4e2d\u3002", "method": "\u7ed3\u5408\u8868\u5f81\u5de5\u7a0b\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff0c\u5728LLM\u5185\u90e8\u72b6\u6001\u4e2d\u8bc6\u522b\u4ee3\u8868\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u5b50\u7a7a\u95f4\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u8bad\u7ec3OCSVM\uff0c\u5728\u9690\u85cf\u72b6\u6001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u7acb\u9c81\u68d2\u8fb9\u754c\uff0c\u5e76\u8bc6\u522b\u4e0e\u76ee\u6807\u4e0a\u4e0b\u6587\u5f3a\u76f8\u5173\u7684LLM\u5185\u90e8\u72b6\u6001\u6700\u4f73\u5c42\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u8bc6\u522b\u7279\u5b9a\u4e0a\u4e0b\u6587\u5b50\u7a7a\u95f4\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5bf9\u8bdd\u662f\u5426\u5728\u9884\u671f\u4e0a\u4e0b\u6587\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6709\u52a9\u4e8e\u68c0\u6d4b\u5bf9\u8bdd\u662f\u5426\u504f\u79bb\u4e0a\u4e0b\u6587\uff0c\u8fd8\u4e3a\u66f4\u597d\u5730\u89e3\u91caLLM\u63d0\u4f9b\u4e86\u7814\u7a76\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u5728LLM\u5185\u90e8\u72b6\u6001\u4e2d\u8bc6\u522b\u4e0a\u4e0b\u6587\u7279\u5b9a\u5b50\u7a7a\u95f4\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.12049", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12049", "abs": "https://arxiv.org/abs/2601.12049", "authors": ["Chenchen Zhao", "Muxi Chen", "Qiang Xu"], "title": "\\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions", "comment": "12 pages, 13 figures", "summary": "Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic's capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models.", "AI": {"tldr": "FocaLogic\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u8868\u793a\u6765\u91cf\u5316\u89e3\u7801\u89c6\u89c9\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u5173\u952e\u89c6\u89c9\u533a\u57df\u5e76\u8f6c\u5316\u4e3a\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u767d\u76d2\u6a21\u578b\u8bbf\u95ee\uff0c\u8981\u4e48\u7f3a\u4e4f\u5b9a\u91cf\u4e25\u8c28\u6027\uff0c\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "FocaLogic\u8bc6\u522b\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u7684\u6700\u5c0f\u53ef\u89e3\u91ca\u89c6\u89c9\u533a\u57df\u5b50\u96c6\uff08\u89c6\u89c9\u7126\u70b9\uff09\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u7cbe\u786e\u7d27\u51d1\u7684\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u7126\u70b9\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u53d1\u6563\u5ea6\u7b49\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660eFocaLogic\u80fd\u591f\u63ed\u793a\u8bad\u7ec3\u8bf1\u5bfc\u7684\u96c6\u4e2d\u6027\u3001\u901a\u8fc7\u6cdb\u5316\u63d0\u9ad8\u7126\u70b9\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5728\u504f\u89c1\u548c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u5f02\u5e38\u7126\u70b9\u7b49\u5173\u952e\u6d1e\u5bdf\u3002", "conclusion": "FocaLogic\u4e3a\u89e3\u91ca\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b9a\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12781", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12781", "abs": "https://arxiv.org/abs/2601.12781", "authors": ["Hyejin Park", "Junhyuk Kwon", "Suha Kwak", "Jungseul Ok"], "title": "VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension", "comment": null, "summary": "Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.", "AI": {"tldr": "VIRO\u6846\u67b6\u901a\u8fc7\u5d4c\u5165\u8f7b\u91cf\u7ea7\u64cd\u4f5c\u7ea7\u9a8c\u8bc1\u5668\u6765\u89e3\u51b3\u795e\u7ecf\u7b26\u53f7REC\u4e2d\u7684\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u5728\u76ee\u6807\u5b58\u5728\u548c\u65e0\u76ee\u6807\u573a\u666f\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7REC\u65b9\u6cd5\u5047\u8bbe\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u51c6\u786e\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7ea7\u8054\u9519\u8bef\uff1a\u9519\u8bef\u68c0\u6d4b\u548c\u65e0\u6548\u5173\u7cfb\u5728\u63a8\u7406\u94fe\u4e2d\u4f20\u64ad\uff0c\u5373\u4f7f\u56fe\u50cf\u4e2d\u6ca1\u6709\u76ee\u6807\u4e5f\u4f1a\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u8bef\u62a5", "method": "\u5f15\u5165\u9a8c\u8bc1\u96c6\u6210\u63a8\u7406\u64cd\u4f5c\u7b26\uff08VIRO\uff09\u6846\u67b6\uff0c\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u5d4c\u5165\u8f7b\u91cf\u7ea7\u64cd\u4f5c\u7ea7\u9a8c\u8bc1\u5668\uff0c\u6bcf\u4e2a\u64cd\u4f5c\u7b26\u6267\u884c\u5e76\u9a8c\u8bc1\u5176\u8f93\u51fa\uff08\u5982\u5bf9\u8c61\u5b58\u5728\u6027\u6216\u7a7a\u95f4\u5173\u7cfb\uff09\uff0c\u4ece\u800c\u5728\u9a8c\u8bc1\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u9c81\u68d2\u5904\u7406\u65e0\u76ee\u6807\u60c5\u51b5", "result": "\u5728\u76ee\u6807\u5b58\u5728\u548c\u65e0\u76ee\u6807\u8bbe\u7f6e\u4e0b\u8fbe\u523061.1%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u7b2c\u4e00\u4eba\u79f0\u6570\u636e\u4e0a\u5c55\u793a\u6cdb\u5316\u80fd\u529b\uff1b\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u7a0b\u5e8f\u5931\u8d25\u7387\u4f4e\u4e8e0.3%\uff0c\u901a\u8fc7\u89e3\u8026\u7a0b\u5e8f\u751f\u6210\u4e0e\u6267\u884c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027", "conclusion": "VIRO\u901a\u8fc7\u96c6\u6210\u64cd\u4f5c\u7ea7\u9a8c\u8bc1\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7b26\u53f7REC\u4e2d\u7684\u7ea7\u8054\u9519\u8bef\u95ee\u9898\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12369", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12369", "abs": "https://arxiv.org/abs/2601.12369", "authors": ["Ming Zhang", "Jiabao Zhuang", "Wenqing Jing", "Ziyu Kong", "Jingyi Deng", "Yujiong Shen", "Kexin Tan", "Yuhang Zhao", "Ning Luo", "Renzhe Zheng", "Jiahui Lin", "Mingqi Wu", "Long Ma", "Yi Zou", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies", "comment": null, "summary": "Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.", "AI": {"tldr": "TaxoBench\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u81ea\u52a8\u751f\u6210\u7efc\u8ff0\u65f6\u7684\u6838\u5fc3\u80fd\u529b\uff1a\u68c0\u7d22\u5173\u952e\u8bba\u6587\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u77e5\u8bc6\u7ed3\u6784\u3002\u57fa\u4e8e72\u7bc7\u9ad8\u88ab\u5f15\u8ba1\u7b97\u673a\u79d1\u5b66\u7efc\u8ff0\u6784\u5efa\uff0c\u5305\u542b3,815\u4e2a\u7cbe\u786e\u5206\u7c7b\u7684\u5f15\u7528\u4f5c\u4e3a\u57fa\u51c6\u3002\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6700\u4f73\u4ee3\u7406\u4ec5\u80fd\u53ec\u56de20.9%\u7684\u4e13\u5bb6\u9009\u62e9\u8bba\u6587\uff0c\u7ec4\u7ec7\u80fd\u529b\u4e5f\u6709\u9650\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u88ab\u5e7f\u6cdb\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u7efc\u8ff0\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e13\u5bb6\u4e00\u6837\u64b0\u5199\u7efc\u8ff0\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6d41\u7545\u6027\u6216\u5f15\u7528\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u6838\u5fc3\u80fd\u529b\uff08\u68c0\u7d22\u5173\u952e\u8bba\u6587\u548c\u7ec4\u7ec7\u77e5\u8bc6\u7ed3\u6784\uff09\u7684\u8bc4\u4f30\u3002", "method": "\u4ece72\u7bc7\u9ad8\u88ab\u5f15\u8ba1\u7b97\u673a\u79d1\u5b66\u7efc\u8ff0\u4e2d\u624b\u52a8\u63d0\u53d6\u4e13\u5bb6\u6784\u5efa\u7684\u5206\u7c7b\u6811\uff0c\u5305\u542b3,815\u4e2a\u7cbe\u786e\u5206\u7c7b\u7684\u5f15\u7528\u4f5c\u4e3a\u57fa\u51c6\u3002\u652f\u6301\u4e24\u79cd\u8bc4\u4f30\u6a21\u5f0f\uff1a\u6df1\u5ea6\u7814\u7a76\u6a21\u5f0f\uff08\u7aef\u5230\u7aef\u68c0\u7d22\u548c\u7ec4\u7ec7\uff09\u548c\u81ea\u5e95\u5411\u4e0a\u6a21\u5f0f\uff08\u9694\u79bb\u7ec4\u7ec7\u7ed3\u6784\u80fd\u529b\uff09\u3002\u8bc4\u4f30\u4e867\u4e2a\u9886\u5148\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548c12\u4e2a\u524d\u6cbfLLM\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u53cc\u91cd\u74f6\u9888\uff1a\u6700\u4f73\u4ee3\u7406\u4ec5\u80fd\u53ec\u56de20.9%\u7684\u4e13\u5bb6\u9009\u62e9\u8bba\u6587\uff1b\u5373\u4f7f\u5728\u5b8c\u7f8e\u8f93\u5165\u6761\u4ef6\u4e0b\uff0c\u6700\u4f73\u6a21\u578b\u5728\u7ec4\u7ec7\u7ed3\u6784\u65b9\u9762\u4ec5\u8fbe\u52300.31 ARI\uff08\u8c03\u6574\u5170\u5fb7\u6307\u6570\uff09\u3002\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u8ddd\u79bb\u4e13\u5bb6\u7ea7\u7efc\u8ff0\u64b0\u5199\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\u3002", "conclusion": "TaxoBench\u586b\u8865\u4e86\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u6838\u5fc3\u80fd\u529b\u7684\u7a7a\u767d\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u5f53\u524d\u4ee3\u7406\u5728\u68c0\u7d22\u5173\u952e\u8bba\u6587\u548c\u7ec4\u7ec7\u77e5\u8bc6\u7ed3\u6784\u65b9\u9762\u4ecd\u8fdc\u672a\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u3002\u8be5\u57fa\u51c6\u516c\u5f00\u53ef\u7528\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5f3a\u5927\u7684\u81ea\u52a8\u7efc\u8ff0\u751f\u6210\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.12051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12051", "abs": "https://arxiv.org/abs/2601.12051", "authors": ["Weixin Ye", "Wei Wang", "Yahui Liu", "Yue Song", "Bin Ren", "Wei Bi", "Rita Cucchiara", "Nicu Sebe"], "title": "A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models", "comment": "9 figures, 12 tables", "summary": "In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \\textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack", "AI": {"tldr": "\u63d0\u51faMJP\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u6253\u4e71token\u987a\u5e8f\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u6765\u63a9\u76d6\u4f4d\u7f6e\u4fe1\u606f\uff0c\u589e\u5f3aTransformer\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\uff0cTransformer\u9762\u4e34\u68af\u5ea6\u653b\u51fb\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u4f4d\u7f6e\u5d4c\u5165\u7684\u68af\u5ea6\u5305\u542b\u8db3\u591f\u4fe1\u606f\u53ef\u7528\u4e8e\u91cd\u5efa\u8f93\u5165\u6570\u636e\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMasked Jigsaw Puzzle (MJP)\u6846\u67b6\uff1a1) \u968f\u673a\u6253\u4e71token\u987a\u5e8f\u4ee5\u7834\u574ftoken\u987a\u5e8f\uff1b2) \u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u672a\u77e5\u4f4d\u7f6e\u5d4c\u5165\u6765\u63a9\u76d6\u88ab\u6253\u4e71token\u7684\u4f4d\u7f6e\u4fe1\u606f\uff1b3) \u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u4e0d\u4f9d\u8d56\u5c40\u90e8\u7a7a\u95f4\u4fe1\u606f\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMJP\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u6a21\u578b\u5bf9\u68af\u5ea6\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u80fd\u63d0\u5347\u5728\u56fe\u50cf\u5206\u7c7b\uff08\u5982ImageNet-1K\uff09\u548c\u6587\u672c\u60c5\u611f\u5206\u6790\uff08\u5982Yelp\u548cAmazon\uff09\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "MJP\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2dTransformer\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u548c\u6027\u80fd\u63d0\u5347\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u591a\u79cdTransformer\u6a21\u578b\u548c\u4efb\u52a1\u9886\u57df\u3002"}}
{"id": "2601.12804", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12804", "abs": "https://arxiv.org/abs/2601.12804", "authors": ["Hanwei Zhang", "Luo Cheng", "Rui Wen", "Yang Zhang", "Lijun Zhang", "Holger Hermanns"], "title": "SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability", "comment": null, "summary": "Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.", "AI": {"tldr": "SL-CBM \u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u5c40\u90e8\u6027\u7ea6\u675f\uff0c\u6539\u8fdb\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u7a7a\u95f4\u5bf9\u9f50\u80fd\u529b\uff0c\u751f\u6210\u66f4\u5fe0\u5b9e\u7684\u6982\u5ff5\u548c\u7c7b\u522b\u663e\u8457\u6027\u56fe\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u5b58\u5728\u5c40\u90e8\u5fe0\u5b9e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5c06\u6982\u5ff5\u4e0e\u6709\u610f\u4e49\u7684\u56fe\u50cf\u533a\u57df\u8fdb\u884c\u7a7a\u95f4\u5bf9\u9f50\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u7a7a\u95f4\u4e00\u81f4\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSL-CBM\uff0c\u901a\u8fc7\u96c6\u62101x1\u5377\u79ef\u5c42\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u6982\u5ff5\u3001\u56fe\u50cf\u533a\u57df\u548c\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u91c7\u7528\u5bf9\u6bd4\u6027\u548c\u57fa\u4e8e\u71b5\u7684\u6b63\u5219\u5316\u6765\u5e73\u8861\u51c6\u786e\u6027\u3001\u7a00\u758f\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSL-CBM\u663e\u8457\u63d0\u9ad8\u4e86\u5c40\u90e8\u5fe0\u5b9e\u6027\u3001\u89e3\u91ca\u8d28\u91cf\u548c\u5e72\u9884\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "SL-CBM\u5f25\u5408\u4e86\u57fa\u4e8e\u6982\u5ff5\u7684\u63a8\u7406\u548c\u7a7a\u95f4\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u8d56\u7684\u6982\u5ff5\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2601.12374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12374", "abs": "https://arxiv.org/abs/2601.12374", "authors": ["Akram Elbouanani", "Aboubacar Tuo", "Adrian Popescu"], "title": "A Scalable Entity-Based Framework for Auditing Bias in LLMs", "comment": null, "summary": "Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u504f\u89c1\u5ba1\u8ba1\u6846\u67b6\uff0c\u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u4f5c\u4e3a\u63a2\u9488\u6d4b\u91cf\u6a21\u578b\u884c\u4e3a\u4e2d\u7684\u7ed3\u6784\u6027\u5dee\u5f02\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u5927\u89c4\u6a21\u5206\u6790\uff0c\u53d1\u73b0LLMs\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1", "motivation": "\u73b0\u6709LLM\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u751f\u6001\u6548\u5ea6\u4e0e\u7edf\u8ba1\u63a7\u5236\u7684\u6743\u8861\uff1a\u4eba\u5de5\u63d0\u793a\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f7f\u7528\u573a\u666f\uff0c\u81ea\u7136\u4efb\u52a1\u7f3a\u4e4f\u89c4\u6a21\u548c\u4e25\u8c28\u6027\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u504f\u89c1\u5ba1\u8ba1\u6846\u67b6", "method": "\u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u4f5c\u4e3a\u63a2\u9488\u6d4b\u91cf\u6a21\u578b\u884c\u4e3a\u4e2d\u7684\u7ed3\u6784\u6027\u5dee\u5f02\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u53ef\u9760\u590d\u73b0\u81ea\u7136\u6587\u672c\u4e2d\u7684\u504f\u89c1\u6a21\u5f0f\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u5206\u6790", "result": "\u8fdb\u884c\u4e86\u8fc4\u4eca\u6700\u5927\u7684\u504f\u89c1\u5ba1\u8ba1\uff0819\u4ebf\u6570\u636e\u70b9\uff09\uff0c\u53d1\u73b0\u7cfb\u7edf\u6027\u504f\u89c1\uff1a\u60e9\u7f5a\u53f3\u7ffc\u653f\u5ba2\u3001\u504f\u597d\u5de6\u7ffc\u653f\u5ba2\u3001\u504f\u597d\u897f\u65b9\u548c\u5bcc\u88d5\u56fd\u5bb6\u800c\u975e\u5168\u7403\u5357\u65b9\u3001\u504f\u597d\u897f\u65b9\u516c\u53f8\u3001\u60e9\u7f5a\u56fd\u9632\u548c\u5236\u836f\u4f01\u4e1a\uff1b\u6307\u4ee4\u5fae\u8c03\u51cf\u5c11\u504f\u89c1\u4f46\u6a21\u578b\u89c4\u6a21\u589e\u5927\u4f1a\u653e\u5927\u504f\u89c1\uff0c\u4e2d\u6587\u6216\u4fc4\u6587\u63d0\u793a\u4e0d\u4f1a\u51cf\u5f31\u897f\u65b9\u504f\u597d", "conclusion": "LLMs\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u90e8\u7f72\u524d\u5e94\u8fdb\u884c\u4e25\u683c\u5ba1\u8ba1\uff0c\u5f53\u524d\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\u9700\u8981\u89e3\u51b3"}}
{"id": "2601.12052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12052", "abs": "https://arxiv.org/abs/2601.12052", "authors": ["Zaiyan Zhang", "Jie Li", "Shaowei Shi", "Qiangqiang Yuan"], "title": "Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation", "comment": "Submitted to IGARSS 2026 Conference", "summary": "Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15\\% of the parameters, and achieves a 1.4\\% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.", "AI": {"tldr": "TDP-CR\u662f\u4e00\u4e2a\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u4e91\u53bb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u7f16\u7801\u4e91\u5c42\u539a\u5ea6\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u8054\u5408\u6267\u884c\u4e91\u53bb\u9664\u548c\u571f\u5730\u8986\u76d6\u5206\u5272\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u4e91\u53bb\u9664\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u4f4e\u5c42\u4fdd\u771f\u5ea6\uff0c\u5bb9\u6613\u8fc7\u5ea6\u5e73\u6ed1\u7eb9\u7406\u548c\u8fb9\u754c\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u6062\u590d\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002\u5149\u5b66\u9065\u611f\u5f71\u50cf\u5bf9\u5730\u7403\u89c2\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6301\u7eed\u7684\u4e91\u906e\u6321\u9650\u5236\u4e86\u5176\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u63d0\u51faTDP-CR\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u63d0\u793a\u5f15\u5bfc\u878d\u5408\u673a\u5236\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9000\u5316\u63d0\u793a\u7f16\u7801\u4e91\u539a\u5ea6\u548c\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u5168\u5c40\u901a\u9053\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u63d0\u793a\u6761\u4ef6\u7a7a\u95f4\u504f\u7f6e\uff0c\u81ea\u9002\u5e94\u5730\u96c6\u6210\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u4fe1\u606f\u3002\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u8026\u91cd\u5efa\u548c\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728LuojiaSET-OSFCR\u6570\u636e\u96c6\u4e0a\uff0cTDP-CR\u5728PSNR\u4e0a\u8d85\u8fc7\u6700\u5148\u8fdb\u57fa\u7ebf0.18dB\uff0c\u4ec5\u4f7f\u752815%\u7684\u53c2\u6570\uff1b\u5728mIoU\u4e0a\u6bd4\u591a\u4efb\u52a1\u7ade\u4e89\u5bf9\u624b\u6301\u7eed\u63d0\u53471.4%\uff0c\u6709\u6548\u63d0\u4f9b\u5206\u6790\u5c31\u7eea\u6570\u636e\u3002", "conclusion": "TDP-CR\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u6a21\u6001\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0e\u8bed\u4e49\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u4e91\u53bb\u9664\u548c\u571f\u5730\u8986\u76d6\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5206\u6790\u5c31\u7eea\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12822", "abs": "https://arxiv.org/abs/2601.12822", "authors": ["Wenqi Zhang", "Yulin Shen", "Changyue Jiang", "Jiarun Dai", "Geng Hong", "Xudong Pan"], "title": "MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction", "comment": null, "summary": "Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.", "AI": {"tldr": "MirrorGuard\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u63d0\u5347\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u5728\u4fdd\u6301\u4ee3\u7406\u5b9e\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u96c6\u6210\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u80fd\u591f\u901a\u8fc7GUI\u81ea\u4e3b\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4ea4\u4e92\uff0c\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u8fd9\u79cd\u81ea\u4e3b\u6027\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff1a\u6076\u610f\u6307\u4ee4\u6216\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u53ef\u80fd\u89e6\u53d1\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u5e76\u5bfc\u81f4\u6709\u5bb3\u7684\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u68c0\u6d4b\u7684\u963b\u6b62\uff09\u867d\u7136\u80fd\u9632\u6b62\u635f\u5bb3\uff0c\u4f46\u5e38\u5e38\u8fc7\u65e9\u4e2d\u6b62\u4efb\u52a1\uff0c\u964d\u4f4e\u4e86\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faMirrorGuard\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u62df\u8bad\u7ec3\u65b9\u6cd5\u3002\u4e3a\u4e86\u964d\u4f4e\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u6210\u672c\uff0c\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6a21\u62df\u7ba1\u9053\uff0c\u5728\u7eaf\u6587\u672c\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u771f\u5b9e\u7684\u9ad8\u98ce\u9669GUI\u4ea4\u4e92\u8f68\u8ff9\uff0c\u6355\u83b7\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u6a21\u5f0f\u548c\u6f5c\u5728\u7cfb\u7edf\u5371\u5bb3\uff0c\u800c\u65e0\u9700\u6267\u884c\u771f\u5b9e\u64cd\u4f5c\u3002\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0cMirrorGuard\u5b66\u4e60\u5728CUAs\u4ea7\u751f\u548c\u6267\u884c\u4e0d\u5b89\u5168\u64cd\u4f5c\u4e4b\u524d\u62e6\u622a\u5e76\u7ea0\u6b63\u5176\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u548cCUA\u67b6\u6784\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0cMirrorGuard\u663e\u8457\u964d\u4f4e\u4e86\u5b89\u5168\u98ce\u9669\u3002\u4f8b\u5982\uff0c\u5728\u5b57\u8282\u8df3\u52a8\u7684UI-TARS\u7cfb\u7edf\u4e0a\uff0c\u5c06\u4e0d\u5b89\u5168\u7387\u4ece66.5%\u964d\u81f313.0%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u9519\u8bef\u62d2\u7edd\u7387\uff08FRR\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6700\u5148\u8fdb\u7684GuardAgent\u4ec5\u5c06\u4e0d\u5b89\u5168\u7387\u964d\u81f353.9%\uff0c\u4e14FRR\u9ad8\u51fa15.4%\u3002", "conclusion": "\u6a21\u62df\u884d\u751f\u7684\u9632\u5fa1\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u4ee3\u7406\u57fa\u672c\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u73b0\u5b9e\u4e16\u754c\u4fdd\u62a4\u3002MirrorGuard\u8bc1\u660e\u4e86\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u5347CUA\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.12376", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12376", "abs": "https://arxiv.org/abs/2601.12376", "authors": ["Ofek Raban", "Ethan Fetaya", "Gal Chechik"], "title": "LR-DWM: Efficient Watermarking for Diffusion Language Models", "comment": "Submitted to ACL Rolling Review (ARR). 7 pages, 4 figures", "summary": "Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.", "AI": {"tldr": "\u63d0\u51faLR-DWM\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u6c34\u5370\u65b9\u6848\uff0c\u901a\u8fc7\u5de6\u53f3\u90bb\u5c45\u4fe1\u606f\u5d4c\u5165\u6c34\u5370\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\uff0c\u4e0d\u9002\u7528\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u975e\u987a\u5e8f\u8fed\u4ee3\u53bb\u566a\u751f\u6210\u8fc7\u7a0b\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u8ba1\u7b97\u6216\u5185\u5b58\u5f00\u9500\u95ee\u9898", "method": "\u63d0\u51fa\u5de6\u53f3\u6269\u6563\u6c34\u5370\uff08LR-DWM\uff09\u65b9\u6848\uff0c\u5f53\u5de6\u53f3\u90bb\u5c45\u53ef\u7528\u65f6\uff0c\u57fa\u4e8e\u5de6\u53f3\u90bb\u5c45\u4fe1\u606f\u5bf9\u751f\u6210token\u8fdb\u884c\u504f\u7f6e\uff0c\u5b9e\u73b0\u6c34\u5370\u5d4c\u5165", "result": "LR-DWM\u5728\u63a5\u8fd1\u975e\u6c34\u5370\u57fa\u7ebf\u6269\u6563\u6a21\u578b\u7684\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u5f00\u9500\u4e0b\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6c34\u5370\u68c0\u6d4b\uff0c\u5728\u6807\u51c6\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u9ad8\u53ef\u68c0\u6d4b\u6027", "conclusion": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7LR-DWM\u65b9\u6cd5\u9ad8\u6548\u5b9e\u73b0\u6c34\u5370\uff0c\u5728\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u4e0b\u83b7\u5f97\u9ad8\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2601.12055", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12055", "abs": "https://arxiv.org/abs/2601.12055", "authors": ["Lina Meyer", "Felix Wissel", "Tobias Knopp", "Susanne Pfefferle", "Ralf Fliegert", "Maximilian Sandmann", "Liana Uebler", "Franziska M\u00f6ckl", "Bj\u00f6rn-Philipp Diercks", "David Lohr", "Ren\u00e9 Werner"], "title": "Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer", "comment": null, "summary": "Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP.", "AI": {"tldr": "AUTO-DIP\uff1a\u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u53c2\u6570\u81ea\u52a8\u8fc1\u79fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\uff0c\u65e0\u9700\u4e3a\u6bcf\u5f20\u65b0\u56fe\u50cf\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u505c\u6b62\u70b9\u3002", "motivation": "\u4f20\u7edf\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff09\u9700\u8981\u4e3a\u6bcf\u5f20\u65b0\u56fe\u50cf\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u8fed\u4ee3\u505c\u6b62\u70b9\uff0c\u8017\u65f6\u4e14\u9650\u5236\u4e86\u5728\u9700\u8981\u5904\u7406\u5927\u91cf\u56fe\u50cf\u7684\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u4f5c\u8005\u5047\u8bbe\u76f8\u4f3c\u7684\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u5177\u6709\u53ef\u5171\u4eab\u7684\u6700\u4f18DIP\u53c2\u6570\u914d\u7f6e\u3002", "method": "1. \u4ece\u5f00\u6e90\u6570\u636e\u96c6\u751f\u6210\u6821\u51c6\u96c6\uff08110\u5f20\uff09\u548c\u9a8c\u8bc1\u96c6\uff0855\u5f20\uff09\u8bed\u4e49\u4e0d\u540c\u7684\u56fe\u50cf\uff1b2. \u9488\u5bf9\u7406\u60f3U-net\u67b6\u6784\u548c\u505c\u6b62\u70b9\u8fdb\u884c\u7f51\u7edc\u67b6\u6784\u641c\u7d22\uff1b3. \u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\uff08\u663e\u5fae\u955c\u7c7b\u578b\u3001\u6210\u50cf\u6837\u672c\u7b49\uff09\u800c\u975e\u5b9a\u91cf\u56fe\u50cf\u76f8\u4f3c\u6027\u5ea6\u91cf\u8fdb\u884c\u53c2\u6570\u8fc1\u79fb\uff1b4. \u5b9e\u73b0AUTO-DIP\u81ea\u52a8\u53c2\u6570\u8fc1\u79fb\u7ba1\u9053\u3002", "result": "1. \u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u53c2\u6570\u8fc1\u79fb\u6548\u679c\u4f18\u4e8e\u57fa\u4e8e\u5b9a\u91cf\u56fe\u50cf\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u65b9\u6cd5\uff1b2. AUTO-DIP\u5728\u591a\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5f00\u6e90\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebfDIP\uff08\u539f\u59cbDIP\u53c2\u6570\uff09\u548c\u53d8\u5206\u53bb\u566a\u65b9\u6cd5\uff1b3. \u5728\u975e\u5e38\u5608\u6742\u7684\u8f93\u5165\u56fe\u50cf\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff1b4. \u5728\u672c\u5730\u83b7\u53d6\u7684\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u4e0a\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86AUTO-DIP\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "AUTO-DIP\u901a\u8fc7\u57fa\u4e8e\u56fe\u50cf\u5143\u6570\u636e\u76f8\u4f3c\u6027\u7684\u53c2\u6570\u81ea\u52a8\u8fc1\u79fb\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4f18\u5316\u7684DIP\u53bb\u566a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8367\u5149\u663e\u5fae\u955c\u56fe\u50cf\u53bb\u566a\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u56fe\u50cf\u548c\u9ad8\u566a\u58f0\u573a\u666f\u65f6\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2601.12842", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12842", "abs": "https://arxiv.org/abs/2601.12842", "authors": ["Qitong Fang", "Haotian Li", "Xu Wang"], "title": "SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning", "comment": "11 pages, 3 figures. Equal contribution: Qitong Fang and Haotian Li. Corresponding authors: Qitong Fang (fangqitong@student.jlju.edu.cn), Haotian Li (lihaotian@student.jlju.edu.cn), Xu Wang (wangxu@jlju.edu.cn)", "summary": "Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.", "AI": {"tldr": "SCULPT\u662f\u4e00\u79cd\u7ea6\u675f\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u8bc4\u5206\u548c\u526a\u679d\u6765\u63d0\u5347LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u63a8\u7406\u7a33\u5b9a\u6027", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u641c\u7d22\u7b56\u7565\u4f9d\u8d56\u968f\u673a\u63a2\u7d22\uff0c\u7ecf\u5e38\u904d\u5386\u4e0d\u5408\u7406\u7684\u63a8\u7406\u5206\u652f\uff0c\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u4ece\u901a\u7528\u63d0\u793a\u6216\u5f31\u9886\u57df\u5148\u9a8c\u7684\u7b56\u7565\u4e2d\u91c7\u6837\u5019\u9009\u6b65\u9aa4\uff0c\u5bfc\u81f4\u5728\u64cd\u4f5c\u7b26\u3001\u5355\u4f4d\u548c\u683c\u5f0f\u4e0a\u7684\u63a5\u8fd1\u968f\u673a\u6e38\u8d70", "method": "SCULPT\u5c06\u9886\u57df\u611f\u77e5\u8bc4\u5206\u96c6\u6210\u5230MCTS\u7684\u9009\u62e9\u3001\u6269\u5c55\u3001\u6a21\u62df\u548c\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\uff0c\u901a\u8fc7\u7b26\u53f7\u68c0\u67e5\uff08\u7ef4\u5ea6\u4e00\u81f4\u6027\u3001\u7c7b\u578b\u517c\u5bb9\u6027\u3001\u5e45\u5ea6\u5408\u7406\u6027\u3001\u6df1\u5ea6\u63a7\u5236\u548c\u591a\u6837\u6027\uff09\u548c\u7ed3\u6784\u6a21\u5f0f\u6307\u5bfc\u6765\u8bc4\u5206\u548c\u526a\u679d\u52a8\u4f5c", "result": "\u5728\u5339\u914d\u7684LLM\u914d\u7f6e\u4e0b\uff0cSCULPT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e26\u6765\u7a33\u5b9a\u6539\u8fdb\uff1b\u4f7f\u7528GPT-5.2\u7684\u989d\u5916\u7ed3\u679c\u8bc4\u4f30\u4e86\u6267\u884c\u5668\u53ef\u8f6c\u79fb\u6027\u548c\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd", "conclusion": "\u9886\u57df\u611f\u77e5\u7ea6\u675f\u53ef\u4ee5\u5728\u4fdd\u6301\u6548\u7387\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027"}}
{"id": "2601.12389", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12389", "abs": "https://arxiv.org/abs/2601.12389", "authors": ["Lakshya Tomar", "Vinayak Abrol", "Puneet Agarwal"], "title": "NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages", "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.", "AI": {"tldr": "NADIR\u662f\u4e00\u4e2a\u65b0\u578b\u975e\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u97f3\u8bd1\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u540c\u65f6\u5b9e\u73b013\u500d\u52a0\u901f\uff0c\u663e\u8457\u51cf\u5c11\u5404\u7c7b\u9519\u8bef\u3002", "motivation": "\u8bb8\u591a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff08\u5982\u591a\u8bed\u8a00\u97f3\u8bd1\u3001\u4ee3\u7801\u91cd\u6784\u3001\u8bed\u6cd5\u6821\u6b63\u7b49\uff09\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u867d\u7136\u51c6\u786e\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u800c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u901f\u5ea6\u5feb\u4f46\u5b58\u5728\u5e7b\u89c9\u548c\u957f\u5ea6\u63a7\u5236\u95ee\u9898\u3002\u9700\u8981\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u63d0\u51faNADIR\u67b6\u6784\uff0c\u7ed3\u5408\u5dee\u5206\u53d8\u6362\u5668\u548c\u6df7\u5408\u4e13\u5bb6\u673a\u5236\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u5efa\u6a21\u590d\u6742\u5b57\u7b26\u6620\u5c04\u800c\u65e0\u9700\u5e8f\u5217\u4f9d\u8d56\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u8bed\u8a00\u97f3\u8bd1\u4efb\u52a1\u8bbe\u8ba1\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u57fa\u7ebf\uff0cNADIR\u5b9e\u73b0\u8d85\u8fc713\u500d\u52a0\u901f\uff0c\u5e73\u5747\u5b57\u7b26\u9519\u8bef\u7387\u4e3a15.78%\uff08\u81ea\u56de\u5f52\u6a21\u578b\u4e3a14.44%\uff0c\u6807\u51c6\u975e\u81ea\u56de\u5f52\u6a21\u578b\u4e3a21.88%\uff09\u3002\u663e\u8457\u51cf\u5c11\u91cd\u590d\u9519\u8bef49.53%\u3001\u66ff\u6362\u9519\u8bef24.45%\u3001\u7701\u7565\u9519\u8bef32.92%\u3001\u63d2\u5165\u9519\u8bef16.87%\u3002", "conclusion": "NADIR\u4e3a\u6784\u5efa\u5feb\u901f\u53ef\u9760\u7684\u975e\u81ea\u56de\u5f52\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u84dd\u56fe\uff0c\u6709\u6548\u5f25\u5408\u4e86\u81ea\u56de\u5f52\u51c6\u786e\u6027\u4e0e\u5b9e\u65f6\u5927\u89c4\u6a21\u90e8\u7f72\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.12062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12062", "abs": "https://arxiv.org/abs/2601.12062", "authors": ["Xiaomei Yang", "Xizhan Gao", "Antai Liu", "Kang Wei", "Fa Zhu", "Guang Feng", "Xiaofeng Qu", "Sijie Niu"], "title": "Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification", "comment": null, "summary": "The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features' discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.", "AI": {"tldr": "\u63d0\u51faLSMRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5e8f\u5217\u7ea7\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3VVI-ReID\u4e2d\u9ad8\u6548\u65f6\u7a7a\u5efa\u6a21\u3001\u5145\u5206\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u663e\u5f0f\u6a21\u6001\u7ea7\u635f\u5931\u6307\u5bfc\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u751f\u6210\u6a21\u6001\u5171\u4eab\u8bed\u8a00\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u9ad8\u6548\u65f6\u7a7a\u5efa\u6a21\u3001\u5145\u5206\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u663e\u5f0f\u6a21\u6001\u7ea7\u635f\u5931\u6307\u5bfc\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faLSMRL\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) STFL\u6a21\u5757\u57fa\u4e8eCLIP\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\u5b9e\u73b0\u9ad8\u6548\u65f6\u7a7a\u5efa\u6a21\uff1b2) SD\u6a21\u5757\u5c06\u6a21\u6001\u5171\u4eab\u8bed\u8a00\u63d0\u793a\u6269\u6563\u5230\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u7279\u5f81\u4e2d\u5efa\u7acb\u521d\u6b65\u6a21\u6001\u4e00\u81f4\u6027\uff1b3) CMI\u6a21\u5757\u5229\u7528\u53cc\u5411\u8de8\u6a21\u6001\u81ea\u6ce8\u610f\u529b\u6d88\u9664\u6b8b\u4f59\u6a21\u6001\u5dee\u8ddd\u5e76\u7ec6\u5316\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u3002\u5f15\u5165\u4e24\u79cd\u6a21\u6001\u7ea7\u635f\u5931\u589e\u5f3a\u7279\u5f81\u5224\u522b\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5927\u89c4\u6a21VVI-ReID\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLSMRL\u65b9\u6cd5\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LSMRL\u65b9\u6cd5\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5e8f\u5217\u7ea7\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86VVI-ReID\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002"}}
{"id": "2601.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12856", "abs": "https://arxiv.org/abs/2601.12856", "authors": ["Liping Huang", "Gaoxi Xiao", "Stefan Ma", "Hechang Chen", "Shisong Tang", "Flora Salim"], "title": "Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data", "comment": "9 pages, 9 figures. It's accepted by WWW 2026 Web4Good Track. To make accessible earlier, authors would like to put it on arxiv before the conference", "summary": "Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u516c\u5f00\u767b\u9769\u70ed\u75c5\u4f8b\u6570\u636e\u6316\u6398\u57ce\u5e02\u533a\u57df\u95f4\u6f5c\u5728\u4f20\u64ad\u94fe\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u9690\u85cf\u4f20\u64ad\u7f51\u7edc\uff0c\u5b9e\u73b0\u70ed\u70b9\u9884\u6d4b\u5e76\u9a8c\u8bc1\u4f20\u64ad\u6a21\u5f0f\u4e00\u81f4\u6027\uff0c\u5728\u65b0\u52a0\u5761\u6848\u4f8b\u4e2d\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u767b\u9769\u70ed\u5728\u70ed\u5e26\u57ce\u5e02\u5730\u533a\u6301\u7eed\u6784\u6210\u516c\u5171\u536b\u751f\u6311\u6218\uff0c\u9700\u8981\u4ece\u88ab\u52a8\u5e94\u5bf9\u8f6c\u5411\u4e3b\u52a8\u5e72\u9884\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u75c5\u4f8b\u89c6\u4e3a\u5b64\u7acb\u62a5\u544a\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u533a\u57df\u95f4\u7684\u4f20\u64ad\u52a8\u6001\u5173\u7cfb\u8fdb\u884c\u9884\u6d4b\u3002", "method": "\u5f00\u53d1\u65b0\u578b\u6846\u67b6\uff0c\u4ece\u516c\u5f00\u767b\u9769\u70ed\u75c5\u4f8b\u6570\u636e\u4e2d\u6316\u6398\u57ce\u5e02\u533a\u57df\u95f4\u7684\u6f5c\u5728\u4f20\u64ad\u94fe\u63a5\u3002\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u9690\u85cf\u4f20\u64ad\u7f51\u7edc\uff0c\u5efa\u6a21\u70ed\u70b9\u5f62\u6210\u5982\u4f55\u53d7\u90bb\u8fd1\u533a\u57df\u6d41\u884c\u52a8\u6001\u5f71\u54cd\u3002\u5229\u7528\u8fde\u7eed\u6570\u5468\u7684\u7f51\u7edc\u7a33\u5b9a\u6027\u9a8c\u8bc1\u4f20\u64ad\u6a21\u5f0f\u4e00\u81f4\u6027\u3002", "result": "\u5728\u65b0\u52a0\u57612013-2018\u5e74\u548c2020\u5e74\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u4ec5\u9700\u56db\u5468\u70ed\u70b9\u5386\u53f2\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5e73\u5747F\u5206\u65700.79\u3002\u5b66\u4e60\u5230\u7684\u4f20\u64ad\u94fe\u63a5\u4e0e\u901a\u52e4\u6d41\u52a8\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e3a\u57ce\u5e02\u8303\u56f4\u4f20\u64ad\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u516c\u5f00\u7f51\u7edc\u75c5\u4f8b\u6570\u636e\u8f6c\u5316\u4e3a\u9884\u6d4b\u6027\u548c\u89e3\u91ca\u6027\u8d44\u6e90\uff0c\u901a\u8fc7\u6316\u6398\u548c\u9a8c\u8bc1\u9690\u85cf\u4f20\u64ad\u52a8\u6001\uff0c\u4e3a\u516c\u5171\u536b\u751f\u89c4\u5212\u3001\u65e9\u671f\u5e72\u9884\u548c\u57ce\u5e02\u97e7\u6027\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u5de5\u5177\u3002"}}
{"id": "2601.12419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12419", "abs": "https://arxiv.org/abs/2601.12419", "authors": ["Mahammad Namazov", "Tom\u00e1\u0161 Koref", "Ivan Habernal"], "title": "Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification", "comment": null, "summary": "Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's \"reasons\" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6bd4\u8f83\u5206\u6790\u6846\u67b6\uff0c\u8bc4\u4f30\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u5728\u6cd5\u5f8b\u6587\u672c\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u6a21\u578b\u9884\u6d4b\u7406\u7531\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u7406\u7531\u5b58\u5728\u663e\u8457\u5dee\u5f02", "motivation": "\u6cd5\u5f8b\u9886\u57df\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u54ea\u79cd\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6700\u9002\u5408\u6cd5\u5f8b\u7ed3\u679c\u9884\u6d4b\u4ecd\u5b58\u5728\u7591\u95ee", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u6bd4\u8f83\u5206\u6790\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u79cd\u7406\u7531\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fe0\u5b9e\u5ea6\uff08\u6807\u51c6\u5316\u5145\u5206\u6027\u548c\u5168\u9762\u6027\u6307\u6807\uff09\u548c\u5408\u7406\u6027\uff08\u6cd5\u5f8b\u4e13\u5bb6\u8bc4\u4f30\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u6d4b\u8bd5LLM\u4f5c\u4e3a\u6cd5\u5b98\u7684\u53ef\u884c\u6027", "result": "\u6a21\u578b\u9884\u6d4b\u8fdd\u89c4\u7684\u7406\u7531\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u7684\u7406\u7531\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c3d\u7ba1\u5b9a\u91cf\u5206\u6790\u7ed3\u679c\u5f88\u6709\u524d\u666f\u4e14\u4e0b\u6e38\u5206\u7c7b\u6027\u80fd\u5408\u7406", "conclusion": "\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u6a21\u578b\u9884\u6d4b\u7406\u7531\u4e0e\u4e13\u5bb6\u7406\u7531\u7684\u5dee\u5f02\u63d0\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027"}}
{"id": "2601.12066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12066", "abs": "https://arxiv.org/abs/2601.12066", "authors": ["Zijie Lou", "Xiangwei Feng", "Jiaxin Wang", "Xiaochao Qu", "Luoqi Liu", "Ting Liu"], "title": "Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation", "comment": null, "summary": "Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6865\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\uff0c\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\uff0c\u5229\u7528\u6e90\u89c6\u9891\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u7b56\u7565\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u4ece\u9ad8\u65af\u566a\u58f0\u5f00\u59cb\u751f\u6210\uff0c\u4e22\u5f03\u4e86\u539f\u59cb\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u5148\u9a8c\uff0c\u5bfc\u81f4\u5bf9\u8c61\u64e6\u9664\u4e0d\u5b8c\u6574\u6216\u751f\u6210\u5185\u5bb9\u4e0e\u573a\u666f\u7269\u7406\u903b\u8f91\u51b2\u7a81\u3002", "method": "\u5c06\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u968f\u673a\u6865\u6a21\u578b\u8fdb\u884c\u7684\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u4efb\u52a1\uff0c\u5efa\u7acb\u4ece\u6e90\u89c6\u9891\uff08\u542b\u5bf9\u8c61\uff09\u5230\u76ee\u6807\u89c6\u9891\uff08\u5bf9\u8c61\u79fb\u9664\uff09\u7684\u76f4\u63a5\u968f\u673a\u8def\u5f84\u3002\u63d0\u51fa\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\u7b56\u7565\uff0c\u6839\u636e\u63a9\u7801\u7279\u5f81\u52a8\u6001\u8c03\u5236\u8f93\u5165\u5d4c\u5165\uff0c\u5e73\u8861\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u751f\u6210\u7075\u6d3b\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u9891\u5bf9\u8c61\u79fb\u9664\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u5230\u89c6\u9891\u8f6c\u6362\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u968f\u673a\u6865\u6a21\u578b\u548c\u81ea\u9002\u5e94\u63a9\u7801\u8c03\u5236\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u8f93\u5165\u89c6\u9891\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u5bf9\u8c61\u79fb\u9664\u548c\u903b\u8f91\u4e00\u81f4\u7684\u586b\u5145\u3002"}}
{"id": "2601.12912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12912", "abs": "https://arxiv.org/abs/2601.12912", "authors": ["Andreas Br\u00e4nnstr\u00f6m", "Juan Carlos Nieves"], "title": "Human Emotion Verification by Action Languages via Answer Set Programming", "comment": "Under consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "C-MT\u662f\u4e00\u79cd\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u548c\u8f6c\u79fb\u7cfb\u7edf\u6784\u5efa\u7684\u52a8\u4f5c\u8bed\u8a00\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u5728\u53ef\u89c2\u5bdf\u52a8\u4f5c\u5e8f\u5217\u4e0b\u7684\u6f14\u5316\uff0c\u7279\u522b\u5173\u6ce8\u60c5\u7eea\u7b49\u5fc3\u7406\u72b6\u6001\u7684\u591a\u7ef4\u914d\u7f6e\u548c\u53d7\u63a7\u53d8\u5316\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u884c\u4e3a\u63a7\u5236\u7684\u9700\u6c42\uff0c\u5e76\u9650\u5236\u52a8\u4f5c\u5bf9\u5fc3\u7406\u72b6\u6001\u7684\u4e0d\u826f\u526f\u4f5c\u7528\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5f62\u5f0f\u5316\u8868\u793a\u5fc3\u7406\u72b6\u6001\u52a8\u6001\u53d8\u5316\u7684\u8bed\u8a00\u3002\u73b0\u6709\u7684\u52a8\u4f5c\u8bed\u8a00\u5728\u5efa\u6a21\u5fc3\u7406\u72b6\u6001\u6f14\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u5fc3\u7406\u72b6\u6001\u53d8\u5316\u539f\u5219\u7684\u7ea6\u675f\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\u548c\u8f6c\u79fb\u7cfb\u7edf\u6784\u5efaC-MT\u8bed\u8a00\uff0c\u6574\u5408\u60c5\u7eea\u8bc4\u4f30\u7406\u8bba\u7b49\u5fc3\u7406\u5b66\u7406\u8bba\uff0c\u5c06\u5fc3\u7406\u72b6\u6001\u5f62\u5f0f\u5316\u4e3a\u591a\u7ef4\u914d\u7f6e\u3002\u5f15\u5165\u65b0\u7684\u56e0\u679c\u89c4\u5219\"forbids to cause\"\u548c\u4e13\u95e8\u7684\u5fc3\u7406\u72b6\u6001\u52a8\u6001\u8868\u8fbe\u5f0f\uff0c\u5c06\u5fc3\u7406\u53d8\u5316\u539f\u5219\u8f6c\u5316\u4e3a\u8f6c\u79fb\u7ea6\u675f\u548c\u4e0d\u53d8\u6027\u5c5e\u6027\uff0c\u901a\u8fc7\u8f68\u8ff9\u5206\u6790\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "C-MT\u8bed\u8a00\u80fd\u591f\u5bf9\u5fc3\u7406\u72b6\u6001\u7684\u52a8\u6001\u6f14\u5316\u8fdb\u884c\u53d7\u63a7\u63a8\u7406\uff0c\u652f\u6301\u901a\u8fc7\u5206\u6790\u9075\u5faa\u4e0d\u540c\u5fc3\u7406\u5b66\u539f\u5219\u7684\u8f68\u8ff9\u6765\u6bd4\u8f83\u4e0d\u540c\u7684\u53d8\u5316\u52a8\u6001\u3002\u8be5\u6846\u67b6\u5df2\u5e94\u7528\u4e8e\u60c5\u7eea\u9a8c\u8bc1\u6a21\u578b\u7684\u8bbe\u8ba1\u3002", "conclusion": "C-MT\u8bed\u8a00\u4e3a\u5efa\u6a21\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u6f14\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u53d7\u63a7\u5fc3\u7406\u72b6\u6001\u53d8\u5316\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u786e\u4fdd\u5fc3\u7406\u72b6\u6001\u53d8\u5316\u7b26\u5408\u7279\u5b9a\u7684\u5fc3\u7406\u5b66\u539f\u5219\u3002"}}
{"id": "2601.12430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12430", "abs": "https://arxiv.org/abs/2601.12430", "authors": ["Tsan Tsai Chan", "Varsha Suresh", "Anisha Saha", "Michael Hahn", "Vera Demberg"], "title": "System-Mediated Attention Imbalances Make Vision-Language Models Say Yes", "comment": "Under review", "summary": "Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u4e0e\u7cfb\u7edf\u6a21\u6001\u6ce8\u610f\u529b\u5206\u914d\u5931\u8861\u6709\u5173\uff0c\u901a\u8fc7\u56e0\u679c\u6027\u5730\u5c06\u6ce8\u610f\u529b\u4ece\u7cfb\u7edf\u6a21\u6001\u91cd\u65b0\u5206\u914d\u5230\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\uff0c\u53ef\u4ee5\u6709\u6548\u6291\u5236\"yes-bias\"\u5e7b\u89c9\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7f13\u89e3VLM\u5e7b\u89c9\u7684\u7b56\u7565\u901a\u5e38\u504f\u5411\u56fe\u50cf\u4e2d\u5fc3\u89e3\u91ca\uff0c\u4f18\u5148\u589e\u52a0\u56fe\u50cf\u6ce8\u610f\u529b\u800c\u8f83\u5c11\u8003\u8651\u5176\u4ed6\u6a21\u6001\u7684\u4f5c\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u4ecb\u5bfc\u89e3\u91ca\uff0c\u8ba4\u4e3a\u7cfb\u7edf\u6743\u91cd\u5197\u4f59\u5bfc\u81f4\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u7684\u6ce8\u610f\u529b\u51cf\u5c11\u662f\u5e7b\u89c9\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u4ecb\u5bfc\u7684\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u5f52\u56e0\u4e8e\u529f\u80fd\u5197\u4f59\u7684\u7cfb\u7edf\u6743\u91cd\u3002\u901a\u8fc7\u56e0\u679c\u6027\u5730\u91cd\u65b0\u5206\u914d\u6ce8\u610f\u529b\uff0c\u5c06\u7cfb\u7edf\u6a21\u6001\u7684\u6ce8\u610f\u529b\u8f6c\u79fb\u5230\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u4e0a\uff0c\u6765\u6291\u5236\"yes-bias\"\u5e7b\u89c9\u3002", "result": "\u56e0\u679c\u6027\u5730\u91cd\u65b0\u5206\u914d\u6ce8\u610f\u529b\u4ece\u7cfb\u7edf\u6a21\u6001\u5230\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\uff0c\u663e\u8457\u6291\u5236\u4e86\"yes-bias\"\u5e7b\u89c9\uff0c\u901a\u5e38\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8bc1\u636e\u8868\u660e\u7cfb\u7edf\u4ecb\u5bfc\u7684\u6ce8\u610f\u529b\u5931\u8861\u901a\u8fc7\u9f13\u52b1\u4f9d\u8d56\u7c97\u7cd9\u8f93\u5165\u8868\u5f81\u800c\u5bfc\u81f4yes-bias\u3002", "conclusion": "\u7cfb\u7edf\u6ce8\u610f\u529b\u662fVLM\u5e7b\u89c9\u7684\u5173\u952e\u56e0\u7d20\uff0c\u53ef\u4f5c\u4e3a\u7f13\u89e3\u5e7b\u89c9\u7684\u6709\u6548\u6760\u6746\u3002\u7cfb\u7edf\u4ecb\u5bfc\u7684\u6ce8\u610f\u529b\u6846\u67b6\u4e3a\u7406\u89e3VLM\u5e7b\u89c9\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u7ecf\u9a8c\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u5206\u914d\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12067", "abs": "https://arxiv.org/abs/2601.12067", "authors": ["VSS Tejaswi Abburi", "Ananya Singhal", "Saurabh J. Shigwan", "Nitin Kumar"], "title": "ARMARecon: An ARMA Convolutional Filter based Graph Neural Network for Neurodegenerative Dementias Classification", "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Early detection of neurodegenerative diseases such as Alzheimer's Disease (AD) and Frontotemporal Dementia (FTD) is essential for reducing the risk of progression to severe disease stages. As AD and FTD propagate along white-matter regions in a global, graph-dependent manner, graph-based neural networks are well suited to capture these patterns. Hence, we introduce ARMARecon, a unified graph learning framework that integrates Autoregressive Moving Average (ARMA) graph filtering with a reconstruction-driven objective to enhance feature representation and improve classification accuracy. ARMARecon effectively models both local and global connectivity by leveraging 20-bin Fractional Anisotropy (FA) histogram features extracted from white-matter regions, while mitigating over-smoothing. Overall, ARMARecon achieves superior performance compared to state-of-the-art methods on the multi-site dMRI datasets ADNI and NIFD.", "AI": {"tldr": "ARMARecon\uff1a\u4e00\u79cd\u7ed3\u5408ARMA\u56fe\u6ee4\u6ce2\u548c\u91cd\u5efa\u76ee\u6807\u7684\u7edf\u4e00\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u989d\u989e\u53f6\u75f4\u5446\u7684\u65e9\u671f\u68c0\u6d4b\uff0c\u5229\u7528\u767d\u8d28\u533a\u57dfFA\u76f4\u65b9\u56fe\u7279\u5f81\uff0c\u5728ADNI\u548cNIFD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u548c\u989d\u989e\u53f6\u75f4\u5446\uff08FTD\uff09\u7b49\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u4e8e\u964d\u4f4e\u8fdb\u5c55\u4e3a\u4e25\u91cd\u9636\u6bb5\u7684\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8eAD\u548cFTD\u6cbf\u7740\u767d\u8d28\u533a\u57df\u4ee5\u5168\u5c40\u3001\u56fe\u4f9d\u8d56\u7684\u65b9\u5f0f\u4f20\u64ad\uff0c\u57fa\u4e8e\u56fe\u7684\u795e\u7ecf\u7f51\u7edc\u975e\u5e38\u9002\u5408\u6355\u6349\u8fd9\u4e9b\u6a21\u5f0f\u3002", "method": "\u63d0\u51faARMARecon\u7edf\u4e00\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u81ea\u56de\u5f52\u79fb\u52a8\u5e73\u5747\uff08ARMA\uff09\u56fe\u6ee4\u6ce2\u4e0e\u91cd\u5efa\u9a71\u52a8\u76ee\u6807\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002\u5229\u7528\u4ece\u767d\u8d28\u533a\u57df\u63d0\u53d6\u768420-bin\u5206\u6570\u5404\u5411\u5f02\u6027\uff08FA\uff09\u76f4\u65b9\u56fe\u7279\u5f81\uff0c\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u548c\u5168\u5c40\u8fde\u63a5\u6027\uff0c\u540c\u65f6\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "ARMARecon\u5728\u591a\u7ad9\u70b9dMRI\u6570\u636e\u96c6ADNI\u548cNIFD\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ARMARecon\u6846\u67b6\u901a\u8fc7\u7ed3\u5408ARMA\u56fe\u6ee4\u6ce2\u548c\u91cd\u5efa\u76ee\u6807\uff0c\u6709\u6548\u6355\u6349\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5728\u767d\u8d28\u533a\u57df\u7684\u4f20\u64ad\u6a21\u5f0f\uff0c\u4e3aAD\u548cFTD\u7684\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u56fe\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.12913", "categories": ["cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.12913", "abs": "https://arxiv.org/abs/2601.12913", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Francesco Giannini", "Alberto Termine", "Filippo Bonchi", "Mateja Jamnik", "Giuseppe Marra"], "title": "Actionable Interpretability Must Be Defined in Terms of Symmetries", "comment": null, "summary": "This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5f53\u524dAI\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u73b0\u6709\u5b9a\u4e49\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\uff0c\u65e0\u6cd5\u63a8\u5bfc\u51fa\u5177\u4f53\u7684\u5efa\u6a21\u548c\u63a8\u7406\u89c4\u5219\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u64cd\u4f5c\u5b9a\u4e49\uff0c\u5e76\u5047\u8bbe\u56db\u79cd\u5bf9\u79f0\u6027\u8db3\u4ee5\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u7684\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u56e0\u4e3a\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\"\u53ef\u64cd\u4f5c\u6027\" - \u5b83\u4eec\u672a\u80fd\u63d0\u4f9b\u53ef\u4ee5\u63a8\u5bfc\u51fa\u5177\u4f53\u5efa\u6a21\u548c\u63a8\u7406\u89c4\u5219\u7684\u5f62\u5f0f\u5316\u539f\u5219\u3002\u8fd9\u4f7f\u5f97\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u96be\u4ee5\u4ea7\u751f\u5b9e\u9645\u53ef\u7528\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u6846\u67b6\u3002\u5047\u8bbe\u56db\u79cd\u5bf9\u79f0\u6027\u8db3\u4ee5\uff1a(1) \u6fc0\u53d1\u6838\u5fc3\u53ef\u89e3\u91ca\u6027\u5c5e\u6027\uff0c(2) \u523b\u753b\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u7c7b\u522b\uff0c(3) \u63a8\u5bfc\u51fa\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u63a8\u7406\u5f62\u5f0f\uff08\u5982\u5bf9\u9f50\u3001\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff09\u4f5c\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u5f62\u5f0f\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u89e3\u91ca\u6027\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5b9a\u4e49\uff0c\u5e76\u7edf\u4e00\u5904\u7406\u5404\u79cd\u53ef\u89e3\u91ca\u6027\u4efb\u52a1\uff0c\u5305\u62ec\u6a21\u578b\u5bf9\u9f50\u3001\u5e72\u9884\u5206\u6790\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9700\u8981\u4ece\u975e\u64cd\u4f5c\u6027\u7684\u5b9a\u4e49\u8f6c\u5411\u57fa\u4e8e\u5bf9\u79f0\u6027\u7684\u53ef\u64cd\u4f5c\u6846\u67b6\u3002\u901a\u8fc7\u56db\u79cd\u5bf9\u79f0\u6027\uff0c\u53ef\u4ee5\u5efa\u7acb\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u6027\u7406\u8bba\uff0c\u4e3a\u5b9e\u9645\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\u3002"}}
{"id": "2601.12465", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12465", "abs": "https://arxiv.org/abs/2601.12465", "authors": ["Miao Peng", "Weizhou Shen", "Nuo Chen", "Chenliang Li", "Ming Yan", "Jia Li"], "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.", "AI": {"tldr": "\u63d0\u51fa\u4e86DeepReasonQA\u6846\u67b6\u548cLongPAS\u65b9\u6cd5\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dRLVR\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u5bc6\u5ea6\u63a8\u7406\u6570\u636e\u6784\u5efa\u548c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u63d0\u5347LLMs\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u5728\u77ed\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u5b58\u5728\"almost-there\"\u73b0\u8c61\uff08\u8f68\u8ff9\u57fa\u672c\u6b63\u786e\u4f46\u6700\u540e\u4e00\u6b65\u5931\u8d25\uff09\u3002\u4f5c\u8005\u53d1\u73b0\u4e24\u4e2a\u539f\u56e0\uff1a1\uff09\u957f\u4e0a\u4e0b\u6587QA\u6570\u636e\u7f3a\u4e4f\u9ad8\u63a8\u7406\u5bc6\u5ea6\uff1b2\uff09RL\u8bad\u7ec3\u4e2d\u5bf9\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\u7684\u60e9\u7f5a\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u4e22\u5931\u3002", "method": "1. DeepReasonQA\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5408\u6210\u6846\u67b6\uff0c\u53ef\u63a7\u6784\u5efa\u9ad8\u96be\u5ea6\u3001\u591a\u8df3\u957f\u4e0a\u4e0b\u6587QA\u5bf9\uff0c\u5305\u542b\u5185\u5728\u63a8\u7406\u94fe\u30022. LongPAS\uff1a\u957f\u4e0a\u4e0b\u6587\u8fc7\u7a0b\u4f18\u52bf\u5851\u9020\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u6548\u6027\u548c\u76f8\u5173\u6027\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u4ece\"almost-there\"\u8f68\u8ff9\u4e2d\u6355\u6349\u5173\u952e\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eRLVR\u57fa\u7ebf\uff0c\u5339\u914d\u524d\u6cbfLLMs\u6027\u80fd\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u53c2\u6570\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u5728\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u548c\u4fdd\u6301RL\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7DeepReasonQA\u63d0\u4f9b\u9ad8\u5bc6\u5ea6\u63a8\u7406\u6570\u636e\u548cLongPAS\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dRLVR\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u5927\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.12076", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12076", "abs": "https://arxiv.org/abs/2601.12076", "authors": ["H. Jiang", "Y. Sun", "Z. Dong", "T. Liu", "Y. Gu"], "title": "CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation", "comment": null, "summary": "Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u9065\u611f\u89c6\u9891\u6307\u79f0\u76ee\u6807\u5206\u5272\u57fa\u51c6RS-RVOS Bench\u548c\u57fa\u4e8e\u8bb0\u5fc6\u8d28\u91cf\u63a7\u5236\u7684\u5728\u7ebf\u5206\u5272\u6846\u67b6MQC-SAM\uff0c\u89e3\u51b3\u5f31\u76ee\u6807\u663e\u8457\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u95ee\u9898\u3002", "motivation": "\u9065\u611f\u89c6\u9891\u6307\u79f0\u76ee\u6807\u5206\u5272\u9762\u4e34\u5f31\u76ee\u6807\u663e\u8457\u6027\u548c\u52a8\u6001\u573a\u666f\u4e2d\u89c6\u89c9\u4fe1\u606f\u622a\u65ad\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e13\u7528\u57fa\u51c6\u3001\u521d\u59cb\u8bb0\u5fc6\u6784\u5efa\u504f\u5dee\u5bfc\u81f4\u5b9e\u4f8b\u5b9a\u4f4d\u4e0d\u51c6\uff0c\u4ee5\u53ca\u65e0\u5dee\u522b\u8bb0\u5fc6\u79ef\u7d2f\u5f15\u5165\u566a\u58f0\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u3002", "method": "1) \u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6RS-RVOS Bench\uff0c\u5305\u542b111\u4e2a\u89c6\u9891\u5e8f\u5217\u3001\u7ea625,000\u5e27\u548c213,000\u4e2a\u65f6\u5e8f\u6307\u79f0\u6807\u6ce8\uff0c\u91c7\u7528\u4e25\u683c\u7684\u56e0\u679c\u611f\u77e5\u6807\u6ce8\u7b56\u7565\uff1b2) \u63d0\u51faMQC-SAM\u6846\u67b6\uff0c\u5305\u542b\u65f6\u5e8f\u8fd0\u52a8\u4e00\u81f4\u6027\u6a21\u5757\u8fdb\u884c\u521d\u59cb\u8bb0\u5fc6\u6821\u51c6\uff0c\u4ee5\u53ca\u89e3\u8026\u6ce8\u610f\u529b\u8bb0\u5fc6\u96c6\u6210\u673a\u5236\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u8bc4\u4f30\u548c\u9009\u62e9\u6027\u66f4\u65b0\u3002", "result": "\u5728RS-RVOS Bench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMQC-SAM\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u636e\u548c\u65b9\u6cd5\u7684\u53cc\u91cd\u8d21\u732e\u63a8\u8fdb\u4e86\u9065\u611f\u89c6\u9891\u6307\u79f0\u76ee\u6807\u5206\u5272\u9886\u57df\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2601.13060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13060", "abs": "https://arxiv.org/abs/2601.13060", "authors": ["Zecheng Li", "Zhihui Cao", "Wenke Huang", "Yudong Zhang", "Keying Qi", "Rui Wang", "Zeyu Zheng", "Jian Zhao", "Hao Zhu", "Hengxin Wu", "Yuran Wang", "Guitao Fan", "Guokun Wu", "Yicong Liu", "Zhilin Gao", "Haikun Xu", "He Yang", "Minqi Xiang", "Xingyu Liu", "Zuojian Wang"], "title": "MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux", "comment": null, "summary": "Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.", "AI": {"tldr": "MagicGUI-RMS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5956\u52b1\u6a21\u578b\u7cfb\u7edf\uff0c\u7528\u4e8eGUI\u4ee3\u7406\u7684\u8f68\u8ff9\u8bc4\u4f30\u3001\u7ea0\u6b63\u53cd\u9988\u548c\u81ea\u6211\u8fdb\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7279\u5b9a\u9886\u57df\u548c\u901a\u7528\u5956\u52b1\u6a21\u578b\u7ed3\u5408\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u52a8\u6570\u636e\u6784\u5efa\u7ba1\u9053\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "GUI\u4ee3\u7406\u5728\u81ea\u4e3b\u4ea4\u4e92\u548c\u4efb\u52a1\u6267\u884c\u65b9\u9762\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u81ea\u52a8\u5316\u8bc4\u4f30\u4ee3\u7406\u8f68\u8ff9\u548c\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4ee5\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u9759\u6001\u89c4\u5219\u9a8c\u8bc1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faMagicGUI-RMS\u591a\u667a\u80fd\u4f53\u5956\u52b1\u6a21\u578b\u7cfb\u7edf\uff0c\u6574\u5408\u7279\u5b9a\u9886\u57df\u5956\u52b1\u6a21\u578b\uff08DS-RM\uff09\u548c\u901a\u7528\u5956\u52b1\u6a21\u578b\uff08GP-RM\uff09\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc4\u4f30\u548c\u8de8\u5f02\u6784GUI\u4efb\u52a1\u7684\u9c81\u68d2\u6cdb\u5316\u3002\u8bbe\u8ba1\u7ed3\u6784\u5316\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u81ea\u52a8\u751f\u6210\u5e73\u8861\u591a\u6837\u7684\u5956\u52b1\u6570\u636e\u96c6\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u56de\u6d41\u673a\u5236\u8bc6\u522b\u9519\u8bef\u52a8\u4f5c\u3001\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u5e76\u6301\u7eed\u589e\u5f3a\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMagicGUI-RMS\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u884c\u4e3a\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u6784\u5efa\u57fa\u4e8e\u5956\u52b1\u9002\u5e94\u7684\u81ea\u6211\u6539\u8fdbGUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u6709\u6548\u7684\u57fa\u7840\u3002", "conclusion": "MagicGUI-RMS\u901a\u8fc7\u81ea\u9002\u5e94\u8f68\u8ff9\u8bc4\u4f30\u3001\u7ea0\u6b63\u53cd\u9988\u548c\u81ea\u6211\u8fdb\u5316\u5b66\u4e60\u80fd\u529b\uff0c\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u751f\u6210\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u81ea\u6211\u6539\u8fdb\u7684GUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2601.12471", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12471", "abs": "https://arxiv.org/abs/2601.12471", "authors": ["Sravanthi Machcha", "Sushrita Yerra", "Sahil Gupta", "Aishwarya Sahoo", "Sharmin Sultana", "Hong Yu", "Zonghai Yao"], "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty", "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026", "summary": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.", "AI": {"tldr": "MedAbstain\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u533b\u5b66\u591a\u9009\u9898\u4e2d\u5f03\u6743\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u5e38\u5728\u4e0d\u786e\u4fe1\u65f6\u65e0\u6cd5\u5f03\u6743\uff0c\u800c\u63d0\u4f9b\u660e\u786e\u5f03\u6743\u9009\u9879\u6bd4\u8f93\u5165\u6270\u52a8\u66f4\u80fd\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u8fc7\u4e8e\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u5728\u4e0d\u786e\u4fe1\u65f6\u80fd\u591f\u5f03\u6743\u5bf9\u4e8e\u53ef\u4fe1\u90e8\u7f72\u540c\u6837\u91cd\u8981\u3002\u533b\u5b66\u591a\u9009\u9898\u573a\u666f\u5177\u6709\u79bb\u6563\u9009\u62e9\u7279\u6027\uff0c\u53ef\u6cdb\u5316\u5230\u667a\u80fd\u4f53\u884c\u52a8\u9009\u62e9\u3002", "method": "\u63d0\u51faMedAbstain\u57fa\u51c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u6574\u5408\u4e86\u5171\u5f62\u9884\u6d4b\u3001\u5bf9\u6297\u6027\u95ee\u9898\u6270\u52a8\u548c\u660e\u786e\u5f03\u6743\u9009\u9879\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u95ed\u6e90LLMs\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u5de5\u7a0b\u7b49\u56e0\u7d20\u5bf9\u5f03\u6743\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u9ad8\u51c6\u786e\u6027\u6a21\u578b\u4e5f\u7ecf\u5e38\u65e0\u6cd5\u5728\u4e0d\u786e\u4fe1\u65f6\u5f03\u6743\u3002\u63d0\u4f9b\u660e\u786e\u5f03\u6743\u9009\u9879\u80fd\u6301\u7eed\u589e\u52a0\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5b89\u5168\u5f03\u6743\uff0c\u6548\u679c\u8fdc\u4f18\u4e8e\u8f93\u5165\u6270\u52a8\u3002\u6269\u5927\u6a21\u578b\u89c4\u6a21\u6216\u4f7f\u7528\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\u5bf9\u6539\u5584\u5f03\u6743\u80fd\u529b\u5e2e\u52a9\u6709\u9650\u3002", "conclusion": "\u5f03\u6743\u673a\u5236\u5bf9\u4e8eLLM\u7684\u53ef\u4fe1\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u9700\u8981\u5e73\u8861\u51c6\u786e\u6027\u548c\u5b89\u5168\u5f03\u6743\u80fd\u529b\u3002"}}
{"id": "2601.12079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12079", "abs": "https://arxiv.org/abs/2601.12079", "authors": ["Jing Zhang", "Bingjie Fan", "Jixiang Zhu", "Zhe Wang"], "title": "EmoLat: Text-driven Image Sentiment Transfer via Emotion Latent Space", "comment": "10 pages, 5 figures", "summary": "We propose EmoLat, a novel emotion latent space that enables fine-grained, text-driven image sentiment transfer by modeling cross-modal correlations between textual semantics and visual emotion features. Within EmoLat, an emotion semantic graph is constructed to capture the relational structure among emotions, objects, and visual attributes. To enhance the discriminability and transferability of emotion representations, we employ adversarial regularization, aligning the latent emotion distributions across modalities. Building upon EmoLat, a cross-modal sentiment transfer framework is proposed to manipulate image sentiment via joint embedding of text and EmoLat features. The network is optimized using a multi-objective loss incorporating semantic consistency, emotion alignment, and adversarial regularization. To support effective modeling, we construct EmoSpace Set, a large-scale benchmark dataset comprising images with dense annotations on emotions, object semantics, and visual attributes. Extensive experiments on EmoSpace Set demonstrate that our approach significantly outperforms existing state-of-the-art methods in both quantitative metrics and qualitative transfer fidelity, establishing a new paradigm for controllable image sentiment editing guided by textual input. The EmoSpace Set and all the code are available at http://github.com/JingVIPLab/EmoLat.", "AI": {"tldr": "EmoLat\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u60c5\u611f\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u5efa\u6a21\u6587\u672c\u8bed\u4e49\u4e0e\u89c6\u89c9\u60c5\u611f\u7279\u5f81\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u5173\u8054\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u60c5\u611f\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u60c5\u611f\u8fc1\u79fb\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u6587\u672c\u5f15\u5bfc\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u6587\u672c\u8bed\u4e49\u4e0e\u89c6\u89c9\u60c5\u611f\u4e4b\u95f4\u590d\u6742\u5173\u7cfb\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u60c5\u611f\u8bed\u4e49\u56fe\u6355\u6349\u60c5\u611f\u3001\u7269\u4f53\u548c\u89c6\u89c9\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b\u4f7f\u7528\u5bf9\u6297\u6b63\u5219\u5316\u589e\u5f3a\u60c5\u611f\u8868\u793a\u7684\u5224\u522b\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff1b\u63d0\u51fa\u8de8\u6a21\u6001\u60c5\u611f\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u548cEmoLat\u7279\u5f81\u7684\u8054\u5408\u5d4c\u5165\u6765\u64cd\u63a7\u56fe\u50cf\u60c5\u611f\uff1b\u4f7f\u7528\u5305\u542b\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u60c5\u611f\u5bf9\u9f50\u548c\u5bf9\u6297\u6b63\u5219\u5316\u7684\u591a\u76ee\u6807\u635f\u5931\u4f18\u5316\u7f51\u7edc\u3002", "result": "\u5728EmoSpace Set\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8fc1\u79fb\u4fdd\u771f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "EmoLat\u5efa\u7acb\u4e86\u6587\u672c\u5f15\u5bfc\u53ef\u63a7\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u7684\u65b0\u8303\u5f0f\uff0cEmoSpace Set\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.13122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13122", "abs": "https://arxiv.org/abs/2601.13122", "authors": ["Gourab K Patro", "Himanshi Agrawal", "Himanshu Gharat", "Supriya Panigrahi", "Nim Sherpa", "Vishal Vaddina", "Dagnachew Birru"], "title": "Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward", "comment": null, "summary": "Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u901a\u7528AI\u7cfb\u7edf\u5b58\u5728\u5e7b\u89c9\u3001\u6bd2\u6027\u7b49\u98ce\u9669\uff0c\u76f8\u6bd4\u4f20\u7edf\u4efb\u52a1\u7279\u5b9aAI\u66f4\u96be\u5b9e\u73b0\u8d1f\u8d23\u4efbAI\uff0c\u63d0\u51fa\u9700\u8981\u91cd\u65b0\u601d\u8003RAI\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acbC2V2\uff08\u63a7\u5236\u3001\u4e00\u81f4\u6027\u3001\u4ef7\u503c\u3001\u771f\u5b9e\u6027\uff09\u6846\u67b6\u6765\u6307\u5bfc\u672a\u6765\u901a\u7528AI\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u53d1\u5c55\u3002", "motivation": "\u73b0\u4ee3\u901a\u7528AI\u7cfb\u7edf\uff08\u5982\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\uff09\u867d\u7136\u80fd\u6267\u884c\u591a\u79cd\u4efb\u52a1\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u3001\u6bd2\u6027\u548c\u523b\u677f\u5370\u8c61\u7b49\u98ce\u9669\uff0c\u4f7f\u5176\u4e0d\u53ef\u4fe1\u3002\u8fd9\u4e9b\u98ce\u9669\u5728\u4f20\u7edf\u4efb\u52a1\u7279\u5b9aAI\u4e2d\u4e0d\u5b58\u5728\u6216\u8f83\u8f7b\u4e14\u6613\u7f13\u89e3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u8d1f\u8d23\u4efbAI\uff08RAI\uff09\u65b9\u6cd5\u3002", "method": "1. \u4ece\u516b\u4e2a\u5e7f\u6cdb\u63a5\u53d7\u7684RAI\u539f\u5219\uff08\u516c\u5e73\u6027\u3001\u9690\u79c1\u3001\u53ef\u89e3\u91ca\u6027\u3001\u9c81\u68d2\u6027\u3001\u5b89\u5168\u6027\u3001\u771f\u5b9e\u6027\u3001\u6cbb\u7406\u3001\u53ef\u6301\u7eed\u6027\uff09\u5206\u6790\u73b0\u4ee3\u901a\u7528AI\u7684\u98ce\u9669\u548c\u8106\u5f31\u6027\uff1b2. \u63d0\u51fa\u8f93\u51fa\u81ea\u7531\u5ea6\uff08DoFo\uff09\u6982\u5ff5\u89e3\u91ca\u901a\u7528AI\u4e0e\u4efb\u52a1\u7279\u5b9aAI\u7684\u5dee\u5f02\uff1b3. \u63a8\u5bfc\u51faC2V2\uff08\u63a7\u5236\u3001\u4e00\u81f4\u6027\u3001\u4ef7\u503c\u3001\u771f\u5b9e\u6027\uff09\u9700\u6c42\u6846\u67b6\uff1b4. \u8bc4\u4f30AI\u5bf9\u9f50\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u63a8\u7406\u589e\u5f3a\u7b49\u6280\u672f\u5728\u6ee1\u8db3C2V2\u9700\u6c42\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u901a\u7528AI\u7cfb\u7edf\u7531\u4e8e\u8f93\u51fa\u81ea\u7531\u5ea6\uff08DoFo\uff09\u9ad8\u4e14\u975e\u786e\u5b9a\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u4efb\u52a1\u7279\u5b9aAI\u66f4\u96be\u5b9e\u73b0\u8d1f\u8d23\u4efbAI\u3002C2V2\u6846\u67b6\u4e3a\u672a\u6765\u901a\u7528AI\u7cfb\u7edf\u7684RAI\u8981\u6c42\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8bbe\u8ba1\u65b9\u5411\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u53ef\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "conclusion": "\u5f00\u53d1\u8d1f\u8d23\u4efb\u901a\u7528AI\u9700\u8981\u901a\u8fc7C2V2\u7ef4\u5ea6\u5f62\u5f0f\u5316\u5efa\u6a21\u5e94\u7528\u6216\u9886\u57df\u76f8\u5173\u7684RAI\u8981\u6c42\uff0c\u5e76\u91c7\u7528\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u7ed3\u5408\u591a\u79cd\u6280\u672f\u6765\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002\u8fd9\u662f\u5b9e\u73b0\u901a\u7528AI\u8d1f\u8d23\u4efb\u53d1\u5c55\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.12473", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12473", "abs": "https://arxiv.org/abs/2601.12473", "authors": ["Renlong Jie", "Chen Chu", "Zhen Wang"], "title": "Capability-Aware Early-Stage Research Idea Evaluation", "comment": null, "summary": "Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f5c\u8005\u4fe1\u606f\u548c\u7814\u7a76\u60f3\u6cd5\u7684\u80fd\u529b\u611f\u77e5\u6846\u67b6\uff0c\u9884\u6d4b\u8bba\u6587\u63a5\u53d7\u5ea6\u548c\u8bc4\u5206\uff0c\u65e0\u9700\u5b8c\u6574\u6587\u672c\u6216\u5b9e\u9a8c\u7ed3\u679c", "motivation": "\u5728\u6982\u5ff5\u9636\u6bb5\uff08\u6295\u5165\u5927\u91cf\u8d44\u6e90\u524d\uff09\u9884\u6d4b\u7814\u7a76\u7ed3\u679c\u5bf9\u4f18\u5316\u79d1\u5b66\u8d44\u6e90\u5206\u914d\u548c\u7814\u7a76\u89c4\u5212\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u624b\u7a3f\u6216\u540c\u884c\u8bc4\u5ba1", "method": "\u63d0\u51fa\u80fd\u529b\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u8deftransformer\u67b6\u6784\u6574\u5408\u4f5c\u8005\u4fe1\u606f\u3001\u80fd\u529b\u8868\u793a\u548c\u7814\u7a76\u60f3\u6cd5\uff0c\u91c7\u7528\u7075\u6d3b\u878d\u5408\u673a\u5236\uff1b\u5f15\u5165\u4e24\u9636\u6bb5\u67b6\u6784\u5b66\u4e60\u7ed9\u5b9a\u4f5c\u8005\u4fe1\u606f\u548c\u60f3\u6cd5\u7684\u80fd\u529b\u8868\u793a", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8ebert-base\u548cbert-large\u7684\u5355\u8def\u6a21\u578b\uff0c\u80fd\u529b\u9884\u6d4b\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u7ec8\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u65e9\u671f\u7814\u7a76\u7ed3\u679c\u9884\u6d4b\u548c\u79d1\u5b66\u8d44\u6e90\u5206\u914d"}}
{"id": "2601.12080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12080", "abs": "https://arxiv.org/abs/2601.12080", "authors": ["Haipeng Zhou", "Zhaohu Xing", "Hongqiu Wang", "Jun Ma", "Ping Li", "Lei Zhu"], "title": "Toward Real-World High-Precision Image Matting and Segmentation", "comment": "Accepted by AAAI2026, Poster", "summary": "High-precision scene parsing tasks, including image matting and dichotomous segmentation, aim to accurately predict masks with extremely fine details (such as hair). Most existing methods focus on salient, single foreground objects. While interactive methods allow for target adjustment, their class-agnostic design restricts generalization across different categories. Furthermore, the scarcity of high-quality annotation has led to a reliance on inharmonious synthetic data, resulting in poor generalization to real-world scenarios. To this end, we propose a Foreground Consistent Learning model, dubbed as FCLM, to address the aforementioned issues. Specifically, we first introduce a Depth-Aware Distillation strategy where we transfer the depth-related knowledge for better foreground representation. Considering the data dilemma, we term the processing of synthetic data as domain adaptation problem where we propose a domain-invariant learning strategy to focus on foreground learning. To support interactive prediction, we contribute an Object-Oriented Decoder that can receive both visual and language prompts to predict the referring target. Experimental results show that our method quantitatively and qualitatively outperforms SOTA methods.", "AI": {"tldr": "\u63d0\u51faFCLM\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u3001\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u548c\u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u89e3\u51b3\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u56fe\u50cf\u62a0\u56fe\u548c\u4e8c\u5206\u5206\u5272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u663e\u8457\u5355\u4e00\u524d\u666f\u5bf9\u8c61\uff0c\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u7c7b\u522b\u4e0d\u53ef\u77e5\u9650\u5236\u8de8\u7c7b\u522b\u6cdb\u5316\uff0c\u4e14\u9ad8\u8d28\u91cf\u6807\u6ce8\u7a00\u7f3a\u5bfc\u81f4\u4f9d\u8d56\u4e0d\u548c\u8c10\u7684\u5408\u6210\u6570\u636e\uff0c\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\u6548\u679c\u5dee\u3002", "method": "\u63d0\u51fa\u524d\u666f\u4e00\u81f4\u5b66\u4e60\u6a21\u578bFCLM\uff1a1\uff09\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u7b56\u7565\uff0c\u8fc1\u79fb\u6df1\u5ea6\u76f8\u5173\u77e5\u8bc6\u6539\u5584\u524d\u666f\u8868\u793a\uff1b2\uff09\u5c06\u5408\u6210\u6570\u636e\u5904\u7406\u89c6\u4e3a\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u51fa\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u7b56\u7565\u4e13\u6ce8\u4e8e\u524d\u666f\u5b66\u4e60\uff1b3\uff09\u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u53ef\u63a5\u6536\u89c6\u89c9\u548c\u8bed\u8a00\u63d0\u793a\u9884\u6d4b\u53c2\u8003\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FCLM\u6a21\u578b\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u84b8\u998f\u3001\u9886\u57df\u4e0d\u53d8\u5b66\u4e60\u548c\u9762\u5411\u5bf9\u8c61\u89e3\u7801\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7cbe\u5ea6\u573a\u666f\u89e3\u6790\u4e2d\u7684\u524d\u666f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.13186", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13186", "abs": "https://arxiv.org/abs/2601.13186", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching", "comment": "33 pages, 19 figures", "summary": "Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTIVS-O\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u7f13\u5b58\u548c\u53ef\u89c2\u6d4b\u6027\u8bc4\u5206\uff0c\u5728\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u5b9e\u73b0\u96f6\u9ad8\u98ce\u9669\u6f0f\u6d1e\u7684\u5b89\u5168\u54cd\u5e94\uff0c\u540c\u65f6\u51cf\u5c1141.6%\u7684LLM\u8c03\u7528\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u900f\u660e\u5ea6\u3002", "motivation": "\u63d0\u793a\u6ce8\u5165\u662f\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2dLLM\u5b89\u5168\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u5bf9\u5b89\u5168\u63a8\u7406\u900f\u660e\u5ea6\u7684\u8003\u91cf\uff0c\u9700\u8981\u5728\u4e25\u683c\u7f13\u89e3\u63aa\u65bd\u4e0e\u53ef\u5ba1\u8ba1\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u6269\u5c55TIVS\u6846\u67b6\u4e3aTIVS-O\uff0c\u52a0\u5165\u8bed\u4e49\u76f8\u4f3c\u6027\u7f13\u5b58\u548c\u53ef\u89c2\u6d4b\u6027\u8bc4\u5206\u6bd4\uff08OSR\uff09\u3002\u91c7\u7528HOPE\u542f\u53d1\u7684\u5d4c\u5957\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u8fde\u7eed\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5728301\u4e2a\u5408\u6210\u63d0\u793a\u4e0a\u6d4b\u8bd510\u79cd\u653b\u51fb\u5bb6\u65cf\uff0c\u4f7f\u75285\u4e2aKPI\u8fdb\u884c\u5b89\u5168\u5206\u6790\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u96f6\u9ad8\u98ce\u9669\u6f0f\u6d1e\u7684\u5b89\u5168\u54cd\u5e94\uff1b\u8bed\u4e49\u7f13\u5b58\u51cf\u5c1141.6%\u7684LLM\u8c03\u7528\uff0c\u964d\u4f4e\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u78b3\u6392\u653e\uff1b5\u79cdTIVS-O\u914d\u7f6e\u63ed\u793a\u4e86\u7f13\u89e3\u4e25\u683c\u6027\u4e0e\u53d6\u8bc1\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u6700\u4f18\u6743\u8861\u3002", "conclusion": "\u53ef\u89c2\u6d4b\u6027\u611f\u77e5\u8bc4\u4f30\u80fd\u63ed\u793a\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u4e2d\u7684\u975e\u5355\u8c03\u6548\u5e94\uff0c\u8bb0\u5fc6\u589e\u5f3a\u667a\u80fd\u4f53\u53ef\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u6700\u5927\u5316\u5b89\u5168\u9c81\u68d2\u6027\u3001\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u8425\u6210\u672c\u8282\u7ea6\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\uff0c\u4e3a\u5b89\u5168\u7eff\u8272LLM\u90e8\u7f72\u63d0\u4f9b\u751f\u4ea7\u5c31\u7eea\u8def\u5f84\u3002"}}
{"id": "2601.12505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12505", "abs": "https://arxiv.org/abs/2601.12505", "authors": ["Ashish Raj Shekhar", "Shiven Agarwal", "Priyanuj Bordoloi", "Yash Shah", "Tejas Anvekar", "Vivek Gupta"], "title": "DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.", "AI": {"tldr": "DoPE\u662f\u4e00\u4e2a\u6587\u6863\u5c42\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5728PDF/HTML\u8003\u8bd5\u6587\u6863\u4e2d\u5d4c\u5165\u8bed\u4e49\u8bf1\u9975\u6765\u9632\u6b62\u548c\u68c0\u6d4bMLLM\u7684\u81ea\u52a8\u89e3\u9898\uff0c\u5229\u7528\u6e32\u67d3-\u89e3\u6790\u5dee\u5f02\u6765\u4fdd\u62a4\u5b66\u672f\u8bda\u4fe1\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u80fd\u591f\u76f4\u63a5\u5904\u7406\u8003\u8bd5\u6587\u6863\uff0c\u5a01\u80c1\u4f20\u7edf\u8bc4\u4f30\u65b9\u5f0f\u548c\u5b66\u672f\u8bda\u4fe1\u3002\u9700\u8981\u5f00\u53d1\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u673a\u5236\u6765\u9632\u6b62\u81ea\u52a8\u5316\u89e3\u9898\u548c\u68c0\u6d4bAI\u4f9d\u8d56\u3002", "method": "\u63d0\u51faDoPE\u6846\u67b6\uff0c\u5305\u542bFewSoRT-Q\u751f\u6210\u95ee\u9898\u7ea7\u8bed\u4e49\u8bf1\u9975\uff0cFewSoRT-D\u5c06\u8bf1\u9975\u5c01\u88c5\u5230\u6c34\u5370\u6587\u6863\u4e2d\u3002\u5229\u7528MLLM\u7ba1\u9053\u4e2d\u7684\u6e32\u67d3-\u89e3\u6790\u5dee\u5f02\uff0c\u5728\u6587\u6863\u521b\u4f5c\u65f6\u5d4c\u5165\u8bed\u4e49\u8bf1\u9975\u3002", "result": "\u5728Integrity-Bench\u57fa\u51c6(1826\u4e2a\u8003\u8bd5)\u4e0a\u8bc4\u4f30\uff1a\u5bf9OpenAI\u548cAnthropic\u7684\u9ed1\u76d2MLLM\uff0c\u68c0\u6d4b\u7387\u8fbe\u523091.4%(\u5047\u9633\u6027\u73878.7%)\uff0c96.3%\u7684\u5c1d\u8bd5\u88ab\u963b\u6b62\u6210\u529f\u5b8c\u6210\u6216\u8bf1\u5bfc\u51fa\u8bf1\u9975\u5bf9\u9f50\u7684\u5931\u8d25\u3002", "conclusion": "DoPE\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6587\u6863\u5c42\u9632\u5fa1\u673a\u5236\uff0c\u65e2\u80fd\u9632\u6b62MLLM\u81ea\u52a8\u89e3\u9898\uff0c\u53c8\u80fd\u68c0\u6d4bAI\u4f9d\u8d56\uff0c\u4e3a\u4fdd\u62a4\u5b66\u672f\u8bda\u4fe1\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7814\u7a76\u7684\u5de5\u5177\u548c\u57fa\u51c6\u3002"}}
{"id": "2601.12082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12082", "abs": "https://arxiv.org/abs/2601.12082", "authors": ["Tiffanie Godelaine", "Maxime Zanella", "Karim El Khoury", "Sa\u00efd Mahmoudi", "Beno\u00eet Macq", "Christophe De Vleeschouwer"], "title": "Conditional Random Fields for Interactive Refinement of Histopathological Predictions", "comment": null, "summary": "Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF.", "AI": {"tldr": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u968f\u673a\u573a\u4f18\u5316\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u5bf9\u764c\u75c7\u68c0\u6d4b\u548c\u5206\u671f\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u4ef7\u503c\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u80fd\u63d0\u4f9b\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u4f46\u9884\u6d4b\u7ed3\u679c\u4e0d\u5b8c\u7f8e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "\u63d0\u51faHistoCRF\u6846\u67b6\uff0c\u5c06\u6761\u4ef6\u968f\u673a\u573a(CRF)\u5e94\u7528\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u5206\u6790\u3002\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u6210\u5bf9\u52bf\u51fd\u6570\uff0c\u4fc3\u8fdb\u6807\u7b7e\u591a\u6837\u6027\u5e76\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u3002\u8003\u8651\u4e09\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\uff1a\u65e0\u6807\u6ce8\u3001\u6709\u4e13\u5bb6\u6807\u6ce8\u3001\u4ee5\u53ca\u8fed\u4ee3\u5f0f\u4eba\u673a\u534f\u540c\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u6db5\u76d6\u4e0d\u540c\u5668\u5b98\u548c\u75be\u75c5\u7684patch\u7ea7\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\uff1a\u65e0\u6807\u6ce8\u65f616.0%\uff0c\u4ec5100\u4e2a\u6807\u6ce8\u65f627.5%\u3002\u4eba\u673a\u534f\u540c\u6807\u6ce8\u8fdb\u4e00\u6b65\u8fbe\u523032.6%\u7684\u63d0\u5347\u3002", "conclusion": "HistoCRF\u80fd\u6709\u6548\u4f18\u5316\u7ec4\u7ec7\u75c5\u7406\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u6216\u4eba\u673a\u534f\u540c\u6807\u6ce8\u5373\u53ef\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13206", "abs": "https://arxiv.org/abs/2601.13206", "authors": ["Neil K. R. Sehgal", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues", "comment": null, "summary": "Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\\% vs. 4\\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\\geq$95\\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u65f6\u95f4\u9650\u5236\u4e0b\u7f3a\u4e4f\u65f6\u95f4\u610f\u8bc6\uff0c\u5bfc\u81f4\u8c08\u5224\u6210\u529f\u7387\u5927\u5e45\u4e0b\u964d\uff0c\u4f46\u901a\u8fc7\u63d0\u4f9b\u5269\u4f59\u65f6\u95f4\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8868\u73b0\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6c9f\u901a\uff08\u5982\u6cbb\u7597\u4f1a\u8bdd\u3001\u5546\u4e1a\u8c08\u5224\uff09\u90fd\u53d7\u5230\u8fde\u7eed\u65f6\u95f4\u9650\u5236\uff0c\u4f46\u5f53\u524dLLM\u67b6\u6784\u548c\u8bc4\u4f30\u534f\u8bae\u5f88\u5c11\u6d4b\u8bd5\u5728\u5b9e\u65f6\u622a\u6b62\u65f6\u95f4\u4e0b\u7684\u65f6\u95f4\u610f\u8bc6\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u8c08\u5224\u5b9e\u9a8c\uff0c\u914d\u5bf9\u667a\u80fd\u4f53\u5728\u4e25\u683c\u622a\u6b62\u65f6\u95f4\u4e0b\u8fdb\u884c\u8c08\u5224\u3002\u8bbe\u7f6e\u4e24\u79cd\u6761\u4ef6\uff1a\u63a7\u5236\u7ec4\uff08\u4ec5\u77e5\u9053\u5168\u5c40\u65f6\u95f4\u9650\u5236\uff09\u548c\u65f6\u95f4\u611f\u77e5\u7ec4\uff08\u6bcf\u8f6e\u63a5\u6536\u5269\u4f59\u65f6\u95f4\u66f4\u65b0\uff09\u3002\u6bd4\u8f83\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u4ea4\u6613\u8fbe\u6210\u7387\u3002", "result": "\u65f6\u95f4\u611f\u77e5\u6761\u4ef6\u4e0b\u7684\u4ea4\u6613\u8fbe\u6210\u7387\u663e\u8457\u66f4\u9ad8\uff08GPT-5.1\u4e3a32% vs 4%\uff09\uff0c\u62a5\u4ef7\u63a5\u53d7\u7387\u9ad8\u51fa\u516d\u500d\u3002\u4f46\u5728\u56de\u5408\u5236\u9650\u5236\u4e0b\uff0c\u76f8\u540cLLM\u80fd\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u4ea4\u6613\u8fbe\u6210\u7387\uff08\u226595%\uff09\u3002", "conclusion": "LLM\u7f3a\u4e4f\u5185\u90e8\u65f6\u95f4\u8ffd\u8e2a\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8bb8\u591a\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002\u5931\u8d25\u6e90\u4e8e\u65f6\u95f4\u8ffd\u8e2a\u800c\u975e\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.12535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12535", "abs": "https://arxiv.org/abs/2601.12535", "authors": ["Ahmed Attia", "Alham Fikri"], "title": "Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning", "comment": null, "summary": "Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7684\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528NLLB\u6a21\u578b\u901a\u8fc7\u5f80\u8fd4\u7ffb\u8bd1\u8fdb\u884c\u81ea\u4e3e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u53d6\u5f97\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e73\u884c\u6570\u636e\u7684\u6536\u96c6\uff0c\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u5173\u6ce8\uff0c\u4f46\u8bb8\u591a\u6539\u8fdb\u65b9\u6cd5\u5c1a\u672a\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5229\u7528\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528NLLB\u6a21\u578b\u8fdb\u884c\u5f80\u8fd4\u7ffb\u8bd1\u81ea\u4e3e\uff1a\u5c06\u82f1\u8bed\u7ffb\u8bd1\u6210\u76ee\u6807\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u518d\u7ffb\u8bd1\u56de\u82f1\u8bed\u3002\u4f7f\u7528chrF++\u548cBLEU\u7ec4\u5408\u4f5c\u4e3a\u91cd\u5efa\u82f1\u8bed\u53e5\u5b50\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728NLLB-MD\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30600M\u548c1.3B\u53c2\u6570\u7684NLLB\u6a21\u578b\uff0c\u5728Central Aymara\u3001Friulian\u3001Wolof\u548cRussian\u7b49\u8bed\u8a00\u4e0a\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u6539\u8fdb\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\u7ffb\u8bd1\u8f93\u51fa\u7684\u6d41\u7545\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u89c4\u6a21\u6269\u5c55\u4e2d\u8fdb\u4e00\u6b65\u53d7\u76ca\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u9884\u8bad\u7ec3\u77e5\u8bc6\u5e76\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u3002\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u662f\u63d0\u5347\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.12090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12090", "abs": "https://arxiv.org/abs/2601.12090", "authors": ["Matej Mok", "Luk\u00e1\u0161 Gajdo\u0161ech", "Michal Mes\u00e1ro\u0161", "Martin Madaras", "Viktor Kocur"], "title": "Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data", "comment": "8 pages", "summary": "The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin's 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\\circ$ rotation error) while not requiring instance-specific CAD models during inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u4e1a\u6599\u7bb1\u76846DoF\u4f4d\u59ff\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b3D\u7ebf\u6bb5\u5e76\u5229\u7528\u51e0\u4f55\u7ea6\u675f\uff0c\u65e0\u9700\u5b9e\u4f8b\u7279\u5b9a\u7684CAD\u6a21\u578b\uff0c\u5728\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216CAD\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u3001\u7269\u4f53\u5b9e\u4f8b\u591a\u53d8\u7684\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u5de5\u4e1a\u6599\u7bb1\u5177\u6709\u7acb\u65b9\u4f53\u51e0\u4f55\u7279\u5f81\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e00\u7279\u6027\u7b80\u5316\u4f4d\u59ff\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u9996\u5148\u68c0\u6d4b\u6599\u7bb1\u9876\u90e8\u8fb9\u7f18\u5bf9\u5e94\u76843D\u7ebf\u6bb5\uff0c\u5c062D\u7ebf\u6bb5\u68c0\u6d4b\u7f51\u7edcLeTR\u6269\u5c55\u5230\u7ed3\u6784\u5316\u70b9\u4e91\u6570\u636e\u3002\u7136\u540e\u901a\u8fc7\u7b80\u5355\u7684\u51e0\u4f55\u5904\u7406\u6d41\u7a0b\uff0c\u5229\u7528\u68c0\u6d4b\u5230\u76843D\u7ebf\u6bb5\u7a33\u5065\u5730\u786e\u5b9a\u6599\u7bb1\u76846DoF\u4f4d\u59ff\u3002", "result": "\u65b9\u6cd5\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u52a0\u5165\u5408\u6210\u8bad\u7ec3\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u771f\u5b9e\u626b\u63cf\u6570\u636e\u7684\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u76846DoF\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u7cbe\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff083cm\u5e73\u79fb\u8bef\u5dee\uff0c8.2\u00b0\u65cb\u8f6c\u8bef\u5dee\uff09\uff0c\u4e14\u63a8\u7406\u65f6\u65e0\u9700\u5b9e\u4f8b\u7279\u5b9a\u7684CAD\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9488\u5bf9\u5de5\u4e1a\u6599\u7bb1\u7684\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc73D\u7ebf\u6bb5\u68c0\u6d4b\u548c\u51e0\u4f55\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76846DoF\u4f4d\u59ff\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u5b9e\u4f8b\u591a\u53d8\u7684\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13233", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.13233", "abs": "https://arxiv.org/abs/2601.13233", "authors": ["Bolin Chen", "Dex Doksoo Lee", "Wei \"Wayne'' Chen", "Wei Chen"], "title": "RAG: A Random-Forest-Based Generative Design Framework for Uncertainty-Aware Design of Metamaterials with Complex Functional Response Requirements", "comment": null, "summary": "Metamaterials design for advanced functionality often entails the inverse design on nonlinear and condition-dependent responses (e.g., stress-strain relation and dispersion relation), which are described by continuous functions. Most existing design methods focus on vector-valued responses (e.g., Young's modulus and bandgap width), while the inverse design of functional responses remains challenging due to their high-dimensionality, the complexity of accommodating design requirements in inverse-design frameworks, and non-existence or non-uniqueness of feasible solutions. Although generative design approaches have shown promise, they are often data-hungry, handle design requirements heuristically, and may generate infeasible designs without uncertainty quantification. To address these challenges, we introduce a RAndom-forest-based Generative approach (RAG). By leveraging the small-data compatibility of random forests, RAG enables data-efficient predictions of high-dimensional functional responses. During the inverse design, the framework estimates the likelihood through the ensemble which quantifies the trustworthiness of generated designs while reflecting the relative difficulty across different requirements. The one-to-many mapping is addressed through single-shot design generation by sampling from the conditional likelihood. We demonstrate RAG on: 1) acoustic metamaterials with prescribed partial passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through responses, using 500 and 1057 samples, respectively. Its data-efficiency is benchmarked against neural networks on a public mechanical metamaterial dataset with nonlinear stress-strain relations. Our framework provides a lightweight, trustworthy pathway to inverse design involving functional responses, expensive simulations, and complex design requirements, beyond metamaterials.", "AI": {"tldr": "\u63d0\u51faRAG\u65b9\u6cd5\uff0c\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u751f\u6210\u5f0f\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u529f\u80fd\u54cd\u5e94\uff08\u5982\u5e94\u529b-\u5e94\u53d8\u5173\u7cfb\u3001\u8272\u6563\u5173\u7cfb\uff09\u7684\u9006\u8bbe\u8ba1\u95ee\u9898\uff0c\u5177\u6709\u6570\u636e\u6548\u7387\u9ad8\u3001\u53ef\u4fe1\u5ea6\u91cf\u5316\u7b49\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u9006\u8bbe\u8ba1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5411\u91cf\u503c\u54cd\u5e94\uff08\u5982\u6768\u6c0f\u6a21\u91cf\u3001\u5e26\u9699\u5bbd\u5ea6\uff09\uff0c\u800c\u529f\u80fd\u54cd\u5e94\u7684\u9006\u8bbe\u8ba1\u9762\u4e34\u9ad8\u7ef4\u6027\u3001\u8bbe\u8ba1\u9700\u6c42\u590d\u6742\u3001\u53ef\u884c\u89e3\u4e0d\u5b58\u5728\u6216\u4e0d\u552f\u4e00\u7b49\u6311\u6218\u3002\u751f\u6210\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u3001\u542f\u53d1\u5f0f\u5904\u7406\u8bbe\u8ba1\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u63d0\u51faRAndom-forest-based Generative approach (RAG)\uff0c\u5229\u7528\u968f\u673a\u68ee\u6797\u7684\u5c0f\u6570\u636e\u517c\u5bb9\u6027\u9ad8\u6548\u9884\u6d4b\u9ad8\u7ef4\u529f\u80fd\u54cd\u5e94\u3002\u5728\u9006\u8bbe\u8ba1\u4e2d\uff0c\u901a\u8fc7\u96c6\u6210\u4f30\u8ba1\u4f3c\u7136\u5ea6\u6765\u91cf\u5316\u751f\u6210\u8bbe\u8ba1\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u4f3c\u7136\u91c7\u6837\u5b9e\u73b0\u5355\u6b21\u8bbe\u8ba1\u751f\u6210\u3002", "result": "\u5728\u58f0\u5b66\u8d85\u6750\u6599\uff08500\u6837\u672c\uff09\u548c\u529b\u5b66\u8d85\u6750\u6599\uff081057\u6837\u672c\uff09\u4e0a\u9a8c\u8bc1\u4e86RAG\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u89c4\u5b9a\u7684\u90e8\u5206\u901a\u5e26/\u963b\u5e26\u548c\u76ee\u6807\u7a81\u8df3\u54cd\u5e94\u3002\u5728\u516c\u5f00\u529b\u5b66\u8d85\u6750\u6599\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u795e\u7ecf\u7f51\u7edc\u76f8\u6bd4\u663e\u793a\u51fa\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "RAG\u4e3a\u6d89\u53ca\u529f\u80fd\u54cd\u5e94\u3001\u6602\u8d35\u4eff\u771f\u548c\u590d\u6742\u8bbe\u8ba1\u9700\u6c42\u7684\u9006\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u6761\u8f7b\u91cf\u7ea7\u3001\u53ef\u4fe1\u8d56\u7684\u9014\u5f84\uff0c\u5176\u5e94\u7528\u8303\u56f4\u53ef\u6269\u5c55\u81f3\u8d85\u6750\u6599\u4ee5\u5916\u7684\u9886\u57df\u3002"}}
{"id": "2601.12549", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12549", "abs": "https://arxiv.org/abs/2601.12549", "authors": ["Ilia Badanin", "Daniil Dzenhaliou", "Imanol Schlag"], "title": "Benchmarking Concept-Spilling Across Languages in LLMs", "comment": null, "summary": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u9c81\u68d2\u6027\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6d4b\u91cf\u6a21\u578b\u5904\u7406\u8de8\u8bed\u8a00\u591a\u4e49\u8bcd\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u8bed\u8a00\u6ea2\u51fa\u73b0\u8c61\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u8bed\u8a00\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5411\u5176\u4ed6\u8bed\u8a00\u8868\u793a\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u751f\u6210\u65f6\u51fa\u73b0\u8bed\u4e49\u5e72\u6270\uff08\u8bed\u8a00\u6ea2\u51fa\u73b0\u8c61\uff09\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u8bed\u4e49\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u6bd4\u8f83\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u610f\u4e49\u751f\u6210\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u4e3a100\u4e2a\u9ad8\u591a\u4e49\u6027\u82f1\u8bed\u5355\u8bcd\u751f\u6210\u4e94\u79cd\u542b\u4e49\uff0c\u6d4b\u91cf\u6a21\u578b\u4f55\u65f6\u5f00\u59cb\u4f7f\u7528\u4e3b\u5bfc\u8bed\u8a00\uff08\u5982\u82f1\u8bed\uff09\u7684\u542b\u4e49\u800c\u975e\u76ee\u6807\u8bed\u8a00\u7684\u542b\u4e49\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5f00\u6e90\u548c\u95ed\u6e90\u591a\u8bed\u8a00LLM\u5728\u4e5d\u79cd\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u7684\u8bed\u4e49\u9c81\u68d2\u6027\u5dee\u5f02\u3002\u8bed\u4e49\u66f4\u5f3a\u7684\u6a21\u578b\u5728\u751f\u6210\u5e8f\u5217\u540e\u671f\u624d\u4f1a\u8f6c\u5411\u4e3b\u5bfc\u8bed\u8a00\u542b\u4e49\uff0c\u800c\u8f83\u5f31\u6a21\u578b\u5219\u66f4\u65e9\u8f6c\u5411\uff0c\u4ece\u800c\u53ef\u4ee5\u5efa\u7acb\u6a21\u578b\u6bd4\u8f83\u7684\u6392\u540d\u7cfb\u7edf\u3002", "conclusion": "\u8d21\u732e\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u8bed\u4e49\u8bc4\u4f30\u57fa\u51c6\u548c\u4e25\u683c\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u4e3a\u5f00\u53d1\u66f4\u8bed\u8a00\u5e73\u8861\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u8981\u6c42\u786e\u5b9a\u9519\u8bef\u6765\u6e90\u7684\u60c5\u51b5\u4e0b\u5bf9\u6a21\u578b\u8fdb\u884c\u539f\u5219\u6027\u6bd4\u8f83\u3002"}}
{"id": "2601.12109", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12109", "abs": "https://arxiv.org/abs/2601.12109", "authors": ["Larissa Ferreira Rodrigues Moreira", "Rodrigo Moreira", "Leonardo Gabriel Ferreira Rodrigues"], "title": "Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification", "comment": null, "summary": "Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.", "AI": {"tldr": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u5c06\u5927\u578bCNN\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b0\u5496\u5561\u53f6\u75c5\u5bb3\u7684\u53ef\u6301\u7eed\u8bbe\u5907\u7aef\u8bca\u65ad\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\u3002", "motivation": "\u5496\u5561\u53f6\u75c5\u5bb3\u7684\u7530\u95f4\u8bca\u65ad\u9762\u4e34\u6311\u6218\uff0c\u867d\u7136AI\u89c6\u89c9\u6a21\u578b\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u53d7\u9650\u4e8e\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u548c\u7f51\u7edc\u8fde\u63a5\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u8ba9\u5728\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u7684\u9ad8\u5bb9\u91cfCNN\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u7d27\u51d1CNN\u6a21\u578b\u3002\u901a\u8fc7\u7b80\u5355\u4f18\u5316\u7684\u96c6\u6210\u65b9\u6cd5\u6574\u5408\u5bc6\u96c6\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5bf9\uff0c\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u7ea6\u675f\u4e0b\u63d0\u5347\u51c6\u786e\u6027\u3002", "result": "\u5728\u7cbe\u5fc3\u6574\u7406\u7684\u5496\u5561\u53f6\u6570\u636e\u96c6\u4e0a\uff0c\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u96c6\u6210\u6a21\u578b\u8fbe\u5230\u4e86\u4e0e\u5148\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u84b8\u998f\u548c\u96c6\u6210\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u53ef\u4ee5\u4e3a\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u7684\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u8bbe\u5907\u7aef\u75c5\u5bb3\u8bca\u65ad\u3002"}}
{"id": "2601.13262", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13262", "abs": "https://arxiv.org/abs/2601.13262", "authors": ["Eric Onyame", "Akash Ghosh", "Subhadip Baidya", "Sriparna Saha", "Xiuying Chen", "Chirag Agarwal"], "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning", "comment": null, "summary": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/", "AI": {"tldr": "CURE-MED\u6846\u67b6\u901a\u8fc7\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u63d0\u5347LLM\u7684\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\u80fd\u529b\uff0c\u572813\u79cd\u8bed\u8a00\u4e0a\u663e\u8457\u6539\u5584\u903b\u8f91\u6b63\u786e\u6027\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u8bed\u8a00\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\u5e94\u7528\u4e2d\u4ecd\u4e0d\u53ef\u9760\uff0c\u963b\u788d\u4e86\u5728\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faCURE-MED\u6846\u67b6\uff1a1) \u5f15\u5165CUREMED-BENCH\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u91c7\u7528\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u4ee3\u7801\u5207\u6362\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5171\u540c\u63d0\u5347\u903b\u8f91\u6b63\u786e\u6027\u548c\u8bed\u8a00\u7a33\u5b9a\u6027\u3002", "result": "\u572813\u79cd\u8bed\u8a00\u4e0a\uff0c7B\u53c2\u6570\u6a21\u578b\u8fbe\u523085.21%\u8bed\u8a00\u4e00\u81f4\u6027\u548c54.35%\u903b\u8f91\u6b63\u786e\u6027\uff1b32B\u53c2\u6570\u6a21\u578b\u8fbe\u523094.96%\u8bed\u8a00\u4e00\u81f4\u6027\u548c70.04%\u903b\u8f91\u6b63\u786e\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CURE-MED\u6846\u67b6\u652f\u6301\u53ef\u9760\u4e14\u516c\u5e73\u7684\u591a\u8bed\u8a00\u533b\u7597\u63a8\u7406\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u591a\u8bed\u8a00\u533b\u7597\u73af\u5883\u4e2dLLM\u7684\u90e8\u7f72\u3002"}}
{"id": "2601.12555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12555", "abs": "https://arxiv.org/abs/2601.12555", "authors": ["Yihong Liu", "Bingyu Xiong", "Hinrich Sch\u00fctze"], "title": "Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models", "comment": "preprint", "summary": "Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.", "AI": {"tldr": "LLMs\u5728\u4e0a\u4e0b\u6587\u4e2d\u4ecb\u7684\u4e8b\u5b9e\u56de\u5fc6\u4e2d\u8868\u73b0\u4e0b\u964d\uff0c\u5c3d\u7ba1\u5927\u6a21\u578b\u5bf9\u6b64\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u5b64\u7acb\u4e8b\u5b9e\u56de\u5fc6\u4e0e\u4e0a\u4e0b\u6587\u4f9d\u8d56\u8bed\u8a00\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u56de\u5fc6\u8bc4\u4f30\u4e3b\u8981\u8bc4\u4f30\u5b64\u7acb\u7684\u4e8b\u5b9e\u68c0\u7d22\uff0c\u4f46\u5728\u81ea\u7136\u8bed\u8a00\u4f7f\u7528\u4e2d\uff0c\u4e8b\u5b9e\u901a\u5e38\u901a\u8fc7\u4e0a\u4e0b\u6587\u95f4\u63a5\u8bbf\u95ee\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u5728\u76ee\u6807\u5b9e\u4f53\u5d4c\u5165\u81ea\u7136\u8bed\u5883\u800c\u975e\u660e\u786e\u67e5\u8be2\u65f6\uff0c\u80fd\u5426\u53ef\u9760\u5730\u8de8\u8bed\u8a00\u68c0\u7d22\u4e8b\u5b9e\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u4fdd\u7559\u5e95\u5c42\u4e8b\u5b9e\u4f46\u901a\u8fc7\u4e0a\u4e0b\u6587\u53e5\u5b50\u5f15\u5165\u6307\u79f0\u4e2d\u4ecb\u7684\u53d7\u63a7\u63d0\u793a\uff1b\u4f7f\u7528\u5408\u6210\u540d\u79f0\u548c\u771f\u5b9e\u540d\u79f0\u8de8\u8bed\u8a00\u6bd4\u8f83\u4ee5\u533a\u5206\u4e0a\u4e0b\u6587\u6548\u5e94\u4e0e\u540d\u79f0\u7279\u5b9a\u5173\u8054\uff1b\u5728\u4e94\u79cd\u8bed\u8a00\u4e2d\u8bc4\u4f30\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u3002", "result": "\u4e0a\u4e0b\u6587\u4e2d\u4ecb\u6301\u7eed\u964d\u4f4e\u4e8b\u5b9e\u56de\u5fc6\u6027\u80fd\uff0c\u4e0d\u540c\u5173\u7cfb\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u5927\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u4e2d\u4ecb\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4e0e\u76f4\u63a5\u67e5\u8be2\u7684\u6027\u80fd\u5dee\u8ddd\u51cf\u5c0f\uff1b\u771f\u5b9e\u540d\u79f0\u548c\u540d\u79f0\u8d77\u6e90\u7684\u5f71\u54cd\u6df7\u5408\u4e14\u65e0\u7cfb\u7edf\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u8bed\u8a00LLMs\u4e2d\u5b64\u7acb\u4e8b\u5b9e\u56de\u5fc6\u4e0e\u4e0a\u4e0b\u6587\u4f9d\u8d56\u8bed\u8a00\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u9ad8\u4f30\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u8bed\u8a00\u4f7f\u7528\u4e2d\u7684\u4e8b\u5b9e\u68c0\u7d22\u80fd\u529b\u3002"}}
{"id": "2601.12111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12111", "abs": "https://arxiv.org/abs/2601.12111", "authors": ["Wyatt McCurdy", "Xin Zhang", "Yuqi Song", "Min Gao"], "title": "RCDN: Real-Centered Detection Network for Robust Face Forgery Identification", "comment": null, "summary": "Image forgery has become a critical threat with the rapid proliferation of AI-based generation tools, which make it increasingly easy to synthesize realistic but fraudulent facial content. Existing detection methods achieve near-perfect performance when training and testing are conducted within the same domain, yet their effectiveness deteriorates substantially in crossdomain scenarios. This limitation is problematic, as new forgery techniques continuously emerge and detectors must remain reliable against unseen manipulations. To address this challenge, we propose the Real-Centered Detection Network (RCDN), a frequency spatial convolutional neural networks(CNN) framework with an Xception backbone that anchors its representation space around authentic facial images. Instead of modeling the diverse and evolving patterns of forgeries, RCDN emphasizes the consistency of real images, leveraging a dual-branch architecture and a real centered loss design to enhance robustness under distribution shifts. Extensive experiments on the DiFF dataset, focusing on three representative forgery types (FE, I2I, T2I), demonstrate that RCDN achieves both state-of-the-art in-domain accuracy and significantly stronger cross-domain generalization. Notably, RCDN reduces the generalization gap compared to leading baselines and achieves the highest cross/in-domain stability ratio, highlighting its potential as a practical solution for defending against evolving and unseen image forgery techniques.", "AI": {"tldr": "RCDN\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u771f\u5b9e\u56fe\u50cf\u4e00\u81f4\u6027\u7684\u9891\u7387\u7a7a\u95f4CNN\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u548c\u771f\u5b9e\u4e2d\u5fc3\u635f\u5931\u8bbe\u8ba1\uff0c\u5728\u8de8\u57df\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\uff0c\u56fe\u50cf\u4f2a\u9020\u6210\u4e3a\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u540c\u57df\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u57df\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u65b0\u7684\u4f2a\u9020\u6280\u672f\u4e0d\u65ad\u6d8c\u73b0\uff0c\u68c0\u6d4b\u5668\u9700\u8981\u5bf9\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u6280\u672f\u4fdd\u6301\u53ef\u9760\u3002", "method": "\u63d0\u51faReal-Centered Detection Network (RCDN)\uff0c\u57fa\u4e8eXception\u9aa8\u5e72\u7f51\u7edc\u7684\u9891\u7387\u7a7a\u95f4CNN\u6846\u67b6\u3002\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\u548c\u771f\u5b9e\u4e2d\u5fc3\u635f\u5931\u8bbe\u8ba1\uff0c\u5c06\u8868\u793a\u7a7a\u95f4\u951a\u5b9a\u5728\u771f\u5b9e\u9762\u90e8\u56fe\u50cf\u5468\u56f4\uff0c\u5f3a\u8c03\u771f\u5b9e\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u800c\u975e\u5efa\u6a21\u591a\u6837\u5316\u7684\u4f2a\u9020\u6a21\u5f0f\u3002", "result": "\u5728DiFF\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u79cd\u4ee3\u8868\u6027\u4f2a\u9020\u7c7b\u578b\uff08FE\u3001I2I\u3001T2I\uff09\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cRCDN\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u57df\u5185\u7cbe\u5ea6\u548c\u663e\u8457\u66f4\u5f3a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u6cdb\u5316\u5dee\u8ddd\uff0c\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u8de8\u57df/\u57df\u5185\u7a33\u5b9a\u6027\u6bd4\u7387\u3002", "conclusion": "RCDN\u901a\u8fc7\u4e13\u6ce8\u4e8e\u771f\u5b9e\u56fe\u50cf\u7684\u4e00\u81f4\u6027\u800c\u975e\u591a\u6837\u5316\u7684\u4f2a\u9020\u6a21\u5f0f\uff0c\u5728\u8de8\u57df\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9632\u5fa1\u4e0d\u65ad\u6f14\u53d8\u548c\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u4f2a\u9020\u6280\u672f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13268", "abs": "https://arxiv.org/abs/2601.13268", "authors": ["Zainab Ghafoor", "Md Shafiqul Islam", "Koushik Howlader", "Md Rasel Khondokar", "Tanusree Bhattacharjee", "Sayantan Chakraborty", "Adrito Roy", "Ushashi Bhattacharjee", "Tirtho Roy"], "title": "Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8fed\u4ee3\u5bf9\u9f50\u63d0\u5347\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\uff0c\u5728900\u4e2a\u4e34\u5e8a\u67e5\u8be2\u4e0a\u5b9e\u73b0\u4f26\u7406\u8fdd\u89c4\u51cf\u5c1189%\u3001\u98ce\u9669\u964d\u7ea792%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u786e\u4fdd\u5176\u4f26\u7406\u5b8c\u6574\u6027\u548c\u5b89\u5168\u5408\u89c4\u6027\u4ecd\u662f\u4e34\u5e8a\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5b89\u5168\u6cbb\u7406\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u4e2a\u751f\u6210\u6a21\u578b\uff08DeepSeek R1\u548cMed-PaLM\uff09\u548c\u4e24\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\uff08LLaMA 3.1\u548cPhi-4\uff09\uff0c\u4f7f\u7528\u7f8e\u56fd\u533b\u5b66\u4f1a\u533b\u5b66\u4f26\u7406\u539f\u5219\u548c\u4e94\u7ea7\u5b89\u5168\u98ce\u9669\u8bc4\u4f30\u534f\u8bae\u8fdb\u884c\u7ed3\u6784\u5316\u8fed\u4ee3\u5bf9\u9f50\u3002", "result": "\u5728\u6db5\u76d69\u4e2a\u4f26\u7406\u9886\u57df\u7684900\u4e2a\u4e34\u5e8a\u67e5\u8be2\u4e0a\uff0cDeepSeek R1\u6536\u655b\u66f4\u5feb\uff08\u5e73\u57472.34\u6b21\u8fed\u4ee3\uff09\uff0cMed-PaLM\u5728\u9690\u79c1\u654f\u611f\u573a\u666f\u8868\u73b0\u66f4\u4f18\uff1b\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u5faa\u73af\u5b9e\u73b0\u4f26\u7406\u8fdd\u89c4\u51cf\u5c1189%\u3001\u98ce\u9669\u964d\u7ea7\u738792%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u533b\u7597AI\u5b89\u5168\u6cbb\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u7cbe\u70bc\u6709\u6548\u63d0\u5347\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.12607", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12607", "abs": "https://arxiv.org/abs/2601.12607", "authors": ["Anurag Acharya", "Timothy Vega", "Rizwan A. Ashraf", "Anshu Sharma", "Derek Parker", "Robert Rallo"], "title": "A Cloud-based Multi-Agentic Workflow for Science", "comment": null, "summary": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u3001\u6a21\u578b\u72ec\u7acb\u7684\u4e91\u7aef\u79d1\u5b66\u52a9\u624b\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u4ee3\u7406\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff0c\u80fd\u591f\u5904\u7406\u4ece\u6587\u732e\u7efc\u8ff0\u5230\u590d\u6742\u6a21\u62df\u7b49\u4efb\u52a1\uff0c\u5728\u50ac\u5316\u5242\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\uff08\u5982\u8fd0\u884c\u6a21\u62df\u6216\u590d\u6742\u51b3\u7b56\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u73b0\u6709\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u5e73\u8861\u6a21\u578b\u3001\u4e91\u670d\u52a1\u5546\u548c\u5916\u90e8\u8d44\u6e90\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u3001\u6a21\u578b\u72ec\u7acb\u7684\u4e91\u7aef\u4ee3\u7406\u6846\u67b6\uff0c\u91c7\u7528\u76d1\u7763\u4ee3\u7406\u534f\u8c03\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u80fd\u529b\u7684\u5b50\u4ee3\u7406\uff0c\u80fd\u591f\u5904\u7406\u4ece\u7b80\u5355\u7684\u6587\u732e\u7efc\u8ff0\u3001\u6570\u636e\u5206\u6790\u5230\u590d\u6742\u7684\u6a21\u62df\u8fd0\u884c\u7b49\u4efb\u52a1\u3002", "result": "\u5728\u50ac\u5316\u5242\u7814\u7a76\u9886\u57df\u6784\u5efa\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u4efb\u52a1\u8def\u7531\u6b63\u786e\u7387\u8fbe90%\uff0c\u5408\u6210\u4efb\u52a1\u5b8c\u6210\u738797.5%\uff0c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u5b8c\u6210\u738791%\uff0c\u6210\u672c\u6548\u76ca\u4f18\u4e8e\u6216\u5ab2\u7f8e\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u534f\u8c03\u591a\u4e2a\u4ee3\u7406\u5904\u7406\u590d\u6742\u79d1\u5b66\u4efb\u52a1\uff0c\u5177\u6709\u53ef\u590d\u5236\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.12119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12119", "abs": "https://arxiv.org/abs/2601.12119", "authors": ["Xiaotong Zhou", "Zhenhui Yuan", "Yi Han", "Tianhua Xu", "Laurence T. Yang"], "title": "CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction", "comment": null, "summary": "Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.", "AI": {"tldr": "\u63d0\u51faCARLA-Round\u4eff\u771f\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u73af\u5c9b\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a7\u5236\u5929\u6c14\u548c\u4ea4\u901a\u5bc6\u5ea6\u6761\u4ef6\uff0c\u91cf\u5316\u5206\u6790\u5404\u56e0\u7d20\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73af\u5c9b\u573a\u666f\u7684\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4ea4\u901a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\u3001\u8fde\u7eed\u7684\u6c47\u5165\u8ba9\u884c\u4ea4\u4e92\u4ee5\u53ca\u7f3a\u4e4f\u4ea4\u901a\u4fe1\u53f7\u800c\u6781\u5177\u6311\u6218\u3002\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5b58\u5728\u89c2\u6d4b\u4e0d\u5b8c\u6574\u548c\u56e0\u7d20\u6df7\u6742\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528CARLA\u4eff\u771f\u5e73\u53f0\u7cfb\u7edf\u8bbe\u8ba1\u6570\u636e\u96c6\uff0c\u63a7\u52365\u79cd\u5929\u6c14\u6761\u4ef6\u548c5\u4e2a\u4ea4\u901a\u5bc6\u5ea6\u7b49\u7ea7\uff08\u670d\u52a1\u6c34\u5e73A-E\uff09\uff0c\u5f62\u621025\u4e2a\u7ed3\u6784\u5316\u573a\u666f\u3002\u6bcf\u4e2a\u573a\u666f\u5305\u542b\u771f\u5b9e\u7684\u9a7e\u9a76\u884c\u4e3a\u6df7\u5408\uff0c\u5e76\u63d0\u4f9b\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u660e\u786e\u6807\u6ce8\u3002", "result": "\u9a8c\u8bc1\u5b9e\u9a8c\u663e\u793a\u4ea4\u901a\u5bc6\u5ea6\u5bf9\u9884\u6d4b\u96be\u5ea6\u5177\u6709\u4e3b\u5bfc\u6027\u7684\u5355\u8c03\u5f71\u54cd\uff0c\u800c\u5929\u6c14\u6761\u4ef6\u5448\u73b0\u975e\u7ebf\u6027\u5f71\u54cd\u3002\u6700\u4f73\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754crounD\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.312m ADE\uff0c\u8bc1\u660e\u4e86\u6709\u6548\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "CARLA-Round\u6570\u636e\u96c6\u901a\u8fc7\u7ed3\u6784\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5bf9\u5f71\u54cd\u56e0\u7d20\u7684\u91cf\u5316\u5206\u6790\uff0c\u8fd9\u5728\u6df7\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u73af\u5c9b\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u591a\u6a21\u6001\u3001\u73b0\u5b9e\u7684\u57fa\u51c6\u3002"}}
{"id": "2601.13327", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13327", "abs": "https://arxiv.org/abs/2601.13327", "authors": ["Po-Yu Liang", "Tobo Duran", "Jun Bai"], "title": "PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion", "comment": null, "summary": "We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model", "AI": {"tldr": "PepEDiff\u662f\u4e00\u4e2a\u76f4\u63a5\u4ece\u86cb\u767d\u8d28\u5d4c\u5165\u7684\u8fde\u7eed\u6f5c\u7a7a\u95f4\u751f\u6210\u80bd\u7ed3\u5408\u5242\u5e8f\u5217\u7684\u96f6\u6837\u672c\u751f\u6210\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u7ed3\u6784\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u5e8f\u5217\u591a\u6837\u6027", "motivation": "\u73b0\u6709\u80bd\u7ed3\u5408\u5242\u751f\u6210\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e2d\u95f4\u7ed3\u6784\u9884\u6d4b\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u5e76\u9650\u5236\u4e86\u5e8f\u5217\u591a\u6837\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u86cb\u767d\u8d28\u5d4c\u5165\u6a21\u578b\u521b\u5efa\u8fde\u7eed\u6f5c\u7a7a\u95f4\uff0c\u901a\u8fc7\u6f5c\u7a7a\u95f4\u63a2\u7d22\u548c\u57fa\u4e8e\u6269\u6563\u7684\u91c7\u6837\u751f\u6210\u80bd\u5e8f\u5217\uff0c\u4e0d\u4f9d\u8d56\u9884\u6d4b\u7ed3\u6784", "result": "\u5728TIGIT\u7b49\u6311\u6218\u6027\u76ee\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u53ef\u836f\u6027\u53e3\u888b\u7684\u5927\u800c\u5e73\u5766\u7684\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u754c\u9762\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "PepEDiff\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u3001\u65e0\u9700\u7ed3\u6784\u7684\u96f6\u6837\u672c\u80bd\u7ed3\u5408\u5242\u8bbe\u8ba1\u6846\u67b6\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u751f\u6210\u8d85\u8d8a\u5df2\u77e5\u7ed3\u5408\u5242\u5206\u5e03\u7684\u65b0\u9896\u5e8f\u5217"}}
{"id": "2601.12618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12618", "abs": "https://arxiv.org/abs/2601.12618", "authors": ["Elham Tajik", "Conrad Borchers", "Bahar Shahrokhian", "Sebastian Simon", "Ali Keramati", "Sonika Pal", "Sreecharan Sankaranarayanan"], "title": "Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems", "comment": "LAK 2026 conference paper, 7 pages", "summary": "Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u8f68\u8ff9\u4f5c\u4e3a\u8fc7\u7a0b\u6570\u636e\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u91cf\u5316\u667a\u80fd\u4f53\u95f4\u7684\u5206\u6b67\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u610f\u4e49\u7684\u5206\u6790\u4fe1\u53f7\uff0c\u4ee5\u589e\u5f3a\u8d28\u6027\u7f16\u7801\u7684\u89e3\u91ca\u5b9e\u8df5\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u81ea\u52a8\u5316\u6216\u4eba\u673a\u534f\u4f5c\u7684\u8d28\u6027\u5206\u6790\u65b9\u6cd5\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u76f8\u5173\u65b9\u6cd5\u8bba\u6807\u51c6\u4ecd\u7136\u6709\u9650\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8d28\u6027\u7f16\u7801\u7684\u4e25\u8c28\u6027\u548c\u89e3\u91ca\u6df1\u5ea6\u3002", "method": "\u4f7f\u7528LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5bf9\u8fd110,000\u4e2a\u4eba\u7c7b\u8f85\u5bfc\u5bf9\u8bdd\u7247\u6bb5\u8fdb\u884c\u7f16\u7801\uff0c\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u5e94\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u7cfb\u7edf\u68c0\u6d4b\u3001\u91cf\u5316\u548c\u89e3\u91ca\u667a\u80fd\u4f53\u95f4\u7684\u5206\u6b67\uff0c\u5c06\u5206\u6b67\u91cd\u6784\u4e3a\u5206\u6790\u4fe1\u53f7\u3002", "result": "LLM\u667a\u80fd\u4f53\u7684\u8bed\u4e49\u63a8\u7406\u76f8\u4f3c\u5ea6\u80fd\u6709\u6548\u533a\u5206\u5171\u8bc6\u4e0e\u5206\u6b67\uff0c\u5e76\u4e0e\u4eba\u7c7b\u7f16\u7801\u53ef\u9760\u6027\u76f8\u5173\u3002\u57fa\u4e8e\u8be5\u6307\u6807\u7684\u8d28\u6027\u5206\u6790\u63ed\u793a\u4e86\u7f16\u7801\u5185\u7684\u7ec6\u5fae\u6559\u5b66\u5b50\u529f\u80fd\uff0c\u5e76\u4e3a\u6982\u5ff5\u7f16\u7801\u672c\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u5b9a\u91cf\u76f8\u4f3c\u5ea6\u6307\u6807\u4e0e\u8d28\u6027\u5ba1\u67e5\uff0c\u8be5\u65b9\u6cd5\u80fd\u901a\u8fc7\u63ed\u793a\u89e3\u91ca\u6b67\u4e49\u6765\u6539\u8fdb\u548c\u52a0\u901f\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7684\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\u5efa\u7acb\uff0c\u5c24\u5176\u5f53LLM\u4e0e\u4eba\u7c7b\u534f\u4f5c\u65f6\u3002\u63a8\u7406\u8f68\u8ff9\u5206\u6b67\u4ee3\u8868\u4e86\u6559\u80b2\u7814\u7a76\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\u548c\u89e3\u91ca\u6df1\u5ea6\u7684\u5b9d\u8d35\u65b0\u5206\u6790\u4fe1\u53f7\u3002"}}
{"id": "2601.12147", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12147", "abs": "https://arxiv.org/abs/2601.12147", "authors": ["Zezhong Fan", "Xiaohan Li", "Topojoy Biswas", "Kaushiki Nag", "Kannan Achan"], "title": "Segment and Matte Anything in a Unified Model", "comment": "AAAI 2026", "summary": "Segment Anything (SAM) has recently pushed the boundaries of segmentation by demonstrating zero-shot generalization and flexible prompting after training on over one billion masks. Despite this, its mask prediction accuracy often falls short of the precision required in real-world applications. While several refinement modules have been proposed to boost SAM's segmentation quality, achieving highly accurate object delineation within a single, unified framework remains an open challenge. Furthermore, interactive image matting, which aims to generate fine-grained alpha mattes guided by diverse user hints, has not yet been explored in the context of SAM. Insights from recent studies highlight strong correlations between segmentation and matting, suggesting the feasibility of a unified model capable of both tasks. In this paper, we introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM that delivers high-quality interactive image segmentation and matting with minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures detailed features from local views, while the Localization Adapter (Local-Adapter) refines mask outputs by recovering subtle boundary details. We also incorporate two prediction heads for each task into the architecture to generate segmentation and matting masks, simultaneously. Trained on a diverse dataset aggregated from publicly available sources, SAMA achieves state-of-the-art performance across multiple segmentation and matting benchmarks, showcasing its adaptability and effectiveness in a wide range of downstream tasks.", "AI": {"tldr": "SAMA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6269\u5c55SAM\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5206\u5272\u548c\u62a0\u56fe\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u5b9a\u4f4d\u7f16\u7801\u5668\u548c\u5c40\u90e8\u9002\u914d\u5668\u63d0\u5347\u8fb9\u754c\u7ec6\u8282\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1SAM\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u7075\u6d3b\u63d0\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a9\u7801\u9884\u6d4b\u7cbe\u5ea6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u663e\u4e0d\u8db3\u3002\u73b0\u6709\u7ec6\u5316\u6a21\u5757\u65e0\u6cd5\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5bf9\u8c61\u63cf\u7ed8\uff0c\u4e14\u4ea4\u4e92\u5f0f\u56fe\u50cf\u62a0\u56fe\u5728SAM\u80cc\u666f\u4e0b\u5c1a\u672a\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u5206\u5272\u4e0e\u62a0\u56fe\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u4e2a\u4efb\u52a1\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u63d0\u51faSegment And Matte Anything (SAMA)\uff0c\u4f5c\u4e3aSAM\u7684\u8f7b\u91cf\u7ea7\u6269\u5c55\u3002\u5f15\u5165\u591a\u89c6\u56fe\u5b9a\u4f4d\u7f16\u7801\u5668(MVLE)\u4ece\u5c40\u90e8\u89c6\u56fe\u6355\u83b7\u8be6\u7ec6\u7279\u5f81\uff0c\u4f7f\u7528\u5c40\u90e8\u9002\u914d\u5668(Local-Adapter)\u901a\u8fc7\u6062\u590d\u7ec6\u5fae\u8fb9\u754c\u7ec6\u8282\u6765\u7ec6\u5316\u63a9\u7801\u8f93\u51fa\u3002\u5728\u67b6\u6784\u4e2d\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u52a0\u5165\u4e24\u4e2a\u9884\u6d4b\u5934\uff0c\u540c\u65f6\u751f\u6210\u5206\u5272\u548c\u62a0\u56fe\u63a9\u7801\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SAMA\u5728\u591a\u4e2a\u5206\u5272\u548c\u62a0\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u989d\u5916\u53c2\u6570\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u56fe\u50cf\u5206\u5272\u548c\u62a0\u56fe\u3002", "conclusion": "SAMA\u6210\u529f\u5730\u5c06\u5206\u5272\u548c\u62a0\u56fe\u4efb\u52a1\u7edf\u4e00\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u5b9a\u4f4d\u7f16\u7801\u5668\u548c\u5c40\u90e8\u9002\u914d\u5668\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u7ec6\u8282\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5bf9\u8c61\u63cf\u7ed8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13358", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13358", "abs": "https://arxiv.org/abs/2601.13358", "authors": ["Samuel Cyrenius Anderson"], "title": "The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models", "comment": "34 pages, 10 figures", "summary": "Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u6269\u5927\u4e0d\u4f1a\u5747\u5300\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u91cd\u6784\u63a8\u7406\u8fc7\u7a0b\u3002\u6cd5\u5f8b\u63a8\u7406\u51fa\u73b0\"\u7ed3\u6676\u5316\"\uff08\u7ef4\u5ea6\u574d\u7f29\u3001\u8f68\u8ff9\u5bf9\u9f50\uff09\uff0c\u79d1\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4fdd\u6301\"\u6db2\u6001\"\uff0c\u4ee3\u7801\u63a8\u7406\u5f62\u6210\"\u6676\u683c\"\u7ed3\u6784\u3002\u63a8\u7406\u6210\u672c\u7531\u6d41\u5f62\u51e0\u4f55\u800c\u975e\u4efb\u52a1\u96be\u5ea6\u51b3\u5b9a\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u6a21\u578b\u89c4\u6a21\u6269\u5927\u80fd\u5747\u5300\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u89c4\u6a21\u6269\u5927\u5982\u4f55\u771f\u6b63\u6539\u53d8\u63a8\u7406\u8fc7\u7a0b\u7684\u7ed3\u6784\u548c\u51e0\u4f55\u7279\u6027\uff0c\u63ed\u793a\u4e0d\u540c\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u5f02\u8d28\u6027\u53d1\u5c55\u89c4\u5f8b\u3002", "method": "\u5206\u679025,000+\u6761\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u6db5\u76d6\u6cd5\u5f8b\u3001\u79d1\u5b66\u3001\u4ee3\u7801\u3001\u6570\u5b66\u56db\u4e2a\u9886\u57df\u548c8B\u300170B\u4e24\u4e2a\u89c4\u6a21\u3002\u4f7f\u7528\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff08\u7ef4\u5ea6\u574d\u7f29\u3001\u8f68\u8ff9\u5bf9\u9f50\u3001\u6d41\u5f62\u89e3\u7f20\uff09\uff0c\u5f15\u5165\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\uff08\u4ece\u521d\u59cb\u5230\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u7684\u6620\u5c04\uff09\uff0c\u5e76\u8bc6\u522b\u632f\u8361\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u9886\u57df\u7279\u5f02\u6027\u76f8\u53d8\uff1a\u6cd5\u5f8b\u63a8\u7406\u51fa\u73b0\u7ed3\u6676\u5316\uff08\u7ef4\u5ea6\u574d\u7f2945%\uff0c\u8f68\u8ff9\u5bf9\u9f5031%\uff0c\u6d41\u5f62\u89e3\u7f2010\u500d\uff09\uff1b\u79d1\u5b66\u548c\u6570\u5b66\u63a8\u7406\u4fdd\u6301\u6db2\u6001\uff1b\u4ee3\u7801\u63a8\u7406\u5f62\u6210\u79bb\u6563\u6676\u683c\u7ed3\u6784\u3002\u795e\u7ecf\u63a8\u7406\u7b97\u5b50\u5728\u6cd5\u5f8b\u63a8\u7406\u4e0a\u8fbe\u523063.6%\u51c6\u786e\u7387\u3002\u53d1\u73b0\u8de8\u9886\u57df\u548c\u89c4\u6a21\u7684\u901a\u7528\u632f\u8361\u7279\u5f81\uff08\u76f8\u5e72\u6027\u7ea6-0.4\uff09\u3002", "conclusion": "\u63a8\u7406\u6210\u672c\u7531\u6d41\u5f62\u51e0\u4f55\u800c\u975e\u4efb\u52a1\u96be\u5ea6\u51b3\u5b9a\uff0c\u8fd9\u4e3a\u5728\u62d3\u6251\u5141\u8bb8\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u63a8\u7406\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002\u6a21\u578b\u89c4\u6a21\u6269\u5927\u89e6\u53d1\u9886\u57df\u7279\u5f02\u6027\u76f8\u53d8\u800c\u975e\u5747\u5300\u80fd\u529b\u63d0\u5347\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u80fd\u529b\u7684\u7ed3\u6784\u5316\u53d1\u5c55\u89c4\u5f8b\u3002"}}
{"id": "2601.12632", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.12632", "abs": "https://arxiv.org/abs/2601.12632", "authors": ["Kriti Bhattarai", "Vipina K. Keloth", "Donald Wright", "Andrew Loza", "Yang Ren", "Hua Xu"], "title": "BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models", "comment": null, "summary": "Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.\n  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.\n  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.\n  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.", "AI": {"tldr": "BioPulse-QA\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b2280\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u8986\u76d6\u836f\u7269\u6807\u7b7e\u3001\u8bd5\u9a8c\u65b9\u6848\u548c\u4e34\u5e8a\u6307\u5357\u7b49\u65b0\u53d1\u5e03\u6587\u6863\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u533b\u5b66\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f7f\u7528\u9759\u6001\u6216\u8fc7\u65f6\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff1b\u5b58\u5728\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff1b\u5ffd\u89c6\u8bed\u8a00\u53d8\u5f02\u9c81\u68d2\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u7b49\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u5f00\u53d1BioPulse-QA\u57fa\u51c6\uff0c\u5305\u542b2280\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\u53ca\u5176\u6270\u52a8\u53d8\u4f53\uff0c\u8986\u76d6\u62bd\u53d6\u5f0f\u548c\u62bd\u8c61\u5f0f\u95ee\u7b54\u683c\u5f0f\u3002\u8bc4\u4f30GPT-4o\u3001GPT-o1\u3001Gemini-2.0-Flash\u548cLLaMA-3.1 8B Instruct\u56db\u79cd\u5728\u57fa\u51c6\u6587\u6863\u53d1\u5e03\u65e5\u671f\u4e4b\u524d\u53d1\u5e03\u7684LLMs\u3002", "result": "GPT-o1\u5728\u836f\u7269\u6807\u7b7e\u4e0a\u83b7\u5f97\u6700\u9ad8\u653e\u677eF1\u5206\u6570\uff080.92\uff09\uff0cGemini-2.0-Flash\u6b21\u4e4b\uff080.90\uff09\u3002\u4e34\u5e8a\u8bd5\u9a8c\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u6765\u6e90\uff0c\u62bd\u53d6\u5f0fF1\u5206\u6570\u4f4e\u81f30.36\u3002\u6539\u5199\u6bd4\u62fc\u5199\u9519\u8bef\u5e26\u6765\u7684\u6027\u80fd\u5dee\u5f02\u66f4\u5927\uff0c\u504f\u89c1\u6d4b\u8bd5\u663e\u793a\u5dee\u5f02\u53ef\u5ffd\u7565\u3002", "conclusion": "BioPulse-QA\u4e3a\u8bc4\u4f30\u751f\u7269\u533b\u5b66LLMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u52a8\u6001\u751f\u7269\u533b\u5b66\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2601.12149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12149", "abs": "https://arxiv.org/abs/2601.12149", "authors": ["Pengfei Zhu", "Xavier Maldague"], "title": "Principal Component Analysis-Based Terahertz Self-Supervised Denoising and Deblurring Deep Neural Networks", "comment": null, "summary": "Terahertz (THz) systems inherently introduce frequency-dependent degradation effects, resulting in low-frequency blurring and high-frequency noise in amplitude images. Conventional image processing techniques cannot simultaneously address both issues, and manual intervention is often required due to the unknown boundary between denoising and deblurring. To tackle this challenge, we propose a principal component analysis (PCA)-based THz self-supervised denoising and deblurring network (THz-SSDD). The network employs a Recorrupted-to-Recorrupted self-supervised learning strategy to capture the intrinsic features of noise by exploiting invariance under repeated corruption. PCA decomposition and reconstruction are then applied to restore images across both low and high frequencies. The performance of the THz-SSDD network was evaluated on four types of samples. Training requires only a small set of unlabeled noisy images, and testing across samples with different material properties and measurement modes demonstrates effective denoising and deblurring. Quantitative analysis further validates the network feasibility, showing improvements in image quality while preserving the physical characteristics of the original signals.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePCA\u7684THz\u81ea\u76d1\u7763\u53bb\u566a\u53bb\u6a21\u7cca\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548cPCA\u5206\u89e3\u91cd\u5efa\u89e3\u51b3THz\u56fe\u50cf\u4f4e\u9891\u6a21\u7cca\u548c\u9ad8\u9891\u566a\u58f0\u95ee\u9898", "motivation": "THz\u7cfb\u7edf\u56fa\u6709\u7684\u9891\u7387\u76f8\u5173\u9000\u5316\u6548\u5e94\u5bfc\u81f4\u632f\u5e45\u56fe\u50cf\u51fa\u73b0\u4f4e\u9891\u6a21\u7cca\u548c\u9ad8\u9891\u566a\u58f0\uff0c\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4e14\u53bb\u566a\u4e0e\u53bb\u6a21\u7cca\u7684\u8fb9\u754c\u672a\u77e5\u9700\u8981\u4eba\u5de5\u5e72\u9884", "method": "\u63d0\u51faTHz-SSDD\u7f51\u7edc\uff0c\u91c7\u7528Recorrupted-to-Recorrupted\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u6355\u6349\u566a\u58f0\u5185\u5728\u7279\u5f81\uff0c\u5229\u7528PCA\u5206\u89e3\u548c\u91cd\u5efa\u6765\u6062\u590d\u4f4e\u9891\u548c\u9ad8\u9891\u56fe\u50cf", "result": "\u5728\u56db\u7c7b\u6837\u672c\u4e0a\u8bc4\u4f30\uff0c\u4ec5\u9700\u5c11\u91cf\u65e0\u6807\u7b7e\u566a\u58f0\u56fe\u50cf\u8bad\u7ec3\uff0c\u5728\u4e0d\u540c\u6750\u6599\u5c5e\u6027\u548c\u6d4b\u91cf\u6a21\u5f0f\u7684\u6837\u672c\u4e0a\u6d4b\u8bd5\u5747\u80fd\u6709\u6548\u53bb\u566a\u548c\u53bb\u6a21\u7cca\uff0c\u5b9a\u91cf\u5206\u6790\u9a8c\u8bc1\u4e86\u7f51\u7edc\u53ef\u884c\u6027", "conclusion": "THz-SSDD\u7f51\u7edc\u80fd\u540c\u65f6\u89e3\u51b3THz\u56fe\u50cf\u7684\u4f4e\u9891\u6a21\u7cca\u548c\u9ad8\u9891\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u7559\u539f\u59cb\u4fe1\u53f7\u7684\u7269\u7406\u7279\u6027"}}
{"id": "2601.13383", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13383", "abs": "https://arxiv.org/abs/2601.13383", "authors": ["Akbar Anbar Jafari", "Cagri Ozcinar", "Gholamreza Anbarjafari"], "title": "A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge", "comment": "15 pages, 3 figures", "summary": "The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.", "AI": {"tldr": "AgentForge\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90Python\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u7b80\u5316LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u5f00\u53d1\uff0c\u63d0\u4f9b\u53ef\u7ec4\u5408\u6280\u80fd\u3001\u7edf\u4e00LLM\u540e\u7aef\u63a5\u53e3\u548c\u58f0\u660e\u5f0f\u914d\u7f6e\u7cfb\u7edf\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u5b58\u5728\u67b6\u6784\u50f5\u5316\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u548c\u590d\u6742\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6c11\u4e3b\u5316LLM\u9a71\u52a8\u81ea\u4e3b\u4ee3\u7406\u6784\u5efa\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAgentForge\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u53ef\u7ec4\u5408\u6280\u80fd\u62bd\u8c61\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\u548c\u5f62\u5f0f\u5316\u8f93\u5165\u8f93\u51fa\u5951\u7ea6\uff1b2) \u7edf\u4e00LLM\u540e\u7aef\u63a5\u53e3\uff0c\u652f\u6301\u4e91\u7aefAPI\u548c\u672c\u5730\u63a8\u7406\u5f15\u64ce\u65e0\u7f1d\u5207\u6362\uff1b3) \u58f0\u660e\u5f0fYAML\u914d\u7f6e\u7cfb\u7edf\uff0c\u5206\u79bb\u4ee3\u7406\u903b\u8f91\u4e0e\u5b9e\u73b0\u7ec6\u8282\u3002\u5c06\u6280\u80fd\u7ec4\u5408\u673a\u5236\u5f62\u5f0f\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u573a\u666f\u7684\u5168\u9762\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cAgentForge\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u540c\u65f6\u76f8\u6bd4LangChain\u51cf\u5c1162%\u5f00\u53d1\u65f6\u95f4\uff0c\u76f8\u6bd4\u76f4\u63a5API\u96c6\u6210\u51cf\u5c1178%\u5f00\u53d1\u65f6\u95f4\u3002\u7f16\u6392\u5ef6\u8fdf\u4f4e\u4e8e100ms\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002\u6846\u67b6\u96c6\u6210\u4e86\u516d\u4e2a\u5185\u7f6e\u6280\u80fd\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6280\u80fd\u5f00\u53d1\u3002", "conclusion": "AgentForge\u586b\u8865\u4e86LLM\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u7075\u6d3b\u6027\u6216\u6027\u80fd\u3002"}}
{"id": "2601.12639", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12639", "abs": "https://arxiv.org/abs/2601.12639", "authors": ["Daniel Vennemeyer", "Punya Syon Pandey", "Phan Anh Duong", "Michael Umeokoli", "Samuel Ratnam"], "title": "Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift", "comment": null, "summary": "Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.", "AI": {"tldr": "\u5fae\u8c03\u76ee\u6807\u5728\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u6743\u8861\u4e2d\u7684\u4f5c\u7528\uff1a\u5c0f\u89c4\u6a21\u65f6\u5f71\u54cd\u4e0d\u5927\uff0c\u5927\u89c4\u6a21\u65f6\u6210\u4e3a\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u89d2\u8272\u7a33\u5b9a\u6027\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20", "motivation": "\u5c3d\u7ba1\u5728\u826f\u6027\u6570\u636e\u4e0a\u5fae\u8c03LLM\u4ecd\u53ef\u80fd\u964d\u4f4e\u5bf9\u9f50\u6027\u548c\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u4f46\u5fae\u8c03\u76ee\u6807\u5728\u5851\u9020\u8fd9\u4e9b\u5b89\u5168\u7ed3\u679c\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u5fae\u8c03\u76ee\u6807\u5bf9\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u6743\u8861\u7684\u5f71\u54cd\u3002", "method": "\u5728\u6570\u636e\u3001\u9886\u57df\u3001\u67b6\u6784\u548c\u4f18\u5316\u56fa\u5b9a\u7684\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u516d\u79cd\u5fae\u8c03\u76ee\u6807\uff1a\u76d1\u7763\u5fae\u8c03\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3001\u6761\u4ef6\u5fae\u8c03\u3001\u63a5\u79cd\u63d0\u793a\u3001\u6bd4\u503c\u6bd4\u504f\u597d\u4f18\u5316\u548cKL\u6b63\u5219\u5316\u5fae\u8c03\u3002\u5728\u5c01\u95ed\u5f0f\u63a8\u7406\u548c\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u76ee\u6807\u9009\u62e9\u5bfc\u81f4\u5b89\u5168\u6027\u4e0e\u80fd\u529b\u524d\u6cbf\u7684\u7cfb\u7edf\u6027\u3001\u89c4\u6a21\u4f9d\u8d56\u6027\u53d8\u5316\u3002\u5c0f\u8bad\u7ec3\u9884\u7b97\u65f6\uff0c\u9c81\u68d2\u6027\u76f8\u4f3c\u4f46\u80fd\u529b\u4e0d\u540c\uff1b\u5927\u9884\u7b97\u65f6\uff0c\u76ee\u6807\u5dee\u5f02\u663e\u8457\uff1a\u76d1\u7763\u548c\u57fa\u4e8e\u504f\u597d\u7684\u5fae\u8c03\u4f7f\u80fd\u529b\u63d0\u5347\u4e0e\u5bf9\u6297\u8106\u5f31\u6027\u3001\u89d2\u8272\u6f02\u79fb\u7d27\u5bc6\u8026\u5408\uff0c\u800c\u9650\u5236\u5b66\u4e60\u4fe1\u53f7\u7684\u76ee\u6807\uff08\u7279\u522b\u662fORPO\u548cKL\u6b63\u5219\u5316\uff09\u80fd\u663e\u8457\u7f13\u89e3\u8fd9\u4e24\u79cd\u95ee\u9898\u3002", "conclusion": "\u5fae\u8c03\u76ee\u6807\u5728\u5c0f\u89c4\u6a21\u65f6\u5bf9\u5b89\u5168\u6027\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u968f\u7740\u8bad\u7ec3\u89c4\u6a21\u589e\u52a0\uff0c\u6210\u4e3a\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6f5c\u5728\u89d2\u8272\u7a33\u5b9a\u6027\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002\u9009\u62e9\u9002\u5f53\u7684\u5fae\u8c03\u76ee\u6807\u5bf9\u4e8e\u5728\u5927\u89c4\u6a21\u5fae\u8c03\u65f6\u4fdd\u6301\u6a21\u578b\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.12150", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12150", "abs": "https://arxiv.org/abs/2601.12150", "authors": ["Mengxuan Hu", "Zihan Guan", "John Kang", "Sheng Li", "Zhongliang Zhou"], "title": "Enhanced Diagnostic Performance via Large-Resolution Inference Optimization for Pathology Foundation Models", "comment": "8 pages", "summary": "Despite their prominent performance on tasks such as ROI classification and segmentation, many pathology foundation models remain constrained by a specific input size e.g. 224 x 224, creating substantial inefficiencies when applied to whole-slide images (WSIs), which span thousands of resolutions. A naive strategy is to either enlarge inputs or downsample the WSIs. However, enlarging inputs results in prohibitive GPU memory consumption, while downsampling alters the microns-per-pixel resolution and obscures critical morphological details. To overcome these limitations, we propose an space- and time- efficient inference strategy that sparsifies attention using spatially aware neighboring blocks and filters out non-informative tokens through global attention scores. This design substantially reduces GPU memory and runtime during high-resolution WSI inference while preserving and even improving the downstream performance, enabling inference at higher resolutions under the same GPU budget. The experimental results show that our method can achieves up to an 7.67% improvement in the ROI classification and compatible results in segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7a7a\u95f4\u548c\u65f6\u95f4\u9ad8\u6548\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u90bb\u8fd1\u5757\u7a00\u758f\u5316\u6ce8\u610f\u529b\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5206\u6570\u8fc7\u6ee4\u975e\u4fe1\u606ftoken\uff0c\u663e\u8457\u964d\u4f4e\u9ad8\u5206\u8fa8\u7387WSI\u63a8\u7406\u65f6\u7684GPU\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4", "motivation": "\u73b0\u6709\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u53d7\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u5c3a\u5bf8\uff08\u5982224\u00d7224\uff09\uff0c\u5728\u5904\u7406\u6570\u5343\u5206\u8fa8\u7387\u7ea7\u522b\u7684\u5168\u5207\u7247\u56fe\u50cf\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u7b80\u5355\u653e\u5927\u8f93\u5165\u4f1a\u5bfc\u81f4GPU\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u800c\u7f29\u5c0f\u56fe\u50cf\u5219\u4f1a\u635f\u5931\u5173\u952e\u5f62\u6001\u7ec6\u8282", "method": "\u63d0\u51fa\u7a7a\u95f4\u548c\u65f6\u95f4\u9ad8\u6548\u63a8\u7406\u7b56\u7565\uff1a1\uff09\u4f7f\u7528\u7a7a\u95f4\u611f\u77e5\u90bb\u8fd1\u5757\u7a00\u758f\u5316\u6ce8\u610f\u529b\u673a\u5236\uff1b2\uff09\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5206\u6570\u8fc7\u6ee4\u975e\u4fe1\u606ftoken\u3002\u8be5\u8bbe\u8ba1\u5728\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eGPU\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u95f4", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728ROI\u5206\u7c7b\u4efb\u52a1\u4e0a\u83b7\u5f97\u9ad8\u8fbe7.67%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e0a\u83b7\u5f97\u517c\u5bb9\u7ed3\u679c\uff0c\u540c\u65f6\u80fd\u5728\u76f8\u540cGPU\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u63a8\u7406", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5168\u5207\u7247\u56fe\u50cf\u65f6\u7684\u6548\u7387\u74f6\u9888\uff0c\u901a\u8fc7\u667a\u80fd\u6ce8\u610f\u529b\u7a00\u758f\u5316\u548ctoken\u8fc7\u6ee4\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42"}}
{"id": "2601.13443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13443", "abs": "https://arxiv.org/abs/2601.13443", "authors": ["H\u00e9ctor Manuel Manzanilla-Granados", "Zaira Navarrete-Cazales", "Miriam Pescador-Rojas", "Tonahtiu Ram\u00edrez-Romero"], "title": "Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models", "comment": "Preprint. This version corresponds to the initial public release of the CUA architecture and associated evaluation metrics", "summary": "The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.\n  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.\n  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.", "AI": {"tldr": "\u63d0\u51fa\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\u539f\u5219\u548c\u8ba4\u77e5\u901a\u7528\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u7ec4\u7ec7\u8ba4\u77e5\u529f\u80fd\u6765\u6539\u8fdbAI\u8f85\u52a9\u63a8\u7406\uff0c\u76f8\u6bd4\u4f20\u7edfLLM\u63a8\u7406\u5177\u6709\u66f4\u597d\u7684\u53ef\u8ffd\u6eaf\u6027\u3001\u8ba4\u77e5\u63a7\u5236\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\u65b9\u5f0f\u5728\u8ba4\u77e5\u4e0a\u7f3a\u4e4f\u7ed3\u6784\uff0c\u5c06\u95ee\u9898\u6846\u67b6\u3001\u77e5\u8bc6\u63a2\u7d22\u3001\u68c0\u7d22\u3001\u65b9\u6cd5\u610f\u8bc6\u548c\u89e3\u91ca\u7b49\u8ba4\u77e5\u529f\u80fd\u6df7\u4e3a\u4e00\u4f53\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u8ffd\u6eaf\u6027\u3001\u524a\u5f31\u4e86\u8ba4\u77e5\u63a7\u5236\u3001\u7834\u574f\u4e86\u53ef\u91cd\u590d\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d23\u4efb\u573a\u666f\u4e2d\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\u539f\u5219\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3a\u8ba4\u77e5\u901a\u7528\u4ee3\u7406\u67b6\u6784\uff0c\u5c06\u63a8\u7406\u7ec4\u7ec7\u4e3a\u63a2\u7d22\u4e0e\u6846\u67b6\u3001\u8ba4\u77e5\u951a\u5b9a\u3001\u5de5\u5177\u4e0e\u65b9\u6cd5\u6620\u5c04\u3001\u89e3\u91ca\u6027\u5408\u6210\u7b49\u4e0d\u540c\u9636\u6bb5\u3002\u6838\u5fc3\u662f\u901a\u7528\u8ba4\u77e5\u5de5\u5177\u6982\u5ff5\uff0c\u5f62\u5f0f\u5316\u5404\u79cd\u5de5\u5177\u624b\u6bb5\u3002", "result": "\u5728\u519c\u4e1a\u9886\u57df\u7684\u591a\u63d0\u793a\u5b9e\u9a8c\u4e2d\uff0cCUA\u63a8\u7406\u8868\u73b0\u51fa\u66f4\u65e9\u4e14\u7ed3\u6784\u5316\u7684\u8ba4\u77e5\u6536\u655b\u3001\u8bed\u4e49\u6269\u5c55\u4e0b\u66f4\u9ad8\u7684\u8ba4\u77e5\u5bf9\u9f50\u5ea6\uff0c\u5e76\u80fd\u7cfb\u7edf\u6027\u5730\u66b4\u9732\u67e5\u8be2\u7684\u5de5\u5177\u666f\u89c2\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7ebfLLM\u63a8\u7406\u5728\u5bf9\u9f50\u5ea6\u4e0a\u53d8\u5316\u66f4\u5927\uff0c\u4e14\u65e0\u6cd5\u663e\u5f0f\u5c55\u73b0\u5de5\u5177\u7ed3\u6784\u3002", "conclusion": "\u663e\u5f0f\u8ba4\u77e5\u5206\u914d\u539f\u5219\u548cCUA\u67b6\u6784\u80fd\u591f\u663e\u8457\u6539\u5584AI\u8f85\u52a9\u63a8\u7406\u7684\u7ed3\u6784\u5316\u7a0b\u5ea6\uff0c\u589e\u5f3a\u53ef\u8ffd\u6eaf\u6027\u3001\u8ba4\u77e5\u63a7\u5236\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4e3a\u9ad8\u8d23\u4efb\u573a\u666f\u4e0b\u7684AI\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\u3002"}}
{"id": "2601.12648", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12648", "abs": "https://arxiv.org/abs/2601.12648", "authors": ["Nafiz Imtiaz Khan", "Kylie Cleland", "Vladimir Filkov", "Roger Eric Goldman"], "title": "Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?", "comment": "51 pages, 12 figures, 8 tables. Feasibility study using retrospective radiology reports. Submitted to JAMIA Open (under review)", "summary": "Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u7a0b\u5e8f\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ee5\u51cf\u5c11\u57f9\u8bad\u4e2d\u7684\u6587\u4e66\u8d1f\u62c5\u3002", "motivation": "\u653e\u5c04\u5b66\u57f9\u8bad\u4e2d\u7684\u7a0b\u5e8f\u75c5\u4f8b\u8bb0\u5f55\u8017\u65f6\u4e14\u624b\u52a8\u8bb0\u5f55\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u8f7b\u5b66\u5458\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6307\u4ee4\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u672c\u5730\u548c\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ece414\u4efd\u4ecb\u5165\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7a0b\u5e8f\u4fe1\u606f\uff0c\u8bc4\u4f30\u6027\u80fd\u6307\u6807\u5305\u62ec\u7075\u654f\u5ea6\u3001\u7279\u5f02\u5ea6\u3001F1\u5206\u6570\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u4ee3\u5e01\u6548\u7387\u3002", "result": "\u672c\u5730\u548c\u5546\u4e1a\u6a21\u578b\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63d0\u53d6\u6027\u80fd\uff0c\u6700\u4f73F1\u5206\u6570\u63a5\u8fd10.87\uff0c\u4f46\u5728\u901f\u5ea6\u548c\u6210\u672c\u4e4b\u95f4\u5b58\u5728\u4e0d\u540c\u6743\u8861\u3002\u81ea\u52a8\u5316\u53ef\u663e\u8457\u51cf\u5c11\u5b66\u5458\u7684\u6587\u4e66\u8d1f\u62c5\u5e76\u63d0\u9ad8\u8bb0\u5f55\u4e00\u81f4\u6027\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u6559\u80b2\u4e2d\u8f85\u52a9\u6587\u6863\u8bb0\u5f55\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u8de8\u673a\u6784\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002"}}
{"id": "2601.12155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12155", "abs": "https://arxiv.org/abs/2601.12155", "authors": ["Xiang Gao", "Xinmu Wang", "Yuanpeng Liu", "Yue Wang", "Junqi Huang", "Wei Chen", "Xianfeng Gu"], "title": "Inverse Rendering for High-Genus 3D Surface Meshes from Multi-view Images with Persistent Homology Priors", "comment": "ICASSP2026 Accepted", "summary": "Reconstructing 3D objects from images is inherently an ill-posed problem due to ambiguities in geometry, appearance, and topology. This paper introduces collaborative inverse rendering with persistent homology priors, a novel strategy that leverages topological constraints to resolve these ambiguities. By incorporating priors that capture critical features such as tunnel loops and handle loops, our approach directly addresses the difficulty of reconstructing high-genus surfaces. The collaboration between photometric consistency from multi-view images and homology-based guidance enables recovery of complex high-genus geometry while circumventing catastrophic failures such as collapsing tunnels or losing high-genus structure. Instead of neural networks, our method relies on gradient-based optimization within a mesh-based inverse rendering framework to highlight the role of topological priors. Experimental results show that incorporating persistent homology priors leads to lower Chamfer Distance (CD) and higher Volume IoU compared to state-of-the-art mesh-based methods, demonstrating improved geometric accuracy and robustness against topological failure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6301\u7eed\u540c\u8c03\u5148\u9a8c\u7684\u534f\u4f5c\u5f0f\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u7ea6\u675f\u89e3\u51b33D\u91cd\u5efa\u4e2d\u7684\u51e0\u4f55\u3001\u5916\u89c2\u548c\u62d3\u6251\u6b67\u4e49\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u9ad8\u4e8f\u683c\u8868\u9762\u91cd\u5efa\u3002", "motivation": "\u4ece\u56fe\u50cf\u91cd\u5efa3D\u7269\u4f53\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u5b58\u5728\u51e0\u4f55\u3001\u5916\u89c2\u548c\u62d3\u6251\u65b9\u9762\u7684\u6b67\u4e49\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u91cd\u5efa\u9ad8\u4e8f\u683c\u8868\u9762\uff08\u5982\u5177\u6709\u96a7\u9053\u6216\u624b\u67c4\u73af\u7684\u590d\u6742\u7ed3\u6784\uff09\u65f6\u5bb9\u6613\u5931\u8d25\uff0c\u51fa\u73b0\u96a7\u9053\u584c\u9677\u6216\u4e22\u5931\u9ad8\u4e8f\u683c\u7ed3\u6784\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u5f0f\u9006\u6e32\u67d3\u4e0e\u6301\u7eed\u540c\u8c03\u5148\u9a8c\u65b9\u6cd5\uff0c\u5c06\u591a\u89c6\u56fe\u56fe\u50cf\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u4e0e\u540c\u8c03\u5f15\u5bfc\u76f8\u7ed3\u5408\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u795e\u7ecf\u7f51\u7edc\uff0c\u800c\u662f\u5728\u57fa\u4e8e\u7f51\u683c\u7684\u9006\u6e32\u67d3\u6846\u67b6\u5185\u4f7f\u7528\u68af\u5ea6\u4f18\u5316\uff0c\u901a\u8fc7\u6301\u7eed\u540c\u8c03\u5148\u9a8c\u6355\u83b7\u5173\u952e\u62d3\u6251\u7279\u5f81\uff08\u5982\u96a7\u9053\u73af\u548c\u624b\u67c4\u73af\uff09\u6765\u6307\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u52a0\u5165\u6301\u7eed\u540c\u8c03\u5148\u9a8c\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f4e\u7684Chamfer\u8ddd\u79bb\u548c\u66f4\u9ad8\u7684\u4f53\u79efIoU\uff0c\u8bc1\u660e\u4e86\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u62d3\u6251\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6539\u8fdb\u3002", "conclusion": "\u6301\u7eed\u540c\u8c03\u5148\u9a8c\u80fd\u6709\u6548\u89e3\u51b33D\u91cd\u5efa\u4e2d\u7684\u62d3\u6251\u6b67\u4e49\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u4e8f\u683c\u8868\u9762\u7684\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u62d3\u6251\u7ea6\u675f\u4e0e\u5149\u5ea6\u4e00\u81f4\u6027\u534f\u4f5c\uff0c\u907f\u514d\u4e86\u62d3\u6251\u5931\u8d25\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u51e0\u4f55\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13462", "abs": "https://arxiv.org/abs/2601.13462", "authors": ["Amine Rostane"], "title": "SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation", "comment": "19 pages, includes figures and tables", "summary": "Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SpatialBench-UC\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u80fd\u529b\u7684\u5c0f\u578b\u53ef\u590d\u73b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u4e2a\u63d0\u793a\u548c\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u8bc4\u4f30\u663e\u793a\u63a5\u5730\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u662f\u5426\u9075\u5faa\u660e\u786e\u7684\u7a7a\u95f4\u6307\u4ee4\u96be\u4ee5\u81ea\u52a8\u5316\uff0c\u56e0\u4e3a\u5bf9\u8c61\u68c0\u6d4b\u5668\u53ef\u80fd\u6f0f\u68c0\u6216\u8fd4\u56de\u591a\u4e2a\u53ef\u80fd\u68c0\u6d4b\uff0c\u7b80\u5355\u51e0\u4f55\u6d4b\u8bd5\u5728\u8fb9\u754c\u60c5\u51b5\u4e0b\u53ef\u80fd\u6a21\u7cca\u4e0d\u6e05\u3002", "method": "\u5f15\u5165SpatialBench-UC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u4e2a\u63d0\u793a\uff0850\u4e2a\u5bf9\u8c61\u5bf9\u00d74\u79cd\u5173\u7cfb\uff09\uff0c\u5206\u7ec4\u4e3a100\u4e2a\u53cd\u4e8b\u5b9e\u5bf9\u3002\u63d0\u4f9b\u7248\u672c\u5316\u63d0\u793a\u3001\u56fa\u5b9a\u914d\u7f6e\u3001\u6bcf\u6837\u672c\u68c0\u67e5\u5668\u8f93\u51fa\u548c\u62a5\u544a\u8868\u683c\uff0c\u5b9e\u73b0\u53ef\u590d\u73b0\u548c\u53ef\u5ba1\u8ba1\u7684\u6a21\u578b\u6bd4\u8f83\u3002\u5305\u542b\u8f7b\u91cf\u7ea7\u4eba\u5de5\u5ba1\u6838\u6765\u6821\u51c6\u68c0\u67e5\u5668\u7684\u5f03\u6743\u8fb9\u754c\u548c\u7f6e\u4fe1\u5ea6\u9608\u503c\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u4e2a\u57fa\u7ebf\u6a21\u578b\uff1aStable Diffusion 1.5\u3001SD 1.5 BoxDiff\u548cSD 1.4 GLIGEN\u3002\u7ed3\u679c\u663e\u793a\u63a5\u5730\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\u548c\u8986\u76d6\u7387\uff0c\u4f46\u5f03\u6743\u4ecd\u7136\u662f\u4e3b\u8981\u56e0\u7d20\uff0c\u4e3b\u8981\u7531\u4e8e\u68c0\u6d4b\u7f3a\u5931\u3002", "conclusion": "\u7a7a\u95f4\u8bc4\u4f30\u672c\u8d28\u4e0a\u662f\u9009\u62e9\u6027\u9884\u6d4b\u95ee\u9898\uff0c\u68c0\u67e5\u5668\u53ef\u4ee5\u5728\u8bc1\u636e\u5f31\u65f6\u5f03\u6743\u5e76\u62a5\u544a\u7f6e\u4fe1\u5ea6\uff0c\u4f7f\u7ed3\u679c\u53ef\u89e3\u91ca\u4e3a\u98ce\u9669\u8986\u76d6\u6743\u8861\u800c\u975e\u5355\u4e00\u5206\u6570\u3002\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u53ef\u590d\u73b0\u548c\u53ef\u5ba1\u8ba1\u7684\u6a21\u578b\u6bd4\u8f83\u3002"}}
{"id": "2601.12658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12658", "abs": "https://arxiv.org/abs/2601.12658", "authors": ["Tianyi Yang", "Nashrah Haque", "Vaishnave Jonnalagadda", "Yuya Jeremy Ong", "Zhehui Chen", "Yanzhao Wu", "Lei Yu", "Divyesh Jadav", "Wenqi Wei"], "title": "Augmenting Question Answering with A Hybrid RAG Approach", "comment": "10 pages, 5 tables, 2 figures; presented at IEEE CogMI 2025", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.", "AI": {"tldr": "SSRAG\u662f\u4e00\u79cd\u7ed3\u5408\u67e5\u8be2\u589e\u5f3a\u3001\u667a\u80fd\u8def\u7531\u548c\u7ed3\u6784\u5316\u68c0\u7d22\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5411\u91cf\u4e0e\u56fe\u6280\u672f\u7ed3\u5408\u63d0\u5347\u95ee\u7b54\u8d28\u91cf", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u68c0\u7d22\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u7b54\u6848\u4e0d\u5b8c\u6574\u6216\u6b21\u4f18\uff0c\u9700\u8981\u6539\u8fdb\u68c0\u7d22\u8fc7\u7a0b\u4ee5\u63d0\u5347\u95ee\u7b54\u8d28\u91cf", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u8bed\u4e49RAG(SSRAG)\u6df7\u5408\u67b6\u6784\uff0c\u6574\u5408\u67e5\u8be2\u589e\u5f3a\u3001\u667a\u80fd\u8def\u7531\u548c\u7ed3\u6784\u5316\u68c0\u7d22\u673a\u5236\uff0c\u7ed3\u5408\u5411\u91cf\u548c\u56fe\u6280\u672f\u8fdb\u884c\u4e0a\u4e0b\u6587\u7edf\u4e00", "result": "\u5728TruthfulQA\u3001SQuAD\u548cWikiQA\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf9\u4e94\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aSSRAG\u76f8\u6bd4\u6807\u51c6RAG\u80fd\u6301\u7eed\u63d0\u5347\u54cd\u5e94\u8d28\u91cf", "conclusion": "SSRAG\u901a\u8fc7\u6539\u8fdb\u68c0\u7d22\u8fc7\u7a0b\u548c\u589e\u5f3a\u4e0a\u4e0b\u6587\u57fa\u7840\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\u548c\u4fe1\u606f\u91cf\uff0c\u4e3a\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684RAG\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12193", "abs": "https://arxiv.org/abs/2601.12193", "authors": ["Shaunak Halbe", "Bhagyashree Puranik", "Jayakrishnan Unnikrishnan", "Kushan Thakkar", "Vimal Bhat", "Toufiq Parag"], "title": "VIRTUE: Versatile Video Retrieval Through Unified Embeddings", "comment": null, "summary": "Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.", "AI": {"tldr": "VIRTUE\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u652f\u6301\u8bed\u6599\u5e93\u7ea7\u68c0\u7d22\u3001\u65f6\u523b\u5b9a\u4f4d\u548c\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u548cLoRA\u9ad8\u6548\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5176\u4ed6MLLM\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e13\u7528\u67b6\u6784\u867d\u7136\u68c0\u7d22\u6027\u80fd\u5f3a\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\uff1b\u800cMLLM\u65b9\u6cd5\u652f\u6301\u4e30\u5bcc\u591a\u6a21\u6001\u641c\u7d22\uff0c\u4f46\u68c0\u7d22\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4e13\u7528\u7cfb\u7edf\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u652f\u6301\u7075\u6d3b\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u53c8\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\u68c0\u7d22\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5171\u4eabMLLM\u9aa8\u5e72\u7f51\u7edc\u751f\u6210\u89c6\u89c9\u548c\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u4fc3\u8fdb\u9ad8\u6548\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u5019\u9009\u641c\u7d22\u3002\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u572870\u4e07\u5bf9\u89c6\u89c9-\u6587\u672c\u6570\u636e\u6837\u672c\u4e0a\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002\u540c\u4e00\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u96f6\u6837\u672c\u65f6\u523b\u68c0\u7d22\uff0c\u5e76\u901a\u8fc7\u91cd\u6392\u5e8f\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5176\u4ed6MLLM\u65b9\u6cd5\uff1b\u96f6\u6837\u672c\u65f6\u523b\u68c0\u7d22\u8fbe\u5230\u7ade\u4e89\u6027\u7ed3\u679c\uff1b\u96f6\u6837\u672c\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff1b\u901a\u8fc7\u91cd\u6392\u5e8f\u540e\uff0c\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\u73b0\u6709MLLM\u7cfb\u7edf\uff0c\u4e0e\u5728\u66f4\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "VIRTUE\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u8bed\u6599\u5e93\u7ea7\u68c0\u7d22\u3001\u65f6\u523b\u7ea7\u68c0\u7d22\u548c\u7ec4\u5408\u591a\u6a21\u6001\u67e5\u8be2\u80fd\u529b\uff0c\u8bc1\u660e\u4e86MLLM\u5728\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u4e13\u7528\u7cfb\u7edf\u76f8\u5f53\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u6a21\u6001\u67e5\u8be2\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2601.13464", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13464", "abs": "https://arxiv.org/abs/2601.13464", "authors": ["Chongyang Gao", "Marco Postiglione", "Julian Baldwin", "Natalia Denisenko", "Isabel Gortner", "Luke Fosdick", "Chiara Pulice", "Sarit Kraus", "V. S. Subrahmanian"], "title": "Context and Transcripts Improve Detection of Deepfake Audios of Public Figures", "comment": null, "summary": "Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668CADD\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u8f6c\u5f55\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5bf9\u5bf9\u6297\u653b\u51fb\u66f4\u9c81\u68d2", "motivation": "\u5f53\u524d\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u4ec5\u5206\u6790\u97f3\u9891\u6587\u4ef6\u672c\u8eab\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u5224\u65ad\u4fe1\u606f\u771f\u4f2a\u65f6\u4f7f\u7528\u7684\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u4fe1\u606f\uff0c\u8fd9\u9650\u5236\u4e86\u68c0\u6d4b\u6548\u679c", "method": "\u521b\u5efa\u8bb0\u8005\u63d0\u4f9b\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6JDD\u548c\u5408\u6210\u97f3\u9891\u6570\u636e\u96c6SYN\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668CADD\u67b6\u6784\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u548c/\u6216\u6587\u672c\u8f6c\u5f55\u4fe1\u606f", "result": "\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u8f6c\u5f55\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff1aF1\u5206\u6570\u63d0\u53475%-37.58%\uff0cAUC\u63d0\u53473.77%-42.79%\uff0cEER\u63d0\u53476.17%-47.83%\u3002CADD\u5bf95\u79cd\u5bf9\u6297\u9003\u907f\u7b56\u7565\u66f4\u9c81\u68d2\uff0c\u6027\u80fd\u4e0b\u964d\u5e73\u5747\u4ec5-0.71%", "conclusion": "\u4e0a\u4e0b\u6587\u548c\u6587\u672c\u4fe1\u606f\u5bf9\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0cCADD\u67b6\u6784\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u5bf9\u6297\u9c81\u68d2\u6027"}}
{"id": "2601.12696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12696", "abs": "https://arxiv.org/abs/2601.12696", "authors": ["Tassallah Abdullahi", "Macton Mgonzo", "Mardiyyah Oduwole", "Paul Okewunmi", "Abraham Owodunni", "Ritambhara Singh", "Carsten Eickhoff"], "title": "UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages", "comment": "12 pages", "summary": "Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.", "AI": {"tldr": "UbuntuGuard\u662f\u9996\u4e2a\u9488\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u57fa\u4e8e\u7b56\u7565\u7684\u5b89\u5168\u57fa\u51c6\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u521b\u5efa\u5bf9\u6297\u6027\u67e5\u8be2\uff0c\u8bc4\u4f3013\u4e2a\u6a21\u578b\u53d1\u73b0\u73b0\u6709\u82f1\u8bed\u4e2d\u5fc3\u57fa\u51c6\u9ad8\u4f30\u591a\u8bed\u8a00\u5b89\u5168\u6027\uff0c\u9700\u8981\u6587\u5316\u63a5\u5730\u6c14\u7684\u5b89\u5168\u57fa\u51c6", "motivation": "\u5f53\u524d\u5b88\u62a4\u6a21\u578b\u4e3b\u8981\u9762\u5411\u897f\u65b9\u4e2d\u5fc3\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u4f4e\u8d44\u6e90\u975e\u6d32\u8bed\u8a00\u9762\u4e34\u5b89\u5168\u6f0f\u6d1e\u3001\u8de8\u8bed\u8a00\u5b89\u5168\u5931\u6548\u548c\u6587\u5316\u9519\u4f4d\u95ee\u9898\uff0c\u73b0\u6709\u5b89\u5168\u7c7b\u522b\u50f5\u5316\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u8bed\u8a00\u548c\u793e\u4f1a\u6587\u5316\u80cc\u666f", "method": "\u5f15\u5165UbuntuGuard\u57fa\u51c6\uff0c\u57fa\u4e8e155\u540d\u9886\u57df\u4e13\u5bb6\uff08\u5305\u62ec\u533b\u7597\u9886\u57df\uff09\u521b\u5efa\u7684\u5bf9\u6297\u6027\u67e5\u8be2\uff0c\u4ece\u4e2d\u63a8\u5bfc\u51fa\u7279\u5b9a\u60c5\u5883\u7684\u5b89\u5168\u7b56\u7565\u548c\u53c2\u8003\u54cd\u5e94\uff0c\u8bc4\u4f3013\u4e2a\u6a21\u578b\uff086\u4e2a\u901a\u7528LLM\u548c7\u4e2a\u5b88\u62a4\u6a21\u578b\uff09\u7684\u4e09\u79cd\u53d8\u4f53\uff1a\u9759\u6001\u3001\u52a8\u6001\u548c\u591a\u8bed\u8a00", "result": "\u53d1\u73b0\u73b0\u6709\u82f1\u8bed\u4e2d\u5fc3\u57fa\u51c6\u9ad8\u4f30\u5b9e\u9645\u591a\u8bed\u8a00\u5b89\u5168\u6027\uff0c\u8de8\u8bed\u8a00\u8f6c\u79fb\u63d0\u4f9b\u90e8\u5206\u4f46\u4e0d\u5145\u5206\u7684\u8986\u76d6\uff0c\u52a8\u6001\u6a21\u578b\u867d\u80fd\u66f4\u597d\u5229\u7528\u63a8\u7406\u65f6\u7b56\u7565\uff0c\u4f46\u4ecd\u96be\u4ee5\u5b8c\u5168\u672c\u5730\u5316\u975e\u6d32\u8bed\u8a00\u60c5\u5883", "conclusion": "\u8feb\u5207\u9700\u8981\u591a\u8bed\u8a00\u3001\u6587\u5316\u63a5\u5730\u6c14\u7684\u5b89\u5168\u57fa\u51c6\uff0c\u4ee5\u5f00\u53d1\u53ef\u9760\u4e14\u516c\u5e73\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b88\u62a4\u6a21\u578b\uff0c\u786e\u4fdd\u5b89\u5168\u7cfb\u7edf\u53cd\u6620\u672c\u5730\u89c4\u8303\u548c\u6587\u5316\u671f\u671b"}}
{"id": "2601.12224", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12224", "abs": "https://arxiv.org/abs/2601.12224", "authors": ["Meng Wei", "Kun Yuan", "Shi Li", "Yue Zhou", "Long Bai", "Nassir Navab", "Hongliang Ren", "Hong Joo Lee", "Tom Vercauteren", "Nicolas Padoy"], "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion", "comment": null, "summary": "Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.", "AI": {"tldr": "SurgRef\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fd0\u52a8\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ffd\u8e2a\u624b\u672f\u5668\u68b0\u7684\u8fd0\u52a8\u8f68\u8ff9\u800c\u975e\u9759\u6001\u5916\u89c2\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u624b\u672f\u5668\u68b0\u5206\u5272\uff0c\u5728\u906e\u6321\u3001\u6a21\u7cca\u6216\u964c\u751f\u672f\u8bed\u60c5\u51b5\u4e0b\u4ecd\u80fd\u51c6\u786e\u8bc6\u522b\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u573a\u666f\u4e2d\u7684\u6307\u4ee3\u5206\u5272\u4efb\u52a1\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7279\u5f81\u548c\u9884\u5b9a\u4e49\u5668\u68b0\u540d\u79f0\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u624b\u672f\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7406\u89e3\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u63cf\u8ff0\u5e76\u51c6\u786e\u5b9a\u4f4d\u624b\u672f\u5668\u68b0\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5668\u68b0\u88ab\u906e\u6321\u3001\u63cf\u8ff0\u6a21\u7cca\u6216\u4f7f\u7528\u4e0d\u719f\u6089\u672f\u8bed\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faSurgRef\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u5f15\u5bfc\u5c06\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u8868\u8fbe\u4e0e\u5668\u68b0\u8fd0\u52a8\u8f68\u8ff9\u76f8\u7ed3\u5408\u3002\u6846\u67b6\u5173\u6ce8\u5668\u68b0\u5982\u4f55\u968f\u65f6\u95f4\u79fb\u52a8\u548c\u4ea4\u4e92\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u5916\u89c2\u7279\u5f81\u3002\u540c\u65f6\u521b\u5efa\u4e86Ref-IMotion\u6570\u636e\u96c6\uff0c\u5305\u542b\u5bc6\u96c6\u7684\u65f6\u7a7a\u63a9\u7801\u548c\u4e30\u5bcc\u7684\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "SurgRef\u5728\u4e0d\u540c\u624b\u672f\u7a0b\u5e8f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9c81\u68d2\u7684\u8bed\u8a00\u9a71\u52a8\u624b\u672f\u89c6\u9891\u5206\u5272\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc7\u8fd0\u52a8\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0cSurgRef\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5e76\u5206\u5272\u624b\u672f\u5668\u68b0\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\uff0c\u63a8\u52a8\u4e86\u667a\u80fd\u624b\u672f\u5ba4\u548c\u81ea\u4e3b\u624b\u672f\u673a\u5668\u4eba\u8f85\u52a9\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.13465", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13465", "abs": "https://arxiv.org/abs/2601.13465", "authors": ["Yimeng Min", "Carla P. Gomes"], "title": "Graph Neural Networks are Heuristics", "comment": null, "summary": "We demonstrate that a single training trajectory can transform a graph neural network into an unsupervised heuristic for combinatorial optimization. Focusing on the Travelling Salesman Problem, we show that encoding global structural constraints as an inductive bias enables a non-autoregressive model to generate solutions via direct forward passes, without search, supervision, or sequential decision-making. At inference time, dropout and snapshot ensembling allow a single model to act as an implicit ensemble, reducing optimality gaps through increased solution diversity. Our results establish that graph neural networks do not require supervised training nor explicit search to be effective. Instead, they can internalize global combinatorial structure and function as strong, learned heuristics. This reframes the role of learning in combinatorial optimization: from augmenting classical algorithms to directly instantiating new heuristics.", "AI": {"tldr": "\u5355\u4e2a\u8bad\u7ec3\u8f68\u8ff9\u53ef\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u8f6c\u5316\u4e3a\u7ec4\u5408\u4f18\u5316\u7684\u65e0\u76d1\u7763\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u65e0\u9700\u641c\u7d22\u3001\u76d1\u7763\u6216\u5e8f\u5217\u51b3\u7b56\uff0c\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u751f\u6210\u89e3", "motivation": "\u63a2\u7d22\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u65b0\u89d2\u8272\uff0c\u4ece\u589e\u5f3a\u7ecf\u5178\u7b97\u6cd5\u8f6c\u53d8\u4e3a\u76f4\u63a5\u5b9e\u4f8b\u5316\u65b0\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u8bc1\u660e\u795e\u7ecf\u7f51\u7edc\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u6216\u663e\u5f0f\u641c\u7d22\u5373\u53ef\u6709\u6548\u5de5\u4f5c", "method": "\u5c06\u5168\u5c40\u7ed3\u6784\u7ea6\u675f\u7f16\u7801\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u975e\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u751f\u6210\u89e3\uff1b\u63a8\u7406\u65f6\u4f7f\u7528dropout\u548c\u5feb\u7167\u96c6\u6210\uff0c\u4f7f\u5355\u4e2a\u6a21\u578b\u4f5c\u4e3a\u9690\u5f0f\u96c6\u6210\uff0c\u589e\u52a0\u89e3\u591a\u6837\u6027", "result": "\u6210\u529f\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u8f6c\u5316\u4e3a\u65c5\u884c\u5546\u95ee\u9898\u7684\u65e0\u76d1\u7763\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6700\u4f18\u6027\u95f4\u9699\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5185\u5316\u5168\u5c40\u7ec4\u5408\u7ed3\u6784\u5e76\u4f5c\u4e3a\u5f3a\u5b66\u4e60\u542f\u53d1\u5f0f", "conclusion": "\u56fe\u795e\u7ecf\u7f51\u7edc\u65e0\u9700\u76d1\u7763\u8bad\u7ec3\u6216\u663e\u5f0f\u641c\u7d22\u5373\u53ef\u6709\u6548\u5de5\u4f5c\uff0c\u80fd\u591f\u5185\u5316\u5168\u5c40\u7ec4\u5408\u7ed3\u6784\u5e76\u4f5c\u4e3a\u5f3a\u5b66\u4e60\u542f\u53d1\u5f0f\uff0c\u8fd9\u91cd\u65b0\u5b9a\u4e49\u4e86\u5b66\u4e60\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u89d2\u8272\uff1a\u4ece\u589e\u5f3a\u7ecf\u5178\u7b97\u6cd5\u8f6c\u53d8\u4e3a\u76f4\u63a5\u5b9e\u4f8b\u5316\u65b0\u542f\u53d1\u5f0f\u7b97\u6cd5"}}
{"id": "2601.12698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12698", "abs": "https://arxiv.org/abs/2601.12698", "authors": ["Qiuyi Qu", "Yicheng Sui", "Yufei Sun", "Rui Chen", "Xiaofei Zhang", "Yuzhi Zhang", "Haofeng Wang", "Ge Lan", "Ning Zhang"], "title": "A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization", "comment": null, "summary": "GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u677f\u7684GPU\u5185\u6838\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u4ee3\u7406\u4e0e\u53c2\u6570\u641c\u7d22\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u6027\u80fd\u63d0\u5347", "motivation": "GPU\u4ee3\u7801\u4f18\u5316\u5bf9HPC\u548c\u5927\u6a21\u578b\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u6216LLM\u76f4\u63a5\u91cd\u5199\uff0c\u5b58\u5728\u53c2\u6570\u9690\u5f0f\u3001\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898", "method": "\u5f15\u5165\u6a21\u677f\u5316\u91cd\u5199\u5c42\uff1a\u5148\u5c06\u5185\u6838\u91cd\u6784\u4e3a\u663e\u5f0f\u53c2\u6570\u5316\u6a21\u677f\uff0c\u518d\u901a\u8fc7\u57fa\u4e8e\u641c\u7d22\u7684\u81ea\u52a8\u8c03\u4f18\u4f18\u5316\u6a21\u677f\u53c2\u6570\uff0c\u7ed3\u5408\u4ee3\u7406\u9a71\u52a8\u7684\u8fed\u4ee3\u5faa\u73af", "result": "\u5728\u771f\u5b9e\u5185\u6838\u4e0a\u5b9e\u9a8c\u663e\u793a\u6700\u4f73\u60c5\u51b5\u4e0b\u901f\u5ea6\u63d0\u5347\u8d85\u8fc73\u500d\uff0c\u76f8\u6bd4\u7eaf\u4ee3\u7406\u76f4\u63a5\u91cd\u5199\u663e\u8457\u51cf\u5c11\u4f18\u5316\u968f\u673a\u6027\uff0c\u8fc7\u7a0b\u66f4\u53ef\u89e3\u91ca", "conclusion": "\u6a21\u677f\u52a0\u641c\u7d22\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u7cfb\u7edf\u7684\u9ad8\u6027\u80fd\u914d\u7f6e\u65b9\u6cd5\uff0c\u53ef\u6269\u5c55\u5230OpenCL\u3001HIP\u7b49\u540e\u7aef\uff0c\u4e3a\u751f\u4ea7\u8d1f\u8f7d\u63d0\u4f9b\u81ea\u52a8\u5316\u6027\u80fd\u4f18\u5316"}}
{"id": "2601.12233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12233", "abs": "https://arxiv.org/abs/2601.12233", "authors": ["Zhenzhen Wang", "Zhongliang Zhou", "Zhuoyu Wen", "Jeong Hwan Kook", "John B Wojcik", "John Kang"], "title": "DiffusionQC: Artifact Detection in Histopathology via Diffusion Model", "comment": "7 pages", "summary": "Digital pathology plays a vital role across modern medicine, offering critical insights for disease diagnosis, prognosis, and treatment. However, histopathology images often contain artifacts introduced during slide preparation and digitization. Detecting and excluding them is essential to ensure reliable downstream analysis. Traditional supervised models typically require large annotated datasets, which is resource-intensive and not generalizable to novel artifact types. To address this, we propose DiffusionQC, which detects artifacts as outliers among clean images using a diffusion model. It requires only a set of clean images for training rather than pixel-level artifact annotations and predefined artifact types. Furthermore, we introduce a contrastive learning module to explicitly enlarge the distribution separation between artifact and clean images, yielding an enhanced version of our method. Empirical results demonstrate superior performance to state-of-the-art and offer cross-stain generalization capacity, with significantly less data and annotations.", "AI": {"tldr": "\u63d0\u51faDiffusionQC\u65b9\u6cd5\uff0c\u4ec5\u9700\u5e72\u51c0\u56fe\u50cf\u8bad\u7ec3\u5373\u53ef\u68c0\u6d4b\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u6216\u9884\u5b9a\u4e49\u4f2a\u5f71\u7c7b\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u6027\u80fd", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u56fe\u50cf\u5e38\u5305\u542b\u5236\u5907\u548c\u6570\u5b57\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7684\u4f2a\u5f71\uff0c\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u4f2a\u5f71\u7c7b\u578b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u4f2a\u5f71\u68c0\u6d4b\u4e3a\u5e72\u51c0\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u4ec5\u9700\u5e72\u51c0\u56fe\u50cf\u8bad\u7ec3\uff1b\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\u663e\u5f0f\u6269\u5927\u4f2a\u5f71\u4e0e\u5e72\u51c0\u56fe\u50cf\u5206\u5e03\u5dee\u5f02", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u8de8\u67d3\u8272\u6cdb\u5316\u80fd\u529b\uff0c\u6240\u9700\u6570\u636e\u548c\u6807\u6ce8\u663e\u8457\u51cf\u5c11", "conclusion": "DiffusionQC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u7684\u4f2a\u5f71\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.13481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13481", "abs": "https://arxiv.org/abs/2601.13481", "authors": ["Jian Zhang", "Zhangqi Wang", "Zhiyuan Wang", "Weiping Fu", "Yu He", "Haiping Zhu", "Qika Lin", "Jun Liu"], "title": "Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement", "comment": null, "summary": "Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.", "AI": {"tldr": "APOLO\u662f\u4e00\u4e2a\u7528\u4e8e\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u60c5\u611f\u8bca\u65ad\u7684\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u63a2\u7d22\u66f4\u7cbe\u7ec6\u7684\u63d0\u793a\u7a7a\u95f4\uff0c\u89e3\u51b3\u60c5\u611f\u5171\u75c5\u548c\u4e34\u5e8a\u7ebf\u7d22\u6316\u6398\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u4e34\u5e8a\u8bb0\u5f55\u3001\u54a8\u8be2\u5bf9\u8bdd\u548c\u5728\u7ebf\u5fc3\u7406\u5065\u5eb7\u793e\u533a\u4e2d\uff0c\u6291\u90c1\u3001\u7126\u8651\u548c\u521b\u4f24\u76f8\u5173\u72b6\u6001\u7b49\u60c5\u611f\u8868\u8fbe\u666e\u904d\u5b58\u5728\uff0c\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9b\u60c5\u611f\u5bf9\u4e8e\u4e34\u5e8a\u5206\u8bca\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u3001\u4e0a\u4e0b\u6587\u5bc6\u96c6\u7684\u533b\u7597\u73af\u5883\u4e2d\uff0c\u5176\u8bca\u65ad\u53ef\u9760\u6027\u5bf9\u63d0\u793a\u8bbe\u8ba1\u9ad8\u5ea6\u654f\u611f\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u60c5\u611f\u5171\u75c5\uff08\u591a\u79cd\u4ea4\u7ec7\u7684\u60c5\u611f\u72b6\u6001\u4f7f\u9884\u6d4b\u590d\u6742\u5316\uff09\u548c\u4e34\u5e8a\u76f8\u5173\u7ebf\u7d22\u7684\u4f4e\u6548\u63a2\u7d22\u3002", "method": "\u63d0\u51faAPOLO\u6846\u67b6\uff0c\u5c06\u6307\u4ee4\u4f18\u5316\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\uff0c\u5305\u62ec\u89c4\u5212\u5668\u3001\u6559\u5e08\u3001\u6279\u8bc4\u8005\u3001\u5b66\u751f\u548c\u76ee\u6807\u89d2\u8272\u3002\u5728\u8fd9\u4e2a\u95ed\u73af\u6846\u67b6\u4e2d\uff0c\u89c4\u5212\u5668\u5b9a\u4e49\u4f18\u5316\u8f68\u8ff9\uff0c\u6559\u5e08-\u6279\u8bc4\u8005-\u5b66\u751f\u667a\u80fd\u4f53\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u4ee5\u589e\u5f3a\u63a8\u7406\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\uff0c\u76ee\u6807\u667a\u80fd\u4f53\u6839\u636e\u6027\u80fd\u8bc4\u4f30\u51b3\u5b9a\u662f\u5426\u7ee7\u7eed\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPOLO\u5728\u9886\u57df\u7279\u5b9a\u548c\u5206\u5c42\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u5728\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u53ef\u4fe1\u8d56LLM\u5e94\u7528\u7684\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u8303\u5f0f\u3002", "conclusion": "APOLO\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u66f4\u5e7f\u6cdb\u548c\u66f4\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u60c5\u611f\u8bca\u65ad\u4e2d\u7684\u5171\u75c5\u95ee\u9898\u548c\u4e34\u5e8a\u7ebf\u7d22\u6316\u6398\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4e3a\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u7684\u9ad8\u98ce\u9669LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12731", "abs": "https://arxiv.org/abs/2601.12731", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nicol\u00f2 Brunello", "Gianluca Demartini"], "title": "A Shared Geometry of Difficulty in Multilingual Language Models", "comment": null, "summary": "Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.", "AI": {"tldr": "LLMs\u5728\u95ee\u9898\u96be\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4e24\u9636\u6bb5\u8868\u793a\uff1a\u6d45\u5c42\u8868\u793a\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u7684\u96be\u5ea6\u4fe1\u53f7\uff0c\u6df1\u5c42\u8868\u793a\u5219\u53d8\u5f97\u8bed\u8a00\u7279\u5b9a\u5316\u3002", "motivation": "\u7814\u7a76LLMs\u4e2d\u95ee\u9898\u96be\u5ea6\u7684\u591a\u8bed\u8a00\u51e0\u4f55\u7ed3\u6784\uff0c\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a\u548c\u6cdb\u5316\u96be\u5ea6\u4fe1\u606f\u3002", "method": "\u4f7f\u7528AMC\u5b50\u96c6\uff08Easy2Hard\u57fa\u51c6\uff09\u7ffb\u8bd1\u621021\u79cd\u8bed\u8a00\uff0c\u5728LLMs\u5185\u90e8\u8868\u793a\u4e0a\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\uff0c\u5206\u6790\u6d45\u5c42\uff08\u65e9\u671f\u5c42\uff09\u548c\u6df1\u5c42\uff08\u540e\u671f\u5c42\uff09\u8868\u793a\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u6df1\u5c42\u8868\u793a\u63a2\u9488\u5728\u76f8\u540c\u8bed\u8a00\u4e0a\u51c6\u786e\u7387\u9ad8\u4f46\u8de8\u8bed\u8a00\u6cdb\u5316\u5dee\uff1b\u6d45\u5c42\u8868\u793a\u63a2\u9488\u8de8\u8bed\u8a00\u6cdb\u5316\u597d\u4f46\u8bed\u8a00\u5185\u6027\u80fd\u8f83\u4f4e\u3002\u8fd9\u8868\u660eLLMs\u5148\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u7684\u96be\u5ea6\u8868\u793a\uff0c\u540e\u53d8\u4e3a\u8bed\u8a00\u7279\u5b9a\u8868\u793a\u3002", "conclusion": "LLMs\u91c7\u7528\u4e24\u9636\u6bb5\u8868\u793a\u8fc7\u7a0b\u5904\u7406\u95ee\u9898\u96be\u5ea6\uff1a\u5148\u62bd\u8c61\u6982\u5ff5\u7a7a\u95f4\uff0c\u540e\u8bed\u8a00\u7279\u5b9a\u8f93\u51fa\u3002\u8fd9\u79cd\u6a21\u5f0f\u4e0d\u4ec5\u9002\u7528\u4e8e\u8bed\u4e49\u5185\u5bb9\uff0c\u4e5f\u9002\u7528\u4e8e\u5143\u8ba4\u77e5\u5c5e\u6027\u5982\u96be\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2601.12243", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12243", "abs": "https://arxiv.org/abs/2601.12243", "authors": ["Shreya Rajpal", "Michal Golovanesky", "Carsten Eickhoff"], "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos", "comment": "22 pages, 6 figures", "summary": "Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u89c6\u9891\u6458\u8981\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89c6\u89c9\u91c7\u6837\u3001\u6807\u7b7e\u9a71\u52a8\u7684\u5173\u952e\u5e27\u951a\u5b9a\u548cLLM\u4e0a\u4e0b\u6587\u9a8c\u8bc1\uff0c\u5728\u4ec5\u91c7\u6837\u4e0d\u52305%\u539f\u59cb\u5e27\u7684\u60c5\u51b5\u4e0b\u4fdd\u755984%\u8bed\u4e49\u5185\u5bb9\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534733%", "motivation": "\u89c6\u9891\u6458\u8981\u5728\u624b\u672f\u57f9\u8bad\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5f88\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u4ece\u57fa\u672c\u89c6\u89c9\u7279\u5f81\u53d1\u5c55\u5230\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u9700\u8981\u66f4\u8bed\u4e49\u57fa\u7840\u3001\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u6458\u8981\u65b9\u6cd5", "method": "\u63d0\u51faPRISM\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u81ea\u9002\u5e94\u89c6\u89c9\u91c7\u6837\uff0c2)\u6807\u7b7e\u9a71\u52a8\u7684\u5173\u952e\u5e27\u951a\u5b9a\uff0c3)\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u9a8c\u8bc1\uff0c\u786e\u4fdd\u9009\u62e9\u6709\u610f\u4e49\u7684\u7a0b\u5e8f\u6027\u8fc7\u6e21\u5e27\uff0c\u8fc7\u6ee4\u901a\u7528\u6216\u5e7b\u89c9\u5185\u5bb9", "result": "\u5728\u6307\u4ee4\u6027\u548c\u6d3b\u52a8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u91c7\u6837\u5c11\u4e8e5%\u539f\u59cb\u5e27\u7684\u60c5\u51b5\u4e0b\u4fdd\u755984%\u8bed\u4e49\u5185\u5bb9\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534733%\uff0c\u5728\u7a0b\u5e8f\u548c\u9886\u57df\u7279\u5b9a\u89c6\u9891\u4efb\u52a1\u4e0a\u90fd\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b", "conclusion": "PRISM\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u8bed\u4e49\u548c\u591a\u6a21\u6001\u5206\u6790\uff0c\u4ea7\u751f\u8bed\u4e49\u57fa\u7840\u3001\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u89c6\u9891\u6458\u8981\uff0c\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u7a0b\u5e8f\u548c\u9886\u57df\u7279\u5b9a\u89c6\u9891\u4efb\u52a1"}}
{"id": "2601.13518", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.13518", "abs": "https://arxiv.org/abs/2601.13518", "authors": ["Jiayi Yuan", "Jonathan N\u00f6ther", "Natasha Jaques", "Goran Radanovi\u0107"], "title": "AgenticRed: Optimizing Agentic Systems for Automated Red-teaming", "comment": "Website: https://yuanjiayiy.github.io/AgenticRed/", "summary": "While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.", "AI": {"tldr": "AgenticRed\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\u8bbe\u8ba1\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u8fed\u4ee3\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5b58\u5728\u4eba\u4e3a\u504f\u89c1\u95ee\u9898\uff0c\u4e14\u63a2\u7d22\u66f4\u5e7f\u6cdb\u8bbe\u8ba1\u7a7a\u95f4\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u7ea2\u961f\u6d4b\u8bd5\u89c6\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u95ee\u9898\u800c\u975e\u7b56\u7565\u4f18\u5316\u95ee\u9898\u3002\u53d7Meta Agent Search\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u4f7f\u7528\u8fdb\u5316\u9009\u62e9\u6f14\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\u3002\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u8fed\u4ee3\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "AgenticRed\u8bbe\u8ba1\u7684\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1a\u5728Llama-2-7B\u4e0a\u8fbe\u523096%\u653b\u51fb\u6210\u529f\u7387\uff08\u63d0\u534736%\uff09\uff0c\u5728Llama-3-8B\u4e0a\u8fbe\u523098%\u3002\u5728\u4e13\u6709\u6a21\u578b\u4e0a\u4e5f\u6709\u5f3a\u8fc1\u79fb\u6027\uff1aGPT-3.5-Turbo\u548cGPT-4o-mini\u8fbe\u5230100%\u653b\u51fb\u6210\u529f\u7387\uff0cClaude-Sonnet-3.5\u8fbe\u523060%\uff08\u63d0\u534724%\uff09\u3002", "conclusion": "\u81ea\u52a8\u5316\u7cfb\u7edf\u8bbe\u8ba1\u662fAI\u5b89\u5168\u8bc4\u4f30\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u80fd\u591f\u8ddf\u4e0a\u5feb\u901f\u53d1\u5c55\u7684\u6a21\u578b\u6b65\u4f10\u3002AgenticRed\u5c55\u793a\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u81ea\u52a8\u8bbe\u8ba1\u548c\u4f18\u5316\u7ea2\u961f\u6d4b\u8bd5\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3aAI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.12748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12748", "abs": "https://arxiv.org/abs/2601.12748", "authors": ["Bin Xie", "Bingbing Xu", "Xueyun Tian", "Yilin Chen", "Huawei Shen"], "title": "Towards Robust Process Reward Modeling via Noise-aware Learning", "comment": null, "summary": "Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \\underline{\\textbf{N}}oise-\\underline{\\textbf{A}}ware \\underline{\\textbf{I}}terative \\underline{\\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\\% absolute gain in average F1 over PRMs trained with noisy supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\u6765\u7f13\u89e3PRM\u8bad\u7ec3\u4e2d\u7684\u566a\u58f0\u76d1\u7763\u95ee\u9898\uff0c\u901a\u8fc7\u53cd\u5c04\u611f\u77e5\u6807\u7b7e\u6821\u6b63\u548c\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6b65\u9aa4\u7ea7\u6b63\u786e\u6027\u5224\u522b\u80fd\u529b", "motivation": "\u73b0\u6709PRM\u4f9d\u8d56\u6602\u8d35\u7684\u6d41\u7a0b\u7ea7\u76d1\u7763\uff0c\u800c\u66ff\u4ee3\u65b9\u6848MCE\u4f1a\u4ea7\u751f\u7b56\u7565\u4f9d\u8d56\u7684\u5956\u52b1\uff0c\u5bfc\u81f4\u6807\u7b7e\u566a\u58f0\uff08\u5305\u62ec\u9519\u8bef\u5956\u52b1\u6b63\u786e\u6b65\u9aa4\u548c\u60e9\u7f5a\u6b63\u786e\u6b65\u9aa4\uff09\uff0c\u9700\u8981\u89e3\u51b3\u566a\u58f0\u76d1\u7763\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u6807\u6ce8\u9636\u6bb5\uff1a\u5f15\u5165\u53cd\u5c04\u611f\u77e5\u6807\u7b7e\u6821\u6b63\u673a\u5236\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u68c0\u6d4b\u4e0e\u5f53\u524d\u63a8\u7406\u6b65\u9aa4\u76f8\u5173\u7684\u53cd\u601d\u548c\u81ea\u6211\u7ea0\u6b63\u884c\u4e3a\uff0c\u6291\u5236\u9ad8\u4f30\u5956\u52b1\uff1b2) \u8bad\u7ec3\u9636\u6bb5\uff1a\u63d0\u51fa\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7fPRM\u80fd\u57fa\u4e8e\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u9010\u6b65\u7ec6\u5316\u566a\u58f0\u6807\u7b7e", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6b65\u9aa4\u7ea7\u6b63\u786e\u6027\u5224\u522b\u80fd\u529b\uff0c\u76f8\u6bd4\u4f7f\u7528\u566a\u58f0\u76d1\u7763\u8bad\u7ec3\u7684PRM\uff0c\u5e73\u5747F1\u5206\u6570\u63d0\u5347\u9ad8\u8fbe27%", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86PRM\u8bad\u7ec3\u4e2d\u7684\u566a\u58f0\u76d1\u7763\u95ee\u9898\uff0c\u901a\u8fc7\u53cd\u5c04\u611f\u77e5\u6807\u7b7e\u6821\u6b63\u548c\u566a\u58f0\u611f\u77e5\u8fed\u4ee3\u8bad\u7ec3\uff0c\u663e\u8457\u6539\u5584\u4e86\u6b65\u9aa4\u7ea7\u63a8\u7406\u8bc4\u4f30\u7684\u8d28\u91cf"}}
{"id": "2601.12249", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12249", "abs": "https://arxiv.org/abs/2601.12249", "authors": ["Ehsan Sadeghi Pour", "Mahdi Esmaeili", "Morteza Romoozi"], "title": "An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion", "comment": "13 page", "summary": "Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\\%, sensitivity of 97.8\\%, specificity of 96.3\\%, F1-score of 98.2\\%, and overall precision of 97.9\\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408PAAC\u548cTransformer\u7684\u4e73\u817a\u764c\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.5%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5973\u6027\u5e38\u89c1\u764c\u75c7\uff0c\u51c6\u786e\u53ca\u65f6\u7684\u8bca\u65ad\u5bf9\u6539\u5584\u6cbb\u7597\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u548c\u5927\u6570\u636e\u96c6\u4e0a\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u91d1\u5b57\u5854\u81ea\u9002\u5e94\u7a7a\u6d1e\u5377\u79ef(PAAC)\u548cTransformer\u67b6\u6784\u7684\u521b\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408Dice Loss\u548cFocal Loss\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002\u4f7f\u7528INbreast\u3001MIAS\u3001DDSM\u6570\u636e\u96c6\uff0c\u7ecf\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u9884\u5904\u7406\uff0c\u56fe\u50cf\u8c03\u6574\u4e3a227\u00d7227\u50cf\u7d20\u3002", "result": "\u6a21\u578b\u5728\u4e73\u817a\u764c\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u51c6\u786e\u738798.5%\u3001\u7075\u654f\u5ea697.8%\u3001\u7279\u5f02\u602796.3%\u3001F1\u5206\u657098.2%\u3001\u7cbe\u786e\u738797.9%\u3002\u4f18\u4e8eBreastNet\u3001DeepMammo\u3001Multi-Scale CNN\u3001Swin-Unet\u3001SegFormer\u7b49\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u5927\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u8bc6\u522b\u764c\u6027\u80bf\u5757\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u4e73\u817a\u764c\u8bca\u65ad\u7684\u53ef\u9760\u9ad8\u6548\u5de5\u5177\uff0c\u6709\u671b\u96c6\u6210\u5230\u533b\u7597\u8bca\u65ad\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2601.13533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13533", "abs": "https://arxiv.org/abs/2601.13533", "authors": ["Changshuo Zhang"], "title": "Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models", "comment": null, "summary": "Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the \"reason first, recommend later\" paradigm to achieve \"reasoning while recommending\", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEGLR\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u7684\u6f5c\u5728\u63a8\u7406\u673a\u5236\u5b9e\u73b0\"\u8fb9\u63a8\u7406\u8fb9\u63a8\u8350\"\uff0c\u89e3\u51b3\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u4e2d\u52a8\u6001\u71b5\u53d8\u5316\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u5217\u8868\u751f\u6210\u8fc7\u7a0b\u4e2d\u6a21\u578b\u96be\u5ea6\u7684\u52a8\u6001\u71b5\u53d8\u5316\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u590d\u6742\u504f\u597d\u3002\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u63a8\u7406\u80fd\u529b\u53d6\u5f97\u7a81\u7834\uff0c\u542f\u53d1\u6211\u4eec\u5f15\u5165\u6f5c\u5728\u63a8\u7406\u673a\u5236\u6765\u964d\u4f4e\u51b3\u7b56\u71b5\u3002", "method": "\u63d0\u51fa\u71b5\u5f15\u5bfc\u6f5c\u5728\u63a8\u7406(EGLR)\u6a21\u578b\uff1a1) \u653e\u5f03\"\u5148\u63a8\u7406\u540e\u63a8\u8350\"\u8303\u5f0f\uff0c\u5b9e\u73b0\"\u8fb9\u63a8\u7406\u8fb9\u63a8\u8350\"\uff1b2) \u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u4ee4\u724c\u548c\u52a8\u6001\u6e29\u5ea6\u8c03\u6574\u5b9e\u73b0\u71b5\u5f15\u5bfc\u7684\u53d8\u957f\u63a8\u7406\uff1b3) \u91c7\u7528\u8f7b\u91cf\u7ea7\u96c6\u6210\u8bbe\u8ba1\uff0c\u65e0\u9700\u590d\u6742\u72ec\u7acb\u6a21\u5757\u6216\u540e\u5904\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u52bf\u5728\u4e8e\u80fd\u4e0e\u73b0\u6709\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u6a21\u578b\u517c\u5bb9\u4ee5\u63d0\u5347\u5176\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u5c55\u793a\u4e86\u5176\u5b9e\u8df5\u90e8\u7f72\u4ef7\u503c\u548c\u7814\u7a76\u6f5c\u529b\u3002", "conclusion": "EGLR\u6a21\u578b\u901a\u8fc7\u71b5\u5f15\u5bfc\u7684\u6f5c\u5728\u63a8\u7406\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u4e2d\u7684\u52a8\u6001\u71b5\u9002\u5e94\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5177\u6709\u826f\u597d\u517c\u5bb9\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.12758", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12758", "abs": "https://arxiv.org/abs/2601.12758", "authors": ["Shenyan Zheng", "Jiayou Zhong", "Anudeex Shetty", "Heng Ji", "Preslav Nakov", "Usman Naseem"], "title": "VISPA: Pluralistic Alignment via Automatic Value Selection and Activation", "comment": "WIP", "summary": "As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.", "AI": {"tldr": "VISPA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u548c\u5185\u90e8\u6fc0\u6d3b\u5f15\u5bfc\u5b9e\u73b0\u76f4\u63a5\u7684\u4ef7\u503c\u8868\u8fbe\u63a7\u5236\uff0c\u5728\u533b\u7597\u7b49\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5e94\u7528\u589e\u591a\uff0c\u9700\u8981\u8ba9\u6a21\u578b\u8f93\u51fa\u53cd\u6620\u591a\u6837\u7684\u4eba\u7c7b\u89c2\u70b9\u800c\u975e\u5355\u4e00\u5e73\u5747\u504f\u597d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8003\u8651\u6709\u9650\u4ef7\u503c\u89c2\uff0c\u8981\u4e48\u4f9d\u8d56\u63d0\u793a\u7ea7\u5e72\u9884\uff0c\u7f3a\u4e4f\u4ef7\u503c\u63a7\u5236\u548c\u4ee3\u8868\u6027\u3002", "method": "VISPA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u89c6\u89d2\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u548c\u5185\u90e8\u6a21\u578b\u6fc0\u6d3b\u5f15\u5bfc\u6765\u5b9e\u73b0\u76f4\u63a5\u7684\u4ef7\u503c\u8868\u8fbe\u63a7\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cVISPA\u5728\u533b\u7597\u53ca\u5176\u4ed6\u9886\u57df\u7684\u6240\u6709\u591a\u89c6\u89d2\u5bf9\u9f50\u6a21\u5f0f\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793aVISPA\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u5f15\u5bfc\u521d\u59cb\u5316\u3001\u6a21\u578b\u548c\u4ef7\u503c\u89c2\u3002", "conclusion": "\u591a\u89c6\u89d2\u5bf9\u9f50\u53ef\u4ee5\u901a\u8fc7\u5185\u90e8\u6fc0\u6d3b\u673a\u5236\u5b9e\u73b0\uff0c\u8fd9\u4e3a\u6784\u5efa\u670d\u52a1\u4e8e\u6240\u6709\u4eba\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2601.12253", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12253", "abs": "https://arxiv.org/abs/2601.12253", "authors": ["Haoran Xu", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo"], "title": "Federated Joint Learning for Domain and Class Generalization", "comment": "ICASSP 2026", "summary": "Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \\textbf{Fed}erated Joint Learning for \\textbf{D}omain and \\textbf{C}lass \\textbf{G}eneralization, termed \\textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.", "AI": {"tldr": "FedDCG\uff1a\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u7c7b\u522b\u6cdb\u5316\u548c\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u57df\u5206\u7ec4\u7b56\u7565\u548c\u53ef\u5b66\u4e60\u7f51\u7edc\u63d0\u5347\u672a\u89c1\u7c7b\u522b\u548c\u672a\u89c1\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u672a\u89c1\u7c7b\u522b\u6216\u672a\u89c1\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u8054\u5408\u6846\u67b6\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u7684\u5927\u89c4\u6a21\u53c2\u6570\u548c\u9884\u8bad\u7ec3\u9700\u6c42\u4f7f\u5f97\u9ad8\u6548\u5fae\u8c03\u53d8\u5f97\u5173\u952e\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u7c7b\u522b\u548c\u57df\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDCG\u8054\u90a6\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u57df\u5206\u7ec4\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u7ec4\u5185\u8bad\u7ec3\u7c7b\u522b\u6cdb\u5316\u7f51\u7edc\u4ee5\u907f\u514d\u51b3\u7b56\u8fb9\u754c\u6df7\u6dc6\uff1b2\uff09\u63a8\u7406\u65f6\u57fa\u4e8e\u57df\u76f8\u4f3c\u6027\u805a\u5408\u7c7b\u522b\u6cdb\u5316\u7ed3\u679c\uff1b3\uff09\u4f7f\u7528\u53ef\u5b66\u4e60\u7f51\u7edc\u589e\u5f3a\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\uff1b4\uff09\u89e3\u8026\u673a\u5236\u5206\u79bb\u901a\u7528\u77e5\u8bc6\u548c\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedDCG\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedDCG\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u540c\u65f6\u5904\u7406\u7c7b\u522b\u548c\u57df\u6cdb\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u57df\u5206\u7ec4\u548c\u89e3\u8026\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.13545", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13545", "abs": "https://arxiv.org/abs/2601.13545", "authors": ["Shirin Shahabi", "Spencer Graham", "Haruna Isah"], "title": "TruthTensor: Evaluating LLMs Human Imitation through Prediction Market Drift and Holistic Reasoning", "comment": "16 pages, 6 figures, 2 tables", "summary": "Evaluating language models and AI agents remains fundamentally challenging because static benchmarks fail to capture real-world uncertainty, distribution shift, and the gap between isolated task accuracy and human-aligned decision-making under evolving conditions. This paper introduces TruthTensor, a novel, reproducible evaluation paradigm that measures Large Language Models (LLMs) not only as prediction engines but as human-imitation systems operating in socially-grounded, high-entropy environments. Building on forward-looking, contamination-free tasks, our framework anchors evaluation to live prediction markets and combines probabilistic scoring to provide a holistic view of model behavior. TruthTensor complements traditional correctness metrics with drift-centric diagnostics and explicit robustness checks for reproducibility. It specify human vs. automated evaluation roles, annotation protocols, and statistical testing procedures to ensure interpretability and replicability of results. In experiments across 500+ real markets (political, economic, cultural, technological), TruthTensor demonstrates that models with similar forecast accuracy can diverge markedly in calibration, drift, and risk-sensitivity, underscoring the need to evaluate models along multiple axes (accuracy, calibration, narrative stability, cost, and resource efficiency). TruthTensor therefore operationalizes modern evaluation best practices, clear hypothesis framing, careful metric selection, transparent compute/cost reporting, human-in-the-loop validation, and open, versioned evaluation contracts, to produce defensible assessments of LLMs in real-world decision contexts. We publicly release TruthTensor at https://truthtensor.com", "AI": {"tldr": "TruthTensor\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u4eba\u7c7b\u5bf9\u9f50\u51b3\u7b56\u65b9\u9762\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5b9e\u65f6\u9884\u6d4b\u5e02\u573a\u548c\u6982\u7387\u8bc4\u5206\u63d0\u4f9b\u5168\u9762\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u5206\u5e03\u504f\u79fb\uff0c\u4ee5\u53ca\u5b64\u7acb\u4efb\u52a1\u51c6\u786e\u6027\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u57fa\u7840\u3001\u9ad8\u71b5\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u524d\u77bb\u6027\u3001\u65e0\u6c61\u67d3\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u951a\u5b9a\u5b9e\u65f6\u9884\u6d4b\u5e02\u573a\u8bc4\u4f30\uff0c\u7ed3\u5408\u6982\u7387\u8bc4\u5206\uff0c\u5305\u542b\u6f02\u79fb\u4e2d\u5fc3\u8bca\u65ad\u548c\u663e\u5f0f\u9c81\u68d2\u6027\u68c0\u67e5\uff0c\u660e\u786e\u4eba\u7c7b\u4e0e\u81ea\u52a8\u5316\u8bc4\u4f30\u89d2\u8272\u3001\u6807\u6ce8\u534f\u8bae\u548c\u7edf\u8ba1\u6d4b\u8bd5\u7a0b\u5e8f\u3002", "result": "\u5728500\u591a\u4e2a\u771f\u5b9e\u5e02\u573a\uff08\u653f\u6cbb\u3001\u7ecf\u6d4e\u3001\u6587\u5316\u3001\u6280\u672f\uff09\u5b9e\u9a8c\u4e2d\uff0cTruthTensor\u663e\u793a\u5177\u6709\u76f8\u4f3c\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6a21\u578b\u5728\u6821\u51c6\u3001\u6f02\u79fb\u548c\u98ce\u9669\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7a81\u663e\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "TruthTensor\u64cd\u4f5c\u5316\u4e86\u73b0\u4ee3\u8bc4\u4f30\u6700\u4f73\u5b9e\u8df5\uff0c\u5305\u62ec\u6e05\u6670\u7684\u5047\u8bbe\u6846\u67b6\u3001\u8c28\u614e\u7684\u6307\u6807\u9009\u62e9\u3001\u900f\u660e\u7684\u8ba1\u7b97/\u6210\u672c\u62a5\u544a\u3001\u4eba\u7c7b\u5728\u73af\u9a8c\u8bc1\u548c\u5f00\u653e\u7684\u7248\u672c\u5316\u8bc4\u4f30\u5408\u540c\uff0c\u4e3aLLM\u5728\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u8fa9\u62a4\u7684\u8bc4\u4f30\u3002"}}
{"id": "2601.12771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12771", "abs": "https://arxiv.org/abs/2601.12771", "authors": ["Keito Inoshita"], "title": "Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory", "comment": null, "summary": "Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.", "AI": {"tldr": "\u63d0\u51faLAMA\u6846\u67b6\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u8054\u60f3\u8bb0\u5fc6\uff0c\u901a\u8fc7\u56de\u5fc6\u540c\u540d\u540d\u4eba\u95f4\u63a5\u63a8\u7406\u56fd\u7c4d\uff0c\u572899\u56fd\u56fd\u7c4d\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u523081.7%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "LLM\u62e5\u6709\u4e30\u5bcc\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u5982\u4f55\u6709\u6548\u63d0\u53d6\u8fd9\u4e9b\u77e5\u8bc6\u4ecd\u5f85\u63a2\u7d22\u3002\u56fd\u7c4d\u548c\u5730\u533a\u9884\u6d4b\u4efb\u52a1\u9700\u8981\u7406\u89e3\u8bed\u8a00\u7279\u5f81\u4ee5\u53ca\u6587\u5316\u548c\u5386\u53f2\u80cc\u666f\uff0cLLM\u7684\u4e16\u754c\u77e5\u8bc6\u7279\u522b\u6709\u4ef7\u503c\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684LLM\u63d0\u793a\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u63a8\u7406\uff0c\u5728\u5e94\u7528\u62bd\u8c61\u8bed\u8a00\u89c4\u5219\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faLLM\u8054\u60f3\u8bb0\u5fc6\u4ee3\u7406(LAMA)\u6846\u67b6\uff0c\u5c06LLM\u4e16\u754c\u77e5\u8bc6\u4f5c\u4e3a\u8054\u60f3\u8bb0\u5fc6\u3002\u4e0d\u76f4\u63a5\u4ece\u540d\u5b57\u63a8\u65ad\u56fd\u7c4d\uff0c\u800c\u662f\u56de\u5fc6\u540c\u540d\u7684\u8457\u540d\u4eba\u7269\uff0c\u901a\u8fc7\u95f4\u63a5\u63a8\u7406\u805a\u5408\u4ed6\u4eec\u7684\u56fd\u7c4d\u3002\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff1a\u4eba\u7269\u4ee3\u7406\u548c\u5a92\u4f53\u4ee3\u7406\uff0c\u5206\u522b\u4e13\u6ce8\u4e8e\u4e0d\u540c\u77e5\u8bc6\u9886\u57df\uff0c\u5e76\u884c\u56de\u5fc6\u8457\u540d\u4eba\u7269\uff0c\u901a\u8fc7\u6295\u7968\u751f\u6210Top-1\u9884\u6d4b\uff0c\u901a\u8fc7\u6761\u4ef6\u5b8c\u6210\u751f\u6210Top-K\u9884\u6d4b\u3002", "result": "\u572899\u56fd\u56fd\u7c4d\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0cLAMA\u8fbe\u52300.817\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfLLM\u63d0\u793a\u65b9\u6cd5\u548c\u795e\u7ecf\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff1aLLM\u5728\u56de\u5fc6\u5177\u4f53\u4f8b\u5b50\u65b9\u9762\u6bd4\u62bd\u8c61\u63a8\u7406\u66f4\u53ef\u9760\uff1b\u57fa\u4e8e\u56de\u5fc6\u7684\u65b9\u6cd5\u5bf9\u4f4e\u9891\u56fd\u7c4d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e0d\u53d7\u6570\u636e\u9891\u7387\u5206\u5e03\u5f71\u54cd\uff1b\u53cc\u4ee3\u7406\u67b6\u6784\u529f\u80fd\u4e92\u8865\uff0c\u4ea7\u751f\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u65b0\u578b\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u8be5\u7cfb\u7edf\u68c0\u7d22\u548c\u805a\u5408LLM\u77e5\u8bc6\u800c\u975e\u63d0\u793a\u63a8\u7406\u3002LLM\u8054\u60f3\u8bb0\u5fc6\u65b9\u6cd5\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5f53\u9700\u8981\u6587\u5316\u548c\u5386\u53f2\u80cc\u666f\u7406\u89e3\u65f6\u3002"}}
{"id": "2601.13546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13546", "abs": "https://arxiv.org/abs/2601.13546", "authors": ["Hui Sun", "Chang Xu", "Haonan Xie", "Hao Li", "Yuhao Huang", "Chuheng Zhang", "Ming Jin", "Xiaoguang Liu", "Gang Wang", "Jiang Bian"], "title": "ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution", "comment": null, "summary": "LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TSEvol\u591a\u667a\u80fd\u4f53\u65f6\u95f4\u5e8f\u5217\u6f14\u5316\u7b97\u6cd5\u3001TSEData-20K\u6570\u636e\u96c6\u3001ChatAD\u7cfb\u5217\u804a\u5929\u673a\u5668\u4eba\u3001TKTO\u4f18\u5316\u65b9\u6cd5\u4ee5\u53caLLADBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3001\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u6b20\u7f3a\u3001\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u5bf9\u5f02\u5e38\u884c\u4e3a\u7684\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u3002", "method": "1) \u63d0\u51faTSEvol\u591a\u667a\u80fd\u4f53\u65f6\u95f4\u5e8f\u5217\u6f14\u5316\u7b97\u6cd5\uff1b2) \u6784\u5efaTSEData-20K\u6570\u636e\u96c6\u5e76\u5f00\u53d1ChatAD\u7cfb\u5217\u804a\u5929\u673a\u5668\u4eba\uff1b3) \u63d0\u51faTKTO\u4f18\u5316\u65b9\u6cd5\u589e\u5f3a\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff1b4) \u5efa\u7acbLLADBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u3002", "result": "ChatAD\u6a21\u578b\u5728\u51c6\u786e\u7387\u63d0\u534734.50%\u3001F1\u5206\u6570\u63d0\u534734.71%\u3001\u8bef\u62a5\u7387\u964d\u4f4e37.42%\uff1b\u901a\u8fc7TKTO\u4f18\u5316\u540e\uff0c\u5728\u5206\u7c7b\u3001\u9884\u6d4b\u548c\u63d2\u8865\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u63a8\u7406\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12812", "abs": "https://arxiv.org/abs/2601.12812", "authors": ["Sushant Kumar Ray", "Gautam Siddharth Kashyap", "Sahil Tripathi", "Nipun Joshi", "Vijay Govindarajan", "Rafiq Ali", "Jiechao Gao", "Usman Naseem"], "title": "Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?", "comment": "Accepted at EACL 2026 (Industry Track)", "summary": "Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.", "AI": {"tldr": "MEDASSESS-X\u662f\u4e00\u4e2a\u4e34\u5e8a\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u5bf9\u9f50\u800c\u975e\u5fae\u8c03\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u6311\u6218\u4e86\u533b\u7597\u4e13\u7528LLM\u5fc5\u7136\u66f4\u4f18\u7684\"\u4e13\u4e1a\u5316\u8c2c\u8bef\"\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u95ee\u7b54\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u533b\u7597\u4e13\u7528LLM\u7684\u5fae\u8c03\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u8986\u76d6\u8303\u56f4\u7a84\u3001\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u9002\u5e94\u6027\u6709\u9650\u7b49\u5b9e\u9645\u95ee\u9898\uff0c\u4e14\u5f62\u6210\u4e86\"\u4e13\u4e1a\u5316\u8c2c\u8bef\"\u7684\u8bef\u533a\u3002", "method": "\u63d0\u51faMEDASSESS-X\u6846\u67b6\uff0c\u91c7\u7528\u63a8\u7406\u65f6\u5bf9\u9f50\u800c\u975e\u76d1\u7763\u5fae\u8c03\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u5411\u91cf\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u5f15\u5bfc\u6a21\u578b\u6fc0\u6d3b\u8d70\u5411\u533b\u5b66\u4e00\u81f4\u63a8\u7406\u3002", "result": "MEDASSESS-X\u5728\u6240\u6709LLM\u5bb6\u65cf\u4e2d\u5747\u5e26\u6765\u4e00\u81f4\u63d0\u5347\uff1a\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53476%\uff0c\u4e8b\u5b9e\u4e00\u81f4\u6027\u63d0\u53477%\uff0c\u5b89\u5168\u9519\u8bef\u7387\u964d\u4f4e\u8fbe50%\u3002", "conclusion": "MEDASSESS-X\u901a\u8fc7\u63a8\u7406\u65f6\u5bf9\u9f50\u7a33\u5b9a\u4e86\u4e34\u5e8a\u95ee\u7b54\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\"\u4e13\u4e1a\u5316\u8c2c\u8bef\"\uff0c\u8bc1\u660e\u901a\u7528LLM\u901a\u8fc7\u9002\u5f53\u5f15\u5bfc\u4e5f\u80fd\u8fbe\u5230\u533b\u7597\u4e13\u7528LLM\u7684\u6548\u679c\u3002"}}
{"id": "2601.12272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12272", "abs": "https://arxiv.org/abs/2601.12272", "authors": ["Shahrzad Esmat", "Mahdi Banisharif", "Ali Jannesari"], "title": "AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search", "comment": "38 pages, 2 figures, 14 tables", "summary": "Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning's graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.\n  We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.", "AI": {"tldr": "AgenticPruner\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0MAC\u7ea6\u675f\u4f18\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u5b66\u4e60\u526a\u679d\u7b56\u7565\uff0c\u80fd\u591f\u5728\u6ee1\u8db3\u8ba1\u7b97\u9884\u7b97\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u51cf\u5c11\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u63a7\u5236\u8ba1\u7b97\u6210\u672c\uff0c\u5bfc\u81f4\u5728\u9700\u8981\u4e25\u683c\u6ee1\u8db3MAC\u64cd\u4f5c\u9884\u7b97\u7684\u90e8\u7f72\u573a\u666f\u4e2d\u63a8\u7406\u5ef6\u8fdf\u4e0d\u53ef\u9884\u6d4b\u3002", "method": "\u63d0\u51faAgenticPruner\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u7b56\u7565\u5b66\u4e60\u5b9e\u73b0MAC\u7ea6\u675f\u4f18\u5316\u3002\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u5206\u6790\u6a21\u578b\u67b6\u6784\u548cMAC\u5206\u5e03\u7684Profiling Agent\u3001\u534f\u8c03\u5de5\u4f5c\u6d41\u7a0b\u7684Master Agent\u3001\u4ee5\u53ca\u57fa\u4e8eClaude 3.5 Sonnet\u4ece\u5386\u53f2\u5c1d\u8bd5\u4e2d\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u7684Analysis Agent\u3002\u57fa\u4e8e\u540c\u6784\u526a\u679d\u7684\u56fe\u7ed3\u6784\u5206\u7ec4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u526a\u679d\u8fed\u4ee3\u4e2d\u7684\u6a21\u5f0f\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u9002\u5e94\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cCNN\u6a21\u578b\uff1aResNet-50\u8fbe\u52301.77G MACs\u548c77.04%\u51c6\u786e\u7387\uff08\u6bd4\u57fa\u7ebf+0.91%\uff09\uff1bResNet-101\u8fbe\u52304.22G MACs\u548c78.94%\u51c6\u786e\u7387\uff08+1.56%\uff09\u3002ConvNeXt-Small\u526a\u679d\u52308.17G MACs\u5b9e\u73b01.41x GPU\u548c1.07x CPU\u52a0\u901f\uff0c\u53c2\u6570\u51cf\u5c1145%\u3002Vision Transformers\u80fd\u591f\u5728\u7528\u6237\u5b9a\u4e49\u7684\u5bb9\u5dee\u8303\u56f4\u5185\uff08\u901a\u5e38+1%\u5230+5%\u8d85\u8c03\uff0c-5%\u5230-15%\u6b20\u8c03\uff09\u6ee1\u8db3MAC\u9884\u7b97\u3002", "conclusion": "AgenticPruner\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u6536\u655b\u5230\u76ee\u6807MAC\u9884\u7b97\uff0c\u76f8\u6bd4\u7f51\u683c\u641c\u7d22\u5c06\u6536\u655b\u6210\u529f\u7387\u4ece48%\u63d0\u5347\u523071%\uff0c\u4e3a\u9700\u8981\u4e25\u683c\u8ba1\u7b97\u4fdd\u8bc1\u7684\u90e8\u7f72\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13558", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13558", "abs": "https://arxiv.org/abs/2601.13558", "authors": ["Mehrab Beikzadeh", "Chenglin Hong", "Cory J Cascalheira", "Callisto Boka", "Majid Sarrafzadeh", "Ian W Holloway"], "title": "Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis", "comment": null, "summary": "Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.", "AI": {"tldr": "\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u548c\u7ea6\u4f1a\u5e94\u7528\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u7537\u7537\u6027\u884c\u4e3a\u8005\u7684\u6027\u98ce\u9669\u884c\u4e3a\u3001\u9152\u7cbe\u4f7f\u7528\u548cPrEP\u4f7f\u7528\u60c5\u51b5", "motivation": "\u7537\u7537\u6027\u884c\u4e3a\u8005\u9762\u4e34\u6027\u4f20\u64ad\u611f\u67d3\u548c\u6709\u5bb3\u996e\u9152\u7684\u9ad8\u98ce\u9669\uff0c\u793e\u4ea4\u5a92\u4f53\u548c\u7ea6\u4f1a\u5e94\u7528\u6587\u672c\u6570\u636e\u53ef\u80fd\u4e3a\u4e2a\u6027\u5316\u516c\u5171\u536b\u751f\u5e72\u9884\u63d0\u4f9b\u65b0\u673a\u4f1a", "method": "\u6536\u96c6\u53c2\u4e0e\u8005\u6587\u672c\u6570\u636e\uff0c\u4f7f\u7528ChatGPT\u5d4c\u5165\u3001BERT\u5d4c\u5165\u3001LIWC\u548c\u57fa\u4e8e\u8bcd\u5178\u7684\u98ce\u9669\u672f\u8bed\u65b9\u6cd5\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6a21\u578b", "result": "\u6a21\u578b\u5728\u9884\u6d4b\u6bcf\u6708\u9157\u9152\u548c\u8d85\u8fc75\u4e2a\u6027\u4f34\u4fa3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff08F1\u5206\u65700.78\uff09\uff0c\u5728\u9884\u6d4bPrEP\u4f7f\u7528\u548c\u91cd\u5ea6\u996e\u9152\u65b9\u9762\u8868\u73b0\u4e2d\u7b49\uff08F1\u5206\u65700.64\u548c0.63\uff09", "conclusion": "\u793e\u4ea4\u5a92\u4f53\u548c\u7ea6\u4f1a\u5e94\u7528\u6587\u672c\u6570\u636e\u80fd\u63d0\u4f9b\u98ce\u9669\u548c\u4fdd\u62a4\u884c\u4e3a\u7684\u6709\u4ef7\u503c\u6d1e\u5bdf\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u6709\u6f5c\u529b\u652f\u6301\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u516c\u5171\u536b\u751f\u5e72\u9884"}}
{"id": "2601.12815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12815", "abs": "https://arxiv.org/abs/2601.12815", "authors": ["Zhaolu Kang", "Junhao Gong", "Qingxi Chen", "Hao Zhang", "Jiaxin Liu", "Rong Fu", "Zhiyuan Feng", "Yuan Wang", "Simon Fong", "Kaiyue Zhou"], "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction", "comment": null, "summary": "Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.", "AI": {"tldr": "\u63d0\u51faJurisMMA\u6846\u67b6\u7528\u4e8e\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u89e3\u5ba1\u5224\u4efb\u52a1\u3001\u6807\u51c6\u5316\u6d41\u7a0b\u5e76\u7ec4\u7ec7\u4e3a\u4e0d\u540c\u9636\u6bb5\uff0c\u540c\u65f6\u6784\u5efa\u5305\u542b10\u4e07+\u4e2d\u56fd\u53f8\u6cd5\u8bb0\u5f55\u7684JurisMM\u591a\u6a21\u6001\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7edf\u8ba1\u5206\u6790\u6216\u89d2\u8272\u6a21\u62df\uff0c\u9762\u4e34\u591a\u91cd\u6307\u63a7\u3001\u591a\u6837\u8bc1\u636e\u548c\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6846\u67b6\u6765\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u6848\u4ef6\u3002", "method": "\u63d0\u51faJurisMMA\u6846\u67b6\uff0c\u5c06\u5ba1\u5224\u4efb\u52a1\u6709\u6548\u5206\u89e3\u3001\u6807\u51c6\u5316\u6d41\u7a0b\u5e76\u7ec4\u7ec7\u4e3a\u4e0d\u540c\u9636\u6bb5\uff1b\u6784\u5efaJurisMM\u5927\u578b\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07+\u8fd1\u671f\u4e2d\u56fd\u53f8\u6cd5\u8bb0\u5f55\uff0c\u6db5\u76d6\u6587\u672c\u548c\u591a\u6a21\u6001\u89c6\u9891-\u6587\u672c\u6570\u636e\u3002", "result": "\u5728JurisMM\u6570\u636e\u96c6\u548c\u57fa\u51c6LawBench\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\uff0c\u8fd8\u80fd\u4e3a\u66f4\u5e7f\u6cdb\u7684\u6cd5\u5f8b\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u6cd5\u5f8b\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u6848\u4ef6\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2601.12282", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12282", "abs": "https://arxiv.org/abs/2601.12282", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training", "comment": null, "summary": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.", "AI": {"tldr": "CytoCLIP\uff1a\u57fa\u4e8eCLIP\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u5927\u8111\u7ec6\u80de\u6784\u7b51\u533a\u57df\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u5168\u533a\u57df\u56fe\u50cf\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5757\u4e0a\u5206\u522b\u8bad\u7ec3\uff0c\u5728\u80ce\u513f\u5927\u8111\u7ec4\u7ec7\u5b66\u5207\u7247\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u8111\u4e0d\u540c\u533a\u57df\u7684\u529f\u80fd\u4e0e\u5176\u72ec\u7279\u7684\u7ec6\u80de\u6784\u7b51\u5bc6\u5207\u76f8\u5173\uff0c\u4f46\u624b\u52a8\u5728\u7ec4\u7ec7\u5b66\u5207\u7247\u4e0a\u5212\u5206\u8fd9\u4e9b\u533a\u57df\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u3002", "method": "\u63d0\u51faCytoCLIP\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684CLIP\u6846\u67b6\u5f00\u53d1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b66\u4e60\u5927\u8111\u7ec6\u80de\u6784\u7b51\u7684\u8054\u5408\u89c6\u89c9-\u6587\u672c\u8868\u793a\u3002\u5305\u542b\u4e24\u4e2a\u53d8\u4f53\uff1a\u4e00\u4e2a\u7528\u4f4e\u5206\u8fa8\u7387\u5168\u533a\u57df\u56fe\u50cf\u8bad\u7ec3\u4ee5\u7406\u89e3\u6574\u4f53\u6a21\u5f0f\uff0c\u53e6\u4e00\u4e2a\u7528\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5757\u8bad\u7ec3\u4ee5\u83b7\u53d6\u7ec6\u80de\u7ea7\u7ec6\u8282\u3002\u4f7f\u7528\u4e0d\u540c\u5b55\u5468\u80ce\u513f\u5927\u8111\u7684NISSL\u67d3\u8272\u7ec4\u7ec7\u5b66\u5207\u7247\u6784\u5efa\u6570\u636e\u96c6\u3002", "result": "\u5728\u533a\u57df\u5206\u7c7b\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6a21\u578b\u5bf9\u7ec6\u80de\u6784\u7b51\u7684\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728\u4e0d\u540c\u5e74\u9f84\u6837\u672c\u548c\u5207\u7247\u5e73\u9762\u7684\u591a\u79cd\u6570\u636e\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0cCytoCLIP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5168\u533a\u57df\u5206\u7c7bF1\u5206\u65700.87\uff0c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5757\u5206\u7c7bF1\u5206\u65700.91\u3002", "conclusion": "CytoCLIP\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5927\u8111\u7ec6\u80de\u6784\u7b51\u7684\u89c6\u89c9-\u6587\u672c\u8868\u793a\uff0c\u4e3a\u81ea\u52a8\u5316\u5927\u8111\u533a\u57df\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u4e13\u5bb6\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387\u3002"}}
{"id": "2601.13559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13559", "abs": "https://arxiv.org/abs/2601.13559", "authors": ["Sun Hui", "Ding Yanfeng", "Huidong Ma", "Chang Xu", "Keyan Jin", "Lizheng Zu", "Cheng Zhong", "xiaoguang Liu", "Gang Wang", "Wentong Cai"], "title": "AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent", "comment": null, "summary": "Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.", "AI": {"tldr": "AgentGC\uff1a\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8fdb\u5316\u5f0f\u57fa\u56e0\u7ec4\u6570\u636e\u65e0\u635f\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08Leader\u548cWorker\uff09\u5b9e\u73b0\u7528\u6237\u53cb\u597d\u754c\u9762\u3001\u8ba4\u77e5\u4f18\u5316\u548c\u9ad8\u6548\u538b\u7f29\uff0c\u5728\u538b\u7f29\u6bd4\u548c\u541e\u5410\u91cf\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u56e0\u7ec4\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4e0d\u53ef\u8fdb\u5316\u3001\u4f4e\u5c42\u6b21\u5efa\u6a21\u3001\u9002\u5e94\u6027\u6709\u9650\u548c\u7528\u6237\u754c\u9762\u4e0d\u53cb\u597d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u81ea\u9002\u5e94\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faAgentGC\u4e09\u5c42\u67b6\u6784\uff1a1\uff09\u7528\u6237\u5c42\u901a\u8fc7Leader\u7ed3\u5408LLM\u63d0\u4f9b\u53cb\u597d\u754c\u9762\uff1b2\uff09\u8ba4\u77e5\u5c42\u7531Leader\u9a71\u52a8\uff0c\u6574\u5408LLM\u8fdb\u884c\u7b97\u6cd5-\u6570\u636e\u96c6-\u7cfb\u7edf\u8054\u5408\u4f18\u5316\uff1b3\uff09\u538b\u7f29\u5c42\u7531Worker\u8d1f\u8d23\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u591a\u77e5\u8bc6\u5b66\u4e60\u6846\u67b6\u6267\u884c\u538b\u7f29\u89e3\u538b\u3002\u652f\u6301\u4e09\u79cd\u6a21\u5f0f\uff1aCP\uff08\u538b\u7f29\u6bd4\u4f18\u5148\uff09\u3001TP\uff08\u541e\u5410\u91cf\u4f18\u5148\uff09\u3001BM\uff08\u5e73\u8861\u6a21\u5f0f\uff09\u3002", "result": "\u57289\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e14\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4e09\u79cd\u6a21\u5f0f\u7684\u5e73\u5747\u538b\u7f29\u6bd4\u63d0\u5347\u5206\u522b\u4e3a16.66%\u300116.11%\u548c16.33%\uff0c\u541e\u5410\u91cf\u63d0\u5347\u5206\u522b\u4e3a4.73\u500d\u30019.23\u500d\u548c9.15\u500d\u3002", "conclusion": "AgentGC\u4f5c\u4e3a\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8fdb\u5316\u5f0f\u57fa\u56e0\u7ec4\u6570\u636e\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548cLLM\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u538b\u7f29\u6027\u80fd\u548c\u541e\u5410\u91cf\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.12844", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12844", "abs": "https://arxiv.org/abs/2601.12844", "authors": ["Julie Ran\u00e7on", "Jean-Fran\u00e7ois Cerisier", "Emilie Remond", "Aur\u00e9lien Nguyen", "Andrew Peterson", "Ladjel Bellatreche"], "title": "Rapport du Projet de Recherche TRAIMA", "comment": "in French language", "summary": "The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{\u00e9}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{\u00e9}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{\u00e9}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.", "AI": {"tldr": "TRAIMA\u9879\u76ee\u63a2\u7d22\u5229\u7528\u673a\u5668\u5b66\u4e60\u81ea\u52a8\u5904\u7406\u6559\u80b2\u573a\u666f\u4e2d\u591a\u6a21\u6001\u4e92\u52a8\uff08\u8bed\u8a00\u3001\u526f\u8bed\u8a00\u3001\u975e\u8bed\u8a00\uff09\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5f53\u524d\u4eba\u5de5\u5206\u6790\u8017\u65f6\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u6559\u5b66\u4e92\u52a8\u7684\u81ea\u52a8\u5316\u5206\u6790\u5efa\u7acb\u65b9\u6cd5\u8bba\u6846\u67b6\u3002", "motivation": "\u6559\u80b2\u4e92\u52a8\u7814\u7a76\u4e2d\uff0c\u5bf9\u8bed\u8a00\u3001\u526f\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u6570\u636e\u7684\u5206\u6790\u76ee\u524d\u5b8c\u5168\u4f9d\u8d56\u4eba\u5de5\u5904\u7406\uff0c\u6781\u5176\u8017\u65f6\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u3002\u9879\u76ee\u65e8\u5728\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u5982\u4f55\u5e2e\u52a9\u5206\u7c7b\u548c\u8bc6\u522b\u8fd9\u7c7b\u591a\u6a21\u6001\u4e92\u52a8\uff0c\u7279\u522b\u662f\u5728\u6cd5\u8bed\u4f5c\u4e3a\u5916\u8bed\u548c\u6bcd\u8bed\u6559\u5b66\u573a\u666f\u4e2d\u7684\u89e3\u91ca\u6027\u548c\u534f\u4f5c\u6027\u5e8f\u5217\u3002", "method": "1) \u7406\u8bba\u6846\u67b6\uff1a\u57fa\u4e8e\u8bdd\u8bed\u5206\u6790\u548c\u4e92\u52a8\u8bed\u8a00\u5b66\uff0c\u5c06\u89e3\u91ca\u6027\u8bdd\u8bed\u7cbe\u786e\u5b9a\u4e49\u4e3a\u4e09\u90e8\u5206\u5e8f\u5217\uff08\u5f00\u573a\u3001\u89e3\u91ca\u6838\u5fc3\u3001\u7ed3\u675f\uff09\uff1b2) \u65b9\u6cd5\u8bba\u57fa\u7840\uff1a\u6df1\u5165\u7814\u7a76\u8f6c\u5f55\u89c4\u8303\uff0c\u6bd4\u8f83\u73b0\u6709\u8f6c\u5f55\u4f53\u7cfb\uff08ICOR\u3001Mondada\u7b49\uff09\u7684\u4f18\u7f3a\u70b9\uff1b3) \u5b9e\u8bc1\u5206\u6790\uff1a\u57fa\u4e8eINTER-EXPLIC\u548cEXPLIC-LEXIC\u8bed\u6599\u5e93\uff08\u7ea630\u5c0f\u65f6\u8bfe\u5802\u4e92\u52a8\uff09\uff0c\u5206\u6790\u6559\u5e08\u624b\u52bf\u3001\u97f5\u5f8b\u7279\u5f81\u7b49\u529f\u80fd\u89d2\u8272\uff1b4) \u6280\u672f\u5e73\u53f0\uff1a\u5229\u7528Techn\u00e9LAB\u5e73\u53f0\u8fdb\u884c\u591a\u6a21\u6001\u6570\u636e\u91c7\u96c6\uff08\u591a\u6444\u50cf\u5934\u89c6\u9891\u3001\u540c\u6b65\u97f3\u9891\u3001\u773c\u52a8\u8ffd\u8e2a\u7b49\uff09\u3002", "result": "\u9879\u76ee\u5c55\u793a\u4e86\u8f6c\u5f55\u5b9e\u8df5\u4e0d\u53ef\u907f\u514d\u7684\u53d8\u5f02\u6027\u548c\u89e3\u91ca\u6027\u7ef4\u5ea6\uff0c\u53d6\u51b3\u4e8e\u7406\u8bba\u7acb\u573a\u548c\u5206\u6790\u76ee\u6807\u3002\u5efa\u7acb\u4e86\u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u517c\u5bb9\u7684\u8f6c\u5f55\u89c4\u8303\u3001\u6807\u6ce8\u7c7b\u522b\u548c\u5206\u6790\u5355\u5143\uff0c\u5f3a\u8c03\u4e86\u7406\u8bba\u660e\u786e\u6027\u548c\u7814\u7a76\u8005\u53cd\u601d\u6027\u7684\u5fc5\u8981\u6027\u3002\u9879\u76ee\u672a\u5f00\u53d1\u51fa\u5b8c\u5168\u53ef\u64cd\u4f5c\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u4f46\u4e3a\u591a\u6a21\u6001\u6559\u5b66\u4e92\u52a8\u7684\u81ea\u52a8\u5904\u7406\u5efa\u7acb\u4e86\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u3002", "conclusion": "TRAIMA\u4e3a\u672a\u6765\u8de8\u5b66\u79d1\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fde\u63a5\u4e86\u6559\u5b66\u6cd5\u3001\u8bdd\u8bed\u5206\u6790\u3001\u591a\u6a21\u6001\u7814\u7a76\u548c\u6559\u80b2\u4eba\u5de5\u667a\u80fd\u3002\u9879\u76ee\u5f3a\u8c03\u81ea\u52a8\u5316\u5904\u7406\u9700\u8981\u7406\u8bba\u660e\u786e\u6027\u548c\u65b9\u6cd5\u4e25\u8c28\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6559\u80b2\u4e92\u52a8\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.12283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12283", "abs": "https://arxiv.org/abs/2601.12283", "authors": ["Bowen Lin", "Fanjiang Ye", "Yihua Liu", "Zhenghui Guo", "Boyuan Zhang", "Weijian Zheng", "Yufan Xu", "Tiancheng Xing", "Yuke Wang", "Chengming Zhang"], "title": "SDiT: Semantic Region-Adaptive for Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art performance in text-to-image synthesis but remain computationally expensive due to the iterative nature of denoising and the quadratic cost of global attention. In this work, we observe that denoising dynamics are spatially non-uniform-background regions converge rapidly while edges and textured areas evolve much more actively. Building on this insight, we propose SDiT, a Semantic Region-Adaptive Diffusion Transformer that allocates computation according to regional complexity. SDiT introduces a training-free framework combining (1) semantic-aware clustering via fast Quickshift-based segmentation, (2) complexity-driven regional scheduling to selectively update informative areas, and (3) boundary-aware refinement to maintain spatial coherence. Without any model retraining or architectural modification, SDiT achieves up to 3.0x acceleration while preserving nearly identical perceptual and semantic quality to full-attention inference.", "AI": {"tldr": "SDiT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u533a\u57df\u81ea\u9002\u5e94\u6269\u6563Transformer\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u805a\u7c7b\u3001\u590d\u6742\u5ea6\u9a71\u52a8\u7684\u533a\u57df\u8c03\u5ea6\u548c\u8fb9\u754c\u611f\u77e5\u7ec6\u5316\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe3\u500d\u52a0\u901f\u3002", "motivation": "\u6269\u6563Transformer\uff08DiTs\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u53bb\u566a\u8fc7\u7a0b\u7684\u8fed\u4ee3\u6027\u548c\u5168\u5c40\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\uff0c\u8ba1\u7b97\u5f00\u9500\u5f88\u5927\u3002\u7814\u7a76\u53d1\u73b0\u53bb\u566a\u52a8\u6001\u5728\u7a7a\u95f4\u4e0a\u662f\u4e0d\u5747\u5300\u7684\u2014\u2014\u80cc\u666f\u533a\u57df\u6536\u655b\u8fc5\u901f\uff0c\u800c\u8fb9\u7f18\u548c\u7eb9\u7406\u533a\u57df\u6f14\u5316\u66f4\u6d3b\u8dc3\u3002", "method": "SDiT\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u5feb\u901fQuickshift\u7684\u8bed\u4e49\u611f\u77e5\u805a\u7c7b\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff1b2\uff09\u590d\u6742\u5ea6\u9a71\u52a8\u7684\u533a\u57df\u8c03\u5ea6\uff0c\u9009\u62e9\u6027\u5730\u66f4\u65b0\u4fe1\u606f\u4e30\u5bcc\u7684\u533a\u57df\uff1b3\uff09\u8fb9\u754c\u611f\u77e5\u7ec6\u5316\u4ee5\u4fdd\u6301\u7a7a\u95f4\u8fde\u8d2f\u6027\u3002", "result": "SDiT\u5728\u4e0d\u8fdb\u884c\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.0\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5b8c\u6574\u6ce8\u610f\u529b\u63a8\u7406\u51e0\u4e4e\u76f8\u540c\u7684\u611f\u77e5\u548c\u8bed\u4e49\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u53bb\u566a\u52a8\u6001\u7684\u7a7a\u95f4\u975e\u5747\u5300\u6027\uff0cSDiT\u4e3a\u6269\u6563Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u727a\u7272\u56fe\u50cf\u8d28\u91cf\uff0c\u4e3a\u5b9e\u65f6\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13562", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13562", "abs": "https://arxiv.org/abs/2601.13562", "authors": ["Zhiguang Liu", "Yi Shang"], "title": "Reasoning is a Modality", "comment": "Code access: https://github.com/lz7fd/Reasoning_is_a_Modality", "summary": "The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.", "AI": {"tldr": "\u63d0\u51fa\u5206\u79bb\u63a8\u7406\u901a\u9053\u7684\u5047\u8bbe\uff0c\u8bbe\u8ba1\u89d2\u8272\u5206\u79bbTransformer\u5757\uff0c\u5728ARC\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u8868\u73b0", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\uff08\u5982LLMs\u548cViTs\uff09\u4e3b\u8981\u4f5c\u4e3a\u884c\u4e3a\u5e8f\u5217\u9884\u6d4b\u673a\u5668\u8fd0\u884c\uff0c\u901a\u8fc7\u5efa\u6a21token\u7edf\u8ba1\u6765\u5339\u914d\u53ef\u89c2\u5bdf\u884c\u4e3a\uff0c\u7f3a\u4e4f\u6301\u4e45\u53ef\u8bfb\u7684\u601d\u7ef4\u72b6\u6001\u3002\u8fd9\u4e0e\u4eba\u7c7b\u884c\u4e3a\u5b58\u5728\u5dee\u8ddd\uff1a\u4eba\u7c7b\u53ef\u4ee5\u901a\u8fc7\u89e3\u7801\u5185\u90e8\u72b6\u6001\u6765\u89e3\u91ca\u884c\u4e3a\uff0c\u800cAI\u7cfb\u7edf\u53ea\u80fd\u4ea7\u751f\u672a\u57fa\u4e8e\u6b64\u7c7b\u72b6\u6001\u7684\u6d41\u7545\u4e8b\u540e\u5408\u7406\u5316\u89e3\u91ca\u3002\u5047\u8bbe\u63a8\u7406\u5e94\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u4e8e\u4f4e\u7ea7\u5de5\u4f5c\u7a7a\u95f4\u7684\u901a\u9053\u5b58\u5728\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89d2\u8272\u5206\u79bbTransformer\u5757\uff0c\u5c06\u5168\u5c40\u63a7\u5236\u5668token\u4e0e\u7f51\u683c\u5de5\u4f5c\u7a7a\u95f4token\u5206\u79bb\uff0c\u5b9e\u73b0\u8fed\u4ee3\u89c4\u5219\u6267\u884c\u3002\u5728VARC\u89c6\u89c9\u4e2d\u5fc3\u534f\u8bae\u4e0b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5c06\u63a8\u7406\u4f5c\u4e3a\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u6765\u89e3\u51b3ARC\u4efb\u52a1\u3002", "result": "\u5728ARC-1\u4e0a\u8fbe\u523062.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0860.2%\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u4e0e\u5bc6\u96c6ViT\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u4e00\u81f4\u7684\u89c4\u5219\u5e94\u7528\u7ed3\u6784\uff0c\u4ece\u6982\u7387\u6591\u70b9\u5411\u63a7\u5236\u5668\u9a71\u52a8\u63a8\u7406\u8f6c\u53d8\u3002", "conclusion": "\u652f\u6301\u63a8\u7406\u4f5c\u4e3a\u72ec\u7acb\u901a\u9053\u7684\u5047\u8bbe\uff0c\u89d2\u8272\u5206\u79bbTransformer\u8bbe\u8ba1\u80fd\u591f\u5b9e\u73b0\u66f4\u7c7b\u4f3c\u4eba\u7c7b\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u4e3aAI\u7cfb\u7edf\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5185\u90e8\u72b6\u6001\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.12868", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12868", "abs": "https://arxiv.org/abs/2601.12868", "authors": ["Shiyue Hu", "Ruizhe Li", "Yanjun Gao"], "title": "Race, Ethnicity and Their Implication on Bias in Large Language Models", "comment": "Work in process", "summary": "Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790LLMs\u4e2d\u79cd\u65cf\u548c\u65cf\u88d4\u4fe1\u606f\u7684\u5185\u90e8\u8868\u793a\u673a\u5236\uff0c\u53d1\u73b0\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5206\u5e03\u5728\u4e0d\u540c\u5355\u5143\u4e2d\uff0c\u5e72\u9884\u53ef\u51cf\u5c11\u504f\u89c1\u4f46\u4ecd\u6709\u6b8b\u7559\u6548\u5e94\u3002", "motivation": "LLMs\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u65f6\uff0c\u79cd\u65cf\u548c\u65cf\u88d4\u4fe1\u606f\u53ef\u80fd\u88ab\u663e\u5f0f\u6216\u9690\u5f0f\u5904\u7406\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u8bb0\u5f55\u7ed3\u679c\u5c42\u9762\u7684\u5dee\u5f02\uff0c\u7f3a\u4e4f\u5bf9\u5185\u90e8\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08\u6bd2\u6027\u751f\u6210\u548c\u4e34\u5e8a\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\uff09\uff0c\u5206\u6790\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u590d\u73b0\u7684\u53ef\u89e3\u91ca\u6027\u7ba1\u9053\uff0c\u7ed3\u5408\u63a2\u6d4b\u3001\u795e\u7ecf\u5143\u7ea7\u5f52\u56e0\u548c\u9488\u5bf9\u6027\u5e72\u9884\u3002", "result": "\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u5206\u5e03\u5728\u4e0d\u540c\u5185\u90e8\u5355\u5143\u4e2d\uff0c\u5b58\u5728\u663e\u8457\u7684\u8de8\u6a21\u578b\u5dee\u5f02\u3002\u4e00\u4e9b\u5355\u5143\u7f16\u7801\u4e86\u9884\u8bad\u7ec3\u4e2d\u7684\u654f\u611f\u6216\u523b\u677f\u5370\u8c61\u5173\u8054\uff0c\u76f8\u540c\u7684\u4eba\u53e3\u7edf\u8ba1\u7ebf\u7d22\u53ef\u80fd\u5f15\u53d1\u4e0d\u540c\u884c\u4e3a\u3002\u5e72\u9884\u6291\u5236\u76f8\u5173\u795e\u7ecf\u5143\u53ef\u51cf\u5c11\u504f\u89c1\u4f46\u4ecd\u6709\u6b8b\u7559\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u4e2d\u79cd\u65cf\u548c\u65cf\u88d4\u4fe1\u606f\u7684\u590d\u6742\u8868\u793a\u673a\u5236\uff0c\u5e72\u9884\u4e3b\u8981\u6539\u53d8\u884c\u4e3a\u800c\u975e\u8868\u793a\u672c\u8eab\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u7f13\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2601.12285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12285", "abs": "https://arxiv.org/abs/2601.12285", "authors": ["Safa C. Medin", "Gengyan Li", "Ziqian Bai", "Ruofei Du", "Leonhard Helminger", "Yinda Zhang", "Stephan J. Garbin", "Philip L. Davidson", "Gregory W. Wornell", "Thabo Beeler", "Abhimitra Meka"], "title": "LegacyAvatars: Volumetric Face Avatars For Traditional Graphics Pipelines", "comment": null, "summary": "We introduce a novel representation for efficient classical rendering of photorealistic 3D face avatars. Leveraging recent advances in radiance fields anchored to parametric face models, our approach achieves controllable volumetric rendering of complex facial features, including hair, skin, and eyes. At enrollment time, we learn a set of radiance manifolds in 3D space to extract an explicit layered mesh, along with appearance and warp textures. During deployment, this allows us to control and animate the face through simple linear blending and alpha compositing of textures over a static mesh. This explicit representation also enables the generated avatar to be efficiently streamed online and then rendered using classical mesh and shader-based rendering on legacy graphics platforms, eliminating the need for any custom engineering or integration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578b\u951a\u5b9a\u8f90\u5c04\u573a\u7684\u65b0\u578b3D\u4eba\u8138\u5316\u8eab\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u7ecf\u5178\u6e32\u67d3", "motivation": "\u4f20\u7edf3D\u4eba\u8138\u5316\u8eab\u6e32\u67d3\u9700\u8981\u590d\u6742\u7684\u81ea\u5b9a\u4e49\u5de5\u7a0b\u548c\u96c6\u6210\uff0c\u65e0\u6cd5\u5728\u4f20\u7edf\u56fe\u5f62\u5e73\u53f0\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u53c8\u80fd\u5b9e\u73b0\u9ad8\u6548\u7ecf\u5178\u6e32\u67d3\u7684\u8868\u793a\u65b9\u6cd5", "method": "\u5229\u7528\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578b\u951a\u5b9a\u7684\u8f90\u5c04\u573a\uff0c\u5b66\u4e603D\u7a7a\u95f4\u4e2d\u7684\u8f90\u5c04\u6d41\u5f62\uff0c\u63d0\u53d6\u663e\u5f0f\u5206\u5c42\u7f51\u683c\u4ee5\u53ca\u5916\u89c2\u548c\u53d8\u5f62\u7eb9\u7406\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u7ebf\u6027\u6df7\u5408\u548calpha\u5408\u6210\u5728\u9759\u6001\u7f51\u683c\u4e0a\u8fdb\u884c\u7eb9\u7406\u63a7\u5236", "result": "\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u4f53\u79ef\u6e32\u67d3\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u9762\u90e8\u7279\u5f81\uff08\u5934\u53d1\u3001\u76ae\u80a4\u3001\u773c\u775b\uff09\uff0c\u751f\u6210\u7684\u5316\u8eab\u53ef\u4ee5\u5728\u7ebf\u9ad8\u6548\u6d41\u5f0f\u4f20\u8f93\uff0c\u5e76\u5728\u4f20\u7edf\u56fe\u5f62\u5e73\u53f0\u4e0a\u4f7f\u7528\u7ecf\u5178\u7f51\u683c\u548c\u7740\u8272\u5668\u6e32\u67d3\uff0c\u65e0\u9700\u81ea\u5b9a\u4e49\u5de5\u7a0b", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u5236\u4e14\u6613\u4e8e\u90e8\u7f72\u76843D\u4eba\u8138\u5316\u8eab\u8868\u793a\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u8f90\u5c04\u573a\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u548c\u4f20\u7edf\u7f51\u683c\u6e32\u67d3\u7684\u6548\u7387\u4f18\u52bf"}}
{"id": "2601.13581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13581", "abs": "https://arxiv.org/abs/2601.13581", "authors": ["Heedou Kim", "Changsik Kim", "Sanghwa Shin", "Jaewoo Kang"], "title": "SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System", "comment": "This paper has been accepted to the EACL 2026 Industry Track", "summary": "Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.", "AI": {"tldr": "ScriptMind\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc8\u9a97\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u72af\u7f6a\u811a\u672c\u63a8\u7406\u3001\u6570\u636e\u96c6\u6784\u5efa\u548c\u8ba4\u77e5\u6a21\u62df\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u8bc8\u9a97\u68c0\u6d4b\u6027\u80fd\u5e76\u589e\u5f3a\u7528\u6237\u8ba4\u77e5\u8b66\u89c9\u3002", "motivation": "\u4f20\u7edf\u8bc8\u9a97\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u4e2a\u6027\u5316\u3001\u591a\u8f6e\u5bf9\u8bdd\u7684\u793e\u4f1a\u5de5\u7a0b\u8bc8\u9a97\uff0c\u800c\u73b0\u6709LLM\u5728\u8bc8\u9a97\u68c0\u6d4b\u4e2d\u7684\u8ba4\u77e5\u8f85\u52a9\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "\u63d0\u51faScriptMind\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u72af\u7f6a\u811a\u672c\u63a8\u7406\u4efb\u52a1(CSIT)\u7528\u4e8e\u8bc8\u9a97\u63a8\u7406\uff0c\u72af\u7f6a\u811a\u672c\u611f\u77e5\u63a8\u7406\u6570\u636e\u96c6(CSID)\u7528\u4e8e\u5fae\u8c03\u5c0f\u578bLLM\uff0c\u8ba4\u77e5\u6a21\u62df\u8bc4\u4f30(CSED)\u7528\u4e8e\u8bc4\u4f30\u5b9e\u65f6\u8ba4\u77e5\u5f71\u54cd\u3002\u4f7f\u7528571\u4e2a\u97e9\u56fd\u7535\u8bdd\u8bc8\u9a97\u6848\u4f8b\u6784\u5efa22,712\u4e2a\u7ed3\u6784\u5316\u8bad\u7ec3\u5b9e\u4f8b\u3002", "result": "\u7ecf\u8fc7ScriptMind\u5fae\u8c03\u768411B\u5c0f\u578bLLM\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u6bd4GPT-4o\u9ad8\u51fa13%\uff0c\u5728\u8bef\u62a5\u51cf\u5c11\u3001\u8bc8\u9a97\u8005\u8bdd\u8bed\u9884\u6d4b\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u3002\u5728\u7535\u8bdd\u8bc8\u9a97\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u5e76\u7ef4\u6301\u4e86\u7528\u6237\u7684\u6000\u7591\u6c34\u5e73\uff0c\u589e\u5f3a\u4e86\u8bc8\u9a97\u8ba4\u77e5\u610f\u8bc6\u3002", "conclusion": "ScriptMind\u4ee3\u8868\u4e86\u5411\u4ee5\u4eba\u4e3a\u672c\u3001\u8ba4\u77e5\u81ea\u9002\u5e94\u7684LLM\u8bc8\u9a97\u9632\u5fa1\u7cfb\u7edf\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5c55\u793a\u4e86LLM\u5728\u589e\u5f3a\u4eba\u7c7b\u8ba4\u77e5\u8b66\u89c9\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12904", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12904", "abs": "https://arxiv.org/abs/2601.12904", "authors": ["Jiahao Wang", "Weiyu Xie", "Mingxing Zhang", "Boxing Zhang", "Jianwei Dong", "Yuening Zhu", "Chen Lin", "Jinqi Tang", "Yaochen Han", "Zhiyuan Ai", "Xianglin Chen", "Yongwei Wu", "Congfeng Jiang"], "title": "From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.", "AI": {"tldr": "FusionRAG\uff1a\u4e00\u79cd\u65b0\u7684RAG\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u5904\u7406\u9636\u6bb5\u5d4c\u5165\u8de8\u5757\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u91cd\u5904\u7406\u9636\u6bb5\u9009\u62e9\u6027\u91cd\u8ba1\u7b97\u5173\u952etoken\u7684KV\u7f13\u5b58\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u91cd\u7528\u68c0\u7d22\u5757\u7684\u9884\u5904\u7406KV\u7f13\u5b58\u6765\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u7f3a\u4e4f\u8de8\u5757\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u663e\u8457\u4e0b\u964d\u3002\u5982\u4f55\u5728\u91cd\u7528\u9884\u8ba1\u7b97KV\u7f13\u5b58\u7684\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51faFusionRAG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u79bb\u7ebf\u9884\u5904\u7406\u9636\u6bb5\uff1a\u5c06\u76f8\u5173\u6587\u672c\u5757\u7684\u4fe1\u606f\u5d4c\u5165\u5230\u6bcf\u4e2a\u5757\u4e2d\uff1b2\uff09\u5728\u7ebf\u91cd\u5904\u7406\u9636\u6bb5\uff1a\u9009\u62e9\u6027\u91cd\u8ba1\u7b97\u6a21\u578b\u5173\u6ce8\u7684token\u7684KV\u7f13\u5b58\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFusionRAG\u5728\u76f8\u540c\u91cd\u8ba1\u7b97\u6bd4\u4f8b\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002\u91cd\u8ba1\u7b97\u5c11\u4e8e15%\u7684token\u5373\u53ef\u5b9e\u73b0\u6bd4\u57fa\u7ebf\u9ad870%\u7684\u5f52\u4e00\u5316F1\u5206\u6570\uff0c\u76f8\u6bd4Full Attention\u51cf\u5c112.66-9.39\u500d\u7684TTFT\u3002", "conclusion": "FusionRAG\u901a\u8fc7\u4f18\u5316\u9884\u5904\u7406\u548c\u91cd\u5904\u7406\u9636\u6bb5\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u4e2dKV\u7f13\u5b58\u91cd\u7528\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2601.12303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12303", "abs": "https://arxiv.org/abs/2601.12303", "authors": ["Shizhan Gong", "Xiaofan Zhang", "Qi Dou"], "title": "Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations", "comment": "AAAI 2026", "summary": "Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP's visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.", "AI": {"tldr": "PCBM-ReD\uff1a\u901a\u8fc7\u8868\u793a\u5206\u89e3\u5b9e\u73b0\u7684\u4e8b\u540e\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u6ce8\u5165\u9884\u8bad\u7ec3\u9ed1\u76d2\u6a21\u578b\uff0c\u81ea\u52a8\u63d0\u53d6\u89c6\u89c9\u6982\u5ff5\u5e76\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u3002\u73b0\u6709\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u6982\u5ff5\u76f8\u5173\u6027\u4e0d\u53ef\u9760\u3001\u6982\u5ff5\u5b9a\u4e49\u975e\u89c6\u89c9\u5316\u6216\u52b3\u52a8\u5bc6\u96c6\u3001\u6a21\u578b\u6216\u6570\u636e\u65e0\u5173\u5047\u8bbe\u7b49\u5c40\u9650\u6027\u3002", "method": "PCBM-ReD\u901a\u8fc7\u8868\u793a\u5206\u89e3\u5c06\u53ef\u89e3\u91ca\u6027\u6ce8\u5165\u9884\u8bad\u7ec3\u6a21\u578b\uff1a1)\u4ece\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u81ea\u52a8\u63d0\u53d6\u89c6\u89c9\u6982\u5ff5\uff1b2)\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u89c6\u89c9\u53ef\u8bc6\u522b\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u6807\u6ce8\u548c\u7b5b\u9009\u6982\u5ff5\uff1b3)\u901a\u8fc7\u91cd\u5efa\u5f15\u5bfc\u4f18\u5316\u9009\u62e9\u72ec\u7acb\u6982\u5ff5\u5b50\u96c6\uff1b4)\u5229\u7528CLIP\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u5c06\u56fe\u50cf\u8868\u793a\u5206\u89e3\u4e3a\u6982\u5ff5\u5d4c\u5165\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u9002\u914d\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u6846\u67b6\u3002", "result": "\u572811\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPCBM-ReD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u7f29\u5c0f\u4e86\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "PCBM-ReD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u4e3a\u9884\u8bad\u7ec3\u9ed1\u76d2\u6a21\u578b\u6ce8\u5165\u6982\u5ff5\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.13589", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.13589", "abs": "https://arxiv.org/abs/2601.13589", "authors": ["HyeYoung Lee"], "title": "Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification", "comment": null, "summary": "This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53AI\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u97f3\u9891\u60c5\u611f\u4fe1\u53f7\u5b9e\u65f6\u751f\u6210\u54cd\u5e94\u5f0f\u5a92\u4f53\u5185\u5bb9\uff0c\u5f3a\u8c03\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u800c\u975e\u5355\u7eaf\u5206\u7c7b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5c06\u60c5\u611f\u72b6\u6001\u8f6c\u5316\u4e3a\u5b89\u5168\u3001\u9002\u9f84\u3001\u53ef\u63a7\u7684\u54cd\u5e94\u5185\u5bb9\u7684\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u5b89\u5168\u54cd\u5e94\u5185\u5bb9\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf", "method": "\u91c7\u7528\u56db\u667a\u80fd\u4f53\u534f\u4f5c\u67b6\u6784\uff1a1)\u57fa\u4e8eCNN\u7684\u60c5\u611f\u8bc6\u522b\u667a\u80fd\u4f53\uff1b2)\u54cd\u5e94\u7b56\u7565\u51b3\u7b56\u667a\u80fd\u4f53\uff1b3)\u5185\u5bb9\u53c2\u6570\u751f\u6210\u667a\u80fd\u4f53\uff1b4)\u5b89\u5168\u9a8c\u8bc1\u667a\u80fd\u4f53\u3002\u5f15\u5165\u663e\u5f0f\u5b89\u5168\u9a8c\u8bc1\u5faa\u73af\uff0c\u5728\u8f93\u51fa\u524d\u8fc7\u6ee4\u751f\u6210\u5185\u5bb9", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b073.2%\u60c5\u611f\u8bc6\u522b\u51c6\u786e\u7387\u300189.4%\u54cd\u5e94\u6a21\u5f0f\u4e00\u81f4\u6027\u3001100%\u5b89\u5168\u5408\u89c4\u6027\uff0c\u63a8\u7406\u5ef6\u8fdf\u4f4e\u4e8e100ms\uff0c\u9002\u5408\u8bbe\u5907\u7aef\u90e8\u7f72", "conclusion": "\u8be5\u6a21\u5757\u5316\u67b6\u6784\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u513f\u7ae5\u76f8\u5173\u5a92\u4f53\u3001\u6cbb\u7597\u5e94\u7528\u548c\u60c5\u611f\u54cd\u5e94\u667a\u80fd\u8bbe\u5907\uff0c\u5b9e\u73b0\u4e86\u4ece\u60c5\u611f\u8bc6\u522b\u5230\u5b89\u5168\u5185\u5bb9\u751f\u6210\u7684\u5b8c\u6574\u95ed\u73af"}}
{"id": "2601.12906", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12906", "abs": "https://arxiv.org/abs/2601.12906", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Yuyao Ge", "Baolong Bi", "Jiayu Yao", "Jun Wan", "Ziling Yin", "Jiafeng Guo", "Xueqi Cheng"], "title": "Gated Differentiable Working Memory for Long-Context Language Modeling", "comment": null, "summary": "Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.", "AI": {"tldr": "Gdwm\u6846\u67b6\u901a\u8fc7\u95e8\u63a7\u5199\u5165\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6548\u7528\u5206\u914d\u68af\u5ea6\u6b65\u6570\uff0c\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u5b9e\u73b04\u500d\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u6311\u6218Transformer\uff1a\u6ce8\u610f\u529b\u5206\u6570\u5728\u6570\u5343\u4e2atoken\u4e2d\u88ab\u7a00\u91ca\uff0c\u5173\u952e\u4fe1\u606f\u5e38\u4e22\u5931\u5728\u4e2d\u95f4\uff0c\u6a21\u578b\u5728\u63a8\u7406\u65f6\u96be\u4ee5\u9002\u5e94\u65b0\u6a21\u5f0f\u3002\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f7f\u7528\u7edf\u4e00\u5199\u5165\u7b56\u7565\uff0c\u6d6a\u8d39\u8ba1\u7b97\u5728\u4f4e\u6548\u7528\u533a\u57df\uff0c\u4e14\u5728\u8bed\u4e49\u5f02\u6784\u4e0a\u4e0b\u6587\u4e2d\u68af\u5ea6\u65b9\u5dee\u9ad8\u3002", "method": "\u5c06\u6d4b\u8bd5\u65f6\u9002\u5e94\u91cd\u6784\u4e3a\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u8bb0\u5fc6\u5de9\u56fa\u95ee\u9898\uff0c\u63d0\u51faGdwm\u6846\u67b6\uff0c\u5f15\u5165\u5199\u5165\u63a7\u5236\u5668\u6765\u95e8\u63a7\u5de9\u56fa\u8fc7\u7a0b\u3002\u63a7\u5236\u5668\u4f30\u8ba1\u4e0a\u4e0b\u6587\u6548\u7528\uff08\u8861\u91cf\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u4fe1\u606f\u7406\u8bba\u6307\u6807\uff09\uff0c\u76f8\u5e94\u5206\u914d\u68af\u5ea6\u6b65\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8986\u76d6\u3002", "result": "\u5728ZeroSCROLLS\u548cLongBench v2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGdwm\u4f7f\u7528\u6bd4\u7edf\u4e00\u57fa\u7ebf\u5c114\u500d\u7684\u68af\u5ea6\u6b65\u6570\uff0c\u5b9e\u73b0\u4e86\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u65b0\u6548\u7387-\u6027\u80fd\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6d4b\u8bd5\u65f6\u9002\u5e94\u89c6\u4e3a\u9884\u7b97\u7ea6\u675f\u7684\u8bb0\u5fc6\u5de9\u56fa\u95ee\u9898\uff0cGdwm\u6846\u67b6\u80fd\u591f\u667a\u80fd\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u6548\u7387\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u8bbe\u5b9a\u4e86\u65b0\u7684\u6548\u7387\u6807\u51c6\u3002"}}
{"id": "2601.12304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12304", "abs": "https://arxiv.org/abs/2601.12304", "authors": ["Wutao Chen", "Huaqin Zou", "Chen Wan", "Lifeng Huang"], "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models", "comment": "Accepted to ICASSP 2026", "summary": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.", "AI": {"tldr": "\u63d0\u51fa2S-GDA\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u591a\u6837\u6027\u7b56\u7565\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u7684\u653b\u51fb\u6210\u529f\u7387", "motivation": "\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u7279\u522b\u662f\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u6270\u52a8\u591a\u6837\u6027\u6709\u9650\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5168\u5c40\u591a\u6837\u6027\u653b\u51fb\u6846\u67b62S-GDA\uff1a1) \u6587\u672c\u6270\u52a8\u9636\u6bb5\uff1a\u7ed3\u5408\u5019\u9009\u6587\u672c\u6269\u5c55\u548c\u5168\u5c40\u611f\u77e5\u66ff\u6362\u5b9e\u73b0\u5168\u5c40\u591a\u6837\u6027\u7b56\u7565\uff1b2) \u89c6\u89c9\u6270\u52a8\u9636\u6bb5\uff1a\u4f7f\u7528\u591a\u5c3a\u5ea6\u8c03\u6574\u548c\u5757\u6d17\u724c\u65cb\u8f6c\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u3002", "result": "\u5728VLP\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c2S-GDA\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u589e\u76ca\u6700\u9ad8\u8fbe11.17%\u3002\u8be5\u6846\u67b6\u6a21\u5757\u5316\uff0c\u53ef\u8f7b\u677e\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u5bf9\u6297\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "2S-GDA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u4e2d\u6270\u52a8\u591a\u6837\u6027\u6709\u9650\u548c\u6d41\u7a0b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ed1\u76d2\u653b\u51fb\u6548\u679c\uff0c\u5177\u6709\u5f88\u597d\u7684\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.13591", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13591", "abs": "https://arxiv.org/abs/2601.13591", "authors": ["Maojun Sun", "Yifei Xie", "Yue Wu", "Ruijian Han", "Binyan Jiang", "Defeng Sun", "Yancheng Yuan", "Jian Huang"], "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems", "comment": null, "summary": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.", "AI": {"tldr": "DSAEval\u662f\u4e00\u4e2a\u5305\u542b641\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5177\u6709\u591a\u6a21\u6001\u73af\u5883\u611f\u77e5\u3001\u591a\u67e5\u8be2\u4ea4\u4e92\u548c\u591a\u7ef4\u8bc4\u4f30\u4e09\u5927\u7279\u70b9\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6570\u636e\u4ee3\u7406\u65e8\u5728\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u4efb\u52a1\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\u7684\u5f00\u653e\u6027\u3001\u8de8\u5206\u7c7b\u548c\u7f3a\u4e4f\u6807\u51c6\u7b54\u6848\u7684\u7279\u70b9\u7ed9\u8bc4\u4f30\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u8fd9\u4e9b\u4ee3\u7406\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1DSAEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b641\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u79d1\u5b66\u95ee\u9898\uff0c\u57fa\u4e8e285\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u89c6\u89c9\u548c\u6587\u672c\uff09\u3002\u8be5\u57fa\u51c6\u5177\u6709\u4e09\u4e2a\u6838\u5fc3\u7279\u5f81\uff1a\u591a\u6a21\u6001\u73af\u5883\u611f\u77e5\u3001\u591a\u67e5\u8be2\u4ea4\u4e92\u548c\u591a\u7ef4\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e8611\u4e2a\u5148\u8fdb\u7684\u4ee3\u7406LLM\uff0c\u7ed3\u679c\u663e\u793aClaude-Sonnet-4.5\u6574\u4f53\u6027\u80fd\u6700\u5f3a\uff0cGPT-5.2\u6700\u6709\u6548\u7387\uff0cMiMo-V2-Flash\u6700\u5177\u6210\u672c\u6548\u76ca\u3002\u591a\u6a21\u6001\u611f\u77e5\u5728\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\u4e0a\u80fd\u5e26\u67652.04%\u523011.30%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5f53\u524d\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u6570\u636e\u548c\u5e38\u89c4\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u9886\u57df\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u7814\u7a76\u4e3a\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.12910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12910", "abs": "https://arxiv.org/abs/2601.12910", "authors": ["Tim Baumg\u00e4rtner", "Iryna Gurevych"], "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment", "comment": null, "summary": "We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.", "AI": {"tldr": "SciCoQA\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u79d1\u5b66\u8bba\u6587\u4e0e\u4ee3\u7801\u5e93\u4e4b\u95f4\u5dee\u5f02\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b611\u4e2a\u5dee\u5f02\u5b9e\u4f8b\uff0881\u4e2a\u771f\u5b9e\uff0c530\u4e2a\u5408\u6210\uff09\uff0c\u6db5\u76d6\u591a\u4e2a\u8ba1\u7b97\u79d1\u5b66\u9886\u57df\u3002\u8bc4\u4f30\u663e\u793aLLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u56f0\u96be\uff0c\u6700\u4f73\u6a21\u578bGPT-5\u4ec5\u80fd\u68c0\u6d4b45.7%\u7684\u771f\u5b9e\u5dee\u5f02\u3002", "motivation": "\u786e\u4fdd\u79d1\u5b66\u8bba\u6587\u4e0e\u5176\u4ee3\u7801\u5b9e\u73b0\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u5bf9\u4e8e\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u68c0\u6d4b\u8bba\u6587\u4e0e\u4ee3\u7801\u5dee\u5f02\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4eceGitHub\u95ee\u9898\u548c\u53ef\u91cd\u590d\u6027\u8bba\u6587\u4e2d\u6536\u96c6\u771f\u5b9e\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u6269\u5c55\u6570\u636e\u96c6\u3002\u8be6\u7ec6\u5206\u6790\u5dee\u5f02\u7c7b\u578b\u548c\u7c7b\u522b\uff0c\u6784\u5efa\u5305\u542b611\u4e2a\u5dee\u5f02\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6AI\u3001\u7269\u7406\u3001\u5b9a\u91cf\u751f\u7269\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u3002", "result": "\u8bc4\u4f3021\u4e2aLLMs\u663e\u793aSciCoQA\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u7701\u7565\u8bba\u6587\u7ec6\u8282\u3001\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u548c\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e4b\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002\u6700\u4f73\u6a21\u578bGPT-5\u4ec5\u80fd\u68c0\u6d4b45.7%\u7684\u771f\u5b9e\u4e16\u754c\u8bba\u6587-\u4ee3\u7801\u5dee\u5f02\u3002", "conclusion": "\u8bba\u6587-\u4ee3\u7801\u5dee\u5f02\u68c0\u6d4b\u662f\u4e00\u4e2a\u56f0\u96be\u4f46\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u73b0\u6709LLMs\u5728\u6b64\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002SciCoQA\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u79d1\u5b66\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.12308", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12308", "abs": "https://arxiv.org/abs/2601.12308", "authors": ["Anurag Kaushish", "Ayan Sar", "Sampurna Roy", "Sudeshna Chakraborty", "Prashant Trivedi", "Tanupriya Choudhury", "Kanav Gupta"], "title": "Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification", "comment": "Accepted in IEEE ICASSP 2026", "summary": "Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\\sim600K$ parameters, offering $20\\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet achieves up to 86.65\\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.", "AI": {"tldr": "AMC-MetaNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5c0f\u6837\u672c\u9065\u611f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u5f15\u5bfc\u7684\u7279\u5f81\u91d1\u5b57\u5854\u3001\u81ea\u9002\u5e94\u901a\u9053\u76f8\u5173\u6027\u6a21\u5757\u548c\u76f8\u5173\u6027\u5f15\u5bfc\u5143\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u6570\u636e\u6807\u6ce8\u5c11\u3001\u57df\u504f\u79fb\u5927\u548c\u591a\u5c3a\u5ea6\u95ee\u9898\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a60\u4e07\uff0c\u6bd4ResNet-18\u5c1120\u500d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523086.65%\u76845-way 5-shot\u51c6\u786e\u7387\u3002", "motivation": "\u9065\u611f\u5c0f\u6837\u672c\u5b66\u4e60\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u663e\u8457\u7684\u57df\u504f\u79fb\u4ee5\u53ca\u5730\u7406\u7a7a\u95f4\u5bf9\u8c61\u7684\u591a\u5c3a\u5ea6\u7279\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u6216Transformer\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u591f\u9ad8\u6548\u3002", "method": "\u63d0\u51faAMC-MetaNet\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u76f8\u5173\u6027\u5f15\u5bfc\u7684\u7279\u5f81\u91d1\u5b57\u5854\u6355\u6349\u5c3a\u5ea6\u4e0d\u53d8\u6a21\u5f0f\uff1b2) \u81ea\u9002\u5e94\u901a\u9053\u76f8\u5173\u6027\u6a21\u5757\u5b66\u4e60\u52a8\u6001\u8de8\u5c3a\u5ea6\u5173\u7cfb\uff1b3) \u76f8\u5173\u6027\u5f15\u5bfc\u5143\u5b66\u4e60\u5229\u7528\u76f8\u5173\u6027\u6a21\u5f0f\u800c\u975e\u4f20\u7edf\u7684\u539f\u578b\u5e73\u5747\u3002\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\uff0c\u4ec5\u7ea660\u4e07\u53c2\u6570\u3002", "result": "\u5728EuroSAT\u3001NWPU-RESISC45\u3001UC Merced Land Use\u548cAID\u7b49\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\uff0c5-way 5-shot\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523086.65%\u3002\u63a8\u7406\u901f\u5ea6\u5feb\uff08\u6bcf\u56fe\u50cf<50ms\uff09\uff0c\u53c2\u6570\u91cf\u6bd4ResNet-18\u5c1120\u500d\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "AMC-MetaNet\u662f\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u3001\u5c3a\u5ea6\u611f\u77e5\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5c0f\u6837\u672c\u9065\u611f\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u9065\u611f\u9886\u57df\u7279\u6709\u7684\u591a\u5c3a\u5ea6\u3001\u57df\u504f\u79fb\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u9065\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.13600", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13600", "abs": "https://arxiv.org/abs/2601.13600", "authors": ["Paul He", "Elke Kirschbaum", "Shiva Kasiviswanathan"], "title": "Foundations of Global Consistency Checking with Noisy LLM Oracles", "comment": "Under Review", "summary": "Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u5206\u6cbb\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u81ea\u7136\u8bed\u8a00\u4e8b\u5b9e\u96c6\u5408\u7684\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u8bc6\u522b\u6700\u5c0f\u4e0d\u4e00\u81f4\u5b50\u96c6\u6765\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u566a\u58f0\u95ee\u9898", "motivation": "\u81ea\u7136\u8bed\u8a00\u4e8b\u5b9e\u96c6\u5408\u7684\u5168\u5c40\u4e00\u81f4\u6027\u5bf9\u4e8e\u4e8b\u5b9e\u6838\u67e5\u3001\u6458\u8981\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u8bc4\u4f30\u5c0f\u89c4\u6a21\u4e8b\u5b9e\u5b50\u96c6\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5176\u5224\u65ad\u5b58\u5728\u566a\u58f0\uff0c\u4e14\u6210\u5bf9\u68c0\u67e5\u65e0\u6cd5\u4fdd\u8bc1\u5168\u5c40\u4e00\u81f4\u6027", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u6cbb\u7b97\u6cd5\uff0c\u8bc6\u522b\u6700\u5c0f\u4e0d\u4e00\u81f4\u5b50\u96c6\uff0c\u53ef\u9009\u5730\u901a\u8fc7\u547d\u4e2d\u96c6\u8ba1\u7b97\u6700\u5c0f\u4fee\u590d\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u4f4e\u9636\u591a\u9879\u5f0f\u67e5\u8be2\u590d\u6742\u5ea6", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eLLM\u8bc4\u4f30\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u8bed\u8a00\u4e00\u81f4\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u7136\u8bed\u8a00\u4e8b\u5b9e\u96c6\u5408\u7684\u5168\u5c40\u4e00\u81f4\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u6cbb\u7b97\u6cd5\u514b\u670d\u4e86LLM\u8bc4\u4f30\u7684\u566a\u58f0\u95ee\u9898\u548c\u6307\u6570\u7ea7\u67e5\u8be2\u590d\u6742\u5ea6\u6311\u6218"}}
{"id": "2601.12921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12921", "abs": "https://arxiv.org/abs/2601.12921", "authors": ["Adimulya Kartiyasa", "Bao Gia Cao", "Boyang Li"], "title": "Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs", "comment": null, "summary": "Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIndoSoSci\u6570\u636e\u96c6\uff0c\u4ece151\u4e2a\u5370\u5c3c\u793e\u4f1a\u79d1\u5b66\u671f\u520a\u4e2d\u63d0\u53d6\u6587\u5316\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7RAG\u65b9\u6cd5\u589e\u5f3aLLMs\u5bf9\u5370\u5c3c\u6587\u5316\u7684\u7406\u89e3\uff0c\u5728IndoCulture\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5370\u5c3c\u6587\u5316\u7684\u7406\u89e3\u6709\u9650\uff0c\u800c\u672c\u571f\u793e\u4f1a\u79d1\u5b66\u671f\u520a\u4f5c\u4e3a\u91cd\u8981\u7684\u6587\u5316\u77e5\u8bc6\u6765\u6e90\u88ab\u5ffd\u89c6\u3002\u8fd9\u4e9b\u671f\u520a\u5305\u542b\u5927\u91cf\u672c\u571f\u89c6\u89d2\u7684\u6587\u5316\u7814\u7a76\uff0c\u662f\u63d0\u5347LLMs\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "method": "1) \u4ece151\u4e2a\u5f00\u6e90\u5370\u5c3c\u793e\u4f1a\u79d1\u5b66\u671f\u520a\u521b\u5efaIndoSoSci\u6587\u672c\u6570\u636e\u96c6\uff1b2) \u63d0\u53d6\u4e0e\u5370\u5c3c\u6587\u5316\u76f8\u5173\u7684\u4e8b\u5b9e\uff1b3) \u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u751f\u6210\u7684\u5047\u8bbe\u6587\u6863\u4f5c\u4e3a\u68c0\u7d22\u67e5\u8be2\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728IndoCulture\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002\u7ed3\u5408IndoSoSci\u548c\u5370\u5c3c\u7ef4\u57fa\u767e\u79d1\u540e\uff0c\u5728IndoCulture\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u51c6\u786e\u7387\u3002", "conclusion": "\u5370\u5c3c\u793e\u4f1a\u79d1\u5b66\u671f\u520a\u662f\u63d0\u5347LLMs\u6587\u5316\u7406\u89e3\u80fd\u529b\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u63d0\u51fa\u7684RAG\u65b9\u6cd5\u80fd\u6709\u6548\u6ce8\u5165\u5370\u5c3c\u6587\u5316\u77e5\u8bc6\uff0c\u4e3a\u8de8\u6587\u5316AI\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8303\u4f8b\u3002"}}
{"id": "2601.12312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12312", "abs": "https://arxiv.org/abs/2601.12312", "authors": ["Yongjun Jeon", "Jongmin Shin", "Kanggil Park", "Seonmin Park", "Soyoung Lim", "Jung Yong Kim", "Jinsoo Rhu", "Jongman Kim", "Gyu-Seong Choi", "Namkee Oh", "Kyu-Hwan Jung"], "title": "CurConMix+: A Unified Spatio-Temporal Framework for Hierarchical Surgical Workflow Understanding", "comment": null, "summary": "Surgical action triplet recognition aims to understand fine-grained surgical behaviors by modeling the interactions among instruments, actions, and anatomical targets. Despite its clinical importance for workflow analysis and skill assessment, progress has been hindered by severe class imbalance, subtle visual variations, and the semantic interdependence among triplet components. Existing approaches often address only a subset of these challenges rather than tackling them jointly, which limits their ability to form a holistic understanding. This study builds upon CurConMix, a spatial representation framework. At its core, a curriculum-guided contrastive learning strategy learns discriminative and progressively correlated features, further enhanced by structured hard-pair sampling and feature-level mixup. Its temporal extension, CurConMix+, integrates a Multi-Resolution Temporal Transformer (MRTT) that achieves robust, context-aware understanding by adaptively fusing multi-scale temporal features and dynamically balancing spatio-temporal cues. Furthermore, we introduce LLS48, a new, hierarchically annotated benchmark for complex laparoscopic left lateral sectionectomy, providing step-, task-, and action-level annotations. Extensive experiments on CholecT45 and LLS48 demonstrate that CurConMix+ not only outperforms state-of-the-art approaches in triplet recognition, but also exhibits strong cross-level generalization, as its fine-grained features effectively transfer to higher-level phase and step recognition tasks. Together, the framework and dataset provide a unified foundation for hierarchy-aware, reproducible, and interpretable surgical workflow understanding. The code and dataset will be publicly released on GitHub to facilitate reproducibility and further research.", "AI": {"tldr": "CurConMix+ \u662f\u4e00\u4e2a\u7528\u4e8e\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u7684\u65f6\u7a7a\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u3001\u7ed3\u6784\u5316\u786c\u5bf9\u91c7\u6837\u548c\u591a\u5206\u8fa8\u7387\u65f6\u95f4\u53d8\u6362\u5668\uff0c\u5728 CholecT45 \u548c\u65b0\u7684 LLS48 \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u5c42\u6b21\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u5bf9\u4e8e\u5de5\u4f5c\u6d41\u5206\u6790\u548c\u6280\u80fd\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u89c6\u89c9\u53d8\u5316\u7ec6\u5fae\u3001\u4e09\u5143\u7ec4\u8bed\u4e49\u76f8\u4e92\u4f9d\u8d56\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u51b3\u90e8\u5206\u95ee\u9898\uff0c\u7f3a\u4e4f\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002", "method": "\u57fa\u4e8e CurConMix \u7a7a\u95f4\u8868\u793a\u6846\u67b6\uff0c\u91c7\u7528\u8bfe\u7a0b\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\u5224\u522b\u6027\u548c\u6e10\u8fdb\u76f8\u5173\u7279\u5f81\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u786c\u5bf9\u91c7\u6837\u548c\u7279\u5f81\u7ea7\u6df7\u5408\u3002\u5176\u65f6\u95f4\u6269\u5c55 CurConMix+ \u96c6\u6210\u591a\u5206\u8fa8\u7387\u65f6\u95f4\u53d8\u6362\u5668\uff08MRTT\uff09\uff0c\u81ea\u9002\u5e94\u878d\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u7279\u5f81\u5e76\u52a8\u6001\u5e73\u8861\u65f6\u7a7a\u7ebf\u7d22\u3002\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u5206\u5c42\u6807\u6ce8\u6570\u636e\u96c6 LLS48\u3002", "result": "\u5728 CholecT45 \u548c LLS48 \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCurConMix+ \u5728\u4e09\u5143\u7ec4\u8bc6\u522b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u5c42\u6b21\u6cdb\u5316\u80fd\u529b\uff0c\u5176\u7ec6\u7c92\u5ea6\u7279\u5f81\u53ef\u6709\u6548\u8fc1\u79fb\u5230\u66f4\u9ad8\u5c42\u6b21\u7684\u9636\u6bb5\u548c\u6b65\u9aa4\u8bc6\u522b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u5c42\u6b21\u611f\u77e5\u3001\u53ef\u91cd\u590d\u548c\u53ef\u89e3\u91ca\u7684\u624b\u672f\u5de5\u4f5c\u6d41\u7406\u89e3\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728 GitHub \u4e0a\u516c\u5f00\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.13632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13632", "abs": "https://arxiv.org/abs/2601.13632", "authors": ["Zhiming Xue", "Sichen Zhao", "Yalun Qi", "Xianling Zeng", "Zihan Yu"], "title": "Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning", "comment": null, "summary": "With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.", "AI": {"tldr": "\u63d0\u51faRADR\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u7ec4\u5408\u4f18\u5316\uff0c\u901a\u8fc7\u9884\u6d4b\u62e5\u5835\u98ce\u9669\u5b9e\u73b0\u52a8\u6001\u8def\u5f84\u89c4\u5212\uff0c\u5728\u4fdd\u6301\u8fd0\u8f93\u8ddd\u79bb\u4ec5\u589e\u52a02.1%\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e19.3%\u7684\u62e5\u5835\u98ce\u9669\u66b4\u9732", "motivation": "\u7535\u5546\u7269\u6d41\u7f51\u7edc\u9762\u4e34\u5de8\u5927\u538b\u529b\uff0c\u4f20\u7edf\u9759\u6001\u8def\u7531\u7b56\u7565\u65e0\u6cd5\u5e94\u5bf9\u4ea4\u901a\u62e5\u5835\u548c\u9700\u6c42\u6ce2\u52a8\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u52a8\u6001\u8def\u7531\u89e3\u51b3\u65b9\u6848", "method": "1) \u4f7f\u7528\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u4ece\u79bb\u6563GPS\u6570\u636e\u6784\u5efa\u7269\u6d41\u62d3\u6251\u56fe\uff1b2) \u91c7\u7528GCN+GRU\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u9884\u6d4b\u62e5\u5835\u98ce\u9669\uff1b3) \u5c06\u9884\u6d4b\u7ed3\u679c\u96c6\u6210\u5230\u52a8\u6001\u8fb9\u6743\u91cd\u673a\u5236\u4e2d\u8fdb\u884c\u8def\u5f84\u89c4\u5212", "result": "\u5728Smart Logistics Dataset 2024\u4e0a\u9a8c\u8bc1\uff0cRADR\u7b97\u6cd5\u663e\u8457\u589e\u5f3a\u4f9b\u5e94\u94fe\u97e7\u6027\uff0c\u5728\u9ad8\u62e5\u5835\u573a\u666f\u4e0b\u964d\u4f4e19.3%\u7684\u6f5c\u5728\u62e5\u5835\u98ce\u9669\u66b4\u9732\uff0c\u4ec5\u589e\u52a02.1%\u8fd0\u8f93\u8ddd\u79bb", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u914d\u9001\u6548\u7387\u4e0e\u8fd0\u8425\u5b89\u5168\uff0c\u4e3a\u7269\u6d41\u7f51\u7edc\u63d0\u4f9b\u667a\u80fd\u52a8\u6001\u8def\u7531\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12945", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12945", "abs": "https://arxiv.org/abs/2601.12945", "authors": ["Miao Xie", "Siguang Chen", "Chunli Lv"], "title": "A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits", "comment": "27 pages, 6 table", "summary": "Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.", "AI": {"tldr": "\u8fd9\u662f\u7b2c\u4e00\u7bc7\u7cfb\u7edf\u7efc\u8ff0\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u591a\u81c2\u8001\u864e\u673a\u53cc\u5411\u4ea4\u4e92\u7684\u8bba\u6587\uff0c\u63a2\u8ba8\u4e24\u8005\u5728\u7ec4\u4ef6\u5c42\u9762\u7684\u76f8\u4e92\u589e\u5f3a\uff1a\u8001\u864e\u673a\u7b97\u6cd5\u89e3\u51b3LLM\u4ece\u9884\u8bad\u7ec3\u5230\u4e2a\u6027\u5316\u7b49\u6311\u6218\uff0cLLM\u5219\u91cd\u65b0\u5b9a\u4e49\u8001\u864e\u673a\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u4ef6\u4ee5\u6539\u8fdb\u51b3\u7b56\u3002", "motivation": "LLM\u5df2\u6210\u4e3a\u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u7cfb\u7edf\uff0c\u800c\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e24\u4e2a\u9886\u57df\u4ea4\u53c9\u6f5c\u529b\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u7279\u522b\u662f\u5728\u7ec4\u4ef6\u5c42\u9762\u7684\u53cc\u5411\u4ea4\u4e92\u5206\u6790\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5728\u7ec4\u4ef6\u5c42\u9762\u5206\u6790LLM\u4e0e\u591a\u81c2\u8001\u864e\u673a\u7684\u53cc\u5411\u4ea4\u4e92\u3002\u5206\u6790\u73b0\u6709LLM\u589e\u5f3a\u7684\u8001\u864e\u673a\u7cfb\u7edf\u548c\u8001\u864e\u673a\u589e\u5f3a\u7684LLM\u7cfb\u7edf\uff0c\u6df1\u5165\u63a2\u8ba8\u5176\u8bbe\u8ba1\u3001\u65b9\u6cd5\u8bba\u548c\u6027\u80fd\u8868\u73b0\u3002", "result": "\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\u548c\u4ee3\u8868\u6027\u53d1\u73b0\uff0c\u5c55\u793a\u4e86\u53cc\u5411\u4ea4\u4e92\u7684\u76ca\u5904\uff1a\u8001\u864e\u673a\u7b97\u6cd5\u89e3\u51b3LLM\u4ece\u9884\u8bad\u7ec3\u5230RAG\u548c\u4e2a\u6027\u5316\u7684\u6311\u6218\uff1bLLM\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u81c2\u5b9a\u4e49\u548c\u73af\u5883\u5efa\u6a21\u7b49\u6838\u5fc3\u7ec4\u4ef6\u6765\u589e\u5f3a\u8001\u864e\u673a\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aLLM\u4e0e\u591a\u81c2\u8001\u864e\u673a\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u6587\u732e\u7d22\u5f15\u7684GitHub\u4ed3\u5e93\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.12313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12313", "abs": "https://arxiv.org/abs/2601.12313", "authors": ["Xiangyu Hu", "Yicheng Hong", "Hongchuang Zheng", "Wenjun Zeng", "Bingyao Liu"], "title": "S^2F-Net:A Robust Spatial-Spectral Fusion Framework for Cross-Model AIGC Detection", "comment": "27pages 9figures", "summary": "The rapid development of generative models has imposed an urgent demand for detection schemes with strong generalization capabilities. However, existing detection methods generally suffer from overfitting to specific source models, leading to significant performance degradation when confronted with unseen generative architectures. To address these challenges, this paper proposes a cross-model detection framework called S 2 F-Net, whose core lies in exploring and leveraging the inherent spectral discrepancies between real and synthetic textures. Considering that upsampling operations leave unique and distinguishable frequency fingerprints in both texture-poor and texture-rich regions, we focus our research on the detection of frequency-domain artifacts, aiming to fundamentally improve the generalization performance of the model. Specifically, we introduce a learnable frequency attention module that adaptively weights and enhances discriminative frequency bands by synergizing spatial texture analysis and spectral dependencies.On the AIGCDetectBenchmark, which includes 17 categories of generative models, S 2 F-Net achieves a detection accuracy of 90.49%, significantly outperforming various existing baseline methods in cross-domain detection scenarios.", "AI": {"tldr": "\u63d0\u51faS\u00b2F-Net\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u4e0e\u5408\u6210\u7eb9\u7406\u7684\u9891\u8c31\u5dee\u5f02\u6765\u68c0\u6d4b\u751f\u6210\u56fe\u50cf\uff0c\u572817\u7c7b\u751f\u6210\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523090.49%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u6e90\u6a21\u578b\uff0c\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u751f\u6210\u67b6\u6784\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u578b\u68c0\u6d4b\u6846\u67b6S\u00b2F-Net\uff0c\u6838\u5fc3\u662f\u63a2\u7d22\u548c\u5229\u7528\u771f\u5b9e\u4e0e\u5408\u6210\u7eb9\u7406\u7684\u5185\u5728\u9891\u8c31\u5dee\u5f02\u3002\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u534f\u540c\u7a7a\u95f4\u7eb9\u7406\u5206\u6790\u548c\u9891\u8c31\u4f9d\u8d56\u5173\u7cfb\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u548c\u589e\u5f3a\u5224\u522b\u6027\u9891\u5e26\u3002", "result": "\u5728\u5305\u542b17\u7c7b\u751f\u6210\u6a21\u578b\u7684AIGCDetectBenchmark\u4e0a\uff0cS\u00b2F-Net\u8fbe\u523090.49%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u8de8\u57df\u68c0\u6d4b\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u5404\u79cd\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5173\u6ce8\u4e0a\u91c7\u6837\u64cd\u4f5c\u5728\u9891\u57df\u7559\u4e0b\u7684\u72ec\u7279\u6307\u7eb9\uff0c\u4ece\u6839\u672c\u4e0a\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8de8\u6a21\u578b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13687", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13687", "abs": "https://arxiv.org/abs/2601.13687", "authors": ["Zhichao Liang", "Satoshi Nakamura"], "title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue", "comment": null, "summary": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.", "AI": {"tldr": "SocialMindChange\u662f\u4e00\u4e2a\u65b0\u7684\u52a8\u6001\u5fc3\u7406\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u4e3b\u52a8\u6539\u53d8\u4ed6\u4eba\u5fc3\u7406\u72b6\u6001\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8ffd\u8e2a\u5fc3\u7406\u72b6\u6001\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u5fc3\u7406\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u8ba9\u8bed\u8a00\u6a21\u578b\u5904\u4e8e\u88ab\u52a8\u89d2\u8272\uff0c\u53ea\u8ffd\u8e2a\u5fc3\u7406\u72b6\u6001\u53d8\u5316\u3002\u4f46\u5728\u771f\u5b9e\u793e\u4ea4\u4e92\u52a8\u4e2d\uff0c\u5fc3\u7406\u7406\u8bba\u4e5f\u88ab\u7528\u4e8e\u4e3b\u52a8\u884c\u52a8\u2014\u2014\u8bf4\u8bdd\u8005\u8ba1\u5212\u8bf4\u4ec0\u4e48\u6765\u6539\u53d8\u4ed6\u4eba\u7684\u5fc3\u7406\u72b6\u6001\u8f68\u8ff9\u4ee5\u5b9e\u73b0\u76ee\u6807\u3002", "method": "\u5f15\u5165SocialMindChange\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u5b9a\u4e49\u5305\u542b4\u4e2a\u89d2\u8272\u7684\u793e\u4ea4\u60c5\u5883\u548c5\u4e2a\u8fde\u63a5\u573a\u666f\u3002\u6a21\u578b\u626e\u6f14\u4e00\u4e2a\u89d2\u8272\uff0c\u5728\u4e94\u4e2a\u573a\u666f\u4e2d\u751f\u6210\u5bf9\u8bdd\u4ee5\u8fbe\u5230\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6240\u6709\u53c2\u4e0e\u8005\u4e0d\u65ad\u53d8\u5316\u7684\u72b6\u6001\u4e00\u81f4\u3002\u4f7f\u7528\u7ed3\u6784\u5316\u56db\u6b65\u6846\u67b6\u6784\u5efa\u4e861,200\u4e2a\u793e\u4ea4\u60c5\u5883\uff0c\u6db5\u76d66,000\u4e2a\u573a\u666f\u548c\u8d85\u8fc790,000\u4e2a\u95ee\u9898\u3002", "result": "\u8bc4\u4f30\u5341\u4e2a\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u663e\u793a\uff0c\u5b83\u4eec\u7684\u5e73\u5747\u6027\u80fd\u6bd4\u4eba\u7c7b\u6027\u80fd\u4f4e54.2%\u3002\u8fd9\u4e2a\u5dee\u8ddd\u8868\u660e\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u8fde\u63a5\u7684\u4e92\u52a8\u4e2d\u7ef4\u6301\u548c\u6539\u53d8\u5fc3\u7406\u72b6\u6001\u8868\u5f81\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "SocialMindChange\u57fa\u51c6\u6d4b\u8bd5\u4ece\u8ffd\u8e2a\u5fc3\u7406\u72b6\u6001\u8f6c\u5411\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u6539\u53d8\u5fc3\u7406\u72b6\u6001\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u793e\u4ea4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.12960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12960", "abs": "https://arxiv.org/abs/2601.12960", "authors": ["Ainhoa Vivel-Couso", "Nicol\u00e1s Vila-Blanco", "Mar\u00eda J. Carreira", "Alberto Bugar\u00edn-Diz", "Inmaculada Tom\u00e1s", "Jose M. Alonso-Moral"], "title": "Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images", "comment": "This paper is a preliminary version of an accepted article in Information Systems Frontiers, Springer, Special Issue \"Explainability in Human-Centric AI\". Please cite the final published version of the paper, not this preprint. The final published version can be found at https://link.springer.com/article/10.1007/s10796-025-10682-3", "summary": "Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4e0d\u900f\u660e\u548c\u900f\u660e\u65b9\u6cd5\u7684\u7259\u79d1\u5e74\u9f84\u4f30\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u5757\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u6587\u672c\u89e3\u91ca\uff0c\u63d0\u9ad8AI\u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u867d\u7136\u80fd\u5b9e\u73b0\u4e2a\u6027\u5316\u62a4\u7406\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u4e0d\u900f\u660e\u6027\u5bfc\u81f4\u4fe1\u4efb\u95ee\u9898\u3002\u9700\u8981\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u4ee5\u589e\u5f3a\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ece\u5168\u666f\u56fe\u50cf\u8fdb\u884c\u7259\u79d1\u5e74\u9f84\u4f30\u8ba1\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e0d\u900f\u660e\u548c\u900f\u660e\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u5757\u4ea7\u751f\u4e34\u5e8a\u533b\u751f\u53cb\u597d\u7684\u6587\u672c\u89e3\u91ca\u3002\u89e3\u91ca\u89c4\u5219\u662f\u4e0e\u7259\u79d1\u4e13\u5bb6\u5408\u4f5c\u5236\u5b9a\u7684\uff0c\u5e76\u91c7\u7528\u95ee\u5377\u7531\u4e13\u5bb6\u624b\u52a8\u9a8c\u8bc1\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u4e13\u5bb6\u5728\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u5e73\u5747\u8bc4\u5206\u4e3a4.77\u00b10.12\uff08\u6ee1\u52065\u5206\uff09\uff0c\u663e\u793a\u89e3\u91ca\u8d28\u91cf\u5f88\u9ad8\u3002\u7cfb\u7edf\u5728ALTAI\u68c0\u67e5\u8868\u7684\u4e03\u4e2a\u53ef\u4fe1\u5ea6\u7ef4\u5ea6\u4e0a\u83b7\u5f974.40\u00b10.27\uff08\u6ee1\u52065\u5206\uff09\u7684\u8bc4\u5206\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u6210\u529f\u63d0\u9ad8\u4e86\u7259\u79d1\u5e74\u9f84\u4f30\u8ba1AI\u7684\u900f\u660e\u5ea6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u589e\u5f3a\u4e86\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\uff0c\u4e3a\u533b\u7597AI\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2601.12316", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12316", "abs": "https://arxiv.org/abs/2601.12316", "authors": ["Xinyuan Zhao", "Xianrui Chen", "Ahmad Chaddad"], "title": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer", "comment": "accepted at ICASSP 2026", "summary": "We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49\u00b0, 3.22\u00b0, 10.16\u00b0, and 1.44\u00b0, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u8c03\u5236\u3001\u591a\u5c3a\u5ea6Transformer\u7528\u4e8e3D\u89c6\u7ebf\u4f30\u8ba1\uff0c\u901a\u8fc7\u539f\u578b\u6761\u4ef6\u5316\u3001\u8de8\u5c3a\u5ea6\u878d\u5408\u548c\u6df7\u5408\u4e13\u5bb6\u673a\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5149\u7167\u3001\u5934\u90e8\u59ff\u6001\u3001\u80cc\u666f\u7b49\u53d8\u5316\u65f6\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u6761\u4ef6\u5efa\u6a21\u80fd\u529b\u6765\u9002\u5e94\u8fd9\u4e9b\u8bed\u4e49\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u539f\u578b\u5e93\uff08\u5149\u7167\u3001\u5934\u90e8\u59ff\u6001\u3001\u80cc\u666f\u3001\u65b9\u5411\uff09\u8c03\u5236CLIP\u5168\u5c40\u7279\u5f81\uff0c\u5c06\u539f\u578b\u589e\u5f3a\u7684\u5168\u5c40\u5411\u91cf\u4e0eCLIP patch token\u548c\u9ad8\u5206\u8fa8\u7387CNN token\u5728\u7edf\u4e00\u6ce8\u610f\u529b\u7a7a\u95f4\u4e2d\u878d\u5408\uff0c\u5e76\u7528\u8def\u7531/\u5171\u4eab\u7684\u6df7\u5408\u4e13\u5bb6\u66ff\u6362\u591a\u4e2aFFN\u5757\u4ee5\u589e\u52a0\u6761\u4ef6\u5bb9\u91cf\u3002", "result": "\u5728MPIIFaceGaze\u3001EYEDIAP\u3001Gaze360\u548cETH-XGaze\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52302.49\u00b0\u30013.22\u00b0\u300110.16\u00b0\u548c1.44\u00b0\u7684\u89d2\u8bef\u5dee\uff0c\u76f8\u5bf9\u5148\u524d\u6700\u4f73\u7ed3\u679c\u63d0\u5347\u9ad8\u8fbe64%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u4e49\u8c03\u5236\u591a\u5c3a\u5ea6Transformer\u901a\u8fc7\u539f\u578b\u6761\u4ef6\u5316\u3001\u8de8\u5c3a\u5ea6\u878d\u5408\u548c\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u7ebf\u4f30\u8ba1\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.13709", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13709", "abs": "https://arxiv.org/abs/2601.13709", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "comment": "For associated dataset, see https://github.com/cocochief4/llm-mafia. Published in IEEE ICA 2025, waiting for IEEEXplore proceedings", "summary": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "AI": {"tldr": "GPT-4o\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u300a\u9ed1\u624b\u515a\u300b\u4e2d\u6bd4\u4eba\u7c7b\u66f4\u64c5\u957f\u6b3a\u9a97\uff0c\u901a\u8fc7\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df\u771f\u5b9e\u793e\u4ea4\u60c5\u5883\uff0c\u9ed1\u624b\u515a\u68c0\u6d4b\u5668\u5728LLM\u6e38\u620f\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u7387\u4f4e\u4e8e\u4eba\u7c7b\u6e38\u620f\uff0c\u8868\u660eLLM\u80fd\u66f4\u597d\u5730\u878d\u5165\u7fa4\u4f53\u8fdb\u884c\u6b3a\u9a97\u3002", "motivation": "\u7814\u7a76LLM\u5728\u81ea\u7136\u8bed\u8a00\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u6b3a\u9a97\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u300a\u9ed1\u624b\u515a\u300b\u4e2d\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u53d7\u63a7\u4efb\u52a1\u4e2d\u7684\u6b3a\u9a97\uff0c\u800c\u5bf9\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u6b3a\u9a97\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df35\u573a\u300a\u9ed1\u624b\u515a\u300b\u6e38\u620f\uff0c\u8ba9GPT-4o\u667a\u80fd\u4f53\u53c2\u4e0e\uff1b\u521b\u5efa\u57fa\u4e8eGPT-4-Turbo\u7684\u9ed1\u624b\u515a\u68c0\u6d4b\u5668\uff0c\u5728\u4e0d\u4e86\u89e3\u73a9\u5bb6\u89d2\u8272\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u6e38\u620f\u8bb0\u5f55\u6765\u9884\u6d4b\u9ed1\u624b\u515a\u73a9\u5bb6\uff1b\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u4f5c\u4e3a\u6b3a\u9a97\u8d28\u91cf\u7684\u66ff\u4ee3\u6307\u6807\uff0c\u5e76\u4e0e28\u573a\u4eba\u7c7b\u6e38\u620f\u548c\u968f\u673a\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u9ed1\u624b\u515a\u68c0\u6d4b\u5668\u5728LLM\u6e38\u620f\u4e2d\u7684\u9884\u6d4b\u51c6\u786e\u7387\u4f4e\u4e8e\u4eba\u7c7b\u6e38\u620f\uff0c\u8fd9\u4e00\u7ed3\u679c\u5728\u4e0d\u540c\u6e38\u620f\u5929\u6570\u548c\u68c0\u6d4b\u5230\u7684\u9ed1\u624b\u515a\u6570\u91cf\u4e0a\u4fdd\u6301\u4e00\u81f4\uff0c\u8868\u660eLLM\u80fd\u66f4\u597d\u5730\u878d\u5165\u7fa4\u4f53\uff0c\u6b3a\u9a97\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "LLM\u5728\u793e\u4ea4\u60c5\u5883\u4e2d\u5c55\u73b0\u51fa\u590d\u6742\u7684\u6b3a\u9a97\u80fd\u529b\uff0c\u8fd9\u65e2\u4f53\u73b0\u4e86\u5176\u793e\u4ea4\u667a\u80fd\u7684\u6210\u719f\u5ea6\uff0c\u4e5f\u51f8\u663e\u4e86\u6f5c\u5728\u98ce\u9669\uff1b\u7814\u7a76\u56e2\u961f\u53d1\u5e03\u4e86LLM\u300a\u9ed1\u624b\u515a\u300b\u6e38\u620f\u8bb0\u5f55\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.12973", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12973", "abs": "https://arxiv.org/abs/2601.12973", "authors": ["Shuanghong Huang", "Jinlei Xu", "Youchao Zhou", "Yanghao Zhou", "Xuan Zhao", "Chong Feng", "Wenxuan Zhang"], "title": "Pardon? Evaluating Conversational Repair in Large Audio-Language Models", "comment": null, "summary": "Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u590d\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u56de\u7b54\u548c\u4e0d\u53ef\u56de\u7b54\u97f3\u9891\u8f93\u5165\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165EAR\u8bc4\u5206\u6765\u540c\u65f6\u8bc4\u4f30\u4efb\u52a1\u80fd\u529b\u548c\u4fee\u590d\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7b54\u6848\u51c6\u786e\u6027\u548c\u5bf9\u58f0\u5b66\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5047\u8bbe\u8bed\u97f3\u8f93\u5165\u5728\u8bed\u4e49\u4e0a\u603b\u662f\u53ef\u56de\u7b54\u7684\u3002\u7136\u800c\uff0c\u5728\u73b0\u5b9e\u4ea4\u4e92\u4e2d\uff0c\u5f53\u5173\u952e\u4fe1\u606f\u7f3a\u5931\u65f6\uff0c\u8f93\u5165\u53ef\u80fd\u4e0d\u53ef\u56de\u7b54\uff0c\u73b0\u6709\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u8bc6\u522b\u4e0d\u53ef\u56de\u7b54\u6027\u548c\u542f\u52a8\u9002\u5f53\u5bf9\u8bdd\u4fee\u590d\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4fee\u590d\u611f\u77e5\u7684\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u660e\u786e\u533a\u5206\u53ef\u56de\u7b54\u548c\u4e0d\u53ef\u56de\u7b54\u7684\u97f3\u9891\u8f93\u5165\u3002\u4f7f\u7528\u8bed\u4e49-\u58f0\u5b66\u63a9\u853d\u534f\u8bae\u6784\u5efa\u914d\u5bf9\u8bc4\u4f30\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86EAR\u8bc4\u5206\uff08\u53ef\u8bc4\u4f30\u6027\u610f\u8bc6\u548c\u4fee\u590d\u8bc4\u5206\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u8865\u507f\u6027\u6307\u6807\uff0c\u540c\u65f6\u8bc4\u4f30\u53ef\u56de\u7b54\u6761\u4ef6\u4e0b\u7684\u4efb\u52a1\u80fd\u529b\u548c\u4e0d\u53ef\u56de\u7b54\u6761\u4ef6\u4e0b\u7684\u4fee\u590d\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u53e3\u8bed\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf9\u591a\u79cdLALM\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u7b54\u6848\u51c6\u786e\u6027\u548c\u5bf9\u8bdd\u53ef\u9760\u6027\u4e4b\u95f4\u5b58\u5728\u4e00\u81f4\u5dee\u8ddd\uff1a\u8bb8\u591a\u6a21\u578b\u5728\u8f93\u5165\u53ef\u56de\u7b54\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5927\u591a\u6570\u65e0\u6cd5\u8bc6\u522b\u8bed\u4e49\u4e0d\u53ef\u56de\u7b54\u6027\u5e76\u542f\u52a8\u9002\u5f53\u7684\u5bf9\u8bdd\u4fee\u590d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u4ee5\u51c6\u786e\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u5b9e\u8df5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6fc0\u52b1\u4e86\u5c06\u4e0d\u53ef\u56de\u7b54\u8f93\u5165\u89c6\u4e3a\u4fee\u590d\u548c\u6301\u7eed\u4ea4\u4e92\u7ebf\u7d22\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.12325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12325", "abs": "https://arxiv.org/abs/2601.12325", "authors": ["Eli Passov", "Nathan S. Netanyahu", "Yosi Keller"], "title": "Multi-Sensor Matching with HyperNetworks", "comment": null, "summary": "Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground/aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u8f7b\u91cf\u7ea7\u63cf\u8ff0\u7b26\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u9053\u7f29\u653e\u548c\u6a21\u6001\u7279\u5b9a\u5f52\u4e00\u5316\u589e\u5f3aSiamese CNN\uff0c\u5728VIS-IR\u8de8\u6a21\u6001\u5339\u914d\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\uff08\u5982\u53ef\u89c1\u5149\u4e0e\u7ea2\u5916\uff09\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\u5728\u5904\u7406\u5916\u89c2\u53d8\u5316\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u9ad8\u6548\u63cf\u8ff0\u7b26\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u6a21\u6001\u5dee\u5f02\u3002\u8d85\u7f51\u7edc\u80fd\u7075\u6d3b\u6ce8\u5165\u4e0a\u4e0b\u6587\u548c\u4efb\u52a1\u6761\u4ef6\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u8865\u4e01\u5339\u914d\u3002", "method": "1) \u5728Siamese CNN\u4e2d\u5f15\u5165\u8d85\u7f51\u7edc\u6a21\u5757\uff0c\u8ba1\u7b97\u81ea\u9002\u5e94\u3001\u6bcf\u901a\u9053\u7684\u7f29\u653e\u548c\u504f\u79fb\uff1b2) \u4f7f\u7528\u6761\u4ef6\u5b9e\u4f8b\u5f52\u4e00\u5316\u5728\u6d45\u5c42\u63d0\u4f9b\u6a21\u6001\u7279\u5b9a\u9002\u5e94\uff08\u5982VIS-IR\uff09\uff1b3) \u7ed3\u5408\u4e09\u5143\u7ec4\u635f\u5931\u548c\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728VIS-NIR\u548c\u5176\u4ed6VIS-IR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u989d\u5916\u6570\u636e\u96c6\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff08\u5c3d\u7ba1\u540e\u8005\u63a8\u7406\u6210\u672c\u66f4\u9ad8\uff09\u3002\u540c\u65f6\u53d1\u5e03\u4e86GAP-VIR\u8de8\u5e73\u53f0\uff08\u5730\u9762/\u7a7a\u4e2d\uff09VIS-IR\u8865\u4e01\u6570\u636e\u96c6\uff0850\u4e07\u5bf9\uff09\u3002", "conclusion": "\u8d85\u7f51\u7edc\u589e\u5f3a\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5339\u914d\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.13735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13735", "abs": "https://arxiv.org/abs/2601.13735", "authors": ["Hojin Kim", "Jaehyung Kim"], "title": "Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection", "comment": "15 pages, 4 figures", "summary": "Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8e\u6982\u7387\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u65e0\u6cd5\u6709\u6548\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3b\u8981\u53cd\u6620\u8868\u9762\u6d41\u7545\u6027\u6216\u5206\u5e03\u5148\u9a8c\uff0c\u800c\u975e\u903b\u8f91\u7ed3\u6784\u3002", "motivation": "\u6311\u6218\u73b0\u6709\u5047\u8bbe\uff1a\u6982\u7387\u7f6e\u4fe1\u5ea6\u6307\u6807\u662f\u5426\u80fd\u771f\u6b63\u53cd\u6620\u63a8\u7406\u8d28\u91cf\u3002\u7814\u7a76\u8fd9\u4e9b\u6307\u6807\u662f\u5426\u771f\u6b63\u6355\u6349\u4e86\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u5bf9\u6709\u6548\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e09\u7c7b\u63a8\u7406\u6b65\u9aa4\u95f4\u56e0\u679c\u5173\u7cfb\u6270\u52a8\uff0c\u7cfb\u7edf\u6027\u5730\u7834\u574f\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u6d41\u7545\u6027\u3002\u4f7f\u7528\u786c\u6ce8\u610f\u529b\u63a9\u7801\u7b49\u4e25\u91cd\u5e72\u9884\u63aa\u65bd\uff0c\u9632\u6b62\u6a21\u578b\u5173\u6ce8\u5148\u524d\u63a8\u7406\u6b65\u9aa4\u3002\u6700\u540e\u63d0\u51fa\u5bf9\u6bd4\u56e0\u679c\u5ea6\u91cf\u65b9\u6cd5\uff0c\u660e\u786e\u5206\u79bb\u6b65\u9aa4\u95f4\u56e0\u679c\u4f9d\u8d56\u3002", "result": "\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u4e25\u91cd\u7834\u574f\u63a8\u7406\u6b65\u9aa4\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u9009\u62e9\u51c6\u786e\u7387\u4ec5\u8f7b\u5fae\u4e0b\u964d\u3002\u786c\u6ce8\u610f\u529b\u63a9\u7801\u7b49\u4e25\u91cd\u5e72\u9884\u63aa\u65bd\u4e5f\u672a\u80fd\u663e\u8457\u964d\u4f4e\u9009\u62e9\u6027\u80fd\u3002", "conclusion": "\u73b0\u6709\u6982\u7387\u6307\u6807\u5bf9\u903b\u8f91\u7ed3\u6784\u4e0d\u654f\u611f\uff0c\u4e3b\u8981\u6355\u6349\u8868\u9762\u6d41\u7545\u6027\u6216\u5206\u5e03\u5148\u9a8c\u3002\u63d0\u51fa\u7684\u5bf9\u6bd4\u56e0\u679c\u5ea6\u91cf\u65b9\u6cd5\u80fd\u66f4\u5fe0\u5b9e\u5730\u8fdb\u884c\u8f93\u51fa\u9009\u62e9\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6982\u7387\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.12974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12974", "abs": "https://arxiv.org/abs/2601.12974", "authors": ["Hongyang Ma", "Tiantian Gu", "Huaiyuan Sun", "Huilin Zhu", "Yongxin Wang", "Jie Li", "Wubin Sun", "Zeliang Lian", "Yinghong Zhou", "Yi Gao", "Shirui Wang", "Zhihui Tang"], "title": "Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios", "comment": "29 pages, 15 figures", "summary": "The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping \"Guideline Adherence\" versus \"Decision Quality\" reveals a prevalent \"High Efficacy, Low Safety\" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SCMPE\u57fa\u51c6\uff0c\u8bc4\u4f30\u7259\u79d1LLMs\u4ece\u9759\u6001\u77e5\u8bc6\u4efb\u52a1\u5230\u52a8\u6001\u4e34\u5e8a\u5bf9\u8bdd\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u52a8\u6001\u4e34\u5e8a\u4e92\u52a8\u4e2d\u5b58\u5728\u74f6\u9888\uff0cRAG\u5728\u9759\u6001\u4efb\u52a1\u4e2d\u6709\u6548\u4f46\u5728\u52a8\u6001\u5de5\u4f5c\u6d41\u4e2d\u6548\u679c\u6709\u9650\u3002", "motivation": "\u968f\u7740LLMs\u4ece\u88ab\u52a8\u77e5\u8bc6\u68c0\u7d22\u5668\u5411\u81ea\u4e3b\u4e34\u5e8a\u4ee3\u7406\u8f6c\u53d8\uff0c\u9700\u8981\u4ece\u9759\u6001\u51c6\u786e\u6027\u8bc4\u4f30\u8f6c\u5411\u52a8\u6001\u884c\u4e3a\u53ef\u9760\u6027\u8bc4\u4f30\u3002\u7259\u79d1\u9886\u57df\u7684\u9ad8\u8d28\u91cfAI\u5efa\u8bae\u5bf9\u60a3\u8005\u53c2\u4e0e\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u63a2\u7d22LLMs\u5728\u8be5\u9886\u57df\u7684\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u4e34\u5e8a\u7ba1\u7406\u4e0e\u6027\u80fd\u8bc4\u4f30\uff08SCMPE\uff09\u57fa\u51c6\uff0c\u5168\u9762\u8bc4\u4f30\u4ece\u77e5\u8bc6\u5bfc\u5411\u8bc4\u4f30\uff08\u9759\u6001\u5ba2\u89c2\u4efb\u52a1\uff09\u5230\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u6a21\u62df\uff08\u591a\u8f6e\u6a21\u62df\u60a3\u8005\u4e92\u52a8\uff09\u7684\u6027\u80fd\u3002\u5206\u6790\u6a21\u578b\u5728\u9759\u6001\u4efb\u52a1\u548c\u52a8\u6001\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u91cf\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u9759\u6001\u5ba2\u89c2\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u719f\u7ec3\u5ea6\uff0c\u4f46\u5728\u52a8\u6001\u4e34\u5e8a\u5bf9\u8bdd\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e3b\u8981\u74f6\u9888\u4e0d\u5728\u4e8e\u77e5\u8bc6\u4fdd\u7559\uff0c\u800c\u5728\u4e8e\u4e3b\u52a8\u4fe1\u606f\u6536\u96c6\u548c\u52a8\u6001\u72b6\u6001\u8ddf\u8e2a\u7684\u6311\u6218\u3002\u6620\u5c04\"\u6307\u5357\u9075\u5faa\"\u4e0e\"\u51b3\u7b56\u8d28\u91cf\"\u663e\u793a\u901a\u7528\u6a21\u578b\u5b58\u5728\"\u9ad8\u6548\u6027\u3001\u4f4e\u5b89\u5168\u6027\"\u98ce\u9669\u3002RAG\u5728\u9759\u6001\u4efb\u52a1\u4e2d\u51cf\u8f7b\u5e7b\u89c9\uff0c\u4f46\u5728\u52a8\u6001\u5de5\u4f5c\u6d41\u4e2d\u6548\u679c\u6709\u9650\u4e14\u5f02\u8d28\uff0c\u6709\u65f6\u751a\u81f3\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u5916\u90e8\u77e5\u8bc6\u672c\u8eab\u65e0\u6cd5\u5f25\u8865\u63a8\u7406\u5dee\u8ddd\uff0c\u9700\u8981\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u3002\u8be5\u7814\u7a76\u5b9e\u8bc1\u7ed8\u5236\u4e86\u7259\u79d1LLMs\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u5f25\u5408\u6807\u51c6\u5316\u77e5\u8bc6\u4e0e\u5b89\u5168\u3001\u81ea\u4e3b\u4e34\u5e8a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2601.12326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12326", "abs": "https://arxiv.org/abs/2601.12326", "authors": ["Jing Zhang", "Bingjie Fan"], "title": "EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation", "comment": "11pages,10figures", "summary": "Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.", "AI": {"tldr": "EmoKGEdit\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5173\u8054\u77e5\u8bc6\u56fe\u8c31\u7684\u8bad\u7ec3\u514d\u8d39\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u7cbe\u786e\u5206\u79bb\u60c5\u611f\u4e0e\u5185\u5bb9\uff0c\u4fdd\u6301\u89c6\u89c9\u7ed3\u6784", "motivation": "\u73b0\u6709\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\u65b9\u6cd5\u96be\u4ee5\u4ece\u6f5c\u5728\u5185\u5bb9\u8868\u793a\u4e2d\u89e3\u8026\u60c5\u611f\u7ebf\u7d22\uff0c\u5bfc\u81f4\u60c5\u611f\u8868\u8fbe\u5f31\u4e14\u89c6\u89c9\u7ed3\u6784\u626d\u66f2\u3002\u9700\u8981\u89e3\u51b3\u60c5\u611f\u4e0e\u5185\u5bb9\u5206\u79bb\u95ee\u9898\uff0c\u5b9e\u73b0\u7cbe\u786e\u4e14\u4fdd\u6301\u7ed3\u6784\u7684\u60c5\u611f\u7f16\u8f91\u3002", "method": "1. \u6784\u5efa\u591a\u6a21\u6001\u60c5\u611f\u5173\u8054\u77e5\u8bc6\u56fe\u8c31\uff08MSA-KG\uff09\uff0c\u89e3\u8026\u5bf9\u8c61\u3001\u573a\u666f\u3001\u5c5e\u6027\u3001\u89c6\u89c9\u7ebf\u7d22\u548c\u60c5\u611f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff1b2. MSA-KG\u7f16\u7801\u5bf9\u8c61-\u5c5e\u6027-\u60c5\u611f\u56e0\u679c\u94fe\uff0c\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\u652f\u6301\u601d\u7ef4\u94fe\u63a8\u7406\uff1b3. \u8bbe\u8ba1\u89e3\u8026\u7ed3\u6784-\u60c5\u611f\u7f16\u8f91\u6a21\u5757\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u660e\u786e\u5206\u79bb\u60c5\u611f\u5c5e\u6027\u548c\u5e03\u5c40\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEmoKGEdit\u5728\u60c5\u611f\u4fdd\u771f\u5ea6\u548c\u5185\u5bb9\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "EmoKGEdit\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u89e3\u8026\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u7ed3\u6784\u4fdd\u6301\u7684\u56fe\u50cf\u60c5\u611f\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u60c5\u611f\u4e0e\u5185\u5bb9\u7ea0\u7f20\u95ee\u9898\u3002"}}
{"id": "2601.13752", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13752", "abs": "https://arxiv.org/abs/2601.13752", "authors": ["Chak Tou Leong", "Dingwei Chen", "Heming Xia", "Qingyu Yin", "Sunbowen Lee", "Jian Wang", "Wenjie Li"], "title": "Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering", "comment": "Working in progress", "summary": "Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \\textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.", "AI": {"tldr": "RELIEF\u6846\u67b6\u901a\u8fc7\u8c03\u6574\u5927\u63a8\u7406\u6a21\u578b\u7684\u81ea\u6211\u8ba4\u77e5\u4fe1\u5ff5\u6765\u5851\u9020\u5176\u884c\u4e3a\uff0c\u65e0\u9700\u63a8\u7406\u8f68\u8ff9\u76d1\u7763\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c", "motivation": "\u73b0\u6709\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u548c\u63a8\u7406\u4e0d\u5fe0\u5b9e\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u6216\u9ec4\u91d1\u63a8\u7406\u8f68\u8ff9\u5fae\u8c03\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55", "method": "\u63d0\u51faRELIEF\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u5355logit\u63a2\u6d4b\u6355\u83b7\u6a21\u578b\u7684\u6f5c\u5728\u63a8\u7406\u4fe1\u5ff5\uff0c\u7136\u540e\u5fae\u8c03\u6a21\u578b\u4f7f\u5176\u81ea\u6211\u8ba4\u77e5\u4e0e\u76ee\u6807\u4fe1\u5ff5\u84dd\u56fe\u5bf9\u9f50\uff0c\u4f7f\u7528\u5408\u6210\u7684\u81ea\u6211\u53cd\u601d\u95ee\u7b54\u5bf9\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u6548\u7387\u548c\u5fe0\u5b9e\u6027\u4efb\u52a1\u4e0a\uff0cRELIEF\u5339\u914d\u6216\u4f18\u4e8e\u884c\u4e3a\u76d1\u7763\u548c\u57fa\u4e8e\u504f\u597d\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u8bad\u7ec3\u6210\u672c\u66f4\u4f4e\uff1b\u5206\u6790\u8bc1\u5b9e\u6539\u53d8\u6a21\u578b\u63a8\u7406\u4fe1\u5ff5\u80fd\u6709\u6548\u5851\u9020\u5176\u5b9e\u9645\u884c\u4e3a", "conclusion": "\u5927\u63a8\u7406\u6a21\u578b\u5177\u6709\u53ef\u63a2\u6d4b\u7684\u63a8\u7406\u4fe1\u5ff5\uff0c\u901a\u8fc7\u4fe1\u5ff5\u5de5\u7a0b\u800c\u975e\u63a8\u7406\u8f68\u8ff9\u76d1\u7763\u6765\u5851\u9020\u6a21\u578b\u884c\u4e3a\u662f\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b0\u8303\u5f0f"}}
{"id": "2601.12979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12979", "abs": "https://arxiv.org/abs/2601.12979", "authors": ["Qingyu Lu", "Liang Ding", "Kanjian Zhang", "Jinxia Zhang", "Dacheng Tao"], "title": "The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check", "comment": "Under Review", "summary": "The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a \"bitter lesson\": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5728\u5b9e\u65f6\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u6548\u7387\u4f18\u52bf\u672a\u80fd\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u5728\u5177\u8eab\u667a\u80fd\u4f53\u548c\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u673a\u5236\u624d\u80fd\u53d1\u6325\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u4f5c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63a2\u7d22\u5176\u5728\u5b9e\u65f6\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u9a8c\u8bc1\u6548\u7387\u4f18\u52bf\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "method": "\u5728Agentboard\u548cBFCL\u57fa\u51c6\u4e0a\u5168\u9762\u8bc4\u4f30dLLMs\uff08\u5982LLaDA\u3001Dream\uff09\u5728\u4e24\u79cd\u667a\u80fd\u4f53\u8303\u5f0f\u4e2d\u7684\u8868\u73b0\uff1a\u5177\u8eab\u667a\u80fd\u4f53\uff08\u9700\u8981\u957f\u65f6\u7a0b\u89c4\u5212\uff09\u548c\u5de5\u5177\u8c03\u7528\u667a\u80fd\u4f53\uff08\u9700\u8981\u7cbe\u786e\u683c\u5f0f\uff09\u3002\u5f15\u5165DiffuAgent\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06dLLMs\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u8ba4\u77e5\u6838\u5fc3\u96c6\u6210\u5230\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u3002", "result": "dLLMs\u672a\u80fd\u6210\u4e3a\u53ef\u9760\u7684\u667a\u80fd\u4f53\u9aa8\u5e72\u6a21\u578b\uff0c\u7ecf\u5e38\u5bfc\u81f4\u7cfb\u7edf\u6027\u5931\u8d25\uff1a(1) \u5728\u5177\u8eab\u73af\u5883\u4e2d\uff0cdLLMs\u53cd\u590d\u5c1d\u8bd5\u5931\u8d25\uff0c\u65e0\u6cd5\u5728\u65f6\u95f4\u53cd\u9988\u4e0b\u8fdb\u884c\u5206\u652f\u51b3\u7b56\uff1b(2) \u5728\u5de5\u5177\u8c03\u7528\u73af\u5883\u4e2d\uff0cdLLMs\u5728\u6269\u6563\u566a\u58f0\u4e0b\u65e0\u6cd5\u4fdd\u6301\u7b26\u53f7\u7cbe\u5ea6\uff08\u5982\u4e25\u683c\u7684JSON\u6a21\u5f0f\uff09\u3002dLLMs\u5728\u975e\u56e0\u679c\u89d2\u8272\uff08\u5982\u8bb0\u5fc6\u603b\u7ed3\u548c\u5de5\u5177\u9009\u62e9\uff09\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u8981\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6574\u5408\u56e0\u679c\u3001\u7cbe\u786e\u548c\u903b\u8f91\u57fa\u7840\u63a8\u7406\u673a\u5236\u624d\u80fd\u9002\u7528\u4e8e\u667a\u80fd\u4f53\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524ddLLMs\u4e0d\u9002\u5408\u4f5c\u4e3a\u667a\u80fd\u4f53\u9aa8\u5e72\u6a21\u578b\uff0c\u5176\u6548\u7387\u4f18\u52bf\u672a\u80fd\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u3002dLLMs\u5728\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4e0e\u56e0\u679c\u63a8\u7406\u673a\u5236\u7ed3\u5408\uff0c\u7279\u522b\u662f\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6574\u5408\u7cbe\u786e\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.12329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12329", "abs": "https://arxiv.org/abs/2601.12329", "authors": ["Mithlesh Singla", "Seema Kumari", "Shanmuganathan Raman"], "title": "FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching", "comment": null, "summary": "Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.", "AI": {"tldr": "\u63d0\u51faFlowIID\uff0c\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u65b9\u6cd5\uff0c\u53c2\u6570\u9ad8\u6548\u4e14\u5355\u6b65\u63a8\u7406\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u548c\u5b9e\u65f6\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709IID\u6a21\u578b\u867d\u7136\u6548\u679c\u597d\u4f46\u53c2\u6570\u91cf\u5927\uff0c\u96be\u4ee5\u4e0e\u5176\u4ed6\u6a21\u578b\u7ed3\u5408\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u53c2\u6570\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8e\u6f5c\u5728\u6d41\u5339\u914d\u8bbe\u8ba1FlowIID\u67b6\u6784\uff0c\u7ed3\u5408VAE\u5f15\u5bfc\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u6d41\u5339\u914d\u6a21\u5757\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u53cd\u7167\u7387\u548c\u9634\u5f71\u5206\u89e3", "result": "FlowIID\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u548c\u4f18\u8d8a\u7684\u7ed3\u679c\uff0c\u5c3d\u7ba1\u8bbe\u8ba1\u7d27\u51d1\u4f46\u6027\u80fd\u51fa\u8272", "conclusion": "FlowIID\u53c2\u6570\u9ad8\u6548\u3001\u5355\u6b65\u63a8\u7406\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u548c\u5b9e\u65f6\u89c6\u89c9\u5e94\u7528\u90e8\u7f72"}}
{"id": "2601.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13761", "abs": "https://arxiv.org/abs/2601.13761", "authors": ["Shengda Fan", "Xuyan Ye", "Yankai Lin"], "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution", "comment": null, "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.", "AI": {"tldr": "DARC\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u81ea\u6f14\u5316\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u96be\u5ea6\u6821\u51c6\u95ee\u9898\u751f\u6210\u548c\u975e\u5bf9\u79f0\u81ea\u84b8\u998f\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u81ea\u6f14\u5316\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534710.9\u5206\u3002", "motivation": "\u73b0\u6709\u81ea\u6f14\u5316\u7684\u6846\u67b6\u5b58\u5728\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\uff1a(1) \u6c42\u89e3\u5668\u4f9d\u8d56\u7684\u5956\u52b1\u53cd\u9988\u5bfc\u81f4\u63d0\u95ee\u8005\u7684\u76ee\u6807\u975e\u5e73\u7a33\uff1b(2) \u81ea\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u7528\u4e8e\u76d1\u7763\u6c42\u89e3\u5668\u65f6\u5b58\u5728\u81ea\u4e3e\u8bef\u5dee\u3002", "method": "DARC\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u63d0\u95ee\u8005\u57fa\u4e8e\u660e\u786e\u96be\u5ea6\u7ea7\u522b\u548c\u5916\u90e8\u8bed\u6599\u5408\u6210\u96be\u5ea6\u6821\u51c6\u7684\u95ee\u9898\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u975e\u5bf9\u79f0\u81ea\u84b8\u998f\u673a\u5236\uff0c\u8ba9\u6587\u6863\u589e\u5f3a\u7684\u6559\u5e08\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u6765\u76d1\u7763\u65e0\u6587\u6863\u8bbf\u95ee\u7684\u5b66\u751f\u6c42\u89e3\u5668\u3002", "result": "DARC\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u57289\u4e2a\u63a8\u7406\u57fa\u51c6\u548c3\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u5e73\u5747\u63d0\u534710.9\u5206\uff0c\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u4e14\u65e0\u9700\u4eba\u7c7b\u6807\u6ce8\u5c31\u80fd\u63a5\u8fd1\u5168\u76d1\u7763\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "DARC\u901a\u8fc7\u89e3\u8026\u548c\u975e\u5bf9\u79f0\u81ea\u84b8\u998f\u673a\u5236\u6709\u6548\u7a33\u5b9a\u4e86\u81ea\u6f14\u5316\u8fc7\u7a0b\uff0c\u4e3a\u81ea\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12983", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12983", "abs": "https://arxiv.org/abs/2601.12983", "authors": ["Jesus-German Ortiz-Barajas", "Jonathan Tonglet", "Vivek Gupta", "Iryna Gurevych"], "title": "ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation", "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.", "AI": {"tldr": "ChartAttack\u6846\u67b6\u8bc4\u4f30MLLMs\u751f\u6210\u8bef\u5bfc\u6027\u56fe\u8868\u7684\u98ce\u9669\uff0c\u901a\u8fc7\u6ce8\u5165\u8bef\u5bfc\u5143\u7d20\u964d\u4f4e\u56fe\u8868\u89e3\u8bfb\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u663e\u793aQA\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u88ab\u5e7f\u6cdb\u7528\u4e8e\u4ece\u6570\u636e\u8868\u81ea\u52a8\u751f\u6210\u56fe\u8868\uff0c\u867d\u7136\u63d0\u9ad8\u4e86\u6570\u636e\u5206\u6790\u6548\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u6ee5\u7528\u98ce\u9669\u3002\u9700\u8981\u8bc4\u4f30MLLMs\u5982\u4f55\u88ab\u6ee5\u7528\u6765\u5927\u89c4\u6a21\u751f\u6210\u8bef\u5bfc\u6027\u56fe\u8868\u3002", "method": "\u63d0\u51faChartAttack\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u56fe\u8868\u8bbe\u8ba1\u4e2d\u6ce8\u5165\u8bef\u5bfc\u5143\u7d20\u6765\u8bf1\u5bfc\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u9519\u8bef\u89e3\u8bfb\u3002\u540c\u65f6\u521b\u5efaAttackViz\u6570\u636e\u96c6\uff0c\u5305\u542b\u56fe\u8868\u89c4\u8303\u3001QA\u5bf9\u4ee5\u53ca\u6709\u6548\u7684\u8bef\u5bfc\u5143\u7d20\u548c\u5176\u8bf1\u5bfc\u7684\u9519\u8bef\u7b54\u6848\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u9886\u57df\u5185\u548c\u8de8\u9886\u57df\u8bbe\u7f6e\u4e2d\uff0cChartAttack\u663e\u8457\u964d\u4f4e\u4e86MLLM\u9605\u8bfb\u5668\u7684QA\u6027\u80fd\uff0c\u51c6\u786e\u7387\u5206\u522b\u5e73\u5747\u4e0b\u964d19.6\u5206\u548c14.9\u5206\u3002\u4eba\u7c7b\u7814\u7a76\u663e\u793a\uff0c\u53c2\u4e0e\u8005\u9762\u5bf9ChartAttack\u751f\u6210\u7684\u8bef\u5bfc\u6027\u56fe\u8868\u65f6\uff0c\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d20.2\u5206\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728MLLM-based\u56fe\u8868\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u4e2d\uff0c\u8feb\u5207\u9700\u8981\u52a0\u5f3a\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u8003\u8651\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.12337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12337", "abs": "https://arxiv.org/abs/2601.12337", "authors": ["Jiahui Sheng", "Xiaorun Li", "Shuhan Chen"], "title": "Turbo-GoDec: Exploiting the Cluster Sparsity Prior for Hyperspectral Anomaly Detection", "comment": null, "summary": "As a key task in hyperspectral image processing, hyperspectral anomaly detection has garnered significant attention and undergone extensive research. Existing methods primarily relt on two prior assumption: low-rank background and sparse anomaly, along with additional spatial assumptions of the background. However, most methods only utilize the sparsity prior assumption for anomalies and rarely expand on this hypothesis. From observations of hyperspectral images, we find that anomalous pixels exhibit certain spatial distribution characteristics: they often manifest as small, clustered groups in space, which we refer to as cluster sparsity of anomalies. Then, we combined the cluster sparsity prior with the classical GoDec algorithm, incorporating the cluster sparsity prior into the S-step of GoDec. This resulted in a new hyperspectral anomaly detection method, which we called Turbo-GoDec. In this approach, we modeled the cluster sparsity prior of anomalies using a Markov random field and computed the marginal probabilities of anomalies through message passing on a factor graph. Locations with high anomalous probabilities were treated as the sparse component in the Turbo-GoDec. Experiments are conducted on three real hyperspectral image (HSI) datasets which demonstrate the superior performance of the proposed Turbo-GoDec method in detecting small-size anomalies comparing with the vanilla GoDec (LSMAD) and state-of-the-art anomaly detection methods. The code is available at https://github.com/jiahuisheng/Turbo-GoDec.", "AI": {"tldr": "\u63d0\u51faTurbo-GoDec\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f02\u5e38\u70b9\u7684\u805a\u7c7b\u7a00\u758f\u6027\u5148\u9a8c\u6539\u8fdb\u4f20\u7edfGoDec\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b", "motivation": "\u73b0\u6709\u9ad8\u5149\u8c31\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u79e9\u80cc\u666f\u548c\u7a00\u758f\u5f02\u5e38\u5047\u8bbe\uff0c\u4f46\u5f88\u5c11\u6269\u5c55\u5f02\u5e38\u7684\u7a7a\u95f4\u5206\u5e03\u7279\u6027\u3002\u89c2\u5bdf\u5230\u5f02\u5e38\u50cf\u7d20\u5728\u7a7a\u95f4\u4e0a\u5e38\u5448\u73b0\u5c0f\u89c4\u6a21\u805a\u96c6\u5206\u5e03\uff0c\u5373\"\u805a\u7c7b\u7a00\u758f\u6027\"", "method": "\u5c06\u805a\u7c7b\u7a00\u758f\u6027\u5148\u9a8c\u7ed3\u5408\u5230GoDec\u7b97\u6cd5\u7684S-step\u4e2d\uff0c\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u5efa\u6a21\u5f02\u5e38\u805a\u7c7b\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u56e0\u5b50\u56fe\u4e0a\u7684\u6d88\u606f\u4f20\u9012\u8ba1\u7b97\u5f02\u5e38\u8fb9\u7f18\u6982\u7387\uff0c\u9ad8\u6982\u7387\u4f4d\u7f6e\u4f5c\u4e3aTurbo-GoDec\u7684\u7a00\u758f\u5206\u91cf", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u9ad8\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTurbo-GoDec\u5728\u5c0f\u5c3a\u5bf8\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfGoDec\uff08LSMAD\uff09\u548c\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684Turbo-GoDec\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u5f02\u5e38\u805a\u7c7b\u7a00\u758f\u6027\u5148\u9a8c\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u5c0f\u5c3a\u5bf8\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2601.13770", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-fin.CP", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2601.13770", "abs": "https://arxiv.org/abs/2601.13770", "authors": ["Mostapha Benhenda"], "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance", "comment": null, "summary": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench", "AI": {"tldr": "\u63d0\u51fa\u4e86Look-Ahead-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u91d1\u878dLLM\u4e2d\u7684\u524d\u77bb\u6027\u504f\u5dee\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5e02\u573a\u5468\u671f\u4e2d\u7684\u6027\u80fd\u8870\u51cf\u6765\u533a\u5206\u771f\u5b9e\u9884\u6d4b\u80fd\u529b\u548c\u8bb0\u5fc6\u6027\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u95ee\u7b54\u6d4b\u8bd5LLM\u7684\u5185\u90e8\u524d\u77bb\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u91d1\u878d\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u6a21\u578b\u884c\u4e3a\u7684\u8bc4\u4f30\u3002\u9700\u8981\u533a\u5206\u6a21\u578b\u7684\u771f\u5b9e\u9884\u6d4b\u80fd\u529b\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u8868\u73b0\u3002", "method": "\u521b\u5efa\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u5e02\u573a\u5468\u671f\u4e2d\u7684\u6027\u80fd\u8870\u51cf\uff0c\u5f15\u5165\u591a\u4e2a\u91cf\u5316\u57fa\u7ebf\u5efa\u7acb\u6027\u80fd\u9608\u503c\u3002\u8bc4\u4f30\u5f00\u6e90LLM\u548cPiT-LLM\u7cfb\u5217\u6a21\u578b\u3002", "result": "\u6807\u51c6LLM\u663e\u793a\u51fa\u663e\u8457\u7684\u524d\u77bb\u6027\u504f\u5dee\uff08alpha\u8870\u51cf\uff09\uff0c\u800cPiTinf\u6a21\u578b\u968f\u7740\u89c4\u6a21\u6269\u5927\u5c55\u73b0\u51fa\u6539\u8fdb\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u4e3a\u91d1\u878dLLM\u4e2d\u65f6\u95f4\u504f\u5dee\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u8bc6\u522b\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u6a21\u578b\u7684\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2601.12995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12995", "abs": "https://arxiv.org/abs/2601.12995", "authors": ["Runxuan Liu", "Xianhao Ou", "Xinyan Ma", "Jiyuan Wang", "Jiafeng Liang", "Jiaqi Li", "Tao He", "Zheng Chu", "Rongchuan Mu", "Zekun Wang", "Baoxin Wang", "Dayong Wu", "Ming Liu", "Shijin Wang", "Guoping Hu", "Bing Qin"], "title": "Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models", "comment": null, "summary": "Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.", "AI": {"tldr": "\u63d0\u51faGraph Reasoning Paradigm (GRP)\u548cPASC-GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u63a8\u7406\u66ff\u4ee3\u4f20\u7edf\u6587\u672c\u63a8\u7406\uff0c\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u3001\u76d1\u7763\u7c97\u7c92\u5ea6\u3001\u5956\u52b1\u6b3a\u9a97\u7b49\u95ee\u9898", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u4e3b\u8981\u751f\u6210\u7eaf\u6587\u672c\uff0c\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u8bed\u4e49\u8bc4\u4f30\u9020\u6210\u8bad\u7ec3\u8ba1\u7b97\u74f6\u9888\u3002\u5c3d\u7ba1\u6709RLVR\u4f18\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u76d1\u7763\u7c97\u7c92\u5ea6\u3001\u5956\u52b1\u6b3a\u9a97\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898", "method": "\u63d0\u51faGraph Reasoning Paradigm (GRP)\u5b9e\u73b0\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\uff0c\u4f7f\u7528\u5e26\u6b65\u9aa4\u8ba4\u77e5\u6807\u7b7e\u7684\u56fe\u7ed3\u6784\u8868\u793a\u3002\u57fa\u4e8eGRP\u8bbe\u8ba1PASC-GRPO\u65b9\u6cd5\uff1a\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\u66ff\u4ee3\u8bed\u4e49\u8bc4\u4f30\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u7ed3\u679c\u5956\u52b1\u5b9e\u73b0\u8fc7\u7a0b\u611f\u77e5\u9a8c\u8bc1\uff0c\u91c7\u7528\u5206\u5c42\u88c1\u526a\u4f18\u52bf\u4f30\u8ba1\u7f13\u89e3\u5956\u52b1\u6b3a\u9a97", "result": "\u5b9e\u9a8c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u540e\u7eed\u53d1\u5e03", "conclusion": "GRP\u548cPASC-GRPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dLLM\u63a8\u7406\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387"}}
{"id": "2601.12346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12346", "abs": "https://arxiv.org/abs/2601.12346", "authors": ["Peizhou Huang", "Zixuan Zhong", "Zhongwei Wan", "Donghao Zhou", "Samiul Alam", "Xin Wang", "Zexin Li", "Zhihao Dou", "Li Zhu", "Jing Xiong", "Chaofan Tao", "Yan Xu", "Dimitrios Dimitriadis", "Tuo Zhang", "Mi Zhang"], "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "comment": null, "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMDR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b140\u4e2a\u8de821\u4e2a\u9886\u57df\u7684\u4e13\u5bb6\u8bbe\u8ba1\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u7aef\u5230\u7aef\u8bc1\u636e\u4f7f\u7528\u548c\u5f15\u7528\u62a5\u544a\u751f\u6210\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e09\u4e2a\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9\u7eaf\u6587\u672c\u8bbe\u7f6e\u6216\u77ed\u683c\u5f0f\u591a\u6a21\u6001\u95ee\u7b54\uff0c\u7f3a\u4e4f\u5bf9\u7aef\u5230\u7aef\u591a\u6a21\u6001\u8bc1\u636e\u4f7f\u7528\u7684\u8bc4\u4f30\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u5f15\u7528\u62a5\u544a\u751f\u6210\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b140\u4e2a\u4efb\u52a1\u7684MMDR-Bench\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u56fe\u50cf-\u6587\u672c\u6346\u7ed1\u6570\u636e\uff1b\u63d0\u51fa\u4e86\u4e09\u4e2a\u8bc4\u4f30\u7ec4\u4ef6\uff1aFLAE\u7528\u4e8e\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\uff0cTRACE\u7528\u4e8e\u5f15\u7528\u8bc1\u636e\u5bf9\u9f50\u8bc4\u4f30\uff0cMOSAIC\u7528\u4e8e\u6587\u672c-\u89c6\u89c9\u5b8c\u6574\u6027\u68c0\u67e5\u3002", "result": "\u5bf925\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u751f\u6210\u8d28\u91cf\u3001\u5f15\u7528\u7eaa\u5f8b\u548c\u591a\u6a21\u6001\u57fa\u7840\u4e4b\u95f4\u7684\u7cfb\u7edf\u6743\u8861\uff0c\u8868\u660e\u4ec5\u51ed\u5f3a\u5927\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0d\u80fd\u4fdd\u8bc1\u5fe0\u5b9e\u7684\u8bc1\u636e\u4f7f\u7528\uff0c\u591a\u6a21\u6001\u5b8c\u6574\u6027\u4ecd\u7136\u662f\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u74f6\u9888\u3002", "conclusion": "MMDR-Bench\u586b\u8865\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bc1\u636e\u4f7f\u7528\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2601.13846", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13846", "abs": "https://arxiv.org/abs/2601.13846", "authors": ["Glinskaya Maria"], "title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments", "comment": null, "summary": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.", "AI": {"tldr": "Virtual Urbanism (VU) \u662f\u4e00\u4e2a\u591a\u6a21\u6001AI\u9a71\u52a8\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u57ce\u5e02\u590d\u5236\u54c1\u6765\u91cf\u5316\u57ce\u5e02\u8eab\u4efd\uff0c\u5728\u4e1c\u4eac\u4e5d\u4e2a\u533a\u57df\u7684\u8bd5\u70b9\u7814\u7a76\u4e2d\u5b9e\u73b0\u4e86\u7ea681%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5f00\u53d1\u8ba1\u7b97\u4e0a\u53ef\u5904\u7406\u7684\u91cf\u5316\u57ce\u5e02\u8eab\u4efd\u6307\u6807\uff0c\u63a8\u8fdbAI\u589e\u5f3a\u7684\u57ce\u5e02\u5206\u6790\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u591a\u53c2\u6570\u8eab\u4efd\u5ea6\u91cf\u3002", "method": "\u6574\u5408Stable Diffusion\u548cLoRA\u6a21\u578b\u751f\u6210\u4e1c\u4eac\u4e5d\u4e2a\u533a\u57df\u7684\u5408\u6210\u57ce\u5e02\u5e8f\u5217\uff0c\u6392\u9664\u73b0\u6709\u5bfc\u5411\u6807\u8bb0\u4ee5\u63d0\u53d6\u6838\u5fc3\u8eab\u4efd\u5f62\u6210\u5143\u7d20\uff0c\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u9a8c\u9a8c\u8bc1\u611f\u77e5\u5408\u6cd5\u6027\u3001\u91cf\u5316\u533a\u57df\u7ea7\u8eab\u4efd\u3001\u63a8\u5bfc\u6838\u5fc3\u8eab\u4efd\u5f62\u6210\u5143\u7d20\u3002", "result": "\u5408\u6210\u590d\u5236\u54c1\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u5230\u7ea681%\uff0c\u9a8c\u8bc1\u4e86\u590d\u5236\u54c1\u7684\u6709\u6548\u6027\uff1b\u57ce\u5e02\u8eab\u4efd\u6c34\u5e73\uff08UIL\uff09\u6307\u6807\u80fd\u591f\u8bc4\u4f30\u4e0d\u540c\u533a\u57df\u7684\u8eab\u4efd\u6c34\u5e73\uff1b\u8bed\u4e49\u5206\u6790\u63ed\u793a\u4e86\u6587\u5316\u5d4c\u5165\u7684\u7c7b\u578b\u5b66\u4f5c\u4e3a\u6838\u5fc3\u8eab\u4efd\u5f62\u6210\u5143\u7d20\u3002", "conclusion": "Virtual Urbanism\u662f\u4e00\u4e2a\u53ef\u884c\u7684AI\u589e\u5f3a\u57ce\u5e02\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u591a\u53c2\u6570\u8eab\u4efd\u6307\u6807\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.13018", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13018", "abs": "https://arxiv.org/abs/2601.13018", "authors": ["Ghislain Dorian Tchuente Mondjo"], "title": "Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context", "comment": "Accepted at \"EAI AFRICOMM 2025 - 17th EAI International Conference on Communications and Networks in Africa\"", "summary": "Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.", "AI": {"tldr": "\u63d0\u51faBiAtt-BiRNN-HateXplain\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u548cBiRNN\u5c42\u6539\u8fdb\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u51cf\u5c11\u65e0\u610f\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u5b58\u5728\u89e3\u91ca\u4e0d\u4e00\u81f4\u95ee\u9898\uff1a\u57fa\u4e8eHateXplain\u57fa\u51c6\u7684\u591a\u4efb\u52a1\u65b9\u6cd5\u4e2d\uff0c\u9884\u6d4b\u6ce8\u610f\u529b\u53d8\u5316\u8f83\u5927\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u9884\u6d4b\u4e0d\u7a33\u5b9a\u548c\u5b66\u4e60\u56f0\u96be\u3002\u9700\u8981\u66f4\u900f\u660e\u3001\u80fd\u5904\u7406\u5e8f\u5217\u6570\u636e\u89e3\u91ca\u6027\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u6ce8\u610f\u529bBiRNN HateXplain\u6a21\u578b\uff1a\u7ed3\u5408\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u548cBiRNN\u5c42\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u540c\u65f6\u5904\u7406\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u8003\u8651\u8f93\u5165\u6570\u636e\u7684\u5e8f\u5217\u7279\u6027\uff0c\u63d0\u9ad8\u89e3\u91ca\u51c6\u786e\u6027\u3002", "result": "\u5728HateXplain\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u68c0\u6d4b\u6027\u80fd\u660e\u663e\u63d0\u5347\uff0c\u89e3\u91ca\u6027\u6539\u5584\uff0c\u65e0\u610f\u504f\u89c1\u51cf\u5c11\u3002", "conclusion": "BiAtt-BiRNN-HateXplain\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u89e3\u91ca\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u504f\u89c1\uff0c\u76f8\u6bd4\u590d\u6742LLM\u66f4\u6613\u4e8e\u89e3\u91ca\u3002"}}
{"id": "2601.12357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12357", "abs": "https://arxiv.org/abs/2601.12357", "authors": ["Hailing Jin", "Huiying Li"], "title": "SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence", "comment": null, "summary": "Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.", "AI": {"tldr": "SimpleMatch\uff1a\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u8bed\u4e49\u5bf9\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e0a\u91c7\u6837\u89e3\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u76d1\u7763\u635f\u5931\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1151%\u8bad\u7ec3\u5185\u5b58\u4f7f\u7528", "motivation": "\u73b0\u6709\u8bed\u4e49\u5bf9\u5e94\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u548c\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u6df1\u5c42\u4e0b\u91c7\u6837\u64cd\u4f5c\u4f1a\u5bfc\u81f4\u76f8\u90bb\u5173\u952e\u70b9\u7279\u5f81\u4e0d\u53ef\u9006\u878d\u5408\uff0c\u5f53\u8bed\u4e49\u4e0d\u540c\u7684\u5173\u952e\u70b9\u843d\u5165\u540c\u4e00\u611f\u53d7\u91ce\u65f6\u6027\u80fd\u53d7\u9650", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u4e0a\u91c7\u6837\u89e3\u7801\u5668\u9010\u6b65\u6062\u590d\u7a7a\u95f4\u7ec6\u8282\u81f31/4\u5206\u8fa8\u7387\uff0c\u591a\u5c3a\u5ea6\u76d1\u7763\u635f\u5931\u786e\u4fdd\u4e0a\u91c7\u6837\u7279\u5f81\u4fdd\u7559\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5224\u522b\u7279\u5f81\uff0c\u7a00\u758f\u5339\u914d\u548c\u7a97\u53e3\u5b9a\u4f4d\u4f18\u5316\u8bad\u7ec3\u5185\u5b58\u4f7f\u7528", "result": "\u5728252x252\u5206\u8fa8\u7387\uff08\u6bd4\u5f53\u524dSOTA\u5c0f3.3\u500d\uff09\u4e0b\uff0c\u5728SPair-71k\u57fa\u51c6\u4e0a\u8fbe\u523084.1% PCK@0.1\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u8bad\u7ec3\u5185\u5b58\u51cf\u5c1151%", "conclusion": "SimpleMatch\u4e3a\u8bed\u4e49\u5bf9\u5e94\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u5f3a\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6df1\u5c42\u4e0b\u91c7\u6837\u5bfc\u81f4\u7684\u5173\u952e\u70b9\u7279\u5f81\u878d\u5408\u95ee\u9898"}}
{"id": "2601.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13880", "abs": "https://arxiv.org/abs/2601.13880", "authors": ["Ye Tian", "Zihao Wang", "Onat Gungor", "Xiaoran Fan", "Tajana Rosing"], "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health", "comment": null, "summary": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.", "AI": {"tldr": "\u63d0\u51fa\u4e86LifeAgentBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u957f\u671f\u8de8\u7ef4\u5ea6\u5065\u5eb7\u751f\u6d3b\u65b9\u5f0f\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u5305\u542b22,573\u4e2a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86LifeAgent\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u4e2a\u6027\u5316\u6570\u5b57\u5065\u5eb7\u652f\u6301\u9700\u8981\u5bf9\u5f02\u6784\u751f\u6d3b\u65b9\u5f0f\u4fe1\u53f7\u8fdb\u884c\u957f\u671f\u8de8\u7ef4\u5ea6\u63a8\u7406\uff0c\u4f46\u5f53\u524dLLM\u5728\u6b64\u9886\u57df\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5", "method": "1) \u6784\u5efaLifeAgentBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b22,573\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u68c0\u7d22\u5230\u590d\u6742\u63a8\u7406\uff1b2) \u63d0\u51fa\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6784\u5efa\u6d41\u7a0b\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff1b3) \u63d0\u51faLifeAgent\u57fa\u7ebf\u65b9\u6cd5\uff0c\u96c6\u6210\u591a\u6b65\u8bc1\u636e\u68c0\u7d22\u4e0e\u786e\u5b9a\u6027\u805a\u5408", "result": "\u7cfb\u7edf\u8bc4\u4f30\u4e8611\u4e2a\u9886\u5148LLM\uff0c\u8bc6\u522b\u51fa\u957f\u671f\u805a\u5408\u548c\u8de8\u7ef4\u5ea6\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\uff1bLifeAgent\u65b9\u6cd5\u76f8\u6bd4\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u7ebf\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "LifeAgentBench\u4e3a\u8bc4\u4f30LLM\u5065\u5eb7\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0cLifeAgent\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u73b0\u5b9e\u751f\u6d3b\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e2a\u6027\u5316\u6570\u5b57\u5065\u5eb7\u652f\u6301\u7684\u53d1\u5c55"}}
{"id": "2601.13024", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13024", "abs": "https://arxiv.org/abs/2601.13024", "authors": ["Chongyuan Dai", "Yaling Shen", "Jinpeng Hu", "Zihan Gao", "Jia Li", "Yishun Jiang", "Yaxiong Wang", "Liu Liu", "Zongyuan Ge"], "title": "Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses", "comment": "24 pages, 10 figures, 9 Tables", "summary": "Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \\underline{\\textsc{E}}licited \\underline{\\textsc{D}}istinct \\underline{\\textsc{A}}ffective \\underline{\\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.", "AI": {"tldr": "CEDAR\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u6355\u6349\u6587\u5316\u5f15\u53d1\u7684\u4e0d\u540c\u60c5\u611f\u53cd\u5e94\uff0c\u8bc4\u4f30LLM\u5728\u8de8\u6587\u5316\u60c5\u611f\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u5bf9\u9f50\u7684\u60c5\u611f\u7406\u89e3\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u5730\u7406\u4e8b\u5b9e\u6216\u793e\u4f1a\u4e60\u4fd7\u7b49\u9648\u8ff0\u6027\u77e5\u8bc6\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u540c\u793e\u4f1a\u6587\u5316\u89c6\u89d2\u4e0b\u7684\u4e3b\u89c2\u89e3\u91ca\u5dee\u5f02\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u5728\u6587\u5316\u76f8\u5173\u60c5\u611f\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCEDAR\u57fa\u51c6\u6d4b\u8bd5\u6784\u5efa\u6d41\u7a0b\uff1a\u5229\u7528LLM\u751f\u6210\u4e34\u65f6\u6807\u7b7e\u6765\u8bc6\u522b\u8de8\u6587\u5316\u60c5\u611f\u5dee\u5f02\u5b9e\u4f8b\uff0c\u7136\u540e\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u5de5\u8bc4\u4f30\u83b7\u5f97\u53ef\u9760\u7684\u771f\u5b9e\u6807\u6ce8\uff0c\u5305\u542b7\u79cd\u8bed\u8a00\u300114\u79cd\u7ec6\u7c92\u5ea6\u60c5\u611f\u7c7b\u522b\u300110,962\u4e2a\u5b9e\u4f8b\u3002", "result": "\u5bf917\u4e2a\u4ee3\u8868\u6027\u591a\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8bed\u8a00\u4e00\u81f4\u6027\u4e0e\u6587\u5316\u5bf9\u9f50\u4e4b\u95f4\u5b58\u5728\u5206\u79bb\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u57fa\u4e8e\u6587\u5316\u7684\u60c5\u611f\u7406\u89e3\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "conclusion": "\u6587\u5316\u662f\u60c5\u611f\u5904\u7406\u7684\u57fa\u672c\u51b3\u5b9a\u56e0\u7d20\uff0c\u5f53\u524dLLM\u5728\u6587\u5316\u5bf9\u9f50\u7684\u60c5\u611f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cCEDAR\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u7684\u6587\u5316\u60c5\u611f\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.12358", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12358", "abs": "https://arxiv.org/abs/2601.12358", "authors": ["Omar Y. Goba", "Ahmed Y. Gado", "Catherine M. Elias", "Ahmed Hussein"], "title": "From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u548cLVM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u884c\u4e3a\u6811\u52a8\u6001\u751f\u6210\u4e0e\u9002\u5e94\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7684\u771f\u5b9e\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u6811\uff08BTs\uff09\u867d\u7136\u63d0\u4f9b\u7ed3\u6784\u5316\u51b3\u7b56\u903b\u8f91\uff0c\u4f46\u672c\u8d28\u4e0a\u662f\u9759\u6001\u7684\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u5176\u5728SAE Level 5\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u81ea\u9002\u5e94\u884c\u4e3a\u89c4\u5212\u5668\u6765\u5b89\u5168\u5bfc\u822a\u4e0d\u53ef\u9884\u6d4b\u7684\u771f\u5b9e\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09Descriptor\u4ee3\u7406\u4f7f\u7528\u7b26\u53f7\u94fe\u63d0\u793a\u8bc4\u4f30\u573a\u666f\u5173\u952e\u6027\uff1b2\uff09Planner\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6784\u5efa\u9ad8\u5c42\u5b50\u76ee\u6807\uff1b3\uff09Generator\u4ee3\u7406\u4ee5XML\u683c\u5f0f\u5408\u6210\u53ef\u6267\u884cBT\u5b50\u6811\u3002\u7cfb\u7edf\u96c6\u6210\u5230CARLA+Nav2\u4eff\u771f\u4e2d\uff0c\u4ec5\u5728\u57fa\u7ebfBT\u5931\u8d25\u65f6\u89e6\u53d1\u3002", "result": "\u5728CARLA+Nav2\u4eff\u771f\u4e2d\uff0c\u7cfb\u7edf\u6210\u529f\u5bfc\u822a\u7ed5\u8fc7\u610f\u5916\u969c\u788d\u7269\uff08\u5982\u8857\u9053\u5835\u585e\uff09\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002\u76f8\u6bd4\u9759\u6001BT\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u53ef\u6269\u5c55\u5230\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u5229\u7528LLM\u548cLVM\u52a8\u6001\u751f\u6210\u548c\u9002\u5e94\u884c\u4e3a\u6811\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u63d0\u4f9b\u81ea\u9002\u5e94\u884c\u4e3a\u89c4\u5212\uff0c\u5c55\u793a\u4e86\u5411SAE Level 5\u81ea\u4e3b\u6027\u6269\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.13887", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13887", "abs": "https://arxiv.org/abs/2601.13887", "authors": ["Hong Su"], "title": "Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.", "AI": {"tldr": "\u63d0\u51faHuman Simulation Computation (HSC)\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u667a\u80fd\u7684\u95ed\u73af\u8fc7\u7a0b\uff0c\u901a\u8fc7\u884c\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u589e\u5f3aLLMs\u5728\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b", "motivation": "\u5f53\u524dLLMs\u4ec5\u4f9d\u8d56\u6587\u672c\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f00\u653e\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3001\u63a8\u7406\u9a8c\u8bc1\u548c\u6709\u6548\u64cd\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faHSC\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u5efa\u6a21\u4e3a\u5305\u542b\u601d\u8003\u3001\u884c\u52a8\u3001\u5b66\u4e60\u3001\u53cd\u601d\u548c\u6d3b\u52a8\u8c03\u5ea6\u7684\u8fde\u7eed\u95ed\u73af\u8fc7\u7a0b\u3002\u5f3a\u8c03\u5728\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u548c\u4e0e\u73af\u5883\u4ea4\u4e92\u4e2d\u7684\u4e3b\u52a8\u53c2\u4e0e\uff0c\u884c\u52a8\u4e0d\u4ec5\u7528\u4e8e\u5b9e\u73b0\u76ee\u6807\uff0c\u8fd8\u7528\u4e8e\u81ea\u52a8\u6539\u8fdb\u5185\u90e8\u63a8\u7406\u673a\u5236\u3002\u6574\u5408\u4eba\u7c7b\u5e38\u7528\u601d\u7ef4\u7b56\u7565\uff0c\u5982\u4e3b\u7279\u5f81\u5bfc\u5411\u63a8\u7406\u3001\u901a\u8fc7\u884c\u52a8\u6269\u5c55\u8303\u56f4\u3001\u73af\u5883\u53cd\u9988\u9a71\u52a8\u7684\u5373\u65f6\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4eba\u7c7b\u6a21\u62df\u7b56\u7565\u65e0\u6cd5\u4ec5\u4ece\u8bed\u8a00\u6750\u6599\u4e2d\u5b8c\u5168\u5b66\u4e60\uff0c\u4eba\u7c7b\u5f0f\u63a8\u7406\u8fc7\u7a0b\u548c\u57fa\u4e8e\u884c\u52a8\u7684\u63a8\u7406\u65b9\u6cd5\u5bf9\u4e8e\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u9002\u5e94\u548c\u6709\u6548\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "HSC\u6846\u67b6\u4e3a\u89e3\u51b3LLMs\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5f3a\u8c03\u884c\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u5728\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5f3a\u5927\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.13035", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13035", "abs": "https://arxiv.org/abs/2601.13035", "authors": ["Xu Xiaodan", "Hu Xiaolin"], "title": "SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification", "comment": "in progress", "summary": "Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \\textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\\% on FB15k-237 and +3.4\\% on YAGO3-10.", "AI": {"tldr": "SASA\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u4e49\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u5e38\u5305\u542b\u4e0d\u53ef\u9760\u77e5\u8bc6\uff0c\u73b0\u6709\u4e09\u5143\u7ec4\u5206\u7c7b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5ffd\u7565\u4e0d\u540cKG\u7ec4\u4ef6\u95f4\u7684\u6709\u6548\u8bed\u4e49\u4ea4\u4e92\uff1b2) \u5355\u4e00\u4e8c\u5143\u5206\u7c7b\u8bad\u7ec3\u76ee\u6807\u5bfc\u81f4\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSASA\u6846\u67b6\uff1a1) \u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u5c06\u4e09\u5143\u7ec4\u7f16\u7801\u4e3a\u89e3\u8026\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u5e76\u901a\u8fc7\u66f4\u6709\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\u878d\u5408\uff1b2) \u8bed\u4e49\u611f\u77e5\u5c42\u6b21\u5bf9\u6bd4\u5b66\u4e60\u4f5c\u4e3a\u8f85\u52a9\u8bad\u7ec3\u76ee\u6807\uff0c\u8003\u8651\u5c40\u90e8\u548c\u5168\u5c40\u5c42\u6b21\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff1a\u5728FB15k-237\u4e0a\u51c6\u786e\u7387\u63d0\u53475.9%\uff0c\u5728YAGO3-10\u4e0a\u63d0\u53473.4%\u3002", "conclusion": "SASA\u901a\u8fc7\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u4e49\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4e09\u5143\u7ec4\u5206\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.12366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12366", "abs": "https://arxiv.org/abs/2601.12366", "authors": ["Jiafei Zhang", "Songliang Cao", "Binghui Xu", "Yanan Li", "Weiwei Jia", "Tingting Wu", "Hao Lu", "Weijuan Hu", "Zhiguo Han"], "title": "DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data", "comment": "13 pages, 15 figures and 7 tables", "summary": "DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.", "AI": {"tldr": "DepthCropSeg++\u662f\u4e00\u4e2a\u7528\u4e8e\u4f5c\u7269\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5f00\u653e\u7530\u95f4\u73af\u5883\u4e2d\u5206\u5272\u4e0d\u540c\u4f5c\u7269\u7269\u79cd\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4f5c\u7269\u5206\u5272\u6a21\u578b\u53d7\u9650\u4e8e\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\uff0c\u901a\u5e38\u53ea\u80fd\u5728\u7279\u5b9a\u4f5c\u7269\u7c7b\u578b\u6216\u53d7\u63a7\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u7269\u79cd\u3001\u8de8\u573a\u666f\u6cdb\u5316\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u5148\u524dDepthCropSeg\u5de5\u4f5c\uff0c\u6784\u5efa\u4e86\u5305\u542b28,406\u5f20\u56fe\u50cf\u300130+\u7269\u79cd\u300115\u79cd\u73af\u5883\u6761\u4ef6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u91c7\u7528ViT-Adapter\u67b6\u6784\uff0c\u589e\u5f3a\u52a8\u6001\u4e0a\u91c7\u6837\u4ee5\u6539\u8fdb\u7ec6\u8282\u611f\u77e5\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u81ea\u8bad\u7ec3\u6d41\u7a0b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u7efc\u5408\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523093.11% mIoU\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\uff08+0.36%\uff09\u548c\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5982SAM\uff08+48.57%\uff09\u3002\u5728\u591c\u95f4\u73af\u5883\uff0886.90% mIoU\uff09\u3001\u9ad8\u5bc6\u5ea6\u51a0\u5c42\uff0890.09% mIoU\uff09\u548c\u672a\u89c1\u4f5c\u7269\u54c1\u79cd\uff0890.09% mIoU\uff09\u7b49\u6311\u6218\u6027\u573a\u666f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DepthCropSeg++\u4e3a\u4f5c\u7269\u5206\u5272\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u519c\u4e1a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f00\u653e\u7530\u95f4\u73af\u5883\u4e0b\u7684\u8de8\u7269\u79cd\u5206\u5272\u6311\u6218\u3002"}}
{"id": "2601.13904", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13904", "abs": "https://arxiv.org/abs/2601.13904", "authors": ["Jaeyoung Moon", "Youjin Choi", "Yucheon Park", "David Melhart", "Georgios N. Yannakakis", "Kyung-Joong Kim"], "title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation", "comment": "CHI '26 Accepted paper", "summary": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.", "AI": {"tldr": "PREFAB\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u56de\u987e\u6027\u81ea\u6211\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u60c5\u611f\u53d8\u5316\u533a\u57df\u800c\u975e\u5b8c\u6574\u6807\u6ce8\u6765\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u81ea\u6211\u6807\u6ce8\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5b8c\u6574\u6807\u6ce8\u6574\u4e2a\u4f1a\u8bdd\uff0c\u8fd9\u8fc7\u7a0b\u8017\u65f6\u3001\u8ba4\u77e5\u8d1f\u8377\u5927\u3001\u5bb9\u6613\u75b2\u52b3\u548c\u51fa\u9519\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5\u3002", "method": "\u57fa\u4e8e\u5cf0\u503c-\u7ed3\u675f\u89c4\u5219\u548c\u60c5\u611f\u5e8f\u6570\u8868\u793a\uff0cPREFAB\u4f7f\u7528\u504f\u597d\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u76f8\u5bf9\u60c5\u611f\u53d8\u5316\uff0c\u6307\u5bfc\u6807\u6ce8\u8005\u53ea\u6807\u6ce8\u9009\u5b9a\u7247\u6bb5\uff0c\u5176\u4f59\u90e8\u5206\u901a\u8fc7\u63d2\u503c\u5904\u7406\u3002\u8fd8\u5f15\u5165\u4e86\u9884\u89c8\u673a\u5236\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8f85\u52a9\u6807\u6ce8\u3002", "result": "PREFAB\u5728\u6280\u672f\u6027\u80fd\u7814\u7a76\u548c25\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u60c5\u611f\u53d8\u5316\uff0c\u51cf\u8f7b\u5de5\u4f5c\u8d1f\u8377\uff08\u6709\u6761\u4ef6\u5730\u51cf\u8f7b\u65f6\u95f4\u8d1f\u62c5\uff09\uff0c\u63d0\u9ad8\u6807\u6ce8\u8005\u4fe1\u5fc3\u800c\u4e0d\u964d\u4f4e\u6807\u6ce8\u8d28\u91cf\u3002", "conclusion": "PREFAB\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u9884\u7b97\u60c5\u611f\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7126\u60c5\u611f\u53d8\u5316\u533a\u57df\u800c\u975e\u5b8c\u6574\u6807\u6ce8\uff0c\u5728\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u6ce8\u8d28\u91cf\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6807\u6ce8\u5de5\u5177\u3002"}}
{"id": "2601.13044", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13044", "abs": "https://arxiv.org/abs/2601.13044", "authors": ["Warit Sirichotedumrong", "Adisai Na-Thalang", "Potsawee Manakul", "Pittawat Taveekitworachai", "Sittipong Sripaisarnmongkol", "Kunat Pipatanakul"], "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition", "comment": "Models and datasets are publicly available on https://huggingface.co/collections/typhoon-ai/typhoon-asr-technical-report ; Project Page: https://opentyphoon.ai/model/typhoon-asr-realtime", "summary": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a115M\u53c2\u6570\u7684FastConformer-Transducer\u6a21\u578b\u7528\u4e8e\u6cf0\u8bed\u5b9e\u65f6\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u6587\u672c\u89c4\u8303\u5316\u5b9e\u73b0\u4e0eWhisper Large-v3\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e45\u500d\uff0c\u5e76\u53d1\u5e03\u4e86\u6807\u51c6\u5316\u7684\u6cf0\u8bedASR\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u6cf0\u8bedASR\u9886\u57df\u4e3b\u8981\u88ab\u79bb\u7ebf\u67b6\u6784\uff08\u5982Whisper\uff09\u4e3b\u5bfc\uff0c\u8fd9\u4e9b\u6a21\u578b\u5ef6\u8fdf\u9ad8\uff0c\u4e0d\u9002\u5408\u6d41\u5f0f\u5e94\u7528\u3002\u7f3a\u4e4f\u9ad8\u6548\u7684\u4f4e\u5ef6\u8fdf\u6cf0\u8bed\u8bed\u97f3\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u5b58\u5728\u8f6c\u5f55\u4e00\u81f4\u6027\u548c\u65b9\u8a00\u9002\u5e94\u7b49\u6311\u6218\u3002", "method": "1) \u4f7f\u7528115M\u53c2\u6570\u7684FastConformer-Transducer\u67b6\u6784\uff1b2) \u5f00\u53d1\u4e25\u683c\u7684\u6587\u672c\u89c4\u8303\u5316\u6d41\u7a0b\uff0c\u89e3\u51b3\u6cf0\u8bed\u8f6c\u5f55\u4e2d\u7684\u7cfb\u7edf\u6b67\u4e49\uff08\u5982\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6570\u5b57\u53d1\u97f3\u548c\u91cd\u590d\u6807\u8bb0\uff09\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4f0a\u68ee\u65b9\u8a00\u9002\u5e94\uff1b4) \u521b\u5efa\u5e76\u53d1\u5e03Typhoon ASR Benchmark\u6807\u51c6\u5316\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "\u7d27\u51d1\u6a21\u578b\u76f8\u6bd4Whisper Large-v3\u8ba1\u7b97\u6210\u672c\u964d\u4f4e45\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002\u6587\u672c\u89c4\u8303\u5316\u663e\u8457\u63d0\u5347\u4e86\u8f6c\u5f55\u4e00\u81f4\u6027\uff0c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u65b9\u8a00\u9002\u5e94\u800c\u4e0d\u5f71\u54cd\u6807\u51c6\u6cf0\u8bed\u6027\u80fd\u3002", "conclusion": "\u4e25\u683c\u7684\u6587\u672c\u89c4\u8303\u5316\u53ef\u4ee5\u5339\u914d\u6a21\u578b\u7f29\u653e\u7684\u6548\u679c\uff0c\u4e3a\u6cf0\u8bedASR\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\u3002\u53d1\u5e03\u7684\u6807\u51c6\u5316\u57fa\u51c6\u5c06\u4fc3\u8fdb\u6cf0\u8bed\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u6b65\u3002"}}
{"id": "2601.12373", "categories": ["cs.CV", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12373", "abs": "https://arxiv.org/abs/2601.12373", "authors": ["Amro Khaled", "Farah Khaled", "Omar Riad", "Catherine M. Elias"], "title": "CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology", "comment": null, "summary": "In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.", "AI": {"tldr": "CD-TWINSAFE\u662f\u4e00\u4e2a\u57fa\u4e8eV2I\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u901a\u8fc7\u5b9e\u65f6\u540c\u6b65\u8f66\u8f7d\u9a7e\u9a76\u5806\u6808\u548c\u6570\u5b57\u5b6a\u751f\u5806\u6808\u6765\u63d0\u4f9b\u5b89\u5168\u8b66\u62a5\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u65f6\u76d1\u63a7\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u72b6\u6001\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u589e\u5f3a\u8f66\u8f86\u5bf9\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u53cc\u5806\u6808\u67b6\u6784\uff1a\u8f66\u8f7d\u9a7e\u9a76\u5806\u6808\uff08\u5305\u542b\u7acb\u4f53\u76f8\u673a\u3001\u5b9a\u4f4d\u548c\u611f\u77e5\u6a21\u5757\uff09\u548c\u6570\u5b57\u5b6a\u751f\u5806\u6808\uff08\u8fd0\u884cUnreal Engine 5\u573a\u666f\u590d\u5236\uff09\u3002\u901a\u8fc7ROS2\u6d88\u606f\u548c4G V2I\u901a\u4fe1\u5b9e\u73b0\u6570\u636e\u540c\u6b65\uff0c\u5b9e\u65f6\u66f4\u65b0\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u8f66\u8f86\u548c\u7269\u4f53\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u67b6\u6784\u7684\u6709\u6548\u6027\u548c\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\uff0c\u80fd\u591f\u51c6\u786e\u66f4\u65b0\u6570\u5b57\u5b6a\u751f\u73af\u5883\u5e76\u8fd4\u56de\u5b89\u5168\u8b66\u62a5\u3002", "conclusion": "CD-TWINSAFE\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8eV2I\u7684\u6570\u5b57\u5b6a\u751f\u67b6\u6784\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u5b89\u5168\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u589e\u5f3a\u4e86\u73af\u5883\u611f\u77e5\u548c\u5b89\u5168\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2601.13969", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13969", "abs": "https://arxiv.org/abs/2601.13969", "authors": ["Joaqu\u00edn Polonuer", "Lucas Vittor", "I\u00f1aki Arango", "Ayush Noori", "David A. Clifton", "Luciano Del Corro", "Marinka Zitnik"], "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval", "comment": null, "summary": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.", "AI": {"tldr": "ARK\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u5668\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5168\u5c40\u641c\u7d22\u548c\u90bb\u57df\u63a2\u7d22\u4e24\u79cd\u64cd\u4f5c\u63a7\u5236\u68c0\u7d22\u7684\u5e7f\u5ea6-\u6df1\u5ea6\u6743\u8861\uff0c\u65e0\u9700\u79cd\u5b50\u8282\u70b9\u9009\u62e9\u6216\u9884\u8bad\u7ec3\uff0c\u5728STaRK\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u65e0\u6807\u7b7e\u6a21\u4eff\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u4e2d\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u5668\u8986\u76d6\u5e7f\u4f46\u6df1\u5ea6\u6d45\uff0c\u800c\u57fa\u4e8e\u904d\u5386\u7684\u65b9\u6cd5\u4f9d\u8d56\u79cd\u5b50\u8282\u70b9\u9009\u62e9\uff0c\u5f53\u67e5\u8be2\u6d89\u53ca\u591a\u4e2a\u5b9e\u4f53\u548c\u5173\u7cfb\u65f6\u5bb9\u6613\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5e73\u8861\u5e7f\u5ea6\u4e0e\u6df1\u5ea6\u68c0\u7d22\u7684\u65b9\u6cd5\u3002", "method": "ARK\u91c7\u7528\u4ee3\u7406\u5f0f\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u5668\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e24\u79cd\u64cd\u4f5c\u5de5\u5177\uff1a1) \u5168\u5c40\u8bcd\u6c47\u641c\u7d22\uff08\u5e7f\u5ea6\u5bfc\u5411\uff09\uff0c2) \u5355\u8df3\u90bb\u57df\u63a2\u7d22\uff08\u6df1\u5ea6\u5bfc\u5411\uff09\u3002\u6a21\u578b\u53ef\u4ea4\u66ff\u4f7f\u7528\u8fd9\u4e24\u79cd\u64cd\u4f5c\u8fdb\u884c\u591a\u8df3\u904d\u5386\uff0c\u65e0\u9700\u4f9d\u8d56\u8106\u5f31\u7684\u79cd\u5b50\u8282\u70b9\u9009\u62e9\u3001\u9884\u8bbe\u8df3\u6570\u6216\u68c0\u7d22\u8bad\u7ec3\u3002", "result": "\u5728STaRK\u57fa\u51c6\u4e0a\uff0cARK\u8fbe\u523059.1%\u7684\u5e73\u5747Hit@1\u548c67.4\u7684\u5e73\u5747MRR\uff0c\u6bd4\u68c0\u7d22\u57fa\u7ebf\u548c\u65e0\u8bad\u7ec3\u4ee3\u7406\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u6700\u591a31.4%\u7684Hit@1\u548c28.0%\u7684MRR\u3002\u901a\u8fc7\u65e0\u6807\u7b7e\u6a21\u4eff\u84b8\u998f\u52308B\u6a21\u578b\u540e\uff0c\u5728AMAZON\u3001MAG\u548cPRIME\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4\u57fa\u78408B\u6a21\u578b\u63d0\u5347+7.0\u3001+26.6\u548c+13.5\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\u7684Hit@1\uff0c\u540c\u65f6\u4fdd\u7559\u6559\u5e08\u6a21\u578b\u6700\u591a98.5%\u7684\u6027\u80fd\u3002", "conclusion": "ARK\u901a\u8fc7\u8ba9\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u63a7\u5236\u68c0\u7d22\u7684\u5e7f\u5ea6-\u6df1\u5ea6\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u3002\u5176\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u53ef\u84b8\u998f\u5230\u8f83\u5c0f\u6a21\u578b\u4e2d\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13050", "abs": "https://arxiv.org/abs/2601.13050", "authors": ["Lars Kl\u00f6ser", "Mika Beele", "Bodo Kraft"], "title": "Profiling German Text Simplification with Interpretable Model-Fingerprints", "comment": "Presented at 2nd International Conference on Explainable AI for Neural and Symbolic Systems", "summary": "While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Simplification Profiler\u8bca\u65ad\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u751f\u6210\u7b80\u5316\u6587\u672c\u7684\u591a\u7ef4\u53ef\u89e3\u91ca\u6307\u7eb9\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u7684\u884c\u4e3a\u7279\u5f81\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u8bc4\u5206\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u6587\u672c\u7b80\u5316\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6587\u672c\u7b80\u5316\u884c\u4e3a\u7684\u5168\u9762\u3001\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u9002\u5e94\u4e0d\u540c\u7684\u76ee\u6807\u7fa4\u4f53\u800c\u975e\u5355\u4e00\u7b80\u5316\u98ce\u683c\u3002", "method": "\u5f00\u53d1Simplification Profiler\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u7b80\u5316\u6587\u672c\u751f\u6210\u6a21\u578b\u7684\u591a\u7ef4\u53ef\u89e3\u91ca\u6307\u7eb9\u3002\u91c7\u7528\u5143\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u6d4b\u8bd5\u80fd\u5426\u6839\u636e\u7b80\u5316\u6587\u672c\u53ef\u9760\u8bc6\u522b\u4e0d\u540c\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u4ece\u800c\u9a8c\u8bc1\u6307\u7eb9\u7684\u63cf\u8ff0\u80fd\u529b\u3002", "result": "Profiler\u80fd\u591f\u533a\u5206\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u9ad8\u7ea7\u884c\u4e3a\u5dee\u5f02\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u7ec6\u7c92\u5ea6\u53d8\u5316\u3002\u5b8c\u6574\u7279\u5f81\u96c6\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523071.9%\u7684F1\u5206\u6570\uff0c\u6bd4\u7b80\u5355\u57fa\u7ebf\u63d0\u9ad8\u4e8648\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\u3002", "conclusion": "Simplification Profiler\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u3001\u53ef\u64cd\u4f5c\u7684\u5206\u6790\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u6709\u6548\u548c\u771f\u6b63\u81ea\u9002\u5e94\u7684\u6587\u672c\u7b80\u5316\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u8bed\u8a00\u73af\u5883\u4e2d\u3002"}}
{"id": "2601.12379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12379", "abs": "https://arxiv.org/abs/2601.12379", "authors": ["Jiahui Sheng", "Yidan Shi", "Shu Xiang", "Xiaorun Li", "Shuhan Chen"], "title": "Utilizing the Score of Data Distribution for Hyperspectral Anomaly Detection", "comment": null, "summary": "Hyperspectral images (HSIs) are a type of image that contains abundant spectral information. As a type of real-world data, the high-dimensional spectra in hyperspectral images are actually determined by only a few factors, such as chemical composition and illumination. Thus, spectra in hyperspectral images are highly likely to satisfy the manifold hypothesis. Based on the hyperspectral manifold hypothesis, we propose a novel hyperspectral anomaly detection method (named ScoreAD) that leverages the time-dependent gradient field of the data distribution (i.e., the score), as learned by a score-based generative model (SGM). Our method first trains the SGM on the entire set of spectra from the hyperspectral image. At test time, each spectrum is passed through a perturbation kernel, and the resulting perturbed spectrum is fed into the trained SGM to obtain the estimated score. The manifold hypothesis of HSIs posits that background spectra reside on one or more low-dimensional manifolds. Conversely, anomalous spectra, owing to their unique spectral signatures, are considered outliers that do not conform to the background manifold. Based on this fundamental discrepancy in their manifold distributions, we leverage a generative SGM to achieve hyperspectral anomaly detection. Experiments on the four hyperspectral datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/jiahuisheng/ScoreAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGM\uff09\u7684\u9ad8\u5149\u8c31\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5ScoreAD\uff0c\u5229\u7528\u6570\u636e\u5206\u5e03\u7684\u68af\u5ea6\u573a\uff08\u5206\u6570\uff09\u6765\u533a\u5206\u80cc\u666f\u548c\u5f02\u5e38\u5149\u8c31\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u7684\u5149\u8c31\u7531\u5c11\u6570\u56e0\u7d20\uff08\u5982\u5316\u5b66\u6210\u5206\u548c\u5149\u7167\uff09\u51b3\u5b9a\uff0c\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\u3002\u80cc\u666f\u5149\u8c31\u4f4d\u4e8e\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u800c\u5f02\u5e38\u5149\u8c31\u7531\u4e8e\u72ec\u7279\u7684\u5149\u8c31\u7279\u5f81\u88ab\u89c6\u4e3a\u4e0d\u7b26\u5408\u80cc\u666f\u6d41\u5f62\u7684\u79bb\u7fa4\u70b9\u3002", "method": "\u9996\u5148\u5728\u6574\u4e2a\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5149\u8c31\u96c6\u4e0a\u8bad\u7ec3\u5206\u6570\u751f\u6210\u6a21\u578b\uff08SGM\uff09\u3002\u6d4b\u8bd5\u65f6\uff0c\u6bcf\u4e2a\u5149\u8c31\u901a\u8fc7\u6270\u52a8\u6838\u5904\u7406\uff0c\u7136\u540e\u5c06\u6270\u52a8\u540e\u7684\u5149\u8c31\u8f93\u5165\u8bad\u7ec3\u597d\u7684SGM\u83b7\u53d6\u4f30\u8ba1\u5206\u6570\uff0c\u57fa\u4e8e\u80cc\u666f\u548c\u5f02\u5e38\u5149\u8c31\u5728\u6d41\u5f62\u5206\u5e03\u4e0a\u7684\u5dee\u5f02\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6570\u751f\u6210\u6a21\u578b\u548c\u6d41\u5f62\u5047\u8bbe\u7684ScoreAD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9ad8\u5149\u8c31\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2601.14027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14027", "abs": "https://arxiv.org/abs/2601.14027", "authors": ["Junqi Liu", "Zihao Zhou", "Zekai Zhu", "Marco Dos Santos", "Weikun He", "Jiawei Liu", "Ran Wang", "Yunzhou Xie", "Junqiao Zhao", "Qiufeng Wang", "Lihong Zhi", "Jia Li", "Wenda Li"], "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "comment": null, "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u901a\u7528\u7f16\u7801\u667a\u80fd\u4f53\u4f5c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u5668\u7684\u65b0\u8303\u5f0f\uff0c\u5f00\u53d1\u4e86Numina-Lean-Agent\u7cfb\u7edf\uff0c\u5728Putnam 2025\u7ade\u8d5b\u4e2d\u53d6\u5f97\u6ee1\u5206\u6210\u7ee9\uff0c\u5e76\u6210\u529f\u5f62\u5f0f\u5316\u4e86Brascamp-Lieb\u5b9a\u7406\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u6d41\u6c34\u7ebf\u548c\u8bad\u7ec3\u7684\u5f62\u5f0f\u8bc1\u660e\u5668\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002\u672c\u6587\u63d0\u51fa\u901a\u7528\u7f16\u7801\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u56e0\u4e3a\uff1a(1) \u901a\u7528\u7f16\u7801\u667a\u80fd\u4f53\u4e3a\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u81ea\u7136\u63a5\u53e3\uff1b(2) \u4ec5\u66ff\u6362\u57fa\u7840\u6a21\u578b\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u8bad\u7ec3\uff1b(3) MCP\u652f\u6301\u7075\u6d3b\u6269\u5c55\u548c\u81ea\u4e3b\u8c03\u7528\u4e13\u7528\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u65b0\u8303\u5f0f\u5f00\u53d1Numina-Lean-Agent\u7cfb\u7edf\uff0c\u7ed3\u5408Claude Code\u4e0eNumina-Lean-MCP\uff0c\u5b9e\u73b0\u4e0eLean\u7684\u81ea\u4e3b\u4ea4\u4e92\u3001\u76f8\u5173\u5b9a\u7406\u68c0\u7d22\u3001\u975e\u5f62\u5f0f\u5316\u8bc1\u660e\u548c\u8f85\u52a9\u63a8\u7406\u5de5\u5177\u8c03\u7528\u3002", "result": "\u4f7f\u7528Claude Opus 4.5\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0cNumina-Lean-Agent\u5728Putnam 2025\u7ade\u8d5b\u4e2d\u89e3\u51b3\u4e86\u6240\u670912\u4e2a\u95ee\u9898\uff0812/12\uff09\uff0c\u4e0e\u6700\u4f73\u95ed\u6e90\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e0e\u6570\u5b66\u5bb6\u5408\u4f5c\u6210\u529f\u5f62\u5f0f\u5316\u4e86Brascamp-Lieb\u5b9a\u7406\u3002", "conclusion": "\u901a\u7528\u7f16\u7801\u667a\u80fd\u4f53\u4f5c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u5668\u7684\u65b0\u8303\u5f0f\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cNumina-Lean-Agent\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u5f3a\u5927\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u4e3a\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13099", "abs": "https://arxiv.org/abs/2601.13099", "authors": ["Abdellah El Mekki", "Samar M. Magdy", "Houdaifa Atou", "Ruwa AbuHweidi", "Baraah Qawasmeh", "Omer Nacar", "Thikra Al-hibiri", "Razan Saadie", "Hamzah Alsayadi", "Nadia Ghezaiel Hammouda", "Alshima Alkhazimi", "Aya Hamod", "Al-Yas Al-Ghafri", "Wesam El-Sayed", "Asila Al sharji", "Mohamad Ballout", "Anas Belfathi", "Karim Ghaddar", "Serry Sibaee", "Alaa Aoun", "Areej Asiri", "Lina Abureesh", "Ahlam Bashiti", "Majdal Yousef", "Abdulaziz Hafiz", "Yehdih Mohamed", "Emira Hamedtou", "Brakehe Brahim", "Rahaf Alhamouri", "Youssef Nafea", "Aya El Aatar", "Walid Al-Dhabyani", "Emhemed Hamed", "Sara Shatnawi", "Fakhraddin Alwajih", "Khalid Elkhidir", "Ashwag Alasmari", "Abdurrahman Gerrio", "Omar Alshahri", "AbdelRahim A. Elmadany", "Ismail Berrada", "Amir Azad Adli Alkathiri", "Fadi A Zaraket", "Mustafa Jarrar", "Yahya Mohamed El Hadj", "Hassan Alhuzali", "Muhammad Abdul-Mageed"], "title": "Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs", "comment": "Project resources will be available here: https://github.com/UBC-NLP/Alexandria", "summary": "Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \\textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.", "AI": {"tldr": "Alexandria\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u793e\u533a\u9a71\u52a8\u3001\u4eba\u5de5\u7ffb\u8bd1\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u673a\u5668\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u8986\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u300111\u4e2a\u9ad8\u5f71\u54cd\u529b\u9886\u57df\uff0c\u5305\u542b\u57ce\u5e02\u7ea7\u65b9\u8a00\u5143\u6570\u636e\u548c\u6027\u522b\u914d\u7f6e\u6807\u6ce8\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u662f\u9ad8\u5ea6\u53cc\u8a00\u5236\u7684\u8bed\u8a00\uff0c\u65e5\u5e38\u4ea4\u6d41\u4e3b\u8981\u4f7f\u7528\u65b9\u8a00\u800c\u975e\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5bf9\u65b9\u8a00\u8f93\u5165\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u6570\u767e\u4e07\u4f7f\u7528\u8005\u7684\u5b9e\u7528\u6027\u3002", "method": "\u6784\u5efaAlexandria\u6570\u636e\u96c6\uff1a1\uff09\u8986\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u300111\u4e2a\u9ad8\u5f71\u54cd\u529b\u9886\u57df\uff1b2\uff09\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u7ec6\u7c92\u5ea6\uff0c\u5173\u8054\u57ce\u5e02\u6765\u6e90\u5143\u6570\u636e\uff1b3\uff09\u5305\u542b\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\uff0c\u6807\u6ce8\u8bf4\u8bdd\u8005-\u53d7\u8bdd\u8005\u6027\u522b\u914d\u7f6e\uff1b4\uff09\u603b\u8ba1107K\u6837\u672c\u3002", "result": "Alexandria\u6570\u636e\u96c6\u65e2\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u8d44\u6e90\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u673a\u5668\u7ffb\u8bd1\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e25\u683c\u57fa\u51c6\u3002\u901a\u8fc7\u5bf9\u963f\u62c9\u4f2f\u8bed\u611f\u77e5LLMs\u7684\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8de8\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7ffb\u8bd1\u7684\u80fd\u529b\u548c\u6301\u7eed\u6311\u6218\u3002", "conclusion": "Alexandria\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7ffb\u8bd1\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u65b9\u8a00\u5143\u6570\u636e\u548c\u6027\u522b\u914d\u7f6e\u6807\u6ce8\uff0c\u4e3a\u7814\u7a76\u548c\u6539\u8fdb\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u673a\u5668\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u540c\u65f6\u66b4\u9732\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65b9\u8a00\u7ffb\u8bd1\u65b9\u9762\u7684\u663e\u8457\u6311\u6218\u3002"}}
{"id": "2601.12382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12382", "abs": "https://arxiv.org/abs/2601.12382", "authors": ["Furkan Yuceyalcin", "Abdurrahim Yilmaz", "Burak Temelkuran"], "title": "A Hierarchical Benchmark of Foundation Models for Dermatology", "comment": null, "summary": "Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a \"granularity gap\" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.", "AI": {"tldr": "\u8bc4\u4f3010\u4e2a\u57fa\u7840\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5c42\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u7528\u533b\u5b66\u6a21\u578b\u64c5\u957f\u9ad8\u7ea7\u7b5b\u67e5\u4f46\u7ec6\u7c92\u5ea6\u5206\u7c7b\u8f83\u5dee\uff0c\u800c\u76ae\u80a4\u75c5\u4e13\u7528\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u6574\u4f53\u6027\u80fd\u8f83\u4f4e", "motivation": "\u5f53\u524d\u76ae\u80a4\u75c5\u5b66\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5c06\u590d\u6742\u7684\u8bca\u65ad\u5206\u7c7b\u7b80\u5316\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u8fd9\u79cd\u8fc7\u5ea6\u7b80\u5316\u63a9\u76d6\u4e86\u6a21\u578b\u6267\u884c\u7ec6\u7c92\u5ea6\u9274\u522b\u8bca\u65ad\u7684\u80fd\u529b\uff0c\u800c\u8fd9\u5bf9\u4e8e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528DERM12345\u6570\u636e\u96c6\uff08\u5305\u542b40\u4e2a\u75c5\u53d8\u4e9a\u7c7b\uff09\uff0c\u8ba1\u7b9710\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u51bb\u7ed3\u5d4c\u5165\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6a21\u578b\uff0c\u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5f15\u5165\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u56db\u4e2a\u4e34\u5e8a\u7c92\u5ea6\u7ea7\u522b\u7684\u6027\u80fd", "result": "\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u5b58\u5728\"\u7c92\u5ea6\u5dee\u8ddd\"\uff1aMedImageInsights\u5728\u4e8c\u5143\u6076\u6027\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0897.52%\u52a0\u6743F1\u5206\u6570\uff09\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea640\u7c7b\u4e9a\u578b\u5206\u7c7b\u4e2d\u964d\u81f365.50%\uff1b\u800cMedSigLip\uff0869.79%\uff09\u548c\u76ae\u80a4\u75c5\u4e13\u7528\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u6574\u4f53\u6027\u80fd\u8f83\u4f4e", "conclusion": "\u901a\u7528\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5bf9\u4e8e\u9ad8\u7ea7\u7b5b\u67e5\u975e\u5e38\u6709\u6548\uff0c\u4f46\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u533a\u5206\u9700\u8981\u4e13\u95e8\u7684\u5efa\u6a21\u7b56\u7565"}}
{"id": "2601.14096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14096", "abs": "https://arxiv.org/abs/2601.14096", "authors": ["Benedikt Hartl", "L\u00e9o Pio-Lopez", "Chris Fields", "Michael Levin"], "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems", "comment": "41 pages, 5 figures", "summary": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8ba4\u77e5\u7684\u666e\u9002\u539f\u7406\uff1a\u901a\u8fc7\u8fed\u4ee3\u8bef\u5dee\u6700\u5c0f\u5316\u8fdb\u884c\u5d4c\u5165\u7a7a\u95f4\u7684\u91cd\u6620\u5c04\u4e0e\u5bfc\u822a\uff0c\u8fd9\u4e00\u539f\u7406\u9002\u7528\u4e8e\u4ece\u751f\u7269\u7cfb\u7edf\u5230\u4eba\u5de5\u667a\u80fd\u7684\u5404\u79cd\u667a\u80fd\u4f53\u3002", "motivation": "\u5bfb\u6c42\u8de8\u4e0d\u540c\u8d77\u6e90\u3001\u7ec4\u6210\u548c\u57fa\u8d28\u7684\u667a\u80fd\u4f53\uff08\u4ece\u4e9a\u7ec6\u80de\u5316\u5b66\u7f51\u7edc\u5230\u751f\u7269\u7fa4\u4f53\uff09\u4e2d\u95ee\u9898\u89e3\u51b3\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u53d1\u73b0\u5c3a\u5ea6\u4e0d\u53d8\u7684\u51b3\u7b56\u539f\u7406\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u53ef\u7531\u4e24\u4e2a\u540c\u7b49\u91cd\u8981\u7684\u4e0d\u53d8\u91cf\u8868\u5f81\uff1a(1)\u5d4c\u5165\u7a7a\u95f4\u7684\u91cd\u6620\u5c04\uff0c(2)\u5728\u8fd9\u4e9b\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u3002\u751f\u7269\u96c6\u4f53\u901a\u8fc7\u5206\u5e03\u5f0f\u8bef\u5dee\u6821\u6b63\u91cd\u6620\u5c04\u8f6c\u5f55\u3001\u5f62\u6001\u3001\u751f\u7406\u62163D\u7a7a\u95f4\uff1bAI\u7cfb\u7edf\u901a\u8fc7\u5c06\u6570\u636e\u91cd\u6620\u5c04\u5230\u6f5c\u5728\u5d4c\u5165\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u8bba\u8bc1\u4e86\u901a\u8fc7\u8fed\u4ee3\u8bef\u5dee\u6700\u5c0f\u5316\u8fdb\u884c\u5d4c\u5165\u7a7a\u95f4\u91cd\u6620\u5c04\u4e0e\u5bfc\u822a\u7684\u53cc\u91cd\u539f\u7406\u6784\u6210\u4e86\u8ba4\u77e5\u7684\u57fa\u8d28\u72ec\u7acb\u4e0d\u53d8\u91cf\u3002", "conclusion": "\u8ba4\u8bc6\u5230\u8fd9\u4e00\u5171\u4eab\u673a\u5236\u4e0d\u4ec5\u63ed\u793a\u4e86\u751f\u547d\u7cfb\u7edf\u4e0e\u4eba\u5de5\u6a21\u578b\u4e4b\u95f4\u7684\u6df1\u523b\u76f8\u4f3c\u6027\uff0c\u8fd8\u4e3a\u8de8\u5c3a\u5ea6\u5de5\u7a0b\u5316\u81ea\u9002\u5e94\u667a\u80fd\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2601.13105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13105", "abs": "https://arxiv.org/abs/2601.13105", "authors": ["Liu Kaipeng", "Wu Ling"], "title": "Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification", "comment": "19pages, 1figure", "summary": "This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7LoRA\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408RAG\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u82f1\u8bed\u53cc\u53ca\u7269\u7ed3\u6784\u7684\u81ea\u52a8\u8bc6\u522b\uff0c\u5728BNC\u8bed\u6599\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u82f1\u8bed\u53cc\u53ca\u7269\u7ed3\u6784\uff08\u5982\"give someone something\"\uff09\u7684\u81ea\u52a8\u8bc6\u522b\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff08\u5982\u53e5\u6cd5\u5206\u6790\u3001\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\uff09\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u5f62\u5f0f\u5339\u914d\uff0c\u96be\u4ee5\u5904\u7406\u8bed\u4e49\u590d\u6742\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08LoRA\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u53cc\u53ca\u7269\u7ed3\u6784\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "1. \u4f7f\u7528\u82f1\u56fd\u56fd\u5bb6\u8bed\u6599\u5e93\uff08BNC\uff09\u7684\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff1b2. \u91c7\u7528LoRA\uff08Low-Rank Adaptation\uff09\u6280\u672f\u5bf9Qwen3-8B\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b3. \u6784\u5efa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u7406\u8bba\u77e5\u8bc6\u4e0e\u6a21\u578b\u63a8\u7406\uff1b4. \u5bf9\u6bd4\u5b9e\u9a8c\uff1aLoRA\u5fae\u8c03\u6a21\u578b vs. \u539f\u751fQwen3-MAX\u6a21\u578b vs. \u7eaf\u7406\u8bbaRAG\u7cfb\u7edf\u3002", "result": "1. LoRA\u5fae\u8c03\u7684Qwen3-8B\u6a21\u578b\u5728\u53cc\u53ca\u7269\u7ed3\u6784\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff1b2. \u663e\u8457\u4f18\u4e8e\u539f\u751fQwen3-MAX\u6a21\u578b\u548c\u7eaf\u7406\u8bbaRAG\u7cfb\u7edf\uff1b3. \u9519\u8bef\u5206\u6790\u663e\u793a\uff0c\u5fae\u8c03\u4f7f\u6a21\u578b\u4ece\u8868\u9762\u5f62\u5f0f\u5339\u914d\u8f6c\u5411\u57fa\u4e8e\u8bed\u4e49\u7684\u7406\u89e3\uff1b4. \u8bc1\u660e\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5728\u53e5\u6cd5\u7ed3\u6784\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LoRA\u5fae\u8c03\u7ed3\u5408RAG\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u82f1\u8bed\u53cc\u53ca\u7269\u7ed3\u6784\u7684\u8bc6\u522b\u80fd\u529b\u3002\u5fae\u8c03\u4f7f\u6a21\u578b\u4ece\u8868\u9762\u6a21\u5f0f\u5339\u914d\u8f6c\u5411\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u53e5\u6cd5\u7ed3\u6784\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.12391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12391", "abs": "https://arxiv.org/abs/2601.12391", "authors": ["Dasith de Silva Edirimuni", "Ajmal Saeed Mian"], "title": "Class-Partitioned VQ-VAE and Latent Flow Matching for Point Cloud Scene Generation", "comment": "Accepted to AAAI 2026, Main Technical Track", "summary": "Most 3D scene generation methods are limited to only generating object bounding box parameters while newer diffusion methods also generate class labels and latent features. Using object size or latent feature, they then retrieve objects from a predefined database. For complex scenes of varied, multi-categorical objects, diffusion-based latents cannot be effectively decoded by current autoencoders into the correct point cloud objects which agree with target classes. We introduce a Class-Partitioned Vector Quantized Variational Autoencoder (CPVQ-VAE) that is trained to effectively decode object latent features, by employing a pioneering $\\textit{class-partitioned codebook}$ where codevectors are labeled by class. To address the problem of $\\textit{codebook collapse}$, we propose a $\\textit{class-aware}$ running average update which reinitializes dead codevectors within each partition. During inference, object features and class labels, both generated by a Latent-space Flow Matching Model (LFMM) designed specifically for scene generation, are consumed by the CPVQ-VAE. The CPVQ-VAE's class-aware inverse look-up then maps generated latents to codebook entries that are decoded to class-specific point cloud shapes. Thereby, we achieve pure point cloud generation without relying on an external objects database for retrieval. Extensive experiments reveal that our method reliably recovers plausible point cloud scenes, with up to 70.4% and 72.3% reduction in Chamfer and Point2Mesh errors on complex living room scenes.", "AI": {"tldr": "\u63d0\u51faCPVQ-VAE\u548cLFMM\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u6570\u636e\u5e93\u7684\u7eaf\u70b9\u4e91\u573a\u666f\u751f\u6210\uff0c\u901a\u8fc7\u7c7b\u522b\u5206\u533a\u7801\u672c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u89e3\u7801\u95ee\u9898", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u751f\u6210\u8fb9\u754c\u6846\u53c2\u6570\uff0c\u6269\u6563\u65b9\u6cd5\u867d\u7136\u80fd\u751f\u6210\u7c7b\u522b\u6807\u7b7e\u548c\u6f5c\u5728\u7279\u5f81\uff0c\u4f46\u590d\u6742\u591a\u7c7b\u522b\u573a\u666f\u4e2d\uff0c\u5f53\u524d\u81ea\u7f16\u7801\u5668\u65e0\u6cd5\u6709\u6548\u89e3\u7801\u6f5c\u5728\u7279\u5f81\u4e3a\u6b63\u786e\u7684\u70b9\u4e91\u5bf9\u8c61\u3002\u9700\u8981\u89e3\u51b3\u6269\u6563\u6f5c\u5728\u7279\u5f81\u89e3\u7801\u95ee\u9898\uff0c\u5b9e\u73b0\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u8c61\u6570\u636e\u5e93\u7684\u7eaf\u70b9\u4e91\u751f\u6210\u3002", "method": "1. \u63d0\u51fa\u7c7b\u522b\u5206\u533a\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CPVQ-VAE\uff09\uff0c\u4f7f\u7528\u7c7b\u522b\u5206\u533a\u7801\u672c\uff0c\u7801\u5411\u91cf\u6309\u7c7b\u522b\u6807\u8bb0\uff1b2. \u63d0\u51fa\u7c7b\u522b\u611f\u77e5\u8fd0\u884c\u5e73\u5747\u66f4\u65b0\uff0c\u89e3\u51b3\u7801\u672c\u574d\u7f29\u95ee\u9898\uff1b3. \u8bbe\u8ba1\u4e13\u95e8\u7528\u4e8e\u573a\u666f\u751f\u6210\u7684\u6f5c\u5728\u7a7a\u95f4\u6d41\u5339\u914d\u6a21\u578b\uff08LFMM\uff09\uff0c\u751f\u6210\u5bf9\u8c61\u7279\u5f81\u548c\u7c7b\u522b\u6807\u7b7e\uff1b4. \u901a\u8fc7\u7c7b\u522b\u611f\u77e5\u9006\u67e5\u627e\u5c06\u751f\u6210\u6f5c\u5728\u6620\u5c04\u5230\u7801\u672c\u6761\u76ee\uff0c\u89e3\u7801\u4e3a\u7c7b\u522b\u7279\u5b9a\u7684\u70b9\u4e91\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u80fd\u53ef\u9760\u6062\u590d\u5408\u7406\u7684\u70b9\u4e91\u573a\u666f\uff0c\u5728\u590d\u6742\u5ba2\u5385\u573a\u666f\u4e0a\uff0cChamfer\u8bef\u5dee\u51cf\u5c1170.4%\uff0cPoint2Mesh\u8bef\u5dee\u51cf\u5c1172.3%\u3002\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u8c61\u6570\u636e\u5e93\u7684\u7eaf\u70b9\u4e91\u751f\u6210\u3002", "conclusion": "\u63d0\u51fa\u7684CPVQ-VAE\u548cLFMM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u591a\u7c7b\u522b3D\u573a\u666f\u751f\u6210\u4e2d\u7684\u89e3\u7801\u95ee\u9898\uff0c\u901a\u8fc7\u7c7b\u522b\u5206\u533a\u7801\u672c\u548c\u7c7b\u522b\u611f\u77e5\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7eaf\u70b9\u4e91\u573a\u666f\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.14171", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14171", "abs": "https://arxiv.org/abs/2601.14171", "authors": ["Qianli Ma", "Chang Guo", "Zhiheng Tian", "Siyu Wang", "Jipeng Xiao", "Yuanhao Yue", "Zhipeng Zhang"], "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "comment": null, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "AI": {"tldr": "RebuttalAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u53cd\u9a73\u751f\u6210\u91cd\u6784\u4e3a\u4ee5\u8bc1\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c4\u5212\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u89e3\u8bc4\u5ba1\u610f\u89c1\u3001\u6784\u5efa\u6df7\u5408\u4e0a\u4e0b\u6587\u3001\u96c6\u6210\u5916\u90e8\u641c\u7d22\uff0c\u751f\u6210\u53ef\u68c0\u67e5\u7684\u54cd\u5e94\u8ba1\u5212\uff0c\u5728\u8986\u76d6\u5ea6\u3001\u5fe0\u5b9e\u5ea6\u548c\u7b56\u7565\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u53cd\u9a73\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u76f4\u63a5\u6587\u672c\u751f\u6210\u95ee\u9898\uff0c\u5b58\u5728\u5e7b\u89c9\u3001\u5ffd\u89c6\u6279\u8bc4\u610f\u89c1\u3001\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u57fa\u7840\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u5bf9\u9f50\u5ba1\u7a3f\u4eba\u610f\u56fe\u548c\u7a3f\u4ef6\u7ec6\u8282\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRebuttalAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u5c06\u590d\u6742\u53cd\u9988\u5206\u89e3\u4e3a\u539f\u5b50\u5316\u5173\u6ce8\u70b9\uff1b2) \u52a8\u6001\u6784\u5efa\u6df7\u5408\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u538b\u7f29\u6458\u8981\u548c\u9ad8\u4fdd\u771f\u6587\u672c\uff1b3) \u96c6\u6210\u81ea\u4e3b\u6309\u9700\u5916\u90e8\u641c\u7d22\u6a21\u5757\u5904\u7406\u9700\u8981\u5916\u90e8\u6587\u732e\u7684\u5173\u5207\uff1b4) \u5728\u8d77\u8349\u524d\u751f\u6210\u53ef\u68c0\u67e5\u7684\u54cd\u5e94\u8ba1\u5212\uff0c\u786e\u4fdd\u6bcf\u4e2a\u8bba\u70b9\u90fd\u6709\u660e\u786e\u7684\u5185\u5916\u90e8\u8bc1\u636e\u652f\u6491\u3002", "result": "\u5728\u63d0\u51fa\u7684RebuttalBench\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u6d41\u6c34\u7ebf\u5728\u8986\u76d6\u5ea6\u3001\u5fe0\u5b9e\u5ea6\u548c\u7b56\u7565\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u63a7\u7684\u8f85\u52a9\u5de5\u5177\u3002", "conclusion": "RebuttalAgent\u901a\u8fc7\u5c06\u53cd\u9a73\u751f\u6210\u91cd\u6784\u4e3a\u8bc1\u636e\u4e2d\u5fc3\u7684\u89c4\u5212\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u63a7\u7684\u540c\u884c\u8bc4\u5ba1\u8f85\u52a9\u7cfb\u7edf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2601.13111", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13111", "abs": "https://arxiv.org/abs/2601.13111", "authors": ["Hassan Soliman", "Vivek Gupta", "Dan Roth", "Iryna Gurevych"], "title": "CORE-T: COherent REtrieval of Tables for Text-to-SQL", "comment": "Preprint under review. Code and data available at: https://github.com/UKPLab/arxiv2026-core-t", "summary": "Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.", "AI": {"tldr": "CORE-T\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM\u751f\u6210\u8868\u76ee\u7684\u5143\u6570\u636e\u548c\u9884\u8ba1\u7b97\u8868\u517c\u5bb9\u6027\u7f13\u5b58\uff0c\u6539\u8fdb\u591a\u8868\u6587\u672c\u5230SQL\u4e2d\u7684\u8868\u9009\u62e9\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u6587\u672c\u5230SQL\u5de5\u4f5c\u6d41\u901a\u5e38\u9700\u8981\u8fde\u63a5\u591a\u4e2a\u8868\uff0c\u800c\u51c6\u786e\u68c0\u7d22\u76f8\u5173\u8868\u96c6\u5408\u6210\u4e3a\u7aef\u5230\u7aef\u6027\u80fd\u7684\u5173\u952e\u74f6\u9888\u3002\u5728\u5f00\u653e\u4e66\u7c4d\u8bbe\u7f6e\u4e2d\uff0c\u67e5\u8be2\u9700\u8981\u5728\u6765\u81ea\u591a\u4e2a\u6e90\u7684\u5927\u578b\u5f02\u6784\u8868\u96c6\u5408\u4e0a\u56de\u7b54\uff0c\u7f3a\u4e4f\u6570\u636e\u5e93\u6807\u8bc6\u7b26\u7b49\u6e05\u6670\u7684\u8303\u56f4\u4fe1\u53f7\u3002", "method": "CORE-T\u6846\u67b6\uff1a1\uff09\u4f7f\u7528LLM\u4e3a\u8868\u751f\u6210\u76ee\u7684\u5143\u6570\u636e\uff1b2\uff09\u9884\u8ba1\u7b97\u8f7b\u91cf\u7ea7\u8868\u517c\u5bb9\u6027\u7f13\u5b58\uff1b3\uff09\u63a8\u7406\u65f6\uff0c\u5bc6\u96c6\u68c0\u7d22\u8fd4\u56detop-K\u5019\u9009\u8868\uff1b4\uff09\u5355\u4e2aLLM\u8c03\u7528\u9009\u62e9\u53ef\u8fde\u63a5\u7684\u5b50\u96c6\uff1b5\uff09\u7b80\u5355\u52a0\u6cd5\u8c03\u6574\u6b65\u9aa4\u6062\u590d\u5f3a\u517c\u5bb9\u8868\u3002", "result": "\u5728Bird\u3001Spider\u548cMMQA\u6570\u636e\u96c6\u4e0a\uff0cCORE-T\u5c06\u8868\u9009\u62e9F1\u63d0\u9ad8\u4e8622.7\u5206\uff0c\u540c\u65f6\u68c0\u7d22\u8868\u51cf\u5c11\u4e8642%\uff0c\u591a\u8868\u6267\u884c\u51c6\u786e\u7387\u5728Bird\u4e0a\u63d0\u9ad8\u4e865.0\u5206\uff0c\u5728MMQA\u4e0a\u63d0\u9ad8\u4e866.9\u5206\uff0c\u4e14\u6bd4LLM\u5bc6\u96c6\u578b\u57fa\u7ebf\u5c11\u75284-5\u500d\u7684token\u3002", "conclusion": "CORE-T\u901a\u8fc7\u7ed3\u5408\u5bc6\u96c6\u68c0\u7d22\u7684\u53ec\u56de\u4f18\u52bf\u4e0eLLM\u9a71\u52a8\u7684\u8868\u9009\u62e9\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8868\u6587\u672c\u5230SQL\u4e2d\u7684\u8868\u9009\u62e9\u6027\u80fd\u548c\u6267\u884c\u51c6\u786e\u7387\u3002"}}
{"id": "2601.12402", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12402", "abs": "https://arxiv.org/abs/2601.12402", "authors": ["Aleksandra Jamr\u00f3z", "Patrycja Wysocka", "Piotr Garbat"], "title": "Weaknesses of Facial Emotion Recognition Systems", "comment": null, "summary": "Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u9762\u90e8\u7684\u60c5\u611f\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u6df1\u5ea6\u7efc\u8ff0\uff0c\u9009\u62e9\u4e86\u4e09\u79cd\u6700\u4f73\u795e\u7ecf\u7f51\u7edc\u65b9\u6848\u548c\u4e09\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u6570\u636e\u96c6\u5dee\u5f02\u3001\u60c5\u611f\u8bc6\u522b\u96be\u5ea6\u4e0d\u5747\u548c\u76f8\u4f3c\u60c5\u611f\u533a\u5206\u65b9\u9762\u7684\u5f31\u70b9\u3002", "motivation": "\u9762\u90e8\u60c5\u611f\u68c0\u6d4b\u662f\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u673a\u5668\u5b66\u4e60\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u79cd\u7c7b\u7e41\u591a\uff0c\u9700\u8981\u8fdb\u884c\u6df1\u5165\u7efc\u8ff0\u548c\u6bd4\u8f83\u7814\u7a76\uff0c\u4ee5\u8bc6\u522b\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u5e76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "1. \u5bf9\u76f8\u5173\u6587\u732e\u548c\u79d1\u5b66\u7814\u7a76\u8fdb\u884c\u6df1\u5ea6\u7efc\u8ff0\uff1b2. \u9009\u62e9\u4e09\u79cd\u6700\u6709\u8da3\u548c\u6700\u4f73\u7684\u60c5\u611f\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u65b9\u6848\uff1b3. \u9009\u62e9\u4e09\u4e2a\u4ee5\u56fe\u50cf\u591a\u6837\u6027\u548c\u6570\u91cf\u7a81\u51fa\u7684\u6570\u636e\u96c6\uff1b4. \u8bad\u7ec3\u9009\u5b9a\u7684\u795e\u7ecf\u7f51\u7edc\uff1b5. \u8fdb\u884c\u4e00\u7cfb\u5217\u6027\u80fd\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u5305\u62ec\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u591a\u4e2a\u5f31\u70b9\uff1a1. \u4e0d\u540c\u6570\u636e\u96c6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b2. \u8bc6\u522b\u67d0\u4e9b\u60c5\u611f\u5b58\u5728\u96be\u5ea6\u4e0d\u5747\u7684\u95ee\u9898\uff1b3. \u533a\u5206\u5bc6\u5207\u76f8\u5173\u7684\u60c5\u7eea\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u9762\u90e8\u60c5\u611f\u68c0\u6d4b\u9886\u57df\u5b58\u5728\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\u3001\u60c5\u611f\u8bc6\u522b\u96be\u5ea6\u5dee\u5f02\u548c\u76f8\u4f3c\u60c5\u611f\u533a\u5206\u56f0\u96be\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u6539\u8fdb\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14192", "abs": "https://arxiv.org/abs/2601.14192", "authors": ["Xiaofang Yang", "Lijun Li", "Heng Zhou", "Tong Zhu", "Xiaoye Qu", "Yuchen Fan", "Qianshan Wei", "Rui Ye", "Li Kang", "Yiran Qin", "Zhiqiang Kou", "Daizong Liu", "Qi Li", "Ning Ding", "Siheng Chen", "Jing Shao"], "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "comment": "35 pages, 200 references", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u95ee\u9898\uff0c\u4ece\u8bb0\u5fc6\u3001\u5de5\u5177\u5b66\u4e60\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u51fa\u53d1\uff0c\u5206\u6790\u4e86\u6210\u672c\uff08\u5ef6\u8fdf\u3001\u4ee4\u724c\u3001\u6b65\u9aa4\u7b49\uff09\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u6548\u7387\u8bc4\u4f30\u6846\u67b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u667a\u80fd\u4f53\u7cfb\u7edf\u6269\u5c55\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6709\u6548\u6027\u800c\u5ffd\u89c6\u4e86\u6548\u7387\u95ee\u9898\u3002\u6548\u7387\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5168\u9762\u63a2\u8ba8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u4f18\u5316\u3002", "method": "\u4ece\u667a\u80fd\u4f53\u7684\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u8bb0\u5fc6\u3001\u5de5\u5177\u5b66\u4e60\u3001\u89c4\u5212\uff09\u51fa\u53d1\uff0c\u7efc\u8ff0\u4e86\u591a\u79cd\u6548\u7387\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a\u901a\u8fc7\u538b\u7f29\u548c\u7ba1\u7406\u9650\u5236\u4e0a\u4e0b\u6587\u3001\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4ee5\u51cf\u5c11\u5de5\u5177\u8c03\u7528\u3001\u91c7\u7528\u53d7\u63a7\u641c\u7d22\u673a\u5236\u7b49\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6548\u7387\u8bc4\u4f30\u65b9\u5f0f\uff1a\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u7684\u6709\u6548\u6027\u6bd4\u8f83\u548c\u540c\u7b49\u6709\u6548\u6027\u6c34\u5e73\u4e0b\u7684\u6210\u672c\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u4e86\u667a\u80fd\u4f53\u6548\u7387\u4f18\u5316\u7684\u5171\u540c\u539f\u5219\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u6709\u6548\u6027-\u6210\u672c\u6743\u8861\u6846\u67b6\uff0c\u6574\u7406\u4e86\u9762\u5411\u6548\u7387\u7684\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u534f\u8bae\u548c\u5e38\u7528\u6548\u7387\u6307\u6807\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u6548\u7387\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u7814\u7a76\u81f3\u5173\u91cd\u8981\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u91cd\u89c6\u3002\u672c\u6587\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7814\u7a76\u89c6\u89d2\uff0c\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u66f4\u597d\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6548\u7387\u4f18\u5316\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.13115", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13115", "abs": "https://arxiv.org/abs/2601.13115", "authors": ["Fengran Mo", "Yifan Gao", "Sha Li", "Hansi Zeng", "Xin Liu", "Zhaoxuan Tan", "Xian Li", "Jianshu Chen", "Dakuo Wang", "Meng Jiang"], "title": "Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8de8\u8f6e\u6b21\u4ea4\u7ec7\u641c\u7d22\u4e0e\u63a8\u7406\uff0c\u5b9e\u73b0\u63a2\u7d22\u6027\u548c\u81ea\u9002\u5e94\u884c\u4e3a\uff0c\u4ee5\u5e94\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7528\u6237\u610f\u56fe\u7684\u52a8\u6001\u6f14\u53d8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u9759\u6001\u7684\u91cd\u5199-\u68c0\u7d22-\u751f\u6210\u6d41\u7a0b\uff0c\u5206\u522b\u4f18\u5316\u4e0d\u540c\u6b65\u9aa4\uff0c\u5ffd\u89c6\u4e86\u6df7\u5408\u4e3b\u52a8\u884c\u52a8\u7684\u8054\u5408\u4f18\u5316\u3002\u867d\u7136\u6df1\u5ea6\u641c\u7d22\u667a\u80fd\u4f53\u5728\u5355\u8f6e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u8054\u5408\u4f18\u5316\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u5904\u7406\u591a\u8f6e\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5bf9\u8bdd\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u5bf9\u8bdd\u8f6e\u6b21\u95f4\u4ea4\u7ec7\u641c\u7d22\u4e0e\u63a8\u7406\uff0c\u4f7f\u7528\u9488\u5bf9\u6f14\u5316\u7528\u6237\u76ee\u6807\u7684\u5b9a\u5236\u5956\u52b1\u51fd\u6570\uff0c\u5b66\u4e60\u63a2\u7d22\u6027\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u591a\u4e2a\u73b0\u6709\u5f3a\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u8f6e\u6b21\u4ea4\u7ec7\u641c\u7d22\u4e0e\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7528\u6237\u610f\u56fe\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u8bdd\u4ea4\u4e92\u6027\u80fd\u3002"}}
{"id": "2601.12423", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12423", "abs": "https://arxiv.org/abs/2601.12423", "authors": ["Antonin Clerc", "Michael Quellmalz", "Moritz Piening", "Philipp Flotho", "Gregor Kornhardt", "Gabriele Steidl"], "title": "HOT-POT: Optimal Transport for Sparse Stereo Matching", "comment": "18 pages, 10 figures, 6 tables", "summary": "Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u7a00\u758f\u7279\u5f81\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u673a\u51e0\u4f55\u7684\u7ebf\u7ea6\u675f\u89e3\u51b3\u906e\u6321\u3001\u8fd0\u52a8\u7b49\u6311\u6218\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9762\u90e8\u6807\u5fd7\u70b9\u5339\u914d", "motivation": "\u7acb\u4f53\u89c6\u89c9\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u3001\u9762\u90e8\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u9762\u4e34\u906e\u6321\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u7578\u53d8\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u7a00\u758f\u7279\u5f81\uff08\u5982\u9762\u90e8\u6807\u5fd7\u70b9\uff09\u7684\u7acb\u4f53\u5339\u914d\u56e0\u53c2\u6570\u654f\u611f\u6027\u800c\u66f4\u52a0\u590d\u6742", "method": "\u4ece\u6700\u4f18\u4f20\u8f93\u89c6\u89d2\u8003\u8651\u76f8\u673a\u51e0\u4f55\u7684\u7ebf\u7ea6\u675f\uff0c\u5c06\u76f8\u673a\u6295\u5f71\u70b9\u5efa\u6a21\u4e3a\uff08\u534a\uff09\u7ebf\uff0c\u4f7f\u7528\u7ecf\u5178\u6781\u7ebf\u8ddd\u79bb\u548c3D\u5c04\u7ebf\u8ddd\u79bb\u91cf\u5316\u5339\u914d\u8d28\u91cf\uff0c\u4f5c\u4e3a\uff08\u90e8\u5206\uff09\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u6210\u672c\u51fd\u6570\uff0c\u5f62\u6210\u9ad8\u6548\u53ef\u89e3\u7684\u5206\u914d\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u5206\u5c42\u6700\u4f18\u4f20\u8f93\u7684\u65e0\u76d1\u7763\u7269\u4f53\u5339\u914d", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7279\u5f81\u548c\u7269\u4f53\u5339\u914d\uff0c\u5728\u9762\u90e8\u5206\u6790\u5e94\u7528\u4e2d\u6210\u529f\u5339\u914d\u4e0d\u540c\u7684\u6807\u5fd7\u70b9\u6807\u6ce8\u89c4\u8303", "conclusion": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u7ebf\u7ea6\u675f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u7279\u5f81\u7acb\u4f53\u5339\u914d\u7684\u6b20\u5b9a\u95ee\u9898\uff0c\u4e3a\u65e0\u76d1\u7763\u7a00\u758f\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9762\u90e8\u6807\u5fd7\u70b9\u5339\u914d\u7b49\u5e94\u7528"}}
{"id": "2601.13137", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13137", "abs": "https://arxiv.org/abs/2601.13137", "authors": ["Yuan Gao", "Zhigang Liu", "Xinyu Yao", "Bo Chen", "Xiaobing Zhao"], "title": "Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains", "comment": "13 pages, 5 figures", "summary": "With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u5bf9\u9f50\u6846\u67b6VC-LLM\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u9886\u57df\u7684\u4ef7\u503c\u89c2\u4e00\u81f4\u6027", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5728\u79cd\u65cf\u3001\u793e\u4f1a\u3001\u653f\u6cbb\u7b49\u654f\u611f\u9886\u57df\u51fa\u73b0\u4e86\u504f\u89c1\u548c\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u654f\u611f\u9886\u57df\u7684\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898", "method": "\u63d0\u51fa\u5bf9\u6297\u5bf9\u9f50\u6846\u67b6\uff1a1\uff09\u6301\u7eed\u9884\u8bad\u7ec3\uff1b2\uff09\u6307\u4ee4\u5fae\u8c03\uff1b3\uff09\u5bf9\u6297\u8bad\u7ec3\uff08\u4f7f\u7528Attacker\u751f\u6210\u4e89\u8bae\u6027\u67e5\u8be2\uff0cActor\u751f\u6210\u4ef7\u503c\u89c2\u4e00\u81f4\u7684\u54cd\u5e94\uff0cCritic\u8fc7\u6ee4\u5e76\u786e\u4fdd\u54cd\u5e94\u8d28\u91cf\uff09\u3002\u8bad\u7ec3\u4e86VC-LLM\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u4e2d\u82f1\u53cc\u8bed\u8bc4\u4f30\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVC-LLM\u5728\u4e2d\u82f1\u6587\u6d4b\u8bd5\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u6297\u5bf9\u9f50\u6846\u67b6\u80fd\u6709\u6548\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u9886\u57df\u7684\u4ef7\u503c\u89c2\u4e00\u81f4\u6027\uff0cVC-LLM\u5728\u4ef7\u503c\u89c2\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2601.12432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.12432", "abs": "https://arxiv.org/abs/2601.12432", "authors": ["Shunyu Huang", "Yunjiao Zhou", "Jianfei Yang"], "title": "SkeFi: Cross-Modal Knowledge Transfer for Wireless Skeleton-Based Action Recognition", "comment": "Published in IEEE Internet of Things Journal", "summary": "Skeleton-based action recognition leverages human pose keypoints to categorize human actions, which shows superior generalization and interoperability compared to regular end-to-end action recognition. Existing solutions use RGB cameras to annotate skeletal keypoints, but their performance declines in dark environments and raises privacy concerns, limiting their use in smart homes and hospitals. This paper explores non-invasive wireless sensors, i.e., LiDAR and mmWave, to mitigate these challenges as a feasible alternative. Two problems are addressed: (1) insufficient data on wireless sensor modality to train an accurate skeleton estimation model, and (2) skeletal keypoints derived from wireless sensors are noisier than RGB, causing great difficulties for subsequent action recognition models. Our work, SkeFi, overcomes these gaps through a novel cross-modal knowledge transfer method acquired from the data-rich RGB modality. We propose the enhanced Temporal Correlation Adaptive Graph Convolution (TC-AGC) with frame interactive enhancement to overcome the noise from missing or inconsecutive frames. Additionally, our research underscores the effectiveness of enhancing multiscale temporal modeling through dual temporal convolution. By integrating TC-AGC with temporal modeling for cross-modal transfer, our framework can extract accurate poses and actions from noisy wireless sensors. Experiments demonstrate that SkeFi realizes state-of-the-art performances on mmWave and LiDAR. The code is available at https://github.com/Huang0035/Skefi.", "AI": {"tldr": "SkeFi\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5229\u7528\u6570\u636e\u4e30\u5bcc\u7684RGB\u6a21\u6001\u6765\u589e\u5f3a\u65e0\u7ebf\u4f20\u611f\u5668\uff08LiDAR\u548cmmWave\uff09\u7684\u9aa8\u67b6\u4f30\u8ba1\u548c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u9ed1\u6697\u73af\u5883\u548c\u9690\u79c1\u9650\u5236\u4e0b\u7684\u52a8\u4f5c\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8eRGB\u6444\u50cf\u5934\u7684\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u5728\u9ed1\u6697\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u4e14\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u667a\u80fd\u5bb6\u5c45\u548c\u533b\u9662\u7b49\u573a\u666f\u7684\u5e94\u7528\u3002\u65e0\u7ebf\u4f20\u611f\u5668\uff08LiDAR\u548cmmWave\uff09\u4f5c\u4e3a\u975e\u4fb5\u5165\u5f0f\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u548c\u566a\u58f0\u5927\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSkeFi\u6846\u67b6\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u4eceRGB\u6a21\u6001\u8fc1\u79fb\u77e5\u8bc6\u3002\u6838\u5fc3\u5305\u62ec\u589e\u5f3a\u7684\u65f6\u5e8f\u76f8\u5173\u81ea\u9002\u5e94\u56fe\u5377\u79ef\uff08TC-AGC\uff09\u548c\u5e27\u4ea4\u4e92\u589e\u5f3a\u6765\u5904\u7406\u7f3a\u5931\u6216\u4e0d\u8fde\u7eed\u5e27\u7684\u566a\u58f0\uff0c\u4ee5\u53ca\u53cc\u65f6\u5e8f\u5377\u79ef\u589e\u5f3a\u591a\u5c3a\u5ea6\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSkeFi\u5728mmWave\u548cLiDAR\u4f20\u611f\u5668\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ece\u566a\u58f0\u65e0\u7ebf\u4f20\u611f\u5668\u4e2d\u63d0\u53d6\u51c6\u786e\u7684\u59ff\u6001\u548c\u52a8\u4f5c\u4fe1\u606f\u3002", "conclusion": "SkeFi\u901a\u8fc7\u521b\u65b0\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u548c\u566a\u58f0\u9c81\u68d2\u6027\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u7ebf\u4f20\u611f\u5668\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u9ed1\u6697\u73af\u5883\u548c\u9690\u79c1\u654f\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13155", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13155", "abs": "https://arxiv.org/abs/2601.13155", "authors": ["Zimeng Wu", "Donghao Wang", "Chaozhe Jin", "Jiaxin Chen", "Yunhong Wang"], "title": "Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference", "comment": null, "summary": "Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\\times$ and 2.29$\\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.", "AI": {"tldr": "SPTS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u6ce8\u610f\u529b\u63a2\u6d4b\u548c\u4f4e\u79e9\u53d8\u6362\u63a2\u6d4b\u9009\u62e9\u6027\u8df3\u8fc7token\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u5ef6\u8fdf\u526a\u679d\u7b56\u7565\uff0c\u5b9e\u73b02.46\u500d\u9884\u586b\u5145\u548c2.29\u500d\u7aef\u5230\u7aef\u751f\u6210\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301SOTA\u6027\u80fd\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u4f46\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709token\u5bfc\u5411\u65b9\u6cd5\uff08\u5982\u526a\u679d\u548c\u8df3\u8fc7\uff09\u5b58\u5728\u52a0\u901f\u6f5c\u529b\u6709\u9650\u3001\u4ee3\u7406\u4fe1\u53f7\u8fc7\u65f6\u548c\u5197\u4f59\u5e72\u6270\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u901f\u5ea6-\u51c6\u786e\u7387\u6743\u8861\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSPTS\u6846\u67b6\uff1a1\uff09\u90e8\u5206\u6ce8\u610f\u529b\u63a2\u6d4b\uff08PAP\uff09\u901a\u8fc7\u90e8\u5206\u524d\u5411\u6ce8\u610f\u529b\u8ba1\u7b97\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684token\uff1b2\uff09\u4f4e\u79e9\u53d8\u6362\u63a2\u6d4b\uff08LTP\uff09\u6784\u5efa\u4f4e\u79e9\u4ee3\u7406\u7f51\u7edc\u9884\u6d4btoken\u53d8\u6362\uff1b3\uff09\u591a\u9636\u6bb5\u5ef6\u8fdf\u526a\u679d\uff08MSDP\uff09\u91cd\u65b0\u5206\u914d\u8df3\u8fc7\u9884\u7b97\u5e76\u5728\u5404\u5c42\u9010\u6b65\u526a\u679d\u5197\u4f59token\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\uff0c\u5b9e\u73b0\u9884\u586b\u51452.46\u500d\u548c\u7aef\u5230\u7aef\u751f\u62102.29\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5148\u8fdb\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "SPTS\u901a\u8fc7\u521b\u65b0\u7684token\u8df3\u8fc7\u7b56\u7565\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12443", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12443", "abs": "https://arxiv.org/abs/2601.12443", "authors": ["Xiaowei Fu", "Lei Zhang"], "title": "Adversarial Defense in Vision-Language Models: An Overview", "comment": null, "summary": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u6027\u9632\u5fa1\u7684\u4e09\u5927\u8303\u5f0f\uff1a\u8bad\u7ec3\u65f6\u9632\u5fa1\u3001\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\u548c\u514d\u8bad\u7ec3\u9632\u5fa1\uff0c\u5206\u6790\u4e86\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u53ca\u5f53\u524d\u6311\u6218\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\u5f15\u53d1\u4e86\u5b89\u5168\u62c5\u5fe7\u3002\u8fd9\u4e9b\u653b\u51fb\u53ef\u80fd\u635f\u5bb3\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u548c\u7cfb\u7edf\u5b89\u5168\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u672c\u6587\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u4e09\u79cd\u4e3b\u8981\u9632\u5fa1\u8303\u5f0f\uff1a1\uff09\u8bad\u7ec3\u65f6\u9632\u5fa1\uff08\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u589e\u5f3a\u9c81\u68d2\u6027\uff09\uff1b2\uff09\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\uff08\u5728\u63a8\u7406\u65f6\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u5904\u7406\u5bf9\u6297\u6837\u672c\uff09\uff1b3\uff09\u514d\u8bad\u7ec3\u9632\u5fa1\uff08\u901a\u8fc7\u4fee\u6539\u8f93\u5165\u6216\u7279\u5f81\u5d4c\u5165\u6765\u7f13\u89e3\u653b\u51fb\u5f71\u54cd\uff09\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u5404\u79cd\u9632\u5fa1\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff1a\u8bad\u7ec3\u65f6\u9632\u5fa1\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u9632\u5fa1\u7075\u6d3b\u4f46\u589e\u52a0\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff1b\u514d\u8bad\u7ec3\u9632\u5fa1\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4f46\u6548\u679c\u53ef\u80fd\u53d7\u9650\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u6027\u9632\u5fa1\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u66f4\u9ad8\u6548\u3001\u901a\u7528\u4e14\u5b9e\u7528\u7684\u9632\u5fa1\u7b56\u7565\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2601.13178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13178", "abs": "https://arxiv.org/abs/2601.13178", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy Burdick", "Philip Resnik", "Roshnik Rahat", "Sarah DeLozier", "Sarah M. Preum"], "title": "Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages", "comment": "19 Pages, 5 Figures", "summary": "Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `\"which message is more medically urgent\" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.\n  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage", "AI": {"tldr": "\u9996\u4e2a\u5927\u89c4\u6a21\u533b\u7597\u5206\u8bca\u516c\u5f00\u6570\u636e\u96c6PMR-Bench\uff0c\u7528\u4e8e\u5f02\u6b65\u95e8\u8bca\u6d88\u606f\u7684\u533b\u7597\u7d27\u6025\u5ea6\u8bc4\u4f30\uff0c\u901a\u8fc7\u6210\u5bf9\u63a8\u7406\u4efb\u52a1\u8bad\u7ec3LLM\u8fdb\u884c\u6d88\u606f\u7d27\u6025\u5ea6\u6392\u5e8f", "motivation": "\u533b\u7597\u5206\u8bca\u5bf9\u8d44\u6e90\u5206\u914d\u548c\u60a3\u8005\u4f18\u5148\u7ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u6765\u7814\u7a76\u5f02\u6b65\u95e8\u8bca\u6d88\u606f\u7684\u533b\u7597\u7d27\u6025\u5ea6\u8bc4\u4f30", "method": "\u5c06\u60a3\u8005\u6d88\u606f\u5206\u8bca\u89c6\u4e3a\u6210\u5bf9\u63a8\u7406\u95ee\u9898\uff0c\u6784\u5efaPMR-Bench\u6570\u636e\u96c6\uff081569\u6761\u6d88\u606f+2000+\u6d4b\u8bd5\u5bf9\uff09\uff0c\u5f00\u53d1\u81ea\u52a8\u6570\u636e\u6807\u6ce8\u7b56\u7565\uff0c\u8bad\u7ec3UrgentReward\uff08Bradley-Terry\u76ee\u6807\uff09\u548cUrgentSFT\uff08\u4e0b\u4e00\u8bcd\u9884\u6d4b\u76ee\u6807\uff09\u4e24\u79cd\u6a21\u578b", "result": "UrgentSFT\u5728PMR-Bench\u4e0a\u8868\u73b0\u6700\u4f73\uff0cUrgentReward\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e2d\u4f18\u52bf\u660e\u663e\uff1b8B\u6a21\u578b\u76f8\u6bd4\u73b0\u62108B\u6a21\u578b\u5728\u6536\u4ef6\u7bb1\u6392\u5e8f\u6307\u6807\u4e0a\u63d0\u534715-16\u4e2a\u767e\u5206\u70b9", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u7597\u6d88\u606f\u5206\u8bca\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u548c\u6709\u6548\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86LLM\u5728\u533b\u7597\u7d27\u6025\u5ea6\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u7ed3\u5408\u771f\u5b9eEHR\u6570\u636e\u7684\u60c5\u51b5\u4e0b"}}
{"id": "2601.12464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12464", "abs": "https://arxiv.org/abs/2601.12464", "authors": ["Yanrui Lu", "Danyang Chen", "Haowen Xiao", "Jiarui Zhu", "Fukang Ge", "Binqian Zou", "Jiali Guan", "Jiayin Liang", "Yuting Wang", "Ziqian Guan", "Xiangcheng Bao", "Jinhao Bi", "Lin Gu", "Jun He", "Yingying Zhu"], "title": "Large-scale EM Benchmark for Multi-Organelle Instance Segmentation in the Wild", "comment": null, "summary": "Accurate instance-level segmentation of organelles in electron microscopy (EM) is critical for quantitative analysis of subcellular morphology and inter-organelle interactions. However, current benchmarks, based on small, curated datasets, fail to capture the inherent heterogeneity and large spatial context of in-the-wild EM data, imposing fundamental limitations on current patch-based methods. To address these limitations, we developed a large-scale, multi-source benchmark for multi-organelle instance segmentation, comprising over 100,000 2D EM images across variety cell types and five organelle classes that capture real-world variability. Dataset annotations were generated by our designed connectivity-aware Label Propagation Algorithm (3D LPA) with expert refinement. We further benchmarked several state-of-the-art models, including U-Net, SAM variants, and Mask2Former. Our results show several limitations: current models struggle to generalize across heterogeneous EM data and perform poorly on organelles with global, distributed morphologies (e.g., Endoplasmic Reticulum). These findings underscore the fundamental mismatch between local-context models and the challenge of modeling long-range structural continuity in the presence of real-world variability. The benchmark dataset and labeling tool will be publicly released soon.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6e90\u591a\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u5f202D EM\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u7ec6\u80de\u7c7b\u578b\u548c5\u79cd\u7ec6\u80de\u5668\u7c7b\u522b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6570\u636e\u91cf\u5c0f\u3001\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5c0f\u578b\u7cbe\u9009\u6570\u636e\u96c6\u7684\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754cEM\u6570\u636e\u7684\u56fa\u6709\u5f02\u8d28\u6027\u548c\u5927\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u8fd9\u5bf9\u57fa\u4e8epatch\u7684\u65b9\u6cd5\u6784\u6210\u4e86\u6839\u672c\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "1) \u5f00\u53d1\u5927\u89c4\u6a21\u591a\u6e90\u591a\u7ec6\u80de\u5668\u5b9e\u4f8b\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u5f202D EM\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u7ec6\u80de\u7c7b\u578b\u548c5\u79cd\u7ec6\u80de\u5668\u7c7b\u522b\uff1b2) \u4f7f\u7528\u8bbe\u8ba1\u7684\u8fde\u901a\u6027\u611f\u77e5\u6807\u7b7e\u4f20\u64ad\u7b97\u6cd5(3D LPA)\u751f\u6210\u6570\u636e\u96c6\u6807\u6ce8\uff0c\u5e76\u8fdb\u884c\u4e13\u5bb6\u7cbe\u4fee\uff1b3) \u5bf9U-Net\u3001SAM\u53d8\u4f53\u548cMask2Former\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u5f02\u8d28EM\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bf9\u5177\u6709\u5168\u5c40\u5206\u5e03\u5f62\u6001\u7684\u7ec6\u80de\u5668(\u5982\u5185\u8d28\u7f51)\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5c40\u90e8\u4e0a\u4e0b\u6587\u6a21\u578b\u4e0e\u5efa\u6a21\u957f\u8ddd\u79bb\u7ed3\u6784\u8fde\u7eed\u6027\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u771f\u5b9e\u4e16\u754c\u5f02\u8d28\u6027\u548c\u957f\u8ddd\u79bb\u7ed3\u6784\u8fde\u7eed\u6027\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6807\u6ce8\u5de5\u5177\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u63a8\u52a8EM\u56fe\u50cf\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.13183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13183", "abs": "https://arxiv.org/abs/2601.13183", "authors": ["Sergio Servantez", "Sarah B. Lawsky", "Rajiv Jain", "Daniel W. Linna", "Kristian Hammond"], "title": "OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand", "comment": "25 pages, 9 Figures, 15 tables", "summary": "Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.", "AI": {"tldr": "OpenExempt\u662f\u4e00\u4e2a\u7528\u4e8e\u6cd5\u5f8b\u63a8\u7406\u8bca\u65ad\u8bc4\u4f30\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7f8e\u56fd\u7834\u4ea7\u6cd5\u7b26\u53f7\u8868\u793a\u52a8\u6001\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\uff0c\u5305\u542b9,765\u4e2a\u6837\u672c\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u957f\u63a8\u7406\u8def\u5f84\u548c\u6df7\u6dc6\u8bed\u53e5\u4e0b\u7684\u6027\u80fd\u60ac\u5d16\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u9759\u6001\u95ee\u7b54\u5bf9\u53ea\u80fd\u63d0\u4f9b\u6027\u80fd\u5feb\u7167\uff0c\u5c06\u590d\u6742\u884c\u4e3a\u538b\u7f29\u4e3a\u5355\u4e00\u51c6\u786e\u7387\u6307\u6807\u3002\u5728\u6cd5\u5f8b\u7b49\u590d\u6742\u89c4\u5219\u9886\u57df\uff0c\u73b0\u6709\u57fa\u51c6\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u5408\u9694\u79bb\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u5f00\u53d1OpenExempt\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u5bb6\u8bbe\u8ba1\u7684\u7f8e\u56fd\u7834\u4ea7\u6cd5\u6cd5\u89c4\u7b26\u53f7\u8868\u793a\uff0c\u52a8\u6001\u751f\u6210\u5927\u91cf\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u53ca\u5176\u673a\u5668\u53ef\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002\u7528\u6237\u53ef\u4ee5\u7cbe\u7ec6\u63a7\u5236\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u8303\u56f4\uff0c\u5355\u72ec\u6d4b\u8bd5\u7279\u5b9a\u63a8\u7406\u6280\u80fd\u3002\u57fa\u4e8e\u6b64\u6784\u5efaOpenExempt\u57fa\u51c6\uff0c\u5305\u542b9,765\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u4e5d\u4e2a\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u572813\u4e2a\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u8f83\u957f\u63a8\u7406\u8def\u5f84\u548c\u5b58\u5728\u6df7\u6dc6\u8bed\u53e5\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u51fa\u73b0\u6025\u5267\u4e0b\u964d\uff08\u6027\u80fd\u60ac\u5d16\uff09\u3002", "conclusion": "OpenExempt\u6846\u67b6\u548c\u57fa\u51c6\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u4e0b\u4e00\u4ee3\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5df2\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2601.12468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12468", "abs": "https://arxiv.org/abs/2601.12468", "authors": ["Yanqi Wu", "Qichao Chen", "Runhe Lai", "Xinhua Lu", "Jia-Xin Zhuang", "Zhilin Zhao", "Wei-Shi Zheng", "Ruixuan Wang"], "title": "DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors", "comment": "9 pages, 9 figures, Accepted by AAAI2026", "summary": "Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.", "AI": {"tldr": "DCAC\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6821\u51c6\u6a21\u5757\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2aID\u7c7b\u522b\u7ef4\u62a4\u7f13\u5b58\u6765\u6536\u96c6\u9ad8\u71b5\u6837\u672c\uff0c\u7f13\u89e3OOD\u6837\u672c\u7684\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728OOD\u68c0\u6d4b\u4e2d\u5b58\u5728\u5173\u952e\u95ee\u9898\uff1a\u5bf9\u672a\u89c1OOD\u6837\u672c\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u88ab\u9884\u6d4b\u4e3a\u540c\u4e00\u7c7b\u522b\u6216\u5177\u6709\u9ad8\u6982\u7387\u7684OOD\u6837\u672c\u5728\u89c6\u89c9\u4e0a\u5f7c\u6b64\u76f8\u4f3c\u5ea6\u9ad8\u4e8e\u771f\u5b9eID\u6837\u672c\uff0c\u8fd9\u4e00\u7c7b\u522b\u7279\u5f02\u6027\u89c2\u5bdf\u542f\u53d1\u4e86DCAC\u7684\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faDCAC\uff08\u52a8\u6001\u7c7b\u522b\u611f\u77e5\u7f13\u5b58\uff09\uff0c\u4e3a\u6bcf\u4e2aID\u7c7b\u522b\u7ef4\u62a4\u72ec\u7acb\u7f13\u5b58\u6536\u96c6\u9ad8\u71b5\u6837\u672c\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e24\u5c42\u6a21\u5757\u5229\u7528\u7f13\u5b58\u89c6\u89c9\u7279\u5f81\u548c\u9884\u6d4b\u6982\u7387\u6765\u6821\u51c6\u539f\u59cb\u9884\u6d4b\u3002\u8be5\u6a21\u5757\u65e0\u9700\u8bad\u7ec3\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u3002", "result": "\u5728\u591a\u4e2aOOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCAC\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u4e0eASH-S\u96c6\u6210\u5728ImageNet OOD\u57fa\u51c6\u4e0a\uff0c\u5c06FPR95\u964d\u4f4e\u4e866.55%\u3002\u8be5\u6a21\u5757\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u9002\u7528\u4e8e\u5355\u6a21\u6001\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "DCAC\u901a\u8fc7\u7c7b\u522b\u7279\u5f02\u6027\u7f13\u5b58\u673a\u5236\u6709\u6548\u7f13\u89e3OOD\u6837\u672c\u7684\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u6821\u51c6\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2601.13217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13217", "abs": "https://arxiv.org/abs/2601.13217", "authors": ["Bingsen Chen", "Boyan Li", "Ping Nie", "Yuyu Zhang", "Xi Ye", "Chen Zhao"], "title": "Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision", "comment": null, "summary": "Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Mr Dre\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u591a\u8f6e\u62a5\u544a\u4fee\u8ba2\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u4ee3\u7406\u5728\u4fee\u8ba2\u65f6\u4f1a\u9000\u531616-27%\u7684\u5148\u524d\u5185\u5bb9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u57fa\u51c6\u5c06\u62a5\u544a\u751f\u6210\u89c6\u4e3a\u5355\u6b21\u5199\u4f5c\u4efb\u52a1\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u7814\u7a76\u8005\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u6216\u540c\u884c\u53cd\u9988\u8fed\u4ee3\u8d77\u8349\u548c\u4fee\u8ba2\u62a5\u544a\u7684\u65b9\u5f0f\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u80fd\u5426\u53ef\u9760\u5730\u6839\u636e\u7528\u6237\u53cd\u9988\u4fee\u8ba2\u62a5\u544a\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5f15\u5165Mr Dre\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u62ec\uff1a(1)\u7edf\u4e00\u7684\u957f\u671f\u62a5\u544a\u8bc4\u4f30\u534f\u8bae\uff0c\u6db5\u76d6\u5168\u9762\u6027\u3001\u4e8b\u5b9e\u6027\u548c\u5448\u73b0\u8d28\u91cf\uff1b(2)\u4eba\u5de5\u9a8c\u8bc1\u7684\u53cd\u9988\u6a21\u62df\u7ba1\u9053\uff0c\u7528\u4e8e\u591a\u8f6e\u4fee\u8ba2\u8bc4\u4f30\u3002\u5206\u6790\u4e86\u4e94\u4e2a\u4e0d\u540c\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u3002", "result": "\u5206\u6790\u663e\u793a\u5173\u952e\u9650\u5236\uff1a\u867d\u7136\u4ee3\u7406\u80fd\u591f\u5904\u7406\u5927\u591a\u6570\u7528\u6237\u53cd\u9988\uff0c\u4f46\u4e5f\u4f1a\u572816-27%\u7684\u5148\u524d\u8986\u76d6\u5185\u5bb9\u548c\u5f15\u7528\u8d28\u91cf\u4e0a\u51fa\u73b0\u9000\u5316\u3002\u5373\u4f7f\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\u5728\u591a\u8f6e\u4fee\u8ba2\u540e\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u7834\u574f\u53cd\u9988\u8303\u56f4\u4e4b\u5916\u7684\u5185\u5bb9\uff0c\u5e76\u4e14\u65e0\u6cd5\u4fdd\u7559\u65e9\u671f\u7f16\u8f91\u3002", "conclusion": "\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u591a\u8f6e\u62a5\u544a\u4fee\u8ba2\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u8fd9\u4e9b\u7f3a\u9677\u65e0\u6cd5\u901a\u8fc7\u63a8\u7406\u65f6\u4fee\u590d\uff08\u5982\u63d0\u793a\u5de5\u7a0b\u548c\u4e13\u95e8\u7684\u62a5\u544a\u4fee\u8ba2\u5b50\u4ee3\u7406\uff09\u8f7b\u6613\u89e3\u51b3\uff0c\u8868\u660e\u9700\u8981\u66f4\u6839\u672c\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2601.13228", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13228", "abs": "https://arxiv.org/abs/2601.13228", "authors": ["Tianqi Du", "Lizhe Fang", "Weijie Yang", "Chenheng Zhang", "Zeming Wei", "Yifei Wang", "Yisen Wang"], "title": "Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation", "comment": null, "summary": "Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.", "AI": {"tldr": "A3\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u81ea\u56de\u5f52\u5efa\u6a21\u6269\u5c55\u4e3a\u4efb\u610f\u987a\u5e8f\u3001\u4efb\u610f\u5b50\u96c6\u7684\u751f\u6210\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6df1\u5ea6\u5efa\u6a21\u4f18\u52bf\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u652f\u6301\u4efb\u610f\u987a\u5e8f\u751f\u6210\u548c\u53cc\u5411\u6761\u4ef6\u5316\uff0c\u4f46\u5355\u6b65\u4f9d\u8d56\u7684\u516c\u5f0f\u9650\u5236\u4e86\u5efa\u6a21\u6df1\u5ea6\uff0c\u5bfc\u81f4\u6837\u672c\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0d\u5982\u81ea\u56de\u5f52\u6a21\u578b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6982\u7387\u4e25\u8c28\u6027\u548c\u591a\u5c42\u4f9d\u8d56\u5efa\u6a21\uff0c\u53c8\u80fd\u7ee7\u627f\u6269\u6563\u6a21\u578b\u7075\u6d3b\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faA3\u6846\u67b6\uff0c\u5c06\u6807\u51c6\u81ea\u56de\u5f52\u5206\u89e3\u6269\u5c55\u5230\u4efb\u610ftoken\u7ec4\u548c\u751f\u6210\u987a\u5e8f\u3002\u901a\u8fc7\u53cc\u6d41\u6ce8\u610f\u529b\u67b6\u6784\u548c\u6e10\u8fdb\u9002\u5e94\u7b56\u7565\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6a21\u578b\u8f6c\u6362\u4e3a\u652f\u6301\u4efb\u610f\u987a\u5e8f\u9884\u6d4b\u7684\u6a21\u578b\u3002", "result": "\u5728\u95ee\u7b54\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u6545\u4e8b\u586b\u5145\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cA3\u4f18\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7075\u6d3b\u7684\u89e3\u7801\u80fd\u529b\u3002", "conclusion": "A3\u4e3a\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u65b0\u9896\u7684\u5efa\u6a21\u8303\u5f0f\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.12493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12493", "abs": "https://arxiv.org/abs/2601.12493", "authors": ["Mehrdad Noori", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Fereshteh Shakeri", "Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation", "comment": "Accepted to WACV 2026", "summary": "Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo\\-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM's downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at https://github.com/Mehrdad-Noori/Histopath-C.", "AI": {"tldr": "\u63d0\u51faHistopath-C\u57fa\u51c6\u6d4b\u8bd5\u548cLATTE\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u67d3\u8272\u3001\u6c61\u67d3\u3001\u6a21\u7cca\u548c\u566a\u58f0\u7b49\u4e25\u91cd\u57df\u504f\u79fb\u95ee\u9898\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "method": "1) \u521b\u5efaHistopath-C\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u7684\u5408\u6210\u635f\u574f\uff1b2) \u63d0\u51faLATTE\u65b9\u6cd5\uff0c\u4e00\u79cd\u5229\u7528\u591a\u6587\u672c\u6a21\u677f\u7684\u8f6c\u5bfc\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\uff0c\u51cf\u5c11\u6a21\u578b\u5bf9\u591a\u6837\u5316\u6587\u672c\u8f93\u5165\u7684\u654f\u611f\u6027\u3002", "result": "LATTE\u65b9\u6cd5\u5728\u591a\u4e2a\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4e3a\u81ea\u7136\u56fe\u50cf\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u8bbe\u8ba1\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u9c81\u68d2\u9002\u5e94\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684Histopath-C\u57fa\u51c6\u548cLATTE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u548c\u589e\u5f3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u504f\u79fb\u6311\u6218\u3002"}}
{"id": "2601.13247", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.13247", "abs": "https://arxiv.org/abs/2601.13247", "authors": ["Baochang Ren", "Yunzhi Yao", "Rui Sun", "Shuofei Qiao", "Ningyu Zhang", "Huajun Chen"], "title": "Aligning Agentic World Models via Knowledgeable Experience Learning", "comment": "Ongoing work", "summary": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.", "AI": {"tldr": "WorldMind\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u7b26\u53f7\u5316\u4e16\u754c\u77e5\u8bc6\u5e93\uff0c\u5229\u7528\u73af\u5883\u53cd\u9988\u89e3\u51b3LLM\u7684\u7269\u7406\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u8de8\u6a21\u578b\u548c\u8de8\u73af\u5883\u7684\u7269\u7406\u53ef\u884c\u6027\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6a21\u6001\u8131\u8282\u95ee\u9898\uff1a\u62e5\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u77e5\u8bc6\u4f46\u7f3a\u4e4f\u5bf9\u7269\u7406\u4e16\u754c\u4e0d\u53d8\u6cd5\u5219\u7684\u7a0b\u5e8f\u6027\u7406\u89e3\uff0c\u5bfc\u81f4\u4ea7\u751f\u7269\u7406\u4e0a\u4e0d\u53ef\u6267\u884c\u7684\u89c4\u5212\uff08\u7269\u7406\u5e7b\u89c9\uff09\u3002\u73b0\u6709\u5bf9\u9f50\u7b56\u7565\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u5c06\u52a8\u6001\u73af\u5883\u89c4\u5219\u538b\u7f29\u5230\u9759\u6001\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u56fa\u6709\u5730\u50f5\u5316\uff0c\u96be\u4ee5\u9002\u5e94\u7269\u7406\u52a8\u6001\u7684\u5f00\u653e\u53ef\u53d8\u6027\u3002", "method": "\u5f15\u5165WorldMind\u6846\u67b6\uff0c\u81ea\u4e3b\u6784\u5efa\u7b26\u53f7\u5316\u4e16\u754c\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u7efc\u5408\u73af\u5883\u53cd\u9988\u6765\u7edf\u4e00\u8fc7\u7a0b\u7ecf\u9a8c\uff08\u901a\u8fc7\u9884\u6d4b\u9519\u8bef\u5f3a\u5236\u6267\u884c\u7269\u7406\u53ef\u884c\u6027\uff09\u548c\u76ee\u6807\u7ecf\u9a8c\uff08\u901a\u8fc7\u6210\u529f\u8f68\u8ff9\u6307\u5bfc\u4efb\u52a1\u6700\u4f18\u6027\uff09\u3002", "result": "\u5728EB-ALFRED\u548cEB-Habitat\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWorldMind\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u663e\u8457\u7684\u8de8\u6a21\u578b\u548c\u8de8\u73af\u5883\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "WorldMind\u901a\u8fc7\u975e\u53c2\u6570\u5316\u7684\u7b26\u53f7\u77e5\u8bc6\u5e93\u6709\u6548\u89e3\u51b3\u4e86LLM\u7684\u7269\u7406\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u53ef\u8fc1\u79fb\u7684\u7269\u7406\u4e16\u754c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u6301\u7eed\u518d\u8bad\u7ec3\u9700\u6c42\u3002"}}
{"id": "2601.12500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12500", "abs": "https://arxiv.org/abs/2601.12500", "authors": ["Yaowu Fan", "Jia Wan", "Tao Han", "Andy J. Ma", "Antoni B. Chan"], "title": "Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods", "comment": null, "summary": "Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u79fb\u52a8\u65e0\u4eba\u673a\u7684\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u8ba1\u6570\u4e0e\u8ffd\u8e2a\u65b9\u6848\uff0c\u5305\u62ec\u65b0\u6570\u636e\u96c6MovingDroneCrowd++\u548cGD3A/DVTrack\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u56fa\u5b9a\u6444\u50cf\u5934\u65b9\u6cd5\u7a7a\u95f4\u8986\u76d6\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u5206\u6790\u9700\u6c42\uff0c\u9700\u8981\u79fb\u52a8\u65e0\u4eba\u673a\u63d0\u4f9b\u7684\u7075\u6d3b\u89c6\u89d2\u548c\u5168\u9762\u8986\u76d6", "method": "\u63d0\u51faGD3A\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5bc6\u5ea6\u56fe\u7684\u89c6\u9891\u4e2a\u4f53\u8ba1\u6570\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5efa\u7acb\u50cf\u7d20\u7ea7\u884c\u4eba\u63cf\u8ff0\u7b26\u5bf9\u5e94\u5173\u7cfb\uff0c\u5206\u89e3\u5168\u5c40\u5bc6\u5ea6\u56fe\u4e3a\u5171\u4eab\u3001\u6d41\u5165\u3001\u6d41\u51fa\u5206\u91cf\uff1bDVTrack\u65b9\u6cd5\uff1a\u901a\u8fc7\u63cf\u8ff0\u7b26\u6295\u7968\u673a\u5236\u5c06\u63cf\u8ff0\u7b26\u7ea7\u5339\u914d\u8f6c\u4e3a\u5b9e\u4f8b\u7ea7\u5173\u8054\u8fdb\u884c\u884c\u4eba\u8ffd\u8e2a", "result": "\u5728MovingDroneCrowd++\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u6570\u8bef\u5dee\u964d\u4f4e47.4%\uff0c\u8ffd\u8e2a\u6027\u80fd\u63d0\u534739.2%", "conclusion": "\u79fb\u52a8\u65e0\u4eba\u673a\u7ed3\u5408GD3A\u548cDVTrack\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u5bc6\u96c6\u4eba\u7fa4\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2601.13251", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13251", "abs": "https://arxiv.org/abs/2601.13251", "authors": ["Ebubekir Tosun", "Mehmet Emin Buldur", "\u00d6zay Ezerceli", "Mahmoud ElHussieni"], "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph", "comment": null, "summary": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.", "AI": {"tldr": "\u63d0\u51fa\u5927\u89c4\u6a21\u8bed\u4e49\u805a\u7c7b\u7cfb\u7edf\uff0c\u89e3\u51b3\u795e\u7ecf\u5d4c\u5165\u65e0\u6cd5\u533a\u5206\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u8def\u8bed\u4e49\u5173\u7cfb\u5224\u522b\u5668\u548c\u8f6f\u5230\u786c\u805a\u7c7b\u7b97\u6cd5\u751f\u6210290\u4e07\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7c07\u3002", "motivation": "\u795e\u7ecf\u5d4c\u5165\u5b58\u5728\u663e\u8457\u76f2\u70b9\uff1a\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\uff0c\u5bfc\u81f4\u63d0\u9ad8\u76f8\u4f3c\u5ea6\u9608\u503c\u4ecd\u65e0\u6cd5\u9632\u6b62\u53cd\u4e49\u8bcd\u88ab\u5f52\u4e3a\u4e00\u7ec4\u3002\u73b0\u6709\u540c\u4e49\u8bcd\u6570\u636e\u5e93\u5728\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7a00\u758f\u3002", "method": "1) \u6784\u5efa84.3\u4e07\u6982\u5ff5\u5bf9\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u540c\u4e49\u3001\u53cd\u4e49\u548c\u5171\u4e0b\u4f4d\u5173\u7cfb\uff0c\u4f7f\u7528Gemini 2.5-Flash LLM\u589e\u5f3a\u548c\u4eba\u5de5\u8bcd\u5178\u9a8c\u8bc1\uff1b2) \u63d0\u51fa\u4e09\u8def\u8bed\u4e49\u5173\u7cfb\u5224\u522b\u5668\uff0c\u5b9e\u73b090%\u5b8fF1\u5206\u6570\uff1b3) \u8bbe\u8ba1\u65b0\u9896\u7684\u8f6f\u5230\u786c\u805a\u7c7b\u7b97\u6cd5\uff0c\u91c7\u7528\u62d3\u6251\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u6269\u5c55-\u526a\u679d\u8fc7\u7a0b\uff0c\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\u548c\u9519\u8bef\u4f20\u9012\u94fe\u3002", "result": "\u5904\u74061500\u4e07\u4e2a\u8bcd\u6c47\u9879\uff0c\u8bc4\u4f305.2\u4ebf\u6f5c\u5728\u5173\u7cfb\uff0c\u6700\u7ec8\u751f\u6210290\u4e07\u4e2a\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7c07\u3002\u7cfb\u7edf\u80fd\u7cbe\u786e\u5206\u914d\u6bcf\u4e2a\u672f\u8bed\u5230\u5355\u4e00\u8bed\u4e49\u8fde\u8d2f\u7684\u7c07\u4e2d\u3002", "conclusion": "\u8be5\u8d44\u6e90\u652f\u6301\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u641c\u7d22\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f62\u6001\u4e30\u5bcc\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u540c\u4e49\u8bcd\u6570\u636e\u5e93\u7a00\u758f\u7684\u95ee\u9898\u3002"}}
{"id": "2601.12507", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12507", "abs": "https://arxiv.org/abs/2601.12507", "authors": ["Ruo Qi", "Linhui Dai", "Yusong Qin", "Chaolei Yang", "Yanshan Li"], "title": "SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection", "comment": null, "summary": "In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.", "AI": {"tldr": "SDCoNet\u63d0\u51fa\u4e00\u79cd\u663e\u8457\u6027\u9a71\u52a8\u7684\u591a\u4efb\u52a1\u534f\u4f5c\u7f51\u7edc\uff0c\u901a\u8fc7\u9690\u5f0f\u7279\u5f81\u5171\u4eab\u5c06\u8d85\u5206\u8fa8\u7387\u548c\u68c0\u6d4b\u4efb\u52a1\u8026\u5408\uff0c\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u4e2d\u590d\u6742\u80cc\u666f\u3001\u5f31\u76ee\u6807\u4fe1\u53f7\u548c\u5c0f\u76ee\u6807\u5c3a\u5ea6\u4f7f\u5f97\u51c6\u786e\u68c0\u6d4b\u7279\u522b\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d28\u91cf\u6210\u50cf\u6761\u4ef6\u4e0b\u3002\u4f20\u7edf\u7684\u4e32\u884c\u8d85\u5206\u8fa8\u7387+\u68c0\u6d4b\u6d41\u6c34\u7ebf\u5b58\u5728\u4f18\u5316\u76ee\u6807\u4e0d\u4e00\u81f4\u3001\u7279\u5f81\u5197\u4f59\u548c\u4efb\u52a1\u95f4\u7f3a\u4e4f\u6709\u6548\u4ea4\u4e92\u7b49\u95ee\u9898\u3002", "method": "1) \u57fa\u4e8eSwin Transformer\u7684\u5171\u4eab\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5206\u5c42\u7a97\u53e3\u79fb\u4f4d\u81ea\u6ce8\u610f\u529b\u652f\u6301\u8de8\u4efb\u52a1\u7279\u5f81\u534f\u4f5c\uff1b2) \u591a\u5c3a\u5ea6\u663e\u8457\u6027\u9884\u6d4b\u6a21\u5757\u751f\u6210\u91cd\u8981\u6027\u5206\u6570\u9009\u62e9\u5173\u952etoken\uff0c\u805a\u7126\u5f31\u76ee\u6807\u533a\u57df\uff1b3) \u68af\u5ea6\u8def\u7531\u7b56\u7565\u7f13\u89e3\u4f18\u5316\u51b2\u7a81\uff0c\u5148\u7a33\u5b9a\u68c0\u6d4b\u8bed\u4e49\uff0c\u518d\u6cbf\u68c0\u6d4b\u5bfc\u5411\u65b9\u5411\u8def\u7531\u8d85\u5206\u8fa8\u7387\u68af\u5ea6\u3002", "result": "\u5728NWPU VHR-10-Split\u3001DOTAv1.5-Split\u548cHRSSD-Split\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5728\u4f4e\u8d28\u91cf\u9065\u611f\u56fe\u50cf\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u7b97\u6cd5\u3002", "conclusion": "SDCoNet\u901a\u8fc7\u663e\u8457\u6027\u9a71\u52a8\u7684\u591a\u4efb\u52a1\u534f\u4f5c\u548c\u68af\u5ea6\u8def\u7531\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8d85\u5206\u8fa8\u7387\u548c\u68c0\u6d4b\u4efb\u52a1\u7684\u534f\u540c\u4f18\u5316\u3002"}}
{"id": "2601.13253", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13253", "abs": "https://arxiv.org/abs/2601.13253", "authors": ["Ebubekir Tosun", "Mehmet Emin Buldur", "\u00d6zay Ezerceli", "Mahmoud ElHussieni"], "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus", "comment": null, "summary": "We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u571f\u8033\u5176\u8bed\uff09\u751f\u6210\u5927\u89c4\u6a21\u8bed\u4e49\u5173\u7cfb\u6570\u636e\u96c6\uff0c\u5305\u542b84.3\u4e07\u4e2a\u8bed\u4e49\u5bf9\uff0c\u6210\u672c\u4ec565\u7f8e\u5143\uff0c\u6bd4\u73b0\u6709\u8d44\u6e90\u89c4\u6a21\u6269\u592710\u500d\u3002", "motivation": "\u89e3\u51b3\u571f\u8033\u5176\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8bed\u4e49\u5173\u7cfb\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8d44\u6e90\u89c4\u6a21\u6709\u9650\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u4e09\u9636\u6bb5\u6df7\u5408\u65b9\u6cd5\uff1a1) \u4f7f\u7528FastText\u5d4c\u5165\u548c\u51dd\u805a\u805a\u7c7b\u8bc6\u522b\u8bed\u4e49\u7c07\uff1b2) \u4f7f\u7528Gemini 2.5-Flash\u8fdb\u884c\u81ea\u52a8\u8bed\u4e49\u5173\u7cfb\u5206\u7c7b\uff1b3) \u6574\u5408\u7cbe\u9009\u8bcd\u5178\u8d44\u6e90\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b84.3\u4e07\u4e2a\u571f\u8033\u5176\u8bed\u8bed\u4e49\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u540c\u4e49\u8bcd\u3001\u53cd\u4e49\u8bcd\u3001\u540c\u4e0b\u4f4d\u8bcd\u4e09\u79cd\u5173\u7cfb\u7c7b\u578b\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u9a8c\u8bc1\u4e2d\uff0c\u5d4c\u5165\u6a21\u578b\u8fbe\u523090%\u7684top-1\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u5206\u7c7b\u6a21\u578b\u8fbe\u523090%\u7684F1-macro\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u4f4e\u6210\u672c\uff0865\u7f8e\u5143\uff09\u663e\u8457\u6269\u5c55\u4e86\u571f\u8033\u5176\u8bed\u8bed\u4e49\u5173\u7cfb\u8d44\u6e90\uff0c\u9a8c\u8bc1\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.12512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12512", "abs": "https://arxiv.org/abs/2601.12512", "authors": ["Mohd Usama", "Belal Ahmad", "Faleh Menawer R Althiyabi"], "title": "Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images", "comment": "14 pages, 9 figures, 2 tables", "summary": "Magnetic Resonance Imaging (MRI) scans acquired from different scanners or institutions often suffer from domain shifts owing to variations in hardware, protocols, and acquisition parameters. This discrepancy degrades the performance of deep learning models trained on source domain data when applied to target domain images. In this study, we propose a Cycle-GAN-based model for unsupervised medical-image domain adaptation. Leveraging CycleGANs, our model learns bidirectional mappings between the source and target domains without paired training data, preserving the anatomical content of the images. By leveraging Cycle-GAN capabilities with content and disparity loss for adaptation tasks, we ensured image-domain adaptation while maintaining image integrity. Several experiments on MRI datasets demonstrated the efficacy of our model in bidirectional domain adaptation without labelled data. Furthermore, research offers promising avenues for improving the diagnostic accuracy of healthcare. The statistical results confirm that our approach improves model performance and reduces domain-related variability, thus contributing to more precise and consistent medical image analysis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCycleGAN\u7684\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u89e3\u51b3MRI\u626b\u63cf\u5728\u4e0d\u540c\u626b\u63cf\u4eea/\u673a\u6784\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u53cc\u5411\u57df\u6620\u5c04\u3002", "motivation": "\u4e0d\u540c\u626b\u63cf\u4eea\u6216\u673a\u6784\u83b7\u53d6\u7684MRI\u626b\u63cf\u5b58\u5728\u57df\u504f\u79fb\uff08\u786c\u4ef6\u3001\u534f\u8bae\u3001\u91c7\u96c6\u53c2\u6570\u5dee\u5f02\uff09\uff0c\u5bfc\u81f4\u5728\u6e90\u57df\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u76ee\u6807\u57df\u56fe\u50cf\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528CycleGAN\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u57df\u9002\u5e94\uff0c\u5b66\u4e60\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u5185\u5bb9\u548c\u5dee\u5f02\u635f\u5931\u4fdd\u6301\u56fe\u50cf\u89e3\u5256\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5728MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\u6709\u6548\u5b9e\u73b0\u53cc\u5411\u57df\u9002\u5e94\uff0c\u7edf\u8ba1\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u57df\u76f8\u5173\u53d8\u5f02\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63d0\u5347\u533b\u7597\u8bca\u65ad\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u4e00\u81f4\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u3002"}}
{"id": "2601.13260", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13260", "abs": "https://arxiv.org/abs/2601.13260", "authors": ["Sawsan Alqahtani", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Tasnim Mohiuddin", "M Saiful Bari"], "title": "Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models", "comment": "Accepted to EACL 2026 (long, main). The first two authors contributed equally", "summary": "Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06\u5206\u8bcd\u89c6\u4e3a\u6838\u5fc3\u5efa\u6a21\u51b3\u7b56\u800c\u975e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u5f3a\u8c03\u5206\u8bcd\u5668\u4e0e\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u3001\u53ef\u9002\u5e94\u7684\u8bed\u8a00\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u5206\u8bcd\u65b9\u6cd5\uff08\u5982BPE\uff09\u867d\u7136\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u4e0e\u8bed\u8a00\u7ed3\u6784\u4e0d\u5bf9\u9f50\u3001\u653e\u5927\u504f\u89c1\u3001\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u9886\u57df\u6d6a\u8d39\u5bb9\u91cf\u7b49\u95ee\u9898\u3002\u5206\u8bcd\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u7ec4\u4ef6\uff0c\u5374\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u548c\u4e00\u81f4\u8bbe\u8ba1\uff0c\u88ab\u4f4e\u4f30\u4e3a\u6280\u672f\u7ec6\u8282\u800c\u975e\u6838\u5fc3\u5efa\u6a21\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u5c06\u5206\u8bcd\u5668\u4e0e\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\uff0c\u8003\u8651\u8bed\u8a00\u3001\u9886\u57df\u548c\u90e8\u7f72\u9700\u6c42\u3002\u5f3a\u8c03\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u900f\u660e\u62a5\u544a\uff0c\u4f7f\u5206\u8bcd\u9009\u62e9\u53ef\u95ee\u8d23\u3001\u53ef\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5c06\u5206\u8bcd\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u95ee\u9898\u800c\u975e\u6280\u672f\u7ec6\u8282\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u3001\u53ef\u9002\u5e94\u7684\u8bed\u8a00\u6280\u672f\u3002\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u900f\u660e\u62a5\u544a\u673a\u5236\u4f7f\u5206\u8bcd\u9009\u62e9\u66f4\u52a0\u8d1f\u8d23\u4efb\u548c\u53ef\u6bd4\u8f83\u3002", "conclusion": "\u5206\u8bcd\u5e94\u88ab\u89c6\u4e3a\u6838\u5fc3\u5efa\u6a21\u51b3\u7b56\u800c\u975e\u9884\u5904\u7406\u6b65\u9aa4\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\u548c\u5206\u8bcd\u5668-\u6a21\u578b\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u900f\u660e\u62a5\u544a\uff0c\u53ef\u4ee5\u521b\u5efa\u66f4\u516c\u5e73\u3001\u9ad8\u6548\u3001\u53ef\u9002\u5e94\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2601.13264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13264", "abs": "https://arxiv.org/abs/2601.13264", "authors": ["Tyler Lizzo", "Larry Heck"], "title": "Unlearning in LLMs: Methods, Evaluation, and Open Challenges", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u6280\u672f\u5206\u4e3a\u6570\u636e\u4e2d\u5fc3\u3001\u53c2\u6570\u4e2d\u5fc3\u3001\u67b6\u6784\u4e2d\u5fc3\u3001\u6df7\u5408\u7b49\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u9057\u5fd8\u6548\u679c\u3001\u77e5\u8bc6\u4fdd\u7559\u548c\u9c81\u68d2\u6027\uff0c\u6700\u540e\u6307\u51fa\u4e86\u53ef\u6269\u5c55\u6027\u3001\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7b49\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5e7f\u6cdb\u90e8\u7f72\u5f15\u53d1\u4e86\u9690\u79c1\u3001\u7248\u6743\u3001\u5b89\u5168\u548c\u504f\u89c1\u7b49\u7d27\u8feb\u95ee\u9898\u3002\u673a\u5668\u9057\u5fd8\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u6027\u5730\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u77e5\u8bc6\u6216\u6570\u636e\u3002", "method": "\u8bba\u6587\u63d0\u4f9b\u4e86LLMs\u9057\u5fd8\u65b9\u6cd5\u7684\u7ed3\u6784\u5316\u6982\u8ff0\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u4e3a\uff1a1\uff09\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\uff1b2\uff09\u53c2\u6570\u4e2d\u5fc3\u65b9\u6cd5\uff1b3\uff09\u67b6\u6784\u4e2d\u5fc3\u65b9\u6cd5\uff1b4\uff09\u6df7\u5408\u65b9\u6cd5\uff1b5\uff09\u5176\u4ed6\u7b56\u7565\u3002\u540c\u65f6\u56de\u987e\u4e86\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ec\u57fa\u51c6\u6d4b\u8bd5\u3001\u6307\u6807\u548c\u6570\u636e\u96c6\u3002", "result": "\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e86\u5f53\u524dLLMs\u9057\u5fd8\u6280\u672f\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5efa\u7acb\u4e86\u5206\u7c7b\u6846\u67b6\u548c\u8bc4\u4f30\u4f53\u7cfb\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u9057\u5fd8\u6280\u672f\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7efc\u5408\u5f53\u524d\u8fdb\u5c55\u548c\u7a81\u51fa\u5f00\u653e\u65b9\u5411\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u548c\u8d1f\u8d23\u4efb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6280\u672f\u63d0\u4f9b\u8def\u7ebf\u56fe\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u6269\u5c55\u6548\u7387\u3001\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3001\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u9057\u5fd8\u3001\u5bf9\u6297\u6027\u91cd\u65b0\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u7b49\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2601.12530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12530", "abs": "https://arxiv.org/abs/2601.12530", "authors": ["Jan Fabian Schmid", "Annika Hagemann"], "title": "XRefine: Attention-Guided Keypoint Match Refinement", "comment": null, "summary": "Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at https://github.com/boschresearch/xrefine.", "AI": {"tldr": "XRefine\uff1a\u4e00\u79cd\u4e0e\u68c0\u6d4b\u5668\u65e0\u5173\u7684\u4e9a\u50cf\u7d20\u5173\u952e\u70b9\u7ec6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u67b6\u6784\u5728\u56fe\u50cf\u5757\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u68c0\u6d4b\u5668\u5185\u90e8\u8868\u793a\uff0c\u80fd\u63d0\u5347\u51e0\u4f55\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u5e38\u4ea7\u751f\u7a7a\u95f4\u4e0d\u51c6\u786e\u7684\u5339\u914d\uff0c\u73b0\u6709\u7ec6\u5316\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u68c0\u6d4b\u5668\u8bbe\u8ba1\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u68c0\u6d4b\u5668\u91cd\u65b0\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faXRefine\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8de8\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u4ec5\u4f7f\u7528\u4ee5\u5339\u914d\u5173\u952e\u70b9\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u5757\u6765\u9884\u6d4b\u7ec6\u5316\u540e\u7684\u5173\u952e\u70b9\u5750\u6807\uff0c\u4e0d\u4f9d\u8d56\u68c0\u6d4b\u5668\u5185\u90e8\u8868\u793a\uff0c\u53ef\u6269\u5c55\u5230\u591a\u89c6\u89d2\u7279\u5f81\u8ddf\u8e2a\u3002", "result": "\u5728MegaDepth\u3001KITTI\u548cScanNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u51e0\u4f55\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7ec6\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "XRefine\u662f\u4e00\u79cd\u6709\u6548\u7684\u68c0\u6d4b\u5668\u65e0\u5173\u4e9a\u50cf\u7d20\u5173\u952e\u70b9\u7ec6\u5316\u65b9\u6cd5\uff0c\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u68c0\u6d4b\u5668\uff0c\u63d0\u53473D\u89c6\u89c9\u4efb\u52a1\u7684\u5339\u914d\u7cbe\u5ea6\u3002"}}
{"id": "2601.13288", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13288", "abs": "https://arxiv.org/abs/2601.13288", "authors": ["Gonzalo Ariel Meyoyan", "Luciano Del Corro"], "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification", "comment": null, "summary": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.", "AI": {"tldr": "\u63d0\u51fa\u5728LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u590d\u7528\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5206\u7c7b\uff0c\u907f\u514d\u989d\u5916\u5b89\u5168\u6a21\u578b\u7684\u5f00\u9500", "motivation": "\u73b0\u6709\u751f\u4ea7LLM\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5355\u72ec\u7684\u5b89\u5168\u5206\u7c7b\u6a21\u578b\uff0c\u8fd9\u4f1a\u589e\u52a0\u5ef6\u8fdf\u3001VRAM\u5360\u7528\u548c\u64cd\u4f5c\u590d\u6742\u6027", "method": "\u5728LLM\u9690\u85cf\u72b6\u6001\u4e0a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u63a2\u9488\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u805a\u5408\u5668\uff08\u5c42\u5185token\u6c47\u603b\u548c\u8de8\u5c42\u805a\u5408\uff09\u8fdb\u884c\u8868\u793a\u9009\u62e9", "result": "\u5728\u5b89\u5168\u548c\u60c5\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4ec5\u4f7f\u7528logit\u7684\u65b9\u6cd5\uff0c\u4e0e\u66f4\u5927\u7684\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u63a8\u7406\u5ef6\u8fdf", "conclusion": "\u901a\u8fc7\u590d\u7528LLM\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\uff0c\u51cf\u5c11VRAM\u548c\u5ef6\u8fdf\u6210\u672c\uff0c\u7b80\u5316\u64cd\u4f5c\u590d\u6742\u6027"}}
{"id": "2601.12533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12533", "abs": "https://arxiv.org/abs/2601.12533", "authors": ["Md. Ahanaf Arif Khan", "Ariful Islam", "Sangeeta Biswas", "Md. Iqbal Aziz Khan", "Subrata Pramanik", "Sanjoy Kumar Chakrabarty", "Bimal Kumar Pramanik"], "title": "BirdsEye-RU: A Dataset For Detecting Faces from Overhead Images", "comment": null, "summary": "Detecting faces in overhead images remains a significant challenge due to extreme scale variations and environmental clutter. To address this, we created the BirdsEye-RU dataset, a comprehensive collection of 2,978 images containing over eight thousand annotated faces. This dataset is specifically designed to capture small and distant faces across diverse environments, containing both drone images and smartphone-captured images from high altitude. We present a detailed description of the BirdsEye-RU dataset in this paper. We made our dataset freely available to the public, and it can be accessed at https://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BirdsEye-RU\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b2,978\u5f20\u56fe\u50cf\u3001\u8d85\u8fc78,000\u4e2a\u6807\u6ce8\u4eba\u8138\u7684\u4fef\u89c6\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\uff0c\u5305\u542b\u65e0\u4eba\u673a\u548c\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u9ad8\u7a7a\u56fe\u50cf\u3002", "motivation": "\u4fef\u89c6\u56fe\u50cf\u4e2d\u7684\u4eba\u8138\u68c0\u6d4b\u9762\u4e34\u6781\u5927\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u5c3a\u5ea6\u53d8\u5316\u6781\u7aef\u548c\u73af\u5883\u6742\u4e71\u3002\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6709\u6548\u5904\u7406\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86BirdsEye-RU\u6570\u636e\u96c6\uff0c\u5305\u542b2,978\u5f20\u56fe\u50cf\u548c\u8d85\u8fc78,000\u4e2a\u6807\u6ce8\u4eba\u8138\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6355\u6349\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\uff0c\u6570\u636e\u96c6\u5305\u542b\u65e0\u4eba\u673a\u548c\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u9ad8\u7a7a\u56fe\u50cf\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4fef\u89c6\u4eba\u8138\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u516c\u5f00\u514d\u8d39\u63d0\u4f9b\uff0c\u53ef\u901a\u8fc7Kaggle\u5e73\u53f0\u8bbf\u95ee\uff1ahttps://www.kaggle.com/datasets/mdahanafarifkhan/birdseye-ru", "conclusion": "BirdsEye-RU\u6570\u636e\u96c6\u4e3a\u89e3\u51b3\u4fef\u89c6\u56fe\u50cf\u4e2d\u5c0f\u5c3a\u5bf8\u548c\u8fdc\u8ddd\u79bb\u4eba\u8138\u68c0\u6d4b\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.13300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13300", "abs": "https://arxiv.org/abs/2601.13300", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "Yu-Hsiang Liu", "An-Zi Yen"], "title": "OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference", "comment": null, "summary": "Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5728\u591a\u9009\u9898\u754c\u9762\u6ce8\u5165\u8bef\u5bfc\u6027\u6307\u4ee4\u9009\u9879\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u51b3\u7b56\u4e0d\u4ec5\u53d7\u754c\u9762\u56e0\u7d20\u5f71\u54cd\uff0c\u8fd8\u4f1a\u88ab\u793e\u4f1a\u7ebf\u7d22\u3001\u6846\u67b6\u6548\u5e94\u548c\u6307\u4ee4\u7b49\u5bfc\u5411\u4fe1\u53f7\u6240\u5f71\u54cd\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u5728\u9009\u62e9\u9898\u754c\u9762\u4e2d\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\"\u9009\u9879\u6ce8\u5165\"\u65b9\u6cd5\uff0c\u5728\u6807\u51c6\u591a\u9009\u9898\u754c\u9762\u4e2d\u6dfb\u52a0\u5305\u542b\u8bef\u5bfc\u6027\u6307\u4ee4\u7684\u989d\u5916\u9009\u9879\uff0c\u6784\u5efa\u5305\u542b3,000\u4e2a\u95ee\u9898\u7684OI-Bench\u57fa\u51c6\uff0c\u6db5\u76d6\u77e5\u8bc6\u3001\u63a8\u7406\u548c\u5e38\u8bc6\u4efb\u52a1\uff0c\u5305\u542b16\u79cd\u6307\u4ee4\u7c7b\u578b\u3002", "result": "\u8bc4\u4f3012\u4e2aLLM\u663e\u793a\u6a21\u578b\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5b58\u5728\u5f02\u8d28\u6027\u3002\u653b\u51fb\u6210\u529f\u7387\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u654f\u611f\u6027\u3002", "conclusion": "OI-Bench\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u57fa\u4e8e\u9009\u62e9\u7684\u754c\u9762\u4e2d\u5bf9\u6307\u4ee4\u5e72\u6270\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6a21\u578b\u8106\u5f31\u6027\u5206\u6790\u548c\u7f13\u89e3\u7b56\u7565\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2601.12534", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12534", "abs": "https://arxiv.org/abs/2601.12534", "authors": ["Marcus Ma", "Jordan Prescott", "Emily Zhou", "Tiantian Feng", "Kleanthis Avramidis", "Gabor Mihaly Toth", "Shrikanth Narayanan"], "title": "Encoding Emotion Through Self-Supervised Eye Movement Reconstruction", "comment": null, "summary": "The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u7684\u65b0\u578b\u6ce8\u89c6\u68c0\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u9884\u6d4b\u60c5\u611f\u8868\u8fbe\u7684\u591a\u6a21\u6001\u6807\u8bb0\uff0c\u5728\u60c5\u611f\u5bf9\u9f50\u548c\u60c5\u611f\u884c\u4e3a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u773c\u52a8\u4e0e\u60c5\u611f\u5173\u7cfb\u7814\u7a76\u5927\u591a\u4f9d\u8d56\u9ad8\u5206\u8fa8\u7387\u4e13\u4e1a\u773c\u52a8\u4eea\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4ece\u81ea\u7136\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u5229\u7528\u773c\u52a8\u9884\u6d4b\u60c5\u611f\u8868\u8fbe\u7684\u591a\u6a21\u6001\u6807\u8bb0\u3002", "method": "1) \u4f7f\u7528USC Shoah\u57fa\u91d1\u4f1a\u5927\u5c60\u6740\u5e78\u5b58\u8005\u8bbf\u8c08\u89c6\u9891\uff1b2) \u5f00\u53d1\u57fa\u4e8e\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u7684\u65b0\u578b\u6ce8\u89c6\u68c0\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u672a\u6807\u8bb0\u89c6\u9891\uff1b3) \u4f7f\u7528\u6a21\u578b\u7f16\u7801\u5668\u5d4c\u5165\u5fae\u8c03\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff1a\u773c\u52a8\u4e0e\u8bed\u97f3\u60c5\u611f\u4f30\u8ba1\u5bf9\u9f50\u3001\u773c\u52a8\u9884\u6d4b\u60c5\u611f\u884c\u4e3a\uff08\u7b11\u3001\u54ed/\u62bd\u6ce3\u3001\u53f9\u6c14\uff09\u3002", "result": "\u65b0\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u60c5\u611f\u7ed3\u679c\uff0c\u89c2\u5bdf\u5230\u9884\u8bad\u7ec3\u6027\u80fd\u4e0e\u60c5\u611f\u5904\u7406\u6027\u80fd\u5448\u6b63\u76f8\u5173\uff0c\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u662f\u7f16\u7801\u60c5\u611f\u4fe1\u53f7\u7684\u6709\u6548\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u76d1\u7763\u773c\u52a8\u91cd\u5efa\u662f\u4ece\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u63d0\u53d6\u60c5\u611f\u4fe1\u53f7\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u60c5\u611f\u8ba1\u7b97\u548c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.13317", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13317", "abs": "https://arxiv.org/abs/2601.13317", "authors": ["Samantha Sudhoff", "Pranav Perumal", "Zhaoqing Wu", "Tunazzina Islam"], "title": "Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse", "comment": null, "summary": "Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u53d1\u73b0\u6846\u67b6\uff0c\u6bd4\u8f83Meta\u4ed8\u8d39\u5e7f\u544a\u548cBluesky\u516c\u5171\u5e16\u5b50\u4e2d\u7684\u6c14\u5019\u8bdd\u8bed\uff0c\u53d1\u73b0\u5e73\u53f0\u6fc0\u52b1\u673a\u5236\u5851\u9020\u4e86\u6c14\u5019\u53d9\u4e8b\u7684\u4e3b\u9898\u7ed3\u6784\u3001\u7acb\u573a\u5bf9\u9f50\u548c\u65f6\u95f4\u54cd\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5206\u6790\u4e0d\u540c\u5e73\u53f0\u7684\u6c14\u5019\u8bdd\u8bed\uff0c\u96be\u4ee5\u533a\u5206\u673a\u6784\u4fe1\u606f\u4e0e\u516c\u4f17\u8868\u8fbe\u3002\u4ed8\u8d39\u5e7f\u544a\u751f\u6001\u7cfb\u7edf\u6fc0\u52b1\u9488\u5bf9\u6027\u6218\u7565\u8bf4\u670d\uff0c\u800c\u516c\u5171\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e3b\u8981\u662f\u6709\u673a\u3001\u7528\u6237\u9a71\u52a8\u7684\u8bdd\u8bed\uff0c\u9700\u8981\u6bd4\u8f83\u5206\u6790\u6765\u7406\u89e3\u5e73\u53f0\u6fc0\u52b1\u673a\u5236\u5982\u4f55\u5f71\u54cd\u6c14\u5019\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u4e3b\u9898\u53d1\u73b0\u548c\u5206\u914d\u6846\u67b6\uff1a\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u805a\u7c7b\u6587\u672c\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b80\u6d01\u3001\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u6807\u7b7e\u3002\u4f7f\u7528\u4eba\u7c7b\u5224\u65ad\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\u8bc4\u4f30\u8bf1\u5bfc\u4e3b\u9898\u8d28\u91cf\uff0c\u901a\u8fc7\u4e0b\u6e38\u7acb\u573a\u9884\u6d4b\u548c\u4e3b\u9898\u5f15\u5bfc\u68c0\u7d22\u4efb\u52a1\u9a8c\u8bc1\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u5206\u67902024\u5e747\u6708\u81f32025\u5e749\u6708Meta\u4ed8\u8d39\u5e7f\u544a\u548cBluesky\u516c\u5171\u5e16\u5b50\u3002", "result": "\u53d1\u73b0\u4ed8\u8d39\u6c14\u5019\u4fe1\u606f\u4e0e\u516c\u5171\u6c14\u5019\u8bdd\u8bed\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e3b\u9898\u6d41\u884c\u5ea6\u56f4\u7ed5\u91cd\u5927\u653f\u6cbb\u4e8b\u4ef6\u53d8\u5316\u3002\u5e73\u53f0\u7ea7\u6fc0\u52b1\u673a\u5236\u53cd\u6620\u5728\u6c14\u5019\u53d9\u4e8b\u7684\u4e3b\u9898\u7ed3\u6784\u3001\u7acb\u573a\u5bf9\u9f50\u548c\u65f6\u95f4\u54cd\u5e94\u6027\u4e2d\u3002\u4ed8\u8d39\u5e7f\u544a\u66f4\u503e\u5411\u4e8e\u6218\u7565\u6027\u8bf4\u670d\uff0c\u800c\u516c\u5171\u5e73\u53f0\u66f4\u591a\u6709\u673a\u8ba8\u8bba\u3002", "conclusion": "\u5e73\u53f0\u6fc0\u52b1\u673a\u5236\u663e\u8457\u5f71\u54cd\u6c14\u5019\u8bdd\u8bed\u7279\u5f81\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u652f\u6301\u8de8\u5f02\u6784\u4f20\u64ad\u73af\u5883\u7684\u6bd4\u8f83\u53d9\u4e8b\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u533a\u5206\u673a\u6784\u4fe1\u606f\u4e0e\u516c\u4f17\u8868\u8fbe\uff0c\u7406\u89e3\u6c14\u5019\u4f20\u64ad\u7684\u52a8\u6001\u53d8\u5316\u3002"}}
{"id": "2601.12551", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.12551", "abs": "https://arxiv.org/abs/2601.12551", "authors": ["Tong Wu"], "title": "PISE: Physics-Anchored Semantically-Enhanced Deep Computational Ghost Imaging for Robust Low-Bandwidth Machine Perception", "comment": "4 pages, 4 figures, 3 tables. Submitted to IEICE Transactions", "summary": "We propose PISE, a physics-informed deep ghost imaging framework for low-bandwidth edge perception. By combining adjoint operator initialization with semantic guidance, PISE improves classification accuracy by 2.57% and reduces variance by 9x at 5% sampling.", "AI": {"tldr": "PISE\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u9b3c\u6210\u50cf\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u611f\u77e5\uff0c\u901a\u8fc7\u7ed3\u5408\u4f34\u968f\u7b97\u5b50\u521d\u59cb\u5316\u548c\u8bed\u4e49\u6307\u5bfc\uff0c\u57285%\u91c7\u6837\u7387\u4e0b\u5c06\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad82.57%\uff0c\u65b9\u5dee\u964d\u4f4e9\u500d\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u5728\u4f4e\u5e26\u5bbd\u6761\u4ef6\u4e0b\u8fdb\u884c\u611f\u77e5\u4efb\u52a1\u65f6\uff0c\u4f20\u7edf\u9b3c\u6210\u50cf\u65b9\u6cd5\u5206\u7c7b\u51c6\u786e\u7387\u4f4e\u3001\u65b9\u5dee\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u6709\u9650\u91c7\u6837\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u611f\u77e5\u3002", "method": "\u63d0\u51faPISE\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u548c\u9b3c\u6210\u50cf\u6280\u672f\uff0c\u91c7\u7528\u4f34\u968f\u7b97\u5b50\u521d\u59cb\u5316\u6765\u4f18\u5316\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u6307\u5bfc\u6765\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u57285%\u91c7\u6837\u7387\u4e0b\uff0cPISE\u5c06\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.57%\uff0c\u540c\u65f6\u5c06\u65b9\u5dee\u964d\u4f4e\u4e869\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u611f\u77e5\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "conclusion": "PISE\u6846\u67b6\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u548c\u8bed\u4e49\u6307\u5bfc\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5e26\u5bbd\u8fb9\u7f18\u611f\u77e5\u7684\u6311\u6218\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u667a\u80fd\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.13319", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13319", "abs": "https://arxiv.org/abs/2601.13319", "authors": ["Peter Sullivan", "AbdelRahim Elmadany", "Alcides Alcoba Inciarte", "Muhammad Abdul-Mageed"], "title": "Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology", "comment": null, "summary": "Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u6570\u636e\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u63d0\u51fa\u4e86Arab Voices\u6807\u51c6\u5316\u6846\u67b6\u6765\u7edf\u4e0031\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d614\u79cd\u65b9\u8a00\uff0c\u4e3a\u65b9\u8a00ASR\u5efa\u7acb\u4e86\u57fa\u51c6\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u6570\u636e\u5728\u9886\u57df\u8986\u76d6\u3001\u65b9\u8a00\u6807\u6ce8\u5b9e\u8df5\u548c\u5f55\u97f3\u6761\u4ef6\u65b9\u9762\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u4f7f\u5f97\u8de8\u6570\u636e\u96c6\u6bd4\u8f83\u548c\u6a21\u578b\u8bc4\u4f30\u53d8\u5f97\u590d\u6742\u3002\u9700\u8981\u6807\u51c6\u5316\u8868\u5f81\u6765\u51cf\u5c11\u788e\u7247\u5316\u5e76\u652f\u6301\u53ef\u91cd\u590d\u8bc4\u4f30\u3002", "method": "1) \u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u6599\u5e93\u8fdb\u884c\u8bed\u8a00\"\u65b9\u8a00\u6027\"\u7684\u8ba1\u7b97\u5206\u6790\uff0c\u7ed3\u5408\u97f3\u9891\u8d28\u91cf\u7684\u5ba2\u89c2\u4ee3\u7406\u6307\u6807\uff1b2) \u5f15\u5165Arab Voices\u6807\u51c6\u5316\u6846\u67b6\uff0c\u7edf\u4e00\u8bbf\u95ee31\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d614\u79cd\u65b9\u8a00\uff0c\u63d0\u4f9b\u534f\u8c03\u7684\u5143\u6570\u636e\u548c\u8bc4\u4f30\u5de5\u5177\uff1b3) \u5bf9\u4e00\u7cfb\u5217\u6700\u8fd1\u7684ASR\u7cfb\u7edf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u5728\u58f0\u5b66\u6761\u4ef6\u548c\u65b9\u8a00\u4fe1\u53f7\u5f3a\u5ea6\u53ca\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\u3002Arab Voices\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u73b0\u4ee3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00ASR\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\u3002", "conclusion": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u6570\u636e\u9700\u8981\u8d85\u8d8a\u7c97\u7c92\u5ea6\u6807\u7b7e\u7684\u6807\u51c6\u5316\u8868\u5f81\u3002Arab Voices\u6846\u67b6\u51cf\u5c11\u4e86\u788e\u7247\u5316\uff0c\u652f\u6301\u53ef\u91cd\u590d\u8bc4\u4f30\uff0c\u5e76\u4e3a\u65b9\u8a00ASR\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\u548c\u57fa\u51c6\u3002"}}
{"id": "2601.12567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12567", "abs": "https://arxiv.org/abs/2601.12567", "authors": ["W\u0142adys\u0142aw Skarbek", "Micha\u0142 Salomonowicz", "Micha\u0142 Kr\u00f3l"], "title": "Camera Pose Revisited", "comment": "30 pages, 9 figures, 9 tables", "summary": "Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective--$n$--Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \\texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB--IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \\texttt{SQPnP} and slightly higher than \\texttt{IPPE}, both prominent \\texttt{PnP-OpenCV} procedures. However, \\texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \\texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.", "AI": {"tldr": "\u63d0\u51faPnP-ProCay78\u7b97\u6cd5\uff0c\u901a\u8fc7Cayley\u53c2\u6570\u5316\u65cb\u8f6c\u548c\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u89e3\u51b3\u5e73\u9762PnP\u95ee\u9898\uff0c\u907f\u514d\u6602\u8d35\u7684\u89e3\u7a7a\u95f4\u641c\u7d22\uff0c\u5728RGB\u548c\u70ed\u6210\u50cf\u76f8\u673a\u4e0a\u9a8c\u8bc1\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\u65b9\u6cd5\u4f46\u7b97\u6cd5\u66f4\u7b80\u5355\u3002", "motivation": "\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76f8\u673a\u6807\u5b9a\u548c\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u3002\u73b0\u6709PnP\u65b9\u6cd5\u5728\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\u65b9\u9762\u5b58\u5728\u7b97\u6cd5\u590d\u6742\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPnP-ProCay78\u7b97\u6cd5\uff1a1) \u4f7f\u7528Cayley\u53c2\u6570\u5316\u8868\u793a\u65cb\u8f6c\uff1b2) \u7ed3\u5408\u91cd\u5efa\u8bef\u5dee\u7684\u7ecf\u5178\u4e8c\u6b21\u5f62\u5f0f\u4e0e\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff1b3) \u57fa\u4e8e\u4e24\u4e2a\u89c4\u8303\u5411\u91cf\u91cd\u5efa\u8bef\u5dee\u5206\u6790\u8fdb\u884c\u786e\u5b9a\u6027\u8d77\u59cb\u70b9\u9009\u62e9\uff0c\u907f\u514d\u6602\u8d35\u7684\u89e3\u7a7a\u95f4\u641c\u7d22\uff1b4) \u5c06\u6295\u5f71\u8bef\u5dee\u6700\u5c0f\u5316\u4e0e\u89e3\u6790\u6d88\u9664\u5e73\u79fb\u91cd\u5efa\u8bef\u5dee\u66ff\u4ee3\u9879\u7ed3\u5408\uff0c\u5f62\u6210\u6df7\u5408\u4ee3\u4ef7\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387RGB\u76f8\u673a\u548c\u4f4e\u5206\u8fa8\u7387\u70ed\u6210\u50cf\u76f8\u673a\u7684\u96c6\u6210RGB-IR\u7cfb\u7edf\u3002\u7ed3\u679c\u8868\u660e\uff1a1) \u6295\u5f71\u7cbe\u5ea6\u4e0e\u6700\u4f18SQPnP\u51e0\u4e4e\u76f8\u540c\uff0c\u7565\u9ad8\u4e8eIPPE\uff1b2) \u7b97\u6cd5\u7ed3\u6784\u663e\u8457\u66f4\u7b80\u5355\uff1b3) Cayley\u7a7a\u95f4\u4e2d\u7684\u4f18\u5316\u8f68\u8ff9\u5206\u6790\u63d0\u4f9b\u4e86\u5bf9\u6536\u655b\u8fc7\u7a0b\u7684\u76f4\u89c2\u7406\u89e3\u3002", "conclusion": "PnP-ProCay78\u7b97\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5177\u6709\u66f4\u7b80\u5355\u7684\u7b97\u6cd5\u7ed3\u6784\uff0c\u5176\u51e0\u4f55\u900f\u660e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4f7f\u5176\u6210\u4e3a\u5e73\u9762PnP\u95ee\u9898\u7684\u6709\u5438\u5f15\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6559\u5b66\u76ee\u7684\u3002"}}
{"id": "2601.13328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13328", "abs": "https://arxiv.org/abs/2601.13328", "authors": ["Geoffrey Churchill", "Steven Skiena"], "title": "Reducing Tokenization Premiums for Low-Resource Languages", "comment": null, "summary": "Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.", "AI": {"tldr": "\u5206\u6790\u6d41\u884c\u8bed\u8a00\u6a21\u578b\u7684tokenizer\u8bbe\u8ba1\u53ca\u5176\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684tokenization\u6ea2\u4ef7\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u540e\u5904\u7406\u589e\u52a0\u8bcd\u6c47\u8868\u6765\u538b\u7f29\u591atoken\u5b57\u7b26\u4e3a\u5355token\u7684\u65b9\u6cd5", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u9762\u4e34\u663e\u8457\u7684tokenization\u6ea2\u4ef7\u95ee\u9898\uff0c\u76f8\u6bd4\u82f1\u8bed\u9700\u8981\u6570\u500dtoken\u6570\u91cf\u6765\u7f16\u7801\u76f8\u540c\u8bed\u4e49\u7684\u53e5\u5b50\uff0c\u5bfc\u81f4API\u548c\u80fd\u6e90\u6210\u672c\u589e\u52a0\uff0c\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\u51cf\u5c11", "method": "\u5206\u6790\u5341\u79cd\u6d41\u884c\u8bed\u8a00\u6a21\u578b\u7684tokenizer\u8bbe\u8ba1\uff0c\u63d0\u51fa\u540e\u5904\u7406\u65b9\u6cd5\uff1a\u901a\u8fc7\u5411\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u4e2d\u6dfb\u52a0\u65b0token\uff0c\u5c06\u591atoken\u5b57\u7b26\u5408\u5e76\u4e3a\u5355token\uff0c\u4ece\u800c\u538b\u7f29\u8f93\u5165", "result": "\u572812\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u4f7f\u7528Llama 3.2 1B\u6a21\u578b\u9a8c\u8bc1\uff0c\u53d1\u73b0\u539f\u59cb\u8f93\u5165\u548c\u538b\u7f29\u8f93\u5165\u5728\u6700\u540e\u9690\u85cf\u72b6\u6001\u4e0a\u5177\u6709\u76f8\u4f3c\u6027", "conclusion": "\u63d0\u51fa\u7684\u540e\u5904\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684tokenization\u6ea2\u4ef7\uff0c\u4e3a\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u63d0\u5347\u6a21\u578b\u6548\u7387\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.12626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12626", "abs": "https://arxiv.org/abs/2601.12626", "authors": ["Raphi Kang", "Hongqiao Chen", "Georgia Gkioxari", "Pietro Perona"], "title": "Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models", "comment": null, "summary": "Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual/geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \\textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: https://github.com/Raphoo/linear-mech-vlms.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7ebf\u6027\u7ed1\u5b9a\u7a7a\u95f4ID\u5230\u6587\u672c\u6fc0\u6d3b\u6765\u7f16\u7801\u7269\u4f53\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00token\u8fdb\u884c\u63a8\u7406\uff0c\u63ed\u793a\u4e86VLM\u5185\u90e8\u65f6\u7a7a\u63a8\u7406\u7684\u7ebf\u6027\u673a\u5236", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u663e\u8457\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5e95\u5c42\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u4f5c\u8005\u5047\u8bbe\u89c6\u89c9/\u51e0\u4f55\u548c\u6587\u672c\u7684\u7a7a\u95f4\u7ed3\u6784\u8868\u793a\u5fc5\u987b\u5728VLM\u8ba1\u7b97\u7684\u67d0\u4e2a\u70b9\u7ed3\u5408\uff0c\u5e76\u63a2\u7d22\u8fd9\u79cd\u7ed3\u5408\u70b9\u662f\u5426\u80fd\u901a\u8fc7\u7ebf\u6027\u6a21\u578b\u56e0\u679c\u89e3\u91ca\u6a21\u578b\u7684\u8f93\u5165-\u8f93\u51fa\u884c\u4e3a", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c\uff0c\u53d1\u73b0VLM\u901a\u8fc7\u5c06\u7a7a\u95f4ID\u7ebf\u6027\u7ed1\u5b9a\u5230\u6587\u672c\u6fc0\u6d3b\u6765\u7f16\u7801\u7269\u4f53\u4f4d\u7f6e\uff0c\u7136\u540e\u901a\u8fc7\u8bed\u8a00token\u8fdb\u884c\u63a8\u7406\u3002\u8fd9\u4e9b\u7a7a\u95f4ID\u5728\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u53ef\u4ee5\u5728\u4e2d\u95f4\u5c42\u7cfb\u7edf\u6027\u5730\u8c03\u8282\u6a21\u578b\u4fe1\u5ff5\u3002\u8fd8\u5c06\u5206\u6790\u6269\u5c55\u5230\u89c6\u9891VLM\uff0c\u8bc6\u522b\u4e86\u7c7b\u4f3c\u7684\u7ebf\u6027\u65f6\u95f4ID\u673a\u5236", "result": "\u7ecf\u9a8c\u8bc1\u660eVLM\u901a\u8fc7\u7ebf\u6027\u7ed1\u5b9a\u7a7a\u95f4ID\u5230\u6587\u672c\u6fc0\u6d3b\u6765\u7f16\u7801\u7269\u4f53\u4f4d\u7f6e\uff0c\u8fd9\u4e9bID\u5728\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u80fd\u7cfb\u7edf\u6027\u5730\u8c03\u8282\u6a21\u578b\u4fe1\u5ff5\u3002\u7a7a\u95f4ID\u53ef\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u8bc6\u522b\u73b0\u6709VLM\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u5728\u89c6\u9891VLM\u4e2d\u4e5f\u53d1\u73b0\u4e86\u7c7b\u4f3c\u7684\u7ebf\u6027\u65f6\u95f4ID\u673a\u5236", "conclusion": "\u901a\u8fc7\u8868\u5f81\u63d0\u51fa\u7684\u65f6\u7a7aID\u673a\u5236\uff0c\u9610\u660e\u4e86VLM\u4e2d\u5148\u524d\u672a\u5145\u5206\u63a2\u7d22\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u5bf9\u9f50\u3001\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u63d0\u4f9b\u539f\u5219\u6027\u6307\u5bfc"}}
{"id": "2601.13330", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13330", "abs": "https://arxiv.org/abs/2601.13330", "authors": ["Jamie Cummins", "Beth Clarke", "Ian Hussey", "Malte Elson"], "title": "RegCheck: A tool for automating comparisons between study registrations and papers", "comment": "15 pages, 1 figure", "summary": "Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.", "AI": {"tldr": "RegCheck\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684LLM\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u3001\u5ba1\u7a3f\u4eba\u548c\u7f16\u8f91\u6bd4\u8f83\u7814\u7a76\u6ce8\u518c\u4e0e\u5bf9\u5e94\u8bba\u6587\uff0c\u4fdd\u6301\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u5728\u5faa\u73af\u4e2d\uff0c\u4fc3\u8fdb\u79d1\u5b66\u900f\u660e\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u7814\u7a76\u6ce8\u518c\u5bf9\u79d1\u5b66\u900f\u660e\u5ea6\u548c\u4e25\u8c28\u6027\u6709\u76ca\uff0c\u4f46\u624b\u52a8\u68c0\u67e5\u6ce8\u518c\u4e0e\u8bba\u6587\u4e4b\u95f4\u7684\u5dee\u5f02\u65e2\u8017\u65f6\u53c8\u8d39\u529b\uff0c\u9700\u8981\u8de8\u683c\u5f0f\u9605\u8bfb\u548c\u8de8\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002AI\u7684\u53d1\u5c55\u4e3a\u4fc3\u8fdb\u8fd9\u4e00\u6d3b\u52a8\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "RegCheck\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684LLM\u8f85\u52a9\u5de5\u5177\uff0c\u8bbe\u8ba1\u7528\u4e8e\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u3001\u5ba1\u7a3f\u4eba\u548c\u7f16\u8f91\u6bd4\u8f83\u7814\u7a76\u6ce8\u518c\u4e0e\u5bf9\u5e94\u8bba\u6587\u3002\u5b83\u4fdd\u6301\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u5728\u5faa\u73af\u4e2d\uff1a1) \u7528\u6237\u51b3\u5b9a\u9700\u8981\u6bd4\u8f83\u54ea\u4e9b\u7279\u5f81\uff1b2) \u5411\u7528\u6237\u5c55\u793a\u6bcf\u4e2a\u7279\u5f81\u6700\u76f8\u5173\u7684\u6587\u672c\uff0c\u4fc3\u8fdb\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u5dee\u5f02\u5224\u65ad\u3002\u5de5\u5177\u8fd8\u751f\u6210\u53ef\u5171\u4eab\u7684\u62a5\u544a\u548c\u552f\u4e00ID\u3002", "result": "RegCheck\u88ab\u8bbe\u8ba1\u4e3a\u53ef\u8de8\u79d1\u5b66\u9886\u57df\u3001\u6ce8\u518c\u548c\u51fa\u7248\u683c\u5f0f\u9002\u5e94\u3002\u8bba\u6587\u6982\u8ff0\u4e86RegCheck\u7684\u52a8\u673a\u3001\u5de5\u4f5c\u6d41\u7a0b\u548c\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u5728\u53ef\u91cd\u590d\u79d1\u5b66\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u793a\u4f8b\u7528\u4f8b\u3002", "conclusion": "RegCheck\u4f5c\u4e3a\u4e00\u4e2aLLM\u8f85\u52a9\u5de5\u5177\uff0c\u901a\u8fc7\u4fdd\u6301\u4eba\u7c7b\u5224\u65ad\u5728\u5faa\u73af\u4e2d\uff0c\u4fc3\u8fdb\u7814\u7a76\u6ce8\u518c\u4e0e\u8bba\u6587\u7684\u6bd4\u8f83\uff0c\u6709\u671b\u6210\u4e3a\u53ef\u91cd\u590d\u79d1\u5b66\u7684\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u9ad8\u7814\u7a76\u900f\u660e\u5ea6\u548c\u4e25\u8c28\u6027\u3002"}}
{"id": "2601.12636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12636", "abs": "https://arxiv.org/abs/2601.12636", "authors": ["Satyaki Roy Chowdhury", "Aswathnarayan Radhakrishnan", "Hsiao Jou Hsu", "Hari Subramoni", "Joachim Moortgat"], "title": "From Bands to Depth: Understanding Bathymetry Decisions on Sentinel-2", "comment": "Accepted by WACV 2026", "summary": "Deploying Sentinel-2 satellite derived bathymetry (SDB) robustly across sites remains challenging. We analyze a Swin-Transformer based U-Net model (Swin-BathyUNet) to understand how it infers depth and when its predictions are trustworthy. A leave-one-band out study ranks spectral importance to the different bands consistent with shallow water optics. We adapt ablation-based CAM to regression (A-CAM-R) and validate the reliability via a performance retention test: keeping only the top-p% salient pixels while neutralizing the rest causes large, monotonic RMSE increase, indicating explanations localize on evidence the model relies on. Attention ablations show decoder conditioned cross attention on skips is an effective upgrade, improving robustness to glint/foam. Cross-region inference (train on one site, test on another) reveals depth-dependent degradation: MAE rises nearly linearly with depth, and bimodal depth distributions exacerbate mid/deep errors. Practical guidance follows: maintain wide receptive fields, preserve radiometric fidelity in green/blue channels, pre-filter bright high variance near shore, and pair light target site fine tuning with depth aware calibration to transfer across regions.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u4e8eSwin-Transformer\u7684U-Net\u6a21\u578b(Swin-BathyUNet)\u5728Sentinel-2\u536b\u661f\u6d4b\u6df1(SDB)\u4e2d\u7684\u5e94\u7528\uff0c\u7814\u7a76\u4e86\u5176\u6df1\u5ea6\u63a8\u65ad\u673a\u5236\u548c\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u4e86\u8de8\u533a\u57df\u90e8\u7f72\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "Sentinel-2\u536b\u661f\u6d4b\u6df1\u6280\u672f\u5728\u4e0d\u540c\u7ad9\u70b9\u95f4\u7684\u7a33\u5065\u90e8\u7f72\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u63a8\u65ad\u6df1\u5ea6\u4ee5\u53ca\u4f55\u65f6\u5176\u9884\u6d4b\u662f\u53ef\u4fe1\u7684\uff0c\u4ee5\u63d0\u9ad8\u8de8\u533a\u57df\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528Swin-Transformer U-Net\u67b6\u6784\uff1b\u8fdb\u884c\u6ce2\u6bb5\u91cd\u8981\u6027\u5206\u6790\uff1b\u5f00\u53d1\u56de\u5f52\u4efb\u52a1\u7684A-CAM-R\u89e3\u91ca\u65b9\u6cd5\uff1b\u8fdb\u884c\u6ce8\u610f\u529b\u6d88\u878d\u5b9e\u9a8c\uff1b\u5b9e\u65bd\u8de8\u533a\u57df\u63a8\u7406\u6d4b\u8bd5\uff08\u5728\u4e00\u4e2a\u7ad9\u70b9\u8bad\u7ec3\uff0c\u5728\u53e6\u4e00\u4e2a\u7ad9\u70b9\u6d4b\u8bd5\uff09\u3002", "result": "\u7eff\u8272\u548c\u84dd\u8272\u6ce2\u6bb5\u5bf9\u6d4b\u6df1\u6700\u91cd\u8981\uff1bA-CAM-R\u80fd\u53ef\u9760\u8bc6\u522b\u6a21\u578b\u4f9d\u8d56\u7684\u8bc1\u636e\u533a\u57df\uff1b\u89e3\u7801\u5668\u6761\u4ef6\u8de8\u6ce8\u610f\u529b\u673a\u5236\u80fd\u63d0\u9ad8\u5bf9\u7729\u5149/\u6ce1\u6cab\u7684\u9c81\u68d2\u6027\uff1b\u8de8\u533a\u57df\u63a8\u7406\u663e\u793a\u8bef\u5dee\u968f\u6df1\u5ea6\u7ebf\u6027\u589e\u52a0\uff0c\u53cc\u5cf0\u6df1\u5ea6\u5206\u5e03\u4f1a\u52a0\u5267\u4e2d/\u6df1\u6c34\u533a\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5b9e\u7528\u6307\u5bfc\uff1a\u4fdd\u6301\u5bbd\u611f\u53d7\u91ce\uff0c\u4fdd\u62a4\u7eff/\u84dd\u901a\u9053\u7684\u8f90\u5c04\u4fdd\u771f\u5ea6\uff0c\u9884\u8fc7\u6ee4\u8fd1\u5cb8\u9ad8\u4eae\u5ea6\u9ad8\u65b9\u5dee\u533a\u57df\uff0c\u7ed3\u5408\u6df1\u5ea6\u611f\u77e5\u6821\u51c6\u8fdb\u884c\u76ee\u6807\u7ad9\u70b9\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u8de8\u533a\u57df\u8fc1\u79fb\u3002"}}
{"id": "2601.13346", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13346", "abs": "https://arxiv.org/abs/2601.13346", "authors": ["Sang Yun Kwon", "AbdelRahim Elmadany", "Muhammad Abdul-Mageed"], "title": "AfroScope: A Framework for Studying the Linguistic Landscape of Africa", "comment": null, "summary": "Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.", "AI": {"tldr": "AfroScope\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u975e\u6d32\u8bed\u8a00\u8bc6\u522b\u6846\u67b6\uff0c\u5305\u542b\u8986\u76d6713\u79cd\u975e\u6d32\u8bed\u8a00\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u5957\u4ef6\uff0c\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u65b9\u6cd5\u63d0\u5347\u6df7\u6dc6\u8bed\u8a00\u7684\u8bc6\u522b\u80fd\u529b", "motivation": "\u73b0\u6709\u8bed\u8a00\u8bc6\u522b\u65b9\u6cd5\u5bf9\u975e\u6d32\u8bed\u8a00\u652f\u6301\u6709\u9650\uff0c\u8bed\u8a00\u8986\u76d6\u6570\u91cf\u4e0d\u8db3\uff0c\u4e14\u96be\u4ee5\u533a\u5206\u5bc6\u5207\u76f8\u5173\u7684\u8bed\u8a00\u53d8\u4f53\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u975e\u6d32\u8bed\u8a00\u8bc6\u522b\u7cfb\u7edf\u6765\u652f\u6301\u4e0b\u6e38NLP\u5e94\u7528", "method": "\u63d0\u51faAfroScope\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542bAfroScope-Data\u6570\u636e\u96c6\u548cAfroScope-Models\u6a21\u578b\u5957\u4ef6\uff1b\u9488\u5bf9\u9ad8\u5ea6\u6df7\u6dc6\u7684\u8bed\u8a00\uff0c\u91c7\u7528\u5206\u5c42\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u4e13\u95e8\u7684Mirror-Serengeti\u5d4c\u5165\u6a21\u578b\u5904\u740629\u79cd\u5bc6\u5207\u76f8\u5173\u7684\u8bed\u8a00", "result": "\u5728\u6df7\u6dc6\u8bed\u8a00\u5b50\u96c6\u4e0a\uff0c\u5206\u5c42\u5206\u7c7b\u65b9\u6cd5\u6bd4\u6700\u4f73\u57fa\u7840\u6a21\u578b\u63d0\u53474.55\u7684\u5b8f\u89c2F1\u5206\u6570\uff1b\u6846\u67b6\u652f\u6301713\u79cd\u975e\u6d32\u8bed\u8a00\uff0c\u63d0\u4f9b\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u548c\u9886\u57df\u6548\u5e94\u7684\u5206\u6790\u6307\u5bfc", "conclusion": "AfroScope\u4f5c\u4e3a\u975e\u6d32\u8bed\u8a00\u8bc6\u522b\u7684\u4f7f\u80fd\u6280\u672f\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u6d4b\u91cf\u975e\u6d32\u6570\u5b57\u6587\u672c\u4e2d\u7684\u8bed\u8a00\u666f\u89c2\uff0c\u76f8\u5173\u6570\u636e\u548c\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03"}}
{"id": "2601.12638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12638", "abs": "https://arxiv.org/abs/2601.12638", "authors": ["Ninnart Fuengfusin", "Keisuke Yoneda", "Naoki Suganuma"], "title": "Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT", "comment": "6 pages, 3 figures", "summary": "LIDAR 3D object detection is one of the important tasks for autonomous vehicles. Ensuring that this task operates in real-time is crucial. Toward this, model quantization can be used to accelerate the runtime. However, directly applying model quantization often leads to performance degradation due to LIDAR's wide numerical distributions and extreme outliers. To address the wide numerical distribution, we proposed a mixed precision framework designed for PointPillars. Our framework first searches for sensitive layers with post-training quantization (PTQ) by quantizing one layer at a time to 8-bit integer (INT8) and evaluating each model for average precision (AP). The top-k most sensitive layers are assigned as floating point (FP). Combinations of these layers are greedily searched to produce candidate mixed precision models, which are finalized with either PTQ or quantization-aware training (QAT). Furthermore, to handle outliers, we observe that using a very small number of calibration data reduces the likelihood of encountering outliers, thereby improving PTQ performance. Our methods provides mixed precision models without training in the PTQ pipeline, while our QAT pipeline achieves the performance competitive to FP models. With TensorRT deployment, our models offer less latency and sizes by up to 2.35 and 2.26 times, respectively.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\u5904\u7406LiDAR 3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6570\u503c\u5206\u5e03\u95ee\u9898\u548c\u5f02\u5e38\u503c\uff0c\u901a\u8fc7\u654f\u611f\u5c42\u641c\u7d22\u548c\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b02.35\u500d\u52a0\u901f\u548c2.26\u500d\u6a21\u578b\u538b\u7f29", "motivation": "LiDAR 3D\u76ee\u6807\u68c0\u6d4b\u9700\u8981\u5b9e\u65f6\u8fd0\u884c\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u6a21\u578b\u91cf\u5316\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662fLiDAR\u6570\u636e\u7684\u5bbd\u6570\u503c\u5206\u5e03\u548c\u6781\u7aef\u5f02\u5e38\u503c", "method": "1) \u4f7f\u7528\u540e\u8bad\u7ec3\u91cf\u5316(PTQ)\u9010\u5c42\u91cf\u5316\u5230INT8\uff0c\u8bc4\u4f30\u5e73\u5747\u7cbe\u5ea6(AP)\u6765\u641c\u7d22\u654f\u611f\u5c42\uff1b2) \u5c06top-k\u654f\u611f\u5c42\u5206\u914d\u4e3a\u6d6e\u70b9(FP)\uff1b3) \u8d2a\u5a6a\u641c\u7d22\u8fd9\u4e9b\u5c42\u7684\u7ec4\u5408\u751f\u6210\u5019\u9009\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\uff1b4) \u4f7f\u7528\u5c11\u91cf\u6821\u51c6\u6570\u636e\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff1b5) \u901a\u8fc7PTQ\u6216\u91cf\u5316\u611f\u77e5\u8bad\u7ec3(QAT)\u6700\u7ec8\u786e\u5b9a\u6a21\u578b", "result": "PTQ\u6d41\u7a0b\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u6df7\u5408\u7cbe\u5ea6\u6a21\u578b\uff0cQAT\u6d41\u7a0b\u8fbe\u5230\u4e0eFP\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002TensorRT\u90e8\u7f72\u4e0b\uff0c\u6a21\u578b\u5ef6\u8fdf\u964d\u4f4e\u6700\u591a2.35\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u6700\u591a2.26\u500d", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LiDAR 3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u91cf\u5316\u6311\u6218\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u6a21\u578b\u6548\u7387"}}
{"id": "2601.13352", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.13352", "abs": "https://arxiv.org/abs/2601.13352", "authors": ["Yuxing Lu", "J. Ben Tamo", "Weichen Zhao", "Nan Sun", "Yishan Zhong", "Wenqi Shi", "Jinzhuo Wang", "May D. Wang"], "title": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction", "comment": "17 pages, 5 figures, 6 tables", "summary": "Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.", "AI": {"tldr": "LLM-as-RNN\uff1a\u5c06\u51bb\u7ed3LLM\u8f6c\u53d8\u4e3a\u5faa\u73af\u9884\u6d4b\u5668\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\u5b9e\u73b0\u5728\u7ebf\u5b66\u4e60\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u65f6\u4e0a\u4e0b\u6587\u5386\u53f2\u4e0d\u53ef\u53d8\uff0c\u4e00\u65e6\u5728\u751f\u6210\u6b65\u9aa4t\u51fa\u9519\uff0c\u6a21\u578b\u7f3a\u4e4f\u53ef\u66f4\u65b0\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u6539\u8fdb\u6b65\u9aa4t+1\u7684\u9884\u6d4b", "method": "\u5c06LLM\u9690\u85cf\u72b6\u6001\u8868\u793a\u4e3a\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e3a\u7ed3\u6784\u5316\u7cfb\u7edf\u63d0\u793a\u6458\u8981\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u6587\u672c\u91cd\u5199\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u66f4\u65b0\u72b6\u6001\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60", "result": "\u5728\u533b\u7597\u3001\u6c14\u8c61\u548c\u91d1\u878d\u4e09\u4e2a\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-as-RNN\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u3001\u5b8c\u6574\u5386\u53f2\u548cMemPrompt\u57fa\u7ebf\uff0c\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad86.5%\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u4eba\u7c7b\u53ef\u8bfb\u5b66\u4e60\u8f68\u8ff9", "conclusion": "LLM-as-RNN\u6210\u529f\u5c06\u51bb\u7ed3LLM\u8f6c\u53d8\u4e3a\u5faa\u73af\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8bb0\u5fc6\u5b9e\u73b0\u6709\u6548\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5e8f\u5217\u9884\u6d4b\u6027\u80fd"}}
{"id": "2601.12664", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12664", "abs": "https://arxiv.org/abs/2601.12664", "authors": ["Elisa Gon\u00e7alves Ribeiro", "Rodrigo Moreira", "Larissa Ferreira Rodrigues Moreira", "Andr\u00e9 Ricardo Backes"], "title": "Generalizable Hyperparameter Optimization for Federated Learning on Non-IID Cancer Images", "comment": "21st International Conference on Computer Vision Theory and Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain", "summary": "Deep learning for cancer histopathology training conflicts with privacy constraints in clinical settings. Federated Learning (FL) mitigates this by keeping data local; however, its performance depends on hyperparameter choices under non-independent and identically distributed (non-IID) client datasets. This paper examined whether hyperparameters optimized on one cancer imaging dataset generalized across non-IID federated scenarios. We considered binary histopathology tasks for ovarian and colorectal cancers. We perform centralized Bayesian hyperparameter optimization and transfer dataset-specific optima to the non-IID FL setup. The main contribution of this study is the introduction of a simple cross-dataset aggregation heuristic by combining configurations by averaging the learning rates and considering the modal optimizers and batch sizes. This combined configuration achieves a competitive classification performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u9488\u5bf9\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\u7684\u8d85\u53c2\u6570\u4f18\u5316\u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8de8\u6570\u636e\u96c6\u805a\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bad\u7ec3\u4e2d\u5b58\u5728\u9690\u79c1\u7ea6\u675f\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\uff0c\u4f46\u5176\u6027\u80fd\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u6570\u636e\u96c6\u4e0b\u9ad8\u5ea6\u4f9d\u8d56\u8d85\u53c2\u6570\u9009\u62e9\uff0c\u9700\u8981\u7814\u7a76\u8d85\u53c2\u6570\u4f18\u5316\u7684\u53ef\u8fc1\u79fb\u6027", "method": "\u5728\u5375\u5de2\u764c\u548c\u7ed3\u76f4\u80a0\u764c\u7684\u4e8c\u5143\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\u4e0a\uff0c\u8fdb\u884c\u96c6\u4e2d\u5f0f\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u7136\u540e\u5c06\u6570\u636e\u96c6\u7279\u5b9a\u6700\u4f18\u53c2\u6570\u8fc1\u79fb\u5230\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u5e76\u5f15\u5165\u7b80\u5355\u7684\u8de8\u6570\u636e\u96c6\u805a\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5", "result": "\u901a\u8fc7\u5e73\u5747\u5b66\u4e60\u7387\u3001\u8003\u8651\u6a21\u6001\u4f18\u5316\u5668\u548c\u6279\u91cf\u5927\u5c0f\u7684\u7ec4\u5408\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6570\u636e\u96c6\u805a\u5408\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u4e3a\u764c\u75c7\u7ec4\u7ec7\u75c5\u7406\u5b66\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8d85\u53c2\u6570\u9009\u62e9\u7b56\u7565"}}
{"id": "2601.13359", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13359", "abs": "https://arxiv.org/abs/2601.13359", "authors": ["Asen Dotsinski", "Panagiotis Eustratiadis"], "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection", "comment": null, "summary": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\"sockpuppetting\"\u7684\u7b80\u5355\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u8f93\u51fa\u5f00\u5934\u63d2\u5165\u63a5\u53d7\u5e8f\u5217\u6765\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u65e0\u9700\u4f18\u5316\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u4fdd\u62a4\u5b83\u4eec\u514d\u53d7\u6076\u610f\u63d0\u793a\u653b\u51fb\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u81ea\u52a8\u5316\u8d8a\u72f1\u65b9\u6cd5\u5982GCG\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u63ed\u793a\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\"sockpuppetting\"\u65b9\u6cd5\uff1a\u5728\u6a21\u578b\u8f93\u51fa\u5f00\u5934\u63d2\u5165\u63a5\u53d7\u5e8f\u5217\uff08\u5982\"Sure, here is how to...\"\uff09\uff0c\u7136\u540e\u8ba9\u6a21\u578b\u5b8c\u6210\u54cd\u5e94\u3002\u53ea\u9700\u5355\u884c\u4ee3\u7801\uff0c\u65e0\u9700\u4f18\u5316\u3002\u8fd8\u63a2\u7d22\u4e86\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u52a9\u624b\u6d88\u606f\u5757\u5185\u4f18\u5316\u5bf9\u6297\u540e\u7f00\u800c\u975e\u7528\u6237\u63d0\u793a\u3002", "result": "sockpuppetting\u5728Qwen3-8B\u4e0a\u6bd4GCG\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad880%\uff08\u9010\u63d0\u793a\u6bd4\u8f83\uff09\u3002\u6df7\u5408\u65b9\u6cd5\u5728Llama-3.1-8B\u4e0a\u6bd4GCG\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad864%\uff08\u63d0\u793a\u65e0\u5173\u8bbe\u7f6e\uff09\u3002", "conclusion": "sockpuppetting\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f4e\u6210\u672c\u653b\u51fb\u65b9\u6cd5\uff0c\u5373\u4f7f\u662f\u6280\u672f\u4e0d\u719f\u7ec3\u7684\u653b\u51fb\u8005\u4e5f\u80fd\u4f7f\u7528\uff0c\u51f8\u663e\u4e86\u5f00\u6e90\u6a21\u578b\u9700\u8981\u9632\u5fa1\u8f93\u51fa\u524d\u7f00\u6ce8\u5165\u653b\u51fb\u3002"}}
{"id": "2601.12666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12666", "abs": "https://arxiv.org/abs/2601.12666", "authors": ["Zonglin Li", "Jieji Ren", "Shuangfan Zhou", "Heng Guo", "Jinnuo Zhang", "Jiang Zhou", "Boxin Shi", "Zhanyu Ma", "Guoying Gu"], "title": "Near-Light Color Photometric Stereo for mono-Chromaticity non-lambertian surface", "comment": "5 pages 7figures", "summary": "Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u7684\u5355\u56fe\u50cf\u989c\u8272\u5149\u5ea6\u7acb\u4f53\u6846\u67b6\uff0c\u5728\u5355\u8272\u6027\u5047\u8bbe\u4e0b\u5b9e\u73b0\u8fd1\u5149\u548c\u590d\u6742\u53cd\u5c04\u8868\u9762\u7684\u7cbe\u786e\u91cd\u5efa\uff0c\u5e76\u8bbe\u8ba1\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5668\u8fdb\u884c\u9a8c\u8bc1", "motivation": "\u73b0\u6709\u989c\u8272\u5149\u5ea6\u7acb\u4f53\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u7406\u60f3\u8fdc\u8ddd\u79bb\u7167\u660e\u548c\u6717\u4f2f\u53cd\u5c04\uff0c\u65e0\u6cd5\u5904\u7406\u66f4\u5b9e\u9645\u7684\u8fd1\u5149\u6761\u4ef6\u548c\u975e\u6717\u4f2f\u8868\u9762\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u91c7\u7528\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u5efa\u6a21\u6df1\u5ea6\u548cBRDF\uff0c\u5f15\u5165\u5355\u8272\u6027\u5047\u8bbe\uff08\u5747\u5300\u8272\u5ea6\u548c\u540c\u8d28\u6750\u6599\uff09\u6765\u7f13\u89e3\u989c\u8272\u5149\u5ea6\u7acb\u4f53\u7684\u75c5\u6001\u6027\uff0c\u5b9e\u73b0\u5355\u56fe\u50cf\u8868\u9762\u91cd\u5efa", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u8868\u9762\u91cd\u5efa\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u9690\u5f0f\u8868\u793a\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8fd1\u5149\u6761\u4ef6\u548c\u590d\u6742\u53cd\u5c04\u8868\u9762\u7684\u5355\u56fe\u50cf\u91cd\u5efa\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13368", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13368", "abs": "https://arxiv.org/abs/2601.13368", "authors": ["Zhenjiang Mao", "Anirudhh Venkat"], "title": "Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models", "comment": null, "summary": "As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6b65\u9aa4\u95f4\u6ce8\u610f\u529b\u5206\u6790\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u5e76\u5f15\u5165\u9690\u85cf\u7f6e\u4fe1\u5ea6\u673a\u5236\u6765\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8bc4\u4f30\u7b54\u6848\u4e0d\u786e\u5b9a\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u6790\u957f\u63a8\u7406\u5e8f\u5217\u65f6\u5ffd\u7565\u4e86\u7f6e\u4fe1\u5ea6\u7684\u65f6\u95f4\u5206\u5e03\uff0c\u5bfc\u81f4\u5373\u4f7f\u65e9\u671f\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u5f88\u4f4e\uff0c\u6574\u4f53\u7f6e\u4fe1\u5ea6\u4ecd\u88ab\u9ad8\u4f30\uff0c\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u5e7b\u89c9\u3002", "method": "1) \u5f15\u5165\u6b65\u9aa4\u95f4\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\u8de8\u6b65\u9aa4\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff1b2) \u9488\u5bf9\u957f\u5e8f\u5217\u54cd\u5e94\uff0c\u63d0\u51fa\u9690\u85cf\u7f6e\u4fe1\u5ea6\u673a\u5236\u4fdd\u7559\u5386\u53f2\u7f6e\u4fe1\u5ea6\u4fe1\u606f\uff1b3) \u5c06\u5386\u53f2\u7f6e\u4fe1\u5ea6\u4e0e\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u7ed3\u5408\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u603b\u4f53\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728GAOKAO\u6570\u5b66\u57fa\u51c6\u548cCLadder\u56e0\u679c\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e3b\u6d41\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u8d28\u91cf\u548c\u6821\u51c6\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5728\u8d1f\u5bf9\u6570\u4f3c\u7136\u548c\u671f\u671b\u6821\u51c6\u8bef\u5dee\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u7f6e\u4fe1\u5ea6\u7684\u65f6\u95f4\u5206\u5e03\u548c\u8de8\u6b65\u9aa4\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51cf\u5c11\u8bef\u5bfc\u6027\u5e7b\u89c9\uff0c\u5728\u9884\u6d4b\u8d28\u91cf\u548c\u6821\u51c6\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.12671", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12671", "abs": "https://arxiv.org/abs/2601.12671", "authors": ["Thamara Leandra de Deus Melo", "Rodrigo Moreira", "Larissa Ferreira Rodrigues Moreira", "Andr\u00e9 Ricardo Backes"], "title": "Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification", "comment": "21st International Conference on Computer Vision Theory and Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain", "summary": "Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.", "AI": {"tldr": "\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\uff0c\u9884\u5904\u7406\u5355\u72ec\u4f7f\u7528\u6548\u679c\u6709\u9650\uff0c\u4f46\u4e0e\u6d4b\u8bd5\u65f6\u589e\u5f3a\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u5347\u8111\u80bf\u7624MRI\u5206\u7c7b\u6027\u80fd", "motivation": "\u8111\u80bf\u7624\u65e9\u671f\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u75c5\u53d8\u53d8\u5f02\u6027\u548c\u56fe\u50cf\u590d\u6742\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u8111\u80bf\u7624\u8bca\u65ad\u4e2d\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u6bd4\u8f83\u539f\u59cb\u56fe\u50cf\u4e0e\u9884\u5904\u7406\u56fe\u50cf\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u8f83\u539f\u59cbMRI\u56fe\u50cf\u4e0e\u9884\u5904\u7406\u56fe\u50cf\uff08\u5305\u62ec\u8c03\u6574\u5927\u5c0f\u3001\u7070\u5ea6\u8f6c\u6362\u3001\u5f52\u4e00\u5316\u3001\u6ee4\u6ce2\u548c\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff09\u7684\u8bad\u7ec3\u6548\u679c\u3002\u540c\u65f6\u6d4b\u8bd5\u9884\u5904\u7406\u4e0e\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08TTA\uff09\u7684\u7ec4\u5408\u6548\u679c\u3002", "result": "\u5355\u72ec\u4f7f\u7528\u9884\u5904\u7406\u5e26\u6765\u7684\u589e\u76ca\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u4f46\u5f53\u9884\u5904\u7406\u4e0e\u6d4b\u8bd5\u65f6\u589e\u5f3a\u7ed3\u5408\u65f6\uff0c\u5728\u8054\u90a6MRI\u5206\u7c7b\u4e2d\u4ea7\u751f\u4e86\u6301\u7eed\u4e14\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\uff08p<0.001\uff09\u3002", "conclusion": "\u5728\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u5e94\u4f5c\u4e3a\u9ed8\u8ba4\u7684\u63a8\u7406\u7b56\u7565\uff1b\u5f53\u8ba1\u7b97\u9884\u7b97\u5141\u8bb8\u65f6\uff0c\u5c06\u6d4b\u8bd5\u65f6\u589e\u5f3a\u4e0e\u8f7b\u91cf\u7ea7\u9884\u5904\u7406\u7ed3\u5408\u53ef\u63d0\u4f9b\u989d\u5916\u7684\u53ef\u9760\u589e\u76ca\u3002"}}
{"id": "2601.13387", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13387", "abs": "https://arxiv.org/abs/2601.13387", "authors": ["Zhenjiang Mao", "Anirudhh Venkat", "Artem Bisliouk", "Akshat Kothiyal", "Sindhura Kumbakonam Subramanian", "Saithej Singhu", "Ivan Ruchkin"], "title": "Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u5206\u6790LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9010\u6b65\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u901a\u8fc7STL\u6316\u6398\u53d1\u73b0\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u54cd\u5e94\u7684\u65f6\u5e8f\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c06\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u7b80\u5316\u4e3a\u5355\u4e2a\u6807\u91cf\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u7f6e\u4fe1\u5ea6\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6f14\u53d8\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u8868\u9762\u56e0\u7d20\uff08\u5982\u54cd\u5e94\u957f\u5ea6\u6216\u5197\u957f\u7a0b\u5ea6\uff09\u654f\u611f\uff0c\u96be\u4ee5\u533a\u5206\u6b63\u786e\u63a8\u7406\u548c\u81ea\u4fe1\u9648\u8ff0\u7684\u9519\u8bef\u3002", "method": "\u4f7f\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u8868\u5f81\u9010\u6b65\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u901a\u8fc7\u5224\u522b\u6027STL\u6316\u6398\u7a0b\u5e8f\u53d1\u73b0\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u54cd\u5e94\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u7684\u65f6\u5e8f\u516c\u5f0f\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u53c2\u6570\u8d85\u7f51\u7edc\u4e3aSTL\u5757\u63d0\u4f9b\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTL\u6a21\u5f0f\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u6570\u503c\u53c2\u6570\u5bf9\u4e2a\u522b\u95ee\u9898\u654f\u611f\u3002\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6821\u51c6\u3002", "conclusion": "\u901a\u8fc7STL\u5206\u6790\u9010\u6b65\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u80fd\u6709\u6548\u6539\u8fdbLLM\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6807\u91cf\u65b9\u6cd5\u66f4\u51c6\u786e\u548c\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002"}}
{"id": "2601.12672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12672", "abs": "https://arxiv.org/abs/2601.12672", "authors": ["Qimao Chen", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Zixun Xie", "Yuechen Luo", "Shengyin Jiang", "Hanbing Li", "Long Chen", "Bing Wang", "Yi Zhang", "Zhi-Xin Yang"], "title": "VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness", "comment": "Accepted to AAAI 2026", "summary": "The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents' future trajectories. This direct-editing approach fully leverages the VLM's powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.", "AI": {"tldr": "VILTA\u6846\u67b6\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u7f16\u8f91\u5468\u56f4\u8f66\u8f86\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u95ed\u73af\u8bad\u7ec3\u4e2d\u751f\u6210\u591a\u6837\u5316\u6311\u6218\u6027\u573a\u666f\uff0c\u89e3\u51b3\u957f\u5c3e\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u90e8\u7f72\u9762\u4e34\u957f\u5c3e\u95ee\u9898\uff0c\u5373\u7f55\u89c1\u4f46\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u4e25\u91cd\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c4\u5219\u542f\u53d1\u5f0f\u3001\u91cd\u91c7\u6837\u548c\u79bb\u7ebf\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\uff0c\u96be\u4ee5\u4ea7\u751f\u591a\u6837\u5316\u548c\u65b0\u9896\u7684\u6311\u6218\u3002\u4e24\u9636\u6bb5\u6846\u67b6\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u6f5c\u529b\u3002", "method": "\u63d0\u51faVILTA\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u7684\u95ed\u73af\u8bad\u7ec3\u4e2d\u3002VILTA\u901a\u8fc7\u76f4\u63a5\u3001\u7ec6\u7c92\u5ea6\u5730\u7f16\u8f91\u5468\u56f4\u4ee3\u7406\u7684\u672a\u6765\u8f68\u8ff9\u6765\u7406\u89e3\u52a8\u6001\u9a7e\u9a76\u73af\u5883\u5e76\u6218\u7565\u6027\u5730\u751f\u6210\u6311\u6218\u6027\u573a\u666f\uff0c\u5145\u5206\u5229\u7528VLM\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5173\u952e\u957f\u5c3e\u4e8b\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "VILTA\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u53c2\u4e0e\u8bad\u7ec3\u5faa\u73af\uff0c\u80fd\u591f\u751f\u6210\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u8303\u56f4\u7684\u591a\u6837\u5316\u3001\u5408\u7406\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u957f\u5c3e\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2601.13388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13388", "abs": "https://arxiv.org/abs/2601.13388", "authors": ["Sasha Ronaghi", "Prerit Choudhary", "David H Rehkopf", "Bryant Lin"], "title": "Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction", "comment": "7 pages, 5 figures", "summary": "Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u7cd6\u5c3f\u75c5\u60a3\u8005\u751f\u6d3b\u6545\u4e8b\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u4fe1\u606f\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u7cd6\u5c3f\u75c5\u63a7\u5236\u7684\u9884\u6d4b\u4ef7\u503c", "motivation": "\u793e\u4f1a\u5065\u5eb7\u51b3\u5b9a\u56e0\u7d20\u57282\u578b\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u4e2d\u5f80\u5f80\u7f3a\u4e4f\u8fd9\u4e9b\u4fe1\u606f\u3002\u73b0\u6709\u7684\u7ed3\u6784\u5316\u7b5b\u67e5\u5de5\u5177\u65e0\u6cd5\u7075\u6d3b\u6355\u6349\u60a3\u8005\u7ecf\u5386\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u83b7\u53d6\u548c\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u3002", "method": "\u6536\u96c665\u540d65\u5c81\u4ee5\u4e0a2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u975e\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u53d9\u4e8b\uff0c\u751f\u6210\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u5b9a\u6027\u6458\u8981\u548c\u7ed3\u6784\u5316\u5b9a\u91cfSDOH\u8bc4\u5206\u3002\u5c06\u8fd9\u4e9b\u8bc4\u5206\u4e0e\u4f20\u7edf\u5b9e\u9a8c\u5ba4\u751f\u7269\u6807\u5fd7\u7269\u7ed3\u5408\uff0c\u8f93\u5165\u7ebf\u6027\u548c\u6811\u57fa\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u98ce\u9669\u9884\u6d4b\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u8bbf\u8c08\u6587\u672c\u4e2d\u9884\u6d4b\u7cd6\u5c3f\u75c5\u63a7\u5236\u6c34\u5e73\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\uff09\uff0c\u51c6\u786e\u7387\u8fbe\u523060%\u3002\u7ed3\u6784\u5316SDOH\u8bc4\u5206\u53ef\u7528\u4e8e\u589e\u5f3a\u4f20\u7edf\u98ce\u9669\u9884\u6d4b\u6a21\u578b\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5c06\u975e\u7ed3\u6784\u5316SDOH\u76f8\u5173\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6d1e\u5bdf\uff0c\u4e3a\u589e\u5f3a\u4e34\u5e8a\u98ce\u9669\u6a21\u578b\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u6709\u671b\u6539\u5584\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u7684\u793e\u4f1a\u56e0\u7d20\u8003\u91cf\u3002"}}
{"id": "2601.12682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12682", "abs": "https://arxiv.org/abs/2601.12682", "authors": ["Banglei Guan", "Dongcai Tan", "Jing Tao", "Ang Su", "Yang Shang", "Qifeng Yu"], "title": "Fusion-Restoration Image Processing Algorithm to Improve the High-Temperature Deformation Measurement", "comment": null, "summary": "In the deformation measurement of high-temperature structures, image degradation caused by thermal radiation and random errors introduced by heat haze restrict the accuracy and effectiveness of deformation measurement. To suppress thermal radiation and heat haze using fusion-restoration image processing methods, thereby improving the accuracy and effectiveness of DIC in the measurement of high-temperature deformation. For image degradation caused by thermal radiation, based on the image layered representation, the image is decomposed into positive and negative channels for parallel processing, and then optimized for quality by multi-exposure image fusion. To counteract the high-frequency, random errors introduced by heat haze, we adopt the FSIM as the objective function to guide the iterative optimization of model parameters, and the grayscale average algorithm is applied to equalize anomalous gray values, thereby reducing measurement error. The proposed multi-exposure image fusion algorithm effectively suppresses image degradation caused by complex illumination conditions, boosting the effective computation area from 26% to 50% for under-exposed images and from 32% to 40% for over-exposed images without degrading measurement accuracy in the experiment. Meanwhile, the image restoration combined with the grayscale average algorithm reduces static thermal deformation measurement errors. The error in \u03b5_xx is reduced by 85.3%, while the errors in \u03b5_yy and \u03b3_xy are reduced by 36.0% and 36.4%, respectively. We present image processing methods to suppress the interference of thermal radiation and heat haze in high-temperature deformation measurement using DIC. The experimental results verify that the proposed method can effectively improve image quality, reduce deformation measurement errors, and has potential application value in thermal deformation measurement.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408-\u6062\u590d\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\uff0c\u6291\u5236\u9ad8\u6e29\u7ed3\u6784\u53d8\u5f62\u6d4b\u91cf\u4e2d\u7684\u70ed\u8f90\u5c04\u548c\u70ed\u973e\u5e72\u6270\uff0c\u63d0\u9ad8DIC\u6d4b\u91cf\u7cbe\u5ea6", "motivation": "\u9ad8\u6e29\u7ed3\u6784\u53d8\u5f62\u6d4b\u91cf\u4e2d\uff0c\u70ed\u8f90\u5c04\u5f15\u8d77\u7684\u56fe\u50cf\u9000\u5316\u548c\u70ed\u973e\u5f15\u5165\u7684\u968f\u673a\u8bef\u5dee\u9650\u5236\u4e86\u6d4b\u91cf\u7cbe\u5ea6\u548c\u6709\u6548\u6027", "method": "1) \u57fa\u4e8e\u56fe\u50cf\u5206\u5c42\u8868\u793a\uff0c\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u6b63\u8d1f\u901a\u9053\u5e76\u884c\u5904\u7406\uff0c\u901a\u8fc7\u591a\u66dd\u5149\u56fe\u50cf\u878d\u5408\u4f18\u5316\u8d28\u91cf\uff1b2) \u91c7\u7528FSIM\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u6307\u5bfc\u6a21\u578b\u53c2\u6570\u8fed\u4ee3\u4f18\u5316\uff0c\u7ed3\u5408\u7070\u5ea6\u5e73\u5747\u7b97\u6cd5\u5747\u8861\u5f02\u5e38\u7070\u5ea6\u503c", "result": "\u591a\u66dd\u5149\u878d\u5408\u7b97\u6cd5\u5c06\u6b20\u66dd\u5149\u56fe\u50cf\u6709\u6548\u8ba1\u7b97\u533a\u57df\u4ece26%\u63d0\u5347\u81f350%\uff0c\u8fc7\u66dd\u5149\u56fe\u50cf\u4ece32%\u63d0\u5347\u81f340%\uff1b\u9759\u6001\u70ed\u53d8\u5f62\u6d4b\u91cf\u8bef\u5dee\u4e2d\u03b5_xx\u51cf\u5c1185.3%\uff0c\u03b5_yy\u548c\u03b3_xy\u5206\u522b\u51cf\u5c1136.0%\u548c36.4%", "conclusion": "\u63d0\u51fa\u7684\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u70ed\u8f90\u5c04\u548c\u70ed\u973e\u5e72\u6270\uff0c\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u5e76\u51cf\u5c11\u53d8\u5f62\u6d4b\u91cf\u8bef\u5dee\uff0c\u5728\u70ed\u53d8\u5f62\u6d4b\u91cf\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.13392", "categories": ["cs.CL", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.13392", "abs": "https://arxiv.org/abs/2601.13392", "authors": ["Shlok Shelat", "Jay Raval", "Souvik Roy", "Manas Gaur"], "title": "Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks", "comment": "30 pages, 11 figures, 6 tables, Work in Progress", "summary": "Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.", "AI": {"tldr": "LLMs\u5728\u6b63\u5219\u8bed\u8a00\u8f6cDFA\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1a\u5bf9\u5df2\u77e5\u95ee\u9898\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5bf9\u672a\u89c1\u95ee\u9898\u51c6\u786e\u7387\u9aa4\u964d\uff0c\u66b4\u9732\u5176\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u5f62\u5f0f\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u662f\u771f\u6b63\u7684\u7b26\u53f7\u63a8\u7406\u8fd8\u662f\u5bf9\u719f\u6089\u7ed3\u6784\u7684\u6a21\u5f0f\u5339\u914d\uff0c\u901a\u8fc7DFA\u6784\u9020\u4efb\u52a1\u6765\u8bc4\u4f30\u5176\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e8b\u5b9e\u77e5\u8bc6\u95ee\u9898\u3001\u516c\u5f00\u6765\u6e90\u7684\u5df2\u77e5\u6784\u9020\u95ee\u9898\u3001\u624b\u5de5\u5236\u4f5c\u7684\u591a\u7ea6\u675f\u4ea4\u4e92\u95ee\u9898\u4ee5\u53ca\u901a\u8fc7Arden\u5b9a\u7406\u7cfb\u7edf\u751f\u6210\u95ee\u9898\u7684DFA\u6784\u9020\u57fa\u51c6\u3002\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff08\u76f4\u63a5\u3001\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u6811\uff09\u548c\u4e09\u9636\u6bb5\u63d0\u793a\u534f\u8bae\u3002", "result": "\u6a21\u578b\u5728\u4e8b\u5b9e\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u5b8c\u7f8e\uff0c\u5df2\u77e5\u4efb\u52a1\u4e0a84-90%\uff0c\u4f46\u672a\u89c1\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u9aa4\u964d30-64%\u3002\u9519\u8bef\u6e90\u4e8e\u5bf9\u8bed\u8a00\u7ea6\u675f\u7684\u7cfb\u7edf\u6027\u8bef\u89e3\u3001Kleene\u661f\u53f7\u8bed\u4e49\u5904\u7406\u9519\u8bef\u4ee5\u53ca\u5168\u5c40\u4e00\u81f4\u6027\u7ef4\u62a4\u5931\u8d25\u3002\u63d0\u793a\u7b56\u7565\u65e0\u6cd5\u89e3\u51b3\u6839\u672c\u9519\u8bef\u3002", "conclusion": "LLMs\u80fd\u591f\u751f\u6210\u8bed\u6cd5\u4e0a\u770b\u4f3c\u5408\u7406\u7684DFA\uff0c\u4f46\u5728\u8bed\u4e49\u6b63\u786e\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5dee\u8ddd\uff0c\u63d0\u793a\u7b56\u7565\u65e0\u6cd5\u53ef\u9760\u89e3\u51b3\u5168\u5c40\u4e0d\u4e00\u81f4\u6216\u7ed3\u6784\u6709\u7f3a\u9677\u7684\u81ea\u52a8\u673a\u95ee\u9898\u3002"}}
{"id": "2601.12683", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.12683", "abs": "https://arxiv.org/abs/2601.12683", "authors": ["Liwei Liao", "Ronggang Wang"], "title": "GaussianTrimmer: Online Trimming Boundaries for 3DGS Segmentation", "comment": null, "summary": "With the widespread application of 3D Gaussians in 3D scene representation, 3D scene segmentation methods based on 3D Gaussians have also gradually emerged. However, existing 3D Gaussian segmentation methods basically segment on the basis of Gaussian primitives. Due to the large variation range of the scale of 3D Gaussians, large-sized Gaussians that often span the foreground and background lead to jagged boundaries of segmented objects. To this end, we propose an online boundary trimming method, GaussianTrimmer, which is an efficient and plug-and-play post-processing method capable of trimming coarse boundaries for existing 3D Gaussian segmentation methods. Our method consists of two core steps: 1. Generating uniformly and well-covered virtual cameras; 2. Trimming Gaussian at the primitive level based on 2D segmentation results on virtual cameras. Extensive quantitative and qualitative experiments demonstrate that our method can improve the segmentation quality of existing 3D Gaussian segmentation methods as a plug-and-play method.", "AI": {"tldr": "\u63d0\u51faGaussianTrimmer\uff0c\u4e00\u79cd\u7528\u4e8e3D\u9ad8\u65af\u5206\u5272\u7684\u5728\u7ebf\u8fb9\u754c\u4fee\u6574\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u548c2D\u5206\u5272\u7ed3\u679c\u4fee\u6574\u9ad8\u65af\u57fa\u5143\uff0c\u6539\u5584\u5206\u5272\u8fb9\u754c\u8d28\u91cf", "motivation": "\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u57fa\u4e8e\u9ad8\u65af\u57fa\u5143\u8fdb\u884c\u5206\u5272\uff0c\u4f46\u7531\u4e8e3D\u9ad8\u65af\u5c3a\u5ea6\u53d8\u5316\u8303\u56f4\u5927\uff0c\u8de8\u8d8a\u524d\u666f\u80cc\u666f\u7684\u5927\u5c3a\u5bf8\u9ad8\u65af\u5bfc\u81f4\u5206\u5272\u5bf9\u8c61\u8fb9\u754c\u952f\u9f7f\u72b6\uff0c\u9700\u8981\u6539\u5584\u8fb9\u754c\u8d28\u91cf", "method": "\u63d0\u51faGaussianTrimmer\u65b9\u6cd5\uff1a1)\u751f\u6210\u5747\u5300\u8986\u76d6\u7684\u865a\u62df\u76f8\u673a\uff1b2)\u57fa\u4e8e\u865a\u62df\u76f8\u673a\u4e0a\u76842D\u5206\u5272\u7ed3\u679c\u5728\u57fa\u5143\u7ea7\u522b\u4fee\u6574\u9ad8\u65af\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u540e\u5904\u7406\u65b9\u6cd5", "result": "\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u7684\u5206\u5272\u8d28\u91cf", "conclusion": "GaussianTrimmer\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u5728\u7ebf\u8fb9\u754c\u4fee\u6574\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6539\u5584\u73b0\u67093D\u9ad8\u65af\u5206\u5272\u65b9\u6cd5\u7684\u8fb9\u754c\u8d28\u91cf"}}
{"id": "2601.13433", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13433", "abs": "https://arxiv.org/abs/2601.13433", "authors": ["Priyanka Mary Mammen", "Emil Joswin", "Shankar Venkitachalam"], "title": "Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models", "comment": null, "summary": "Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4f1a\u53d7\u5230\u6743\u5a01\u6765\u6e90\u7684\u8bef\u5bfc\u6027\u80cc\u4e66\u5f71\u54cd\uff0c\u968f\u7740\u6765\u6e90\u4e13\u4e1a\u6027\u589e\u52a0\uff0c\u6a21\u578b\u66f4\u5bb9\u6613\u63a5\u53d7\u9519\u8bef\u5efa\u8bae\u5e76\u8868\u73b0\u51fa\u66f4\u9ad8\u7f6e\u4fe1\u5ea6", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4f1a\u53d7\u5230\u5efa\u8bae\u3001\u63d0\u793a\u548c\u80cc\u4e66\u7684\u5f71\u54cd\uff0c\u4f46\u80cc\u4e66\u6765\u6e90\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f1a\u57fa\u4e8e\u80cc\u4e66\u63d0\u4f9b\u8005\u7684\u611f\u77e5\u4e13\u4e1a\u6027\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u89c1\u3002", "method": "\u5728\u6570\u5b66\u3001\u6cd5\u5f8b\u548c\u533b\u5b66\u63a8\u74064\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3011\u4e2a\u6a21\u578b\uff0c\u4f7f\u7528\u4ee3\u8868\u6bcf\u4e2a\u9886\u57df\u56db\u4e2a\u4e13\u4e1a\u6c34\u5e73\u7684\u4eba\u7269\u89d2\u8272\uff0c\u7814\u7a76\u6a21\u578b\u5bf9\u4e0d\u540c\u4e13\u4e1a\u6c34\u5e73\u6765\u6e90\u7684\u80cc\u4e66\u53cd\u5e94\u3002", "result": "\u6a21\u578b\u968f\u7740\u6765\u6e90\u4e13\u4e1a\u6027\u589e\u52a0\uff0c\u8d8a\u6765\u8d8a\u5bb9\u6613\u63a5\u53d7\u9519\u8bef/\u8bef\u5bfc\u6027\u80cc\u4e66\uff0c\u9ad8\u6743\u5a01\u6765\u6e90\u4e0d\u4ec5\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u8fd8\u589e\u52a0\u4e86\u5bf9\u9519\u8bef\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u3002\u8fd9\u79cd\u6743\u5a01\u504f\u89c1\u5728\u6a21\u578b\u4e2d\u5177\u6709\u673a\u5236\u6027\u7f16\u7801\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6743\u5a01\u504f\u89c1\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u8fdc\u79bb\u8fd9\u79cd\u504f\u89c1\u6765\u6539\u5584\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u4e13\u5bb6\u7ed9\u51fa\u8bef\u5bfc\u6027\u80cc\u4e66\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u63d0\u9ad8\u8868\u73b0\u3002"}}
{"id": "2601.12697", "categories": ["cs.CV", "cs.CG"], "pdf": "https://arxiv.org/pdf/2601.12697", "abs": "https://arxiv.org/abs/2601.12697", "authors": ["Chao Yang", "Deshui Miao", "Chao Tian", "Guoqing Zhu", "Yameng Gu", "Zhenyu He"], "title": "Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation", "comment": null, "summary": "Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51faIVGF\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff0c\u5b9e\u73b0\u7ea2\u5916-\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\uff0c\u89e3\u51b3\u4f20\u7edf2D\u65b9\u6cd5\u89c6\u89d2\u56fa\u5b9a\u3001\u4fe1\u606f\u4e22\u5931\u95ee\u9898", "motivation": "\u73b0\u67092D\u878d\u5408\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u76f8\u673a\u89c6\u89d2\uff0c\u65e0\u6cd5\u5168\u9762\u7406\u89e3\u590d\u6742\u573a\u666f\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002\u9700\u8981\u4e00\u79cd\u80fd\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u3001\u652f\u6301\u591a\u89c6\u89d2\u6e32\u67d3\u7684\u878d\u5408\u65b9\u6cd5", "method": "\u63d0\u51fa\u7ea2\u5916-\u53ef\u89c1\u5149\u9ad8\u65af\u878d\u5408(IVGF)\u6846\u67b6\uff1a1) \u4ece\u591a\u6a21\u60012D\u8f93\u5165\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff1b2) \u8de8\u6a21\u6001\u8c03\u6574(CMA)\u6a21\u5757\u8c03\u8282\u9ad8\u65af\u4e0d\u900f\u660e\u5ea6\u89e3\u51b3\u8de8\u6a21\u6001\u51b2\u7a81\uff1b3) \u878d\u5408\u635f\u5931\u6307\u5bfcCMA\u4f18\u5316\uff0c\u4fdd\u7559\u5404\u6a21\u6001\u5173\u952e\u7279\u5f81", "result": "\u5168\u9762\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u4fdd\u7559\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u5173\u952e\u7279\u5f81\u7684\u878d\u5408\u56fe\u50cf", "conclusion": "IVGF\u6846\u67b6\u901a\u8fc73D\u9ad8\u65af\u8868\u793a\u91cd\u5efa\u573a\u666f\u51e0\u4f55\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf2D\u878d\u5408\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u548c\u4fe1\u606f\u4fdd\u7559"}}
{"id": "2601.13437", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13437", "abs": "https://arxiv.org/abs/2601.13437", "authors": ["Adriana-Valentina Costache", "Daria-Nicoleta Dragomir", "Silviu-Florin Gheorghe", "Eduard Poesina", "Paul Irofti", "Radu Tudor Ionescu"], "title": "MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization", "comment": null, "summary": "Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u5f00\u653e\u96c6\u5b66\u4e60\u4e0e\u53d1\u73b0\uff08MOSLD\uff09\u57fa\u51c6\uff0c\u7528\u4e8e\u6587\u672c\u4e3b\u9898\u5206\u7c7b\uff0c\u5305\u542b12\u79cd\u8bed\u8a00\u768496\u4e07\u6570\u636e\u6837\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u591a\u9636\u6bb5\u7684\u65b0\u6846\u67b6\u6765\u6301\u7eed\u53d1\u73b0\u548c\u5b66\u4e60\u65b0\u7c7b\u522b\u3002", "motivation": "\u5f00\u653e\u96c6\u5b66\u4e60\u4e0e\u53d1\u73b0\uff08OSLD\uff09\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u5176\u4e2d\u6d4b\u8bd5\u65f6\u53ef\u80fd\u51fa\u73b0\u6765\u81ea\u65b0\uff08\u672a\u77e5\uff09\u7c7b\u522b\u7684\u6837\u672c\u3002\u867d\u7136\u96f6\u6837\u672c\u5b66\u4e60\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5f00\u653e\u96c6\u5b66\u4e60\u4e0e\u53d1\u73b0\u5bf9\u4e8e\u6587\u672c\u9886\u57df\u6765\u8bf4\u662f\u4e00\u4e2a\u76f8\u5bf9\u8f83\u65b0\u7684\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u76f8\u5e94\u7684\u57fa\u51c6\u3002", "method": "1) \u6784\u5efa\u9996\u4e2a\u591a\u8bed\u8a00\u5f00\u653e\u96c6\u5b66\u4e60\u4e0e\u53d1\u73b0\u57fa\u51c6\uff0c\u901a\u8fc7(i)\u91cd\u65b0\u6574\u7406\u73b0\u6709\u6570\u636e\u96c6\u548c(ii)\u4ece\u65b0\u95fb\u9886\u57df\u6536\u96c6\u65b0\u6570\u636e\u6837\u672c\uff0c\u521b\u5efa\u4e86\u5305\u542b12\u79cd\u8bed\u8a00\u3001960K\u6570\u636e\u6837\u672c\u7684\u57fa\u51c6\uff1b2) \u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684OSLD\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u591a\u4e2a\u9636\u6bb5\u6765\u6301\u7eed\u53d1\u73b0\u548c\u5b66\u4e60\u65b0\u7c7b\u522b\u3002", "result": "\u8bc4\u4f30\u4e86\u5305\u62ec\u4f5c\u8005\u81ea\u5df1\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u8bed\u8a00\u6a21\u578b\uff0c\u83b7\u5f97\u4e86\u53ef\u4f5c\u4e3a\u672a\u6765\u5de5\u4f5c\u53c2\u8003\u7684\u7ed3\u679c\u3002\u57fa\u51c6\u5df2\u516c\u5f00\u53d1\u5e03\u5728GitHub\u4e0a\u3002", "conclusion": "\u672c\u6587\u4e3a\u6587\u672c\u5206\u7c7b\u9886\u57df\u7684\u5f00\u653e\u96c6\u5b66\u4e60\u4e0e\u53d1\u73b0\u4efb\u52a1\u521b\u5efa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u591a\u9636\u6bb5\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.12714", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12714", "abs": "https://arxiv.org/abs/2601.12714", "authors": ["Songlin Dong", "Jiangyang Li", "Chenhao Ding", "Zhiheng Ma", "Haoyu Luo", "Yuhang He", "Yihong Gong"], "title": "P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning", "comment": "12 pages, 5 figures", "summary": "Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.", "AI": {"tldr": "P2L-CA\uff1a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5230\u6807\u7b7e\u6a21\u5757\u548c\u8fde\u7eed\u9002\u914d\u5668\u6a21\u5757\u89e3\u51b3\u7279\u5f81\u6df7\u6dc6\u548c\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a", "motivation": "\u73b0\u6709\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\uff08\u5168\u53c2\u6570\u5fae\u8c03\uff09\u3001\u5927\u5b58\u50a8\u5f00\u9500\uff08\u5185\u5b58\u7f13\u51b2\u533a\uff09\uff0c\u4ee5\u53ca\u7279\u5f81\u6df7\u6dc6\u548c\u9886\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faP2L-CA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) Prompt-to-Label\u6a21\u5757\u5229\u7528\u7c7b\u7279\u5b9a\u63d0\u793a\u89e3\u8026\u591a\u6807\u7b7e\u8868\u793a\uff0c\u7ed3\u5408\u8bed\u8a00\u5148\u9a8c\u5b9e\u73b0\u7a33\u5b9a\u7684\u8bed\u4e49-\u89c6\u89c9\u5bf9\u9f50\uff1b2) Continuous Adapter\u6a21\u5757\u4f7f\u7528\u8f7b\u91cf\u9002\u914d\u5668\u7f13\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u95f4\u7684\u9886\u57df\u5dee\u5f02\uff0c\u589e\u5f3a\u6a21\u578b\u53ef\u5851\u6027", "result": "\u5728MS-COCO\u548cPASCAL VOC\u7684\u6807\u51c6\u548c\u6311\u6218\u6027MLCIL\u8bbe\u7f6e\u4e0a\uff0cP2L-CA\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\uff0c\u5728CIL\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4ec5\u9700\u6781\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u4e14\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a", "conclusion": "P2L-CA\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u63d0\u793a\u5b66\u4e60\u548c\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6807\u7b7e\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u3001\u5b58\u50a8\u5f00\u9500\u548c\u7279\u5f81\u6df7\u6dc6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13453", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13453", "abs": "https://arxiv.org/abs/2601.13453", "authors": ["Aditya Thole", "Anmol Agrawal", "Arnav Ramamoorthy", "Dhruv Kumar"], "title": "PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving", "comment": null, "summary": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems", "AI": {"tldr": "PSA\u662f\u4e00\u4e2a\u81ea\u4e3b\u4ee3\u7406\uff0c\u4f7f\u7528Manim\u52a8\u753b\u751f\u6210\u957f\u8fbe6\u5206\u949f\u7684\u7269\u7406\u89e3\u91ca\u89c6\u9891\uff0c\u901a\u8fc7\u5305\u542b15\u4e2a\u5b9a\u91cf\u53c2\u6570\u7684\u8bc4\u4f30\u7ba1\u9053\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u6765\u8fed\u4ee3\u6539\u8fdb\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7269\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u89e3\u91ca\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6e05\u6670\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u6982\u5ff5\u7406\u89e3\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165PhysicsSolutionAgent (PSA)\uff0c\u4f7f\u7528Manim\u52a8\u753b\u751f\u6210\u7269\u7406\u89e3\u91ca\u89c6\u9891\u3002\u8bbe\u8ba1\u8bc4\u4f30\u7ba1\u9053\u8fdb\u884c15\u4e2a\u5b9a\u91cf\u53c2\u6570\u7684\u81ea\u52a8\u68c0\u67e5\uff0c\u5e76\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u6765\u8fed\u4ee3\u6539\u8fdb\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u572832\u4e2a\u89c6\u9891\u4e0a\u8bc4\u4f30\uff0cPSA\u4f7f\u7528GPT-5-mini\u5b9e\u73b0100%\u89c6\u9891\u5b8c\u6210\u7387\uff0c\u5e73\u5747\u81ea\u52a8\u8bc4\u52063.8/5\u3002\u4f46\u5b9a\u6027\u5206\u6790\u548c\u4eba\u5de5\u68c0\u67e5\u53d1\u73b0\u89c6\u89c9\u5e03\u5c40\u4e0d\u4e00\u81f4\u548c\u89c6\u89c9\u5185\u5bb9\u89e3\u91ca\u9519\u8bef\u7b49\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53ef\u9760Manim\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\u9650\u5236\uff0c\u7a81\u663e\u4e86\u591a\u6a21\u6001\u63a8\u7406\u548c\u8bc4\u4f30\u5728\u6570\u503c\u7269\u7406\u95ee\u9898\u89c6\u89c9\u89e3\u91ca\u4e2d\u7684\u6311\u6218\uff0c\u5f3a\u8c03\u672a\u6765\u591a\u6a21\u6001\u6559\u80b2\u7cfb\u7edf\u9700\u8981\u6539\u8fdb\u89c6\u89c9\u7406\u89e3\u3001\u9a8c\u8bc1\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.12715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12715", "abs": "https://arxiv.org/abs/2601.12715", "authors": ["Chengzhou Li", "Ping Guo", "Guanchen Meng", "Qi Jia", "Jinyuan Liu", "Zhu Liu", "Xiaokang Liu", "Yu Liu", "Zhongxuan Luo", "Xin Fan"], "title": "RSOD: Reliability-Guided Sonar Image Object Detection with Extremely Limited Labels", "comment": "Accepted by AAAI 2026,9 pages,10 figures", "summary": "Object detection in sonar images is a key technology in underwater detection systems. Compared to natural images, sonar images contain fewer texture details and are more susceptible to noise, making it difficult for non-experts to distinguish subtle differences between classes. This leads to their inability to provide precise annotation data for sonar images. Therefore, designing effective object detection methods for sonar images with extremely limited labels is particularly important. To address this, we propose a teacher-student framework called RSOD, which aims to fully learn the characteristics of sonar images and develop a pseudo-label strategy suitable for these images to mitigate the impact of limited labels. First, RSOD calculates a reliability score by assessing the consistency of the teacher's predictions across different views. To leverage this score, we introduce an object mixed pseudo-label method to tackle the shortage of labeled data in sonar images. Finally, we optimize the performance of the student by implementing a reliability-guided adaptive constraint. By taking full advantage of unlabeled data, the student can perform well even in situations with extremely limited labels. Notably, on the UATD dataset, our method, using only 5% of labeled data, achieves results that can compete against those of our baseline algorithm trained on 100% labeled data. We also collected a new dataset to provide more valuable data for research in the field of sonar.", "AI": {"tldr": "RSOD\uff1a\u4e00\u79cd\u9488\u5bf9\u58f0\u7eb3\u56fe\u50cf\u8bbe\u8ba1\u7684\u5e08\u751f\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u8bc4\u5206\u3001\u7269\u4f53\u6df7\u5408\u4f2a\u6807\u7b7e\u548c\u81ea\u9002\u5e94\u7ea6\u675f\uff0c\u5728\u6807\u7b7e\u6781\u5ea6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u58f0\u7eb3\u56fe\u50cf\u7eb9\u7406\u7ec6\u8282\u5c11\u3001\u566a\u58f0\u591a\uff0c\u975e\u4e13\u5bb6\u96be\u4ee5\u533a\u5206\u7c7b\u522b\u5dee\u5f02\uff0c\u5bfc\u81f4\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u6807\u6ce8\u6570\u636e\u3002\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u5728\u6807\u7b7e\u6781\u5ea6\u6709\u9650\u60c5\u51b5\u4e0b\u6709\u6548\u7684\u58f0\u7eb3\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRSOD\u5e08\u751f\u6846\u67b6\uff1a1) \u901a\u8fc7\u8bc4\u4f30\u6559\u5e08\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u56fe\u4e0b\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u8ba1\u7b97\u53ef\u9760\u6027\u8bc4\u5206\uff1b2) \u5f15\u5165\u7269\u4f53\u6df7\u5408\u4f2a\u6807\u7b7e\u65b9\u6cd5\u89e3\u51b3\u58f0\u7eb3\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff1b3) \u5b9e\u65bd\u53ef\u9760\u6027\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u7ea6\u675f\u4f18\u5316\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728UATD\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u75285%\u6807\u6ce8\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u57fa\u7ebf\u7b97\u6cd5\u4f7f\u7528100%\u6807\u6ce8\u6570\u636e\u7684\u7ade\u4e89\u6027\u7ed3\u679c\u3002\u540c\u65f6\u6536\u96c6\u4e86\u65b0\u6570\u636e\u96c6\u4e3a\u58f0\u7eb3\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u66f4\u6709\u4ef7\u503c\u7684\u6570\u636e\u3002", "conclusion": "RSOD\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5728\u6807\u7b7e\u6781\u5ea6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u826f\u597d\u7684\u58f0\u7eb3\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u58f0\u7eb3\u56fe\u50cf\u6807\u6ce8\u56f0\u96be\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.13503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13503", "abs": "https://arxiv.org/abs/2601.13503", "authors": ["Kyung Ho Lim", "Byung-Hoon Kim"], "title": "Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives", "comment": null, "summary": "Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.", "AI": {"tldr": "Anonpsy\u662f\u4e00\u4e2a\u7528\u4e8e\u7cbe\u795e\u75c5\u5b66\u53d9\u8ff0\u7684\u53bb\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u5f15\u5bfc\u7684\u8bed\u4e49\u91cd\u5199\u6765\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u8bca\u65ad\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u8bc6\u522b\u65b9\u6cd5\uff08\u5982PHI\u63a9\u7801\u548cLLM\u5408\u6210\u91cd\u5199\uff09\u5728\u6587\u672c\u5c42\u9762\u64cd\u4f5c\uff0c\u5bf9\u4fdd\u7559\u6216\u4fee\u6539\u54ea\u4e9b\u8bed\u4e49\u5143\u7d20\u63a7\u5236\u6709\u9650\u3002\u7cbe\u795e\u75c5\u5b66\u53d9\u8ff0\u4e0d\u4ec5\u5305\u542b\u663e\u5f0f\u6807\u8bc6\u7b26\uff0c\u8fd8\u901a\u8fc7\u5d4c\u5165\u4e34\u5e8a\u7ed3\u6784\u4e2d\u7684\u72ec\u7279\u751f\u6d3b\u4e8b\u4ef6\u7f16\u7801\u60a3\u8005\u8eab\u4efd\u3002", "method": "Anonpsy\u5c06\u53bb\u8bc6\u522b\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56fe\u5f15\u5bfc\u7684\u8bed\u4e49\u91cd\u5199\uff1a1) \u5c06\u53d9\u8ff0\u8f6c\u6362\u4e3a\u7f16\u7801\u4e34\u5e8a\u5b9e\u4f53\u3001\u65f6\u95f4\u951a\u70b9\u548c\u7c7b\u578b\u5173\u7cfb\u7684\u8bed\u4e49\u56fe\uff1b2) \u5e94\u7528\u56fe\u7ea6\u675f\u6270\u52a8\uff0c\u4fee\u6539\u8bc6\u522b\u4e0a\u4e0b\u6587\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u5fc5\u9700\u7ed3\u6784\uff1b3) \u901a\u8fc7\u56fe\u6761\u4ef6LLM\u751f\u6210\u91cd\u65b0\u751f\u6210\u6587\u672c\u3002", "result": "\u572890\u4e2a\u4e34\u5e8a\u533b\u751f\u64b0\u5199\u7684\u7cbe\u795e\u75c5\u5b66\u6848\u4f8b\u53d9\u8ff0\u4e0a\u8bc4\u4f30\uff0cAnonpsy\u5728\u4fdd\u6301\u8bca\u65ad\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u5728\u4e13\u5bb6\u3001\u8bed\u4e49\u548cGPT-5\u8bc4\u4f30\u4e0b\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u4f4e\u518d\u8bc6\u522b\u98ce\u9669\u3002\u4e0e\u5f3a\u5927\u7684\u4ec5LLM\u91cd\u5199\u57fa\u7ebf\u76f8\u6bd4\uff0cAnonpsy\u4ea7\u751f\u663e\u8457\u66f4\u4f4e\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u53ef\u8bc6\u522b\u6027\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u6784\u8868\u793a\u4e0e\u7ea6\u675f\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u4e3a\u7cbe\u795e\u75c5\u5b66\u53d9\u8ff0\u7684\u53bb\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.12719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12719", "abs": "https://arxiv.org/abs/2601.12719", "authors": ["Lin Zhao", "Yushu Wu", "Aleksei Lebedev", "Dishani Lahiri", "Meng Dong", "Arpit Sahni", "Michael Vasilkovsky", "Hao Chen", "Ju Hu", "Aliaksandr Siarohin", "Sergey Tulyakov", "Yanzhi Wang", "Anil Kag", "Yanyu Li"], "title": "S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation", "comment": null, "summary": "Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.", "AI": {"tldr": "S2DiT\u662f\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u9ad8\u6548\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u89c4\u5212\u641c\u7d22\u7684\"\u4e09\u660e\u6cbb\"\u8bbe\u8ba1\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff0c\u5728iPhone\u4e0a\u5b9e\u73b0\u8d85\u8fc710FPS\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u867d\u7136\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6216\u672c\u5730\u751f\u6210\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u79fb\u52a8\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "method": "1. \u63d0\u51faStreaming Sandwich Diffusion Transformer (S2DiT)\u67b6\u6784\uff1b2. \u5f15\u5165\u4e24\u79cd\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff1aLinConv Hybrid Attention (LCHA)\u548cStride Self-Attention (SSA)\uff1b3. \u901a\u8fc7\u9884\u7b97\u611f\u77e5\u52a8\u6001\u89c4\u5212\u641c\u7d22\u53d1\u73b0\"\u4e09\u660e\u6cbb\"\u8bbe\u8ba1\uff1b4. \u63d0\u51fa2\u54081\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u80fd\u529b\u8f6c\u79fb\u5230\u7d27\u51d1\u7684\u5c11\u6b65\u6a21\u578b\u4e2d\u3002", "result": "S2DiT\u5728\u8d28\u91cf\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u670d\u52a1\u5668\u89c6\u9891\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5728iPhone\u4e0a\u5b9e\u73b0\u8d85\u8fc710FPS\u7684\u6d41\u5f0f\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "S2DiT\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u3001\u67b6\u6784\u641c\u7d22\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2601.13537", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13537", "abs": "https://arxiv.org/abs/2601.13537", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Minwoo Lee", "Kyomin Jung"], "title": "When Wording Steers the Evaluation: Framing Bias in LLM judges", "comment": "4 pages", "summary": "Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6846\u67b6\u6548\u5e94\uff0c\u5373\u76f8\u540c\u7684\u8bc4\u4f30\u5185\u5bb9\u56e0\u63d0\u793a\u8bcd\u8868\u8ff0\u65b9\u5f0f\u4e0d\u540c\u800c\u4ea7\u751f\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524dLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u7684\u5fae\u5c0f\u53d8\u5316\u4f1a\u4ea7\u751f\u4e0d\u540c\u54cd\u5e94\uff0c\u4f46\u5728\u8bc4\u4f30\u4efb\u52a1\u4e2d\u9700\u8981\u7a33\u5b9a\u516c\u6b63\u7684\u5224\u65ad\u3002\u5fc3\u7406\u5b66\u4e2d\u7684\u6846\u67b6\u6548\u5e94\u662f\u5426\u4f1a\u5f71\u54cdLLM\u8bc4\u4f30\u7684\u516c\u6b63\u6027\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5fc3\u7406\u5b66\u4e2d\u7684\u6846\u67b6\u6548\u5e94\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e86\u5bf9\u79f0\u7684\u8c13\u8bcd-\u80af\u5b9a\u548c\u8c13\u8bcd-\u5426\u5b9a\u7ed3\u6784\u63d0\u793a\u8bcd\uff0c\u5728\u56db\u4e2a\u9ad8\u98ce\u9669\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7cfb\u7edf\u6027\u5730\u6d4b\u8bd5\u4e8614\u4e2aLLM\u8bc4\u4f30\u8005\u7684\u6846\u67b6\u654f\u611f\u6027\u3002", "result": "\u6240\u6709LLM\u8bc4\u4f30\u8005\u90fd\u8868\u73b0\u51fa\u660e\u663e\u7684\u6846\u67b6\u654f\u611f\u6027\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u540c\u610f\u6216\u62d2\u7edd\u503e\u5411\u65b9\u9762\u663e\u793a\u51fa\u660e\u663e\u5dee\u5f02\uff0c\u6846\u67b6\u504f\u5dee\u662f\u5f53\u524dLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u7279\u5f81\u3002", "conclusion": "\u6846\u67b6\u504f\u5dee\u662fLLM\u8bc4\u4f30\u7cfb\u7edf\u7684\u91cd\u8981\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u6846\u67b6\u611f\u77e5\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u786e\u4fdd\u8bc4\u4f30\u7684\u7a33\u5b9a\u6027\u548c\u516c\u6b63\u6027\u3002"}}
{"id": "2601.12729", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12729", "abs": "https://arxiv.org/abs/2601.12729", "authors": ["Hanyu Zhu", "Zhihao Zhan", "Yuhang Ming", "Liang Li", "Dibo Hou", "Javier Civera", "Wanzeng Kong"], "title": "DC-VLAQ: Query-Residual Aggregation for Robust Visual Place Recognition", "comment": "10 pages, 4 figures, 5 tables", "summary": "One of the central challenges in visual place recognition (VPR) is learning a robust global representation that remains discriminative under large viewpoint changes, illumination variations, and severe domain shifts. While visual foundation models (VFMs) provide strong local features, most existing methods rely on a single model, overlooking the complementary cues offered by different VFMs. However, exploiting such complementary information inevitably alters token distributions, which challenges the stability of existing query-based global aggregation schemes. To address these challenges, we propose DC-VLAQ, a representation-centric framework that integrates the fusion of complementary VFMs and robust global aggregation. Specifically, we first introduce a lightweight residual-guided complementary fusion that anchors representations in the DINOv2 feature space while injecting complementary semantics from CLIP through a learned residual correction. In addition, we propose the Vector of Local Aggregated Queries (VLAQ), a query--residual global aggregation scheme that encodes local tokens by their residual responses to learnable queries, resulting in improved stability and the preservation of fine-grained discriminative cues. Extensive experiments on standard VPR benchmarks, including Pitts30k, Tokyo24/7, MSLS, Nordland, SPED, and AmsterTime, demonstrate that DC-VLAQ consistently outperforms strong baselines and achieves state-of-the-art performance, particularly under challenging domain shifts and long-term appearance changes.", "AI": {"tldr": "DC-VLAQ\uff1a\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7684\u8868\u793a\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u4e92\u8865\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u7a33\u5065\u7684\u5168\u5c40\u805a\u5408\uff0c\u63d0\u5347\u5728\u89c6\u89d2\u53d8\u5316\u3001\u5149\u7167\u53d8\u5316\u548c\u9886\u57df\u504f\u79fb\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u6a21\u578b\u63d0\u4f9b\u7684\u4e92\u8865\u4fe1\u606f\u3002\u7136\u800c\uff0c\u5229\u7528\u8fd9\u4e9b\u4e92\u8865\u4fe1\u606f\u4f1a\u6539\u53d8token\u5206\u5e03\uff0c\u6311\u6218\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u7684\u5168\u5c40\u805a\u5408\u65b9\u6848\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faDC-VLAQ\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u5f15\u5bfc\u4e92\u8865\u878d\u5408\uff0c\u4ee5DINOv2\u7279\u5f81\u7a7a\u95f4\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u6b8b\u5dee\u6821\u6b63\u6ce8\u5165CLIP\u7684\u4e92\u8865\u8bed\u4e49\uff1b2\uff09\u5c40\u90e8\u805a\u5408\u67e5\u8be2\u5411\u91cf\uff0c\u901a\u8fc7\u7f16\u7801\u5c40\u90e8token\u5bf9\u53ef\u5b66\u4e60\u67e5\u8be2\u7684\u6b8b\u5dee\u54cd\u5e94\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u5168\u5c40\u805a\u5408\u3002", "result": "\u5728Pitts30k\u3001Tokyo24/7\u3001MSLS\u3001Nordland\u3001SPED\u3001AmsterTime\u7b49\u6807\u51c6VPR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDC-VLAQ\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6311\u6218\u6027\u9886\u57df\u504f\u79fb\u548c\u957f\u671f\u5916\u89c2\u53d8\u5316\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DC-VLAQ\u901a\u8fc7\u6709\u6548\u878d\u5408\u4e92\u8865\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u7a33\u5065\u7684\u5168\u5c40\u805a\u5408\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e2d\u7684\u8868\u793a\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.13547", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13547", "abs": "https://arxiv.org/abs/2601.13547", "authors": ["Yujia Hu", "Roy Ka-Wei Lee"], "title": "HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations", "comment": "EACL 2026 Main Conference", "summary": "Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \\textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \\textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \\textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.\n  \\textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}", "AI": {"tldr": "HateXScore\u662f\u4e00\u4e2a\u56db\u7ec4\u4ef6\u8bc4\u4f30\u6307\u6807\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u89e3\u91ca\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u5305\u62ec\u7ed3\u8bba\u660e\u786e\u6027\u3001\u5f15\u7528\u5fe0\u5b9e\u6027\u3001\u53d7\u4fdd\u62a4\u7fa4\u4f53\u8bc6\u522b\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f5c\u4e3a\u6807\u51c6\u6307\u6807\u7684\u8865\u5145\u8bca\u65ad\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u8bc4\u4f30\u6846\u67b6\u5f88\u5c11\u8bc4\u4f30\u6587\u672c\u88ab\u5224\u5b9a\u4e3a\u4ec7\u6068\u8a00\u8bba\u7684\u539f\u56e0\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u89e3\u91ca\u63a8\u7406\u8d28\u91cf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u900f\u660e\u548c\u53ef\u4fe1\u7684\u5185\u5bb9\u5ba1\u6838\u5de5\u5177\u3002", "method": "\u63d0\u51faHateXScore\u56db\u7ec4\u4ef6\u6307\u6807\u5957\u4ef6\uff1a1) \u7ed3\u8bba\u660e\u786e\u6027\uff1b2) \u5f15\u7528\u8de8\u5ea6\u7684\u5fe0\u5b9e\u6027\u548c\u56e0\u679c\u57fa\u7840\uff1b3) \u53d7\u4fdd\u62a4\u7fa4\u4f53\u8bc6\u522b\uff08\u53ef\u914d\u7f6e\u7b56\u7565\uff09\uff1b4) \u8fd9\u4e9b\u5143\u7d20\u95f4\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "HateXScore\u80fd\u591f\u63ed\u793a\u6807\u51c6\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u6216F1\u5206\u6570\uff09\u65e0\u6cd5\u53d1\u73b0\u7684\u89e3\u91ca\u6027\u5931\u8d25\u548c\u6807\u6ce8\u4e0d\u4e00\u81f4\u6027\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u4e0eHateXScore\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u53ef\u4fe1\u548c\u900f\u660e\u5ba1\u6838\u5de5\u5177\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "HateXScore\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8bca\u65ad\u8865\u5145\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u89e3\u91ca\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5185\u5bb9\u5ba1\u6838\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.12736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12736", "abs": "https://arxiv.org/abs/2601.12736", "authors": ["Qingtian Zhu", "Xu Cao", "Zhixiang Wang", "Yinqiang Zheng", "Takafumi Taketomi"], "title": "KaoLRM: Repurposing Pre-trained Large Reconstruction Models for Parametric 3D Face Reconstruction", "comment": null, "summary": "We propose KaoLRM to re-target the learned prior of the Large Reconstruction Model (LRM) for parametric 3D face reconstruction from single-view images. Parametric 3D Morphable Models (3DMMs) have been widely used for facial reconstruction due to their compact and interpretable parameterization, yet existing 3DMM regressors often exhibit poor consistency across varying viewpoints. To address this, we harness the pre-trained 3D prior of LRM and incorporate FLAME-based 2D Gaussian Splatting into LRM's rendering pipeline. Specifically, KaoLRM projects LRM's pre-trained triplane features into the FLAME parameter space to recover geometry, and models appearance via 2D Gaussian primitives that are tightly coupled to the FLAME mesh. The rich prior enables the FLAME regressor to be aware of the 3D structure, leading to accurate and robust reconstructions under self-occlusions and diverse viewpoints. Experiments on both controlled and in-the-wild benchmarks demonstrate that KaoLRM achieves superior reconstruction accuracy and cross-view consistency, while existing methods remain sensitive to viewpoint variations. The code is released at https://github.com/CyberAgentAILab/KaoLRM.", "AI": {"tldr": "KaoLRM\u5229\u7528\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u5c06LRM\u7684\u4e09\u5e73\u9762\u7279\u5f81\u6295\u5f71\u5230FLAME\u53c2\u6570\u7a7a\u95f4\uff0c\u5e76\u7ed3\u54082D\u9ad8\u65af\u6e85\u5c04\uff0c\u5b9e\u73b0\u5355\u89c6\u89d2\u56fe\u50cf\u5230\u53c2\u6570\u53163D\u4eba\u8138\u7684\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u53163D\u5f62\u53d8\u6a21\u578b\uff083DMM\uff09\u56de\u5f52\u5668\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u4e00\u81f4\u6027\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5728\u81ea\u906e\u6321\u548c\u591a\u6837\u5316\u89c6\u89d2\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u9884\u8bad\u7ec3\u7684LRM\u6a21\u578b\u76843D\u5148\u9a8c\u77e5\u8bc6\u6765\u6539\u8fdb\u53c2\u6570\u5316\u4eba\u8138\u91cd\u5efa\u3002", "method": "KaoLRM\u5c06\u9884\u8bad\u7ec3\u7684LRM\u4e09\u5e73\u9762\u7279\u5f81\u6295\u5f71\u5230FLAME\u53c2\u6570\u7a7a\u95f4\u6765\u6062\u590d\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u901a\u8fc7\u4e0eFLAME\u7f51\u683c\u7d27\u5bc6\u8026\u5408\u76842D\u9ad8\u65af\u57fa\u5143\u6765\u5efa\u6a21\u5916\u89c2\u3002\u5c06\u57fa\u4e8eFLAME\u76842D\u9ad8\u65af\u6e85\u5c04\u96c6\u6210\u5230LRM\u7684\u6e32\u67d3\u6d41\u7a0b\u4e2d\uff0c\u4f7fFLAME\u56de\u5f52\u5668\u80fd\u591f\u611f\u77e53D\u7ed3\u6784\u3002", "result": "\u5728\u53d7\u63a7\u548c\u91ce\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKaoLRM\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5bf9\u89c6\u89d2\u53d8\u5316\u4ecd\u7136\u654f\u611f\u3002\u8be5\u65b9\u6cd5\u5728\u81ea\u906e\u6321\u548c\u591a\u6837\u5316\u89c6\u89d2\u4e0b\u90fd\u80fd\u4ea7\u751f\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528LRM\u7684\u9884\u8bad\u7ec33D\u5148\u9a8c\u77e5\u8bc6\u5e76\u5c06\u5176\u4e0e\u53c2\u6570\u5316FLAME\u6a21\u578b\u76f8\u7ed3\u5408\uff0cKaoLRM\u663e\u8457\u63d0\u5347\u4e86\u5355\u89c6\u89d2\u4eba\u8138\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u4e3a\u89e3\u51b3\u73b0\u67093DMM\u56de\u5f52\u5668\u7684\u89c6\u89d2\u654f\u611f\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.13575", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13575", "abs": "https://arxiv.org/abs/2601.13575", "authors": ["Thanh-Lam T. Nguyen", "Ngoc-Quang Le", "Quoc-Trung Phu", "Thi-Phuong Le", "Ngoc-Huyen Pham", "Phuong-Nguyen Nguyen", "Hoang-Quynh Le"], "title": "Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews", "comment": null, "summary": "Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.", "AI": {"tldr": "SUDO\u662f\u4e00\u4e2a\u7528\u4e8e\u9690\u5f0f\u6bd4\u8f83\u89c2\u70b9\u6316\u6398\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5305\u542b4,150\u4e2a\u6807\u6ce8\u7684\u8bc4\u8bba\u5bf9\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u7ed3\u6784\u6355\u6349\u65b9\u9762\u7ea7\u63d0\u53ca\u548c\u8bc4\u8bba\u7ea7\u504f\u597d\uff0c\u4e3a\u6ca1\u6709\u663e\u5f0f\u6bd4\u8f83\u7ebf\u7d22\u7684\u7528\u6237\u504f\u597d\u63a8\u65ad\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u6bd4\u8f83\u89c2\u70b9\u6316\u6398\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u663e\u5f0f\u6bd4\u8f83\u8868\u8fbe\uff0c\u4f46\u5728\u771f\u5b9e\u8bc4\u8bba\u4e2d\u4e0d\u5e38\u89c1\uff0c\u800c\u7528\u6237\u5728\u4e0d\u540c\u8bc4\u8bba\u4e2d\u8868\u8fbe\u504f\u597d\u7684\u9690\u5f0f\u6bd4\u8f83\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u53ef\u9760\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u8fd9\u4e00\u4efb\u52a1\u3002", "method": "\u63d0\u51faSUDO\u6570\u636e\u96c6\uff0c\u5305\u542b4,150\u4e2a\u6807\u6ce8\u7684\u8bc4\u8bba\u5bf9\uff0815,191\u4e2a\u53e5\u5b50\uff09\uff0c\u91c7\u7528\u53cc\u5c42\u6b21\u7ed3\u6784\uff1a\u65b9\u9762\u7ea7\u63d0\u53ca\u548c\u8bc4\u8bba\u7ea7\u504f\u597d\u3002\u4f7f\u7528\u4e24\u79cd\u57fa\u7ebf\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1a\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u7136\u4e2d\u7b49\uff0c\u8868\u660e\u8be5\u4efb\u52a1\u5177\u6709\u5185\u5728\u96be\u5ea6\uff0cSUDO\u6210\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "SUDO\u4e3a\u9690\u5f0f\u6bd4\u8f83\u89c2\u70b9\u6316\u6398\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12747", "abs": "https://arxiv.org/abs/2601.12747", "authors": ["Jingkai Li", "Xiaoze Tian", "Yuhang Shen", "Jia Wang", "Dianjie Lu", "Guijuan Zhang", "Zhuoran Zheng"], "title": "SSPFormer: Self-Supervised Pretrained Transformer for MRI Images", "comment": "Undergraduate student as first author submitted to IJCAI", "summary": "The pre-trained transformer demonstrates remarkable generalization ability in natural image processing. However, directly transferring it to magnetic resonance images faces two key challenges: the inability to adapt to the specificity of medical anatomical structures and the limitations brought about by the privacy and scarcity of medical data. To address these issues, this paper proposes a Self-Supervised Pretrained Transformer (SSPFormer) for MRI images, which effectively learns domain-specific feature representations of medical images by leveraging unlabeled raw imaging data. To tackle the domain gap and data scarcity, we introduce inverse frequency projection masking, which prioritizes the reconstruction of high-frequency anatomical regions to enforce structure-aware representation learning. Simultaneously, to enhance robustness against real-world MRI artifacts, we employ frequency-weighted FFT noise enhancement that injects physiologically realistic noise into the Fourier domain. Together, these strategies enable the model to learn domain-invariant and artifact-robust features directly from raw scans. Through extensive experiments on segmentation, super-resolution, and denoising tasks, the proposed SSPFormer achieves state-of-the-art performance, fully verifying its ability to capture fine-grained MRI image fidelity and adapt to clinical application requirements.", "AI": {"tldr": "SSPFormer\uff1a\u4e00\u79cd\u7528\u4e8eMRI\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3Transformer\uff0c\u901a\u8fc7\u9006\u9891\u7387\u6295\u5f71\u63a9\u7801\u548c\u9891\u7387\u52a0\u6743FFT\u566a\u58f0\u589e\u5f3a\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u9886\u57df\u9002\u5e94\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u5206\u5272\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u566a\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3Transformer\u5728\u81ea\u7136\u56fe\u50cf\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8eMRI\u56fe\u50cf\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u65e0\u6cd5\u9002\u5e94\u533b\u5b66\u89e3\u5256\u7ed3\u6784\u7684\u7279\u5f02\u6027\uff1b2\uff09\u533b\u5b66\u6570\u636e\u7684\u9690\u79c1\u6027\u548c\u7a00\u7f3a\u6027\u5e26\u6765\u7684\u9650\u5236\u3002", "method": "\u63d0\u51faSSPFormer\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u9006\u9891\u7387\u6295\u5f71\u63a9\u7801\uff0c\u4f18\u5148\u91cd\u5efa\u9ad8\u9891\u89e3\u5256\u533a\u57df\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u8868\u793a\u5b66\u4e60\uff1b2\uff09\u91c7\u7528\u9891\u7387\u52a0\u6743FFT\u566a\u58f0\u589e\u5f3a\uff0c\u5728\u5085\u91cc\u53f6\u57df\u6ce8\u5165\u751f\u7406\u771f\u5b9e\u566a\u58f0\uff0c\u589e\u5f3a\u5bf9MRI\u4f2a\u5f71\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5206\u5272\u3001\u8d85\u5206\u8fa8\u7387\u548c\u53bb\u566a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSSPFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u5176\u6355\u6349\u7ec6\u7c92\u5ea6MRI\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u9002\u5e94\u4e34\u5e8a\u5e94\u7528\u9700\u6c42\u7684\u80fd\u529b\u3002", "conclusion": "SSPFormer\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u76d1\u7763\u7b56\u7565\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u626b\u63cf\u4e2d\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u548c\u4f2a\u5f71\u9c81\u68d2\u7684\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u9886\u57df\u9002\u5e94\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3aMRI\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2601.13588", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13588", "abs": "https://arxiv.org/abs/2601.13588", "authors": ["Inho Won", "Hangyeol Yoo", "Minkyung Cho", "Jungyeul Park", "Hoyun Song", "KyungTae Lim"], "title": "TREX: Tokenizer Regression for Optimal Data Mixture", "comment": "Accepted to EACL 2026. Long Paper. (19 languages studied: Chinese, Greek, Japanese, etc.)", "summary": "Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.", "AI": {"tldr": "TREX\u6846\u67b6\u901a\u8fc7\u56de\u5f52\u5206\u6790\u9884\u6d4b\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u8bad\u7ec3\u7684\u6700\u4f18\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u907f\u514d\u4f20\u7edf\u542f\u53d1\u5f0f\u6216\u5927\u89c4\u6a21\u641c\u7d22\u7684\u9ad8\u6210\u672c\uff0c\u63d0\u5347\u5206\u8bcd\u5668\u538b\u7f29\u6027\u80fd\u8fbe12%", "motivation": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5206\u8bcd\u5668\u9700\u8981\u7cbe\u5fc3\u63a7\u5236\u8bed\u8a00\u7279\u5b9a\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u5206\u8bcd\u5668\u7684\u538b\u7f29\u6027\u80fd\u4e25\u91cd\u5f71\u54cdLLM\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u6210\u672c\u9ad8\u6602\u7684\u5927\u89c4\u6a21\u641c\u7d22\u6765\u786e\u5b9a\u6700\u4f18\u8bed\u8a00\u6bd4\u4f8b", "method": "TREX\u6846\u67b6\uff1a1\uff09\u5728\u968f\u673a\u6df7\u5408\u6570\u636e\u4e0a\u8bad\u7ec3\u5c0f\u89c4\u6a21\u4ee3\u7406\u5206\u8bcd\u5668\uff1b2\uff09\u6536\u96c6\u538b\u7f29\u7edf\u8ba1\u4fe1\u606f\uff1b3\uff09\u5b66\u4e60\u4ece\u6570\u636e\u6df7\u5408\u5230\u538b\u7f29\u6027\u80fd\u7684\u9884\u6d4b\u6a21\u578b\uff1b4\uff09\u5728\u5927\u89c4\u6a21\u5206\u8bcd\u5668\u8bad\u7ec3\u524d\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u6df7\u5408\u6bd4\u4f8b\u641c\u7d22", "result": "\u4f7f\u7528TREX\u9884\u6d4b\u7684\u6df7\u5408\u6bd4\u4f8b\u8bad\u7ec3\u7684\u5206\u8bcd\u5668\uff0c\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u538b\u7f29\u6548\u7387\u4e0a\u6bd4\u57fa\u4e8eLLaMA3\u548c\u5747\u5300\u5206\u5e03\u7684\u6df7\u5408\u6bd4\u4f8b\u63d0\u5347\u9ad8\u8fbe12%\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027", "conclusion": "TREX\u6846\u67b6\u901a\u8fc7\u56de\u5f52\u5206\u6790\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u8bbe\u8ba1\u4e2d\u51c6\u786e\u6027\u4e0e\u6210\u672c\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12761", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12761", "abs": "https://arxiv.org/abs/2601.12761", "authors": ["Tianqi Zhang", "Ziyi Wang", "Wenzhao Zheng", "Weiliang Chen", "Yuanhui Huang", "Zhengyang Huang", "Jie Zhou", "Jiwen Lu"], "title": "Moaw: Unleashing Motion Awareness for Video Diffusion Models", "comment": null, "summary": "Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.", "AI": {"tldr": "Moaw\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u611f\u77e5\uff0c\u5c06\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u8f6c\u6362\u4e3a\u89c6\u9891\u5230\u5bc6\u96c6\u8ddf\u8e2a\uff0c\u7136\u540e\u901a\u8fc7\u7279\u5f81\u6ce8\u5165\u5b9e\u73b0\u96f6\u6837\u672c\u8fd0\u52a8\u8fc1\u79fb\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u81ea\u7136\u6355\u83b7\u4e86\u5e27\u95f4\u5171\u4eab\u7279\u5f81\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5df2\u6709\u5de5\u4f5c\u5229\u7528\u8fd9\u4e00\u7279\u6027\u8fdb\u884c\u96f6\u6837\u672c\u5149\u6d41\u9884\u6d4b\u548c\u8ddf\u8e2a\u3002\u672c\u6587\u7814\u7a76\u76d1\u7763\u8bad\u7ec3\u662f\u5426\u80fd\u66f4\u5145\u5206\u5730\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u63d0\u51faMoaw\u6846\u67b6\uff1a1) \u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u611f\u77e5\uff0c\u5c06\u6a21\u6001\u4ece\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u8f6c\u6362\u4e3a\u89c6\u9891\u5230\u5bc6\u96c6\u8ddf\u8e2a\uff1b2) \u6784\u5efa\u8fd0\u52a8\u6807\u8bb0\u6570\u636e\u96c6\u8bc6\u522b\u6700\u5f3a\u8fd0\u52a8\u4fe1\u606f\u7684\u7279\u5f81\uff1b3) \u5c06\u8fd9\u4e9b\u7279\u5f81\u6ce8\u5165\u7ed3\u6784\u76f8\u540c\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u7f51\u7edc\u540c\u8d28\u6027\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "result": "Moaw\u6846\u67b6\u5b9e\u73b0\u4e86\u65e0\u9700\u989d\u5916\u9002\u914d\u5668\u7684\u8fd0\u52a8\u8fc1\u79fb\uff0c\u4e3a\u751f\u6210\u5efa\u6a21\u548c\u8fd0\u52a8\u7406\u89e3\u4e4b\u95f4\u7684\u6865\u6881\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u66f4\u7edf\u4e00\u548c\u53ef\u63a7\u7684\u89c6\u9891\u5b66\u4e60\u6846\u67b6\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u76d1\u7763\u8bad\u7ec3\u53ef\u4ee5\u66f4\u5145\u5206\u5730\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8fd0\u52a8\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2601.13590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13590", "abs": "https://arxiv.org/abs/2601.13590", "authors": ["Fan Huang", "Haewoon Kwak", "Jisun An"], "title": "Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions", "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.", "AI": {"tldr": "LLM\u6613\u53d7\u8bf4\u670d\u5f71\u54cd\uff0c\u5c0f\u578b\u6a21\u578b\u5c24\u5176\u987a\u4ece\uff0c\u5143\u8ba4\u77e5\u63d0\u793a\u53cd\u800c\u589e\u52a0\u8106\u5f31\u6027\uff0c\u5bf9\u6297\u5fae\u8c03\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02", "motivation": "\u5c3d\u7ba1LLM\u5e7f\u6cdb\u5e94\u7528\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u8bf4\u670d\u5f71\u54cd\u5e76\u91c7\u7eb3\u53cd\u4e8b\u5b9e\u4fe1\u5ff5\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728SMCR\u901a\u4fe1\u6846\u67b6\u4e0b\u7684\u8bf4\u670d\u8106\u5f31\u6027\uff0c\u4e86\u89e3\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u4fe1\u5ff5\u7a33\u5b9a\u6027\u3002", "method": "\u5728SMCR\u901a\u4fe1\u6846\u67b6\u4e0b\uff0c\u5bf95\u4e2a\u4e3b\u6d41LLM\u548c3\u4e2a\u9886\u57df\uff08\u4e8b\u5b9e\u77e5\u8bc6\u3001\u533b\u5b66QA\u3001\u793e\u4f1a\u504f\u89c1\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5206\u6790\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u5bf9\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4fe1\u5ff5\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002\u7814\u7a76\u5143\u8ba4\u77e5\u63d0\u793a\uff08\u5f15\u53d1\u81ea\u62a5\u544a\u4fe1\u5fc3\uff09\u662f\u5426\u5f71\u54cd\u6297\u8bf4\u670d\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u5bf9\u6297\u5fae\u8c03\u4f5c\u4e3a\u9632\u5fa1\u624b\u6bb5\u7684\u6548\u679c\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u51fa\u6781\u7aef\u987a\u4ece\uff0c80%\u4ee5\u4e0a\u7684\u4fe1\u5ff5\u53d8\u5316\u53d1\u751f\u5728\u7b2c\u4e00\u8f6e\u8bf4\u670d\uff08\u5e73\u5747\u7ed3\u675f\u8f6e\u6b211.1-1.4\uff09\u3002\u5143\u8ba4\u77e5\u63d0\u793a\u53cd\u800c\u589e\u52a0\u8106\u5f31\u6027\uff0c\u52a0\u901f\u4fe1\u5ff5\u4fb5\u8680\u800c\u975e\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u5bf9\u6297\u5fae\u8c03\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\uff1aGPT-4o-mini\u8fbe\u5230\u8fd1\u5b8c\u5168\u9c81\u68d2\u6027\uff0898.6%\uff09\uff0cMistral~7B\u663e\u8457\u6539\u5584\uff0835.7%\u219279.3%\uff09\uff0c\u4f46Llama\u6a21\u578b\u5373\u4f7f\u5728\u81ea\u5df1\u7684\u5931\u8d25\u6848\u4f8b\u4e0a\u5fae\u8c03\u540e\u4ecd\u9ad8\u5ea6\u8106\u5f31\uff08<14%\uff09\u3002", "conclusion": "\u5f53\u524d\u9c81\u68d2\u6027\u5e72\u9884\u63aa\u65bd\u5b58\u5728\u663e\u8457\u7684\u6a21\u578b\u4f9d\u8d56\u6027\u9650\u5236\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u4fe1\u7684LLM\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u8868\u660e\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u6297\u8bf4\u670d\u7b56\u7565\u3002"}}
{"id": "2601.12765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12765", "abs": "https://arxiv.org/abs/2601.12765", "authors": ["Zhi Cai", "Yingjie Gao", "Yanan Zhang", "Xinzhu Ma", "Di Huang"], "title": "Towards Unbiased Source-Free Object Detection via Vision Foundation Models", "comment": null, "summary": "Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.", "AI": {"tldr": "\u63d0\u51faDSOD\u6846\u67b6\u89e3\u51b3\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7VFM\u8f85\u52a9\u7684\u7279\u5f81\u6ce8\u5165\u548c\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u6b63\u5219\u5316\uff0c\u5728\u591a\u4e2a\u8de8\u57df\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u5373\u9002\u5e94\u540e\u7684\u6a21\u578b\u4ecd\u7136\u504f\u5411\u6e90\u57df\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef", "method": "1. \u7edf\u4e00\u7279\u5f81\u6ce8\u5165\u6a21\u5757\uff1a\u901a\u8fc7\u7b80\u5355\u5c3a\u5ea6\u6269\u5c55\u548c\u57df\u611f\u77e5\u81ea\u9002\u5e94\u52a0\u6743\u5c06VFM\u7279\u5f81\u96c6\u6210\u5230CNN\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff1b2. \u8bed\u4e49\u611f\u77e5\u7279\u5f81\u6b63\u5219\u5316\uff1a\u7ea6\u675f\u7279\u5f81\u5b66\u4e60\u9632\u6b62\u8fc7\u62df\u5408\u6e90\u57df\u7279\u5f81\uff1b3. \u63d0\u51faVFM-free\u53d8\u4f53DSOD-distill\uff0c\u91c7\u7528\u53cc\u6559\u5e08\u84b8\u998f\u65b9\u6848", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff1a\u6b63\u5e38\u5230\u96fe\u5929\u9002\u5e94\u8fbe\u523048.1% AP\uff0c\u8de8\u573a\u666f\u9002\u5e94\u8fbe\u523039.3% AP\uff0c\u5408\u6210\u5230\u771f\u5b9e\u9002\u5e94\u8fbe\u523061.4% AP", "conclusion": "DSOD\u6846\u67b6\u901a\u8fc7VFM\u8f85\u52a9\u6709\u6548\u7f13\u89e3\u6e90\u504f\u5dee\u95ee\u9898\uff0c\u5728\u6e90\u81ea\u7531\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8ba1\u7b97\u53d7\u9650\u573a\u666f\u4e0b\u7684\u84b8\u998f\u53d8\u4f53"}}
{"id": "2601.13614", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13614", "abs": "https://arxiv.org/abs/2601.13614", "authors": ["Bo Peng", "Sirui Chen", "Lei Xu", "Chaochao Lu"], "title": "CauScientist: Teaching LLMs to Respect Data for Causal Discovery", "comment": null, "summary": "Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating \"data scientists\" with probabilistic statistics as rigorous \"verifiers\". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.", "AI": {"tldr": "CauScientist\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u5047\u8bbe\u751f\u6210\u548c\u7edf\u8ba1\u9a8c\u8bc1\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u521d\u59cb\u5316\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u9519\u8bef\u8bb0\u5fc6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u56e0\u679c\u56fe\u6784\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53d7\u7edf\u8ba1\u4e0d\u53ef\u533a\u5206\u6027\u548c\u5efa\u6a21\u5047\u8bbe\u5f71\u54cd\uff0c\u800c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u7edf\u8ba1\u8bc1\u636e\uff0c\u8981\u4e48\u5f15\u5165\u672a\u7ecf\u9a8c\u8bc1\u7684\u5148\u9a8c\u77e5\u8bc6\u53ef\u80fd\u8bef\u5bfc\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u534f\u540c\u6846\u67b6\u3002", "method": "\u63d0\u51faCauScientist\u6846\u67b6\uff1a1) \u6df7\u5408\u521d\u59cb\u5316\u9009\u62e9\u4f18\u8d28\u8d77\u59cb\u56fe\uff1b2) \u8fed\u4ee3\u4f18\u5316\uff1aLLM\u4f5c\u4e3a\"\u6570\u636e\u79d1\u5b66\u5bb6\"\u63d0\u51fa\u7ed3\u6784\u4fee\u6539\uff0c\u7edf\u8ba1\u65b9\u6cd5\u4f5c\u4e3a\"\u9a8c\u8bc1\u8005\"\u8fdb\u884c\u4e25\u683c\u9a8c\u8bc1\uff1b3) \u9519\u8bef\u8bb0\u5fc6\u673a\u5236\u6307\u5bfc\u9ad8\u6548\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCauScientist\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\uff1aF1\u5206\u6570\u63d0\u5347\u6700\u9ad8\u8fbe53.8%\uff0c\u53ec\u56de\u7387\u4ece35.0%\u63d0\u5347\u5230100.0%\u3002\u572837\u8282\u70b9\u56fe\u4e0a\uff0c\u76f8\u6bd4Qwen3-32B\u5c06\u7ed3\u6784\u6c49\u660e\u8ddd\u79bb(SHD)\u964d\u4f4e\u4e8644.0%\u3002", "conclusion": "CauScientist\u6210\u529f\u5c06LLM\u7684\u5047\u8bbe\u751f\u6210\u80fd\u529b\u4e0e\u7edf\u8ba1\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.12766", "categories": ["cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12766", "abs": "https://arxiv.org/abs/2601.12766", "authors": ["Lu Yue", "Yue Fan", "Shiwei Lian", "Yu Zhao", "Jiaxin Yu", "Liang Xie", "Feitian Zhang"], "title": "Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration", "comment": null, "summary": "Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/.", "AI": {"tldr": "\u63d0\u51faSpatial-VLN\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u589e\u5f3a\u548c\u591a\u4e13\u5bb6\u63a8\u7406\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u7a7a\u95f4\u611f\u77e5\u74f6\u9888\u95ee\u9898\uff0c\u5728\u590d\u6742\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u867d\u7136\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5e2e\u52a9\u4e0b\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u8fde\u7eed\u73af\u5883\u4e2d\u9762\u4e34\u95e8\u4ea4\u4e92\u3001\u591a\u623f\u95f4\u5bfc\u822a\u548c\u6a21\u7cca\u6307\u4ee4\u6267\u884c\u4e09\u5927\u7a7a\u95f4\u6311\u6218", "method": "\u63d0\u51faSpatial-VLN\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u7a7a\u95f4\u611f\u77e5\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u5168\u666f\u8fc7\u6ee4\u548c\u4e13\u95e8\u7684\u95e8\u4e0e\u533a\u57df\u4e13\u5bb6\u751f\u6210\u7a7a\u95f4\u4e00\u81f4\u3001\u8de8\u89c6\u56fe\u4e00\u81f4\u7684\u611f\u77e5\u8868\u793a\uff1b2) \u63a2\u7d22\u591a\u4e13\u5bb6\u63a8\u7406\u6a21\u5757\uff0c\u4f7f\u7528\u5e76\u884cLLM\u4e13\u5bb6\u5904\u7406\u8def\u5f84\u70b9\u7ea7\u8bed\u4e49\u548c\u533a\u57df\u7ea7\u7a7a\u95f4\u8f6c\u6362\uff0c\u5f53\u4e13\u5bb6\u9884\u6d4b\u4e0d\u4e00\u81f4\u65f6\u6fc0\u6d3b\u67e5\u8be2\u63a2\u7d22\u673a\u5236", "result": "\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u4f7f\u7528\u4f4e\u6210\u672cLLM\uff1b\u901a\u8fc7\u57fa\u4e8e\u4ef7\u503c\u7684\u8def\u5f84\u70b9\u91c7\u6837\u7b56\u7565\u6709\u6548\u7f29\u5c0fSim2Real\u5dee\u8ddd\uff0c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027", "conclusion": "Spatial-VLN\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u5173\u952e\u7a7a\u95f4\u611f\u77e5\u74f6\u9888\uff0c\u901a\u8fc7\u611f\u77e5\u5f15\u5bfc\u7684\u63a2\u7d22\u673a\u5236\u5728\u590d\u6742\u8fde\u7eed\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u771f\u5b9e\u4e16\u754c\u9002\u7528\u6027"}}
{"id": "2601.13630", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13630", "abs": "https://arxiv.org/abs/2601.13630", "authors": ["Zhaopeng Zhang", "Pengcheng Sun", "Lan Zhang", "Chen Tang", "Jiewei Lai", "Yunhao Wang", "Hui Jin"], "title": "Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.", "AI": {"tldr": "\u63d0\u51faAAAC\u6846\u67b6\uff0c\u5229\u7528\u6fc0\u6d3b\u7a7a\u95f4\u51e0\u4f55\u89c4\u5f8b\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u7c7b\u6743\u9650\u63a7\u5236\uff0c\u663e\u8457\u964d\u4f4e\u6743\u9650\u8fdd\u89c4\u7387", "motivation": "LLMs\u5728\u77e5\u8bc6\u5e93\u95ee\u7b54\u4e2d\u53ef\u80fd\u65e0\u610f\u4e2d\u6cc4\u9732\u8d85\u51fa\u7528\u6237\u6743\u9650\u8303\u56f4\u7684\u654f\u611f\u4fe1\u606f\uff0c\u96be\u4ee5\u6ee1\u8db3\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\u9700\u6c42", "method": "\u57fa\u4e8e\u6fc0\u6d3b\u7a7a\u95f4\u51e0\u4f55\u89c4\u5f8b\uff0c\u6784\u5efa\u6743\u9650\u951a\u70b9\u5e93\uff0c\u901a\u8fc7\u591a\u951a\u70b9\u5f15\u5bfc\u673a\u5236\u5c06\u67e5\u8be2\u6fc0\u6d3b\u91cd\u5b9a\u5411\u5230\u6388\u6743\u533a\u57df", "result": "\u5728\u4e09\u4e2aLLM\u5bb6\u65cf\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u6743\u9650\u8fdd\u89c4\u7387\u964d\u4f4e86.5%\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e90.7%\uff0c\u63a8\u7406\u5f00\u9500\u5c0f", "conclusion": "AAAC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u51e0\u4f55\u89c4\u5f8b\u7684\u6743\u9650\u63a7\u5236\u6846\u67b6\uff0c\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027"}}
{"id": "2601.12768", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.12768", "abs": "https://arxiv.org/abs/2601.12768", "authors": ["Zequn Xie", "Boyun Zhang", "Yuxiao Lin", "Tao Jin"], "title": "Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval", "comment": null, "summary": "Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video's inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at https://github.com/boyun-zhang/HVP-Net.", "AI": {"tldr": "HVP-Net\u901a\u8fc7\u63d0\u53d6\u548c\u7cbe\u70bc\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u95f4\u5c42\u7684\u7279\u5f81\uff0c\u6316\u6398\u66f4\u4e30\u5bcc\u7684\u89c6\u9891\u8bed\u4e49\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u89c6\u9891\u5197\u4f59\u548c\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7279\u5f81\u5bfc\u81f4\u7684\u5339\u914d\u7cbe\u5ea6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u89c6\u9891\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u89c6\u9891\u56fa\u6709\u7684\u5197\u4f59\u6027\uff1b2\uff09\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u6700\u7ec8\u5c42\u7279\u5f81\uff0c\u8fd9\u9650\u5236\u4e86\u5339\u914d\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faHVP-Net\uff08\u5206\u5c42\u89c6\u89c9\u611f\u77e5\u7f51\u7edc\uff09\uff0c\u4ece\u89c6\u89c9\u7f16\u7801\u5668\u7684\u591a\u4e2a\u4e2d\u95f4\u5c42\u63d0\u53d6\u548c\u7cbe\u70bc\u7279\u5f81\uff0c\u5728\u4e0d\u540c\u8bed\u4e49\u5c42\u6b21\u4e0a\u4ece\u539f\u59cb\u8865\u4e01\u6807\u8bb0\u4e2d\u9010\u6b65\u84b8\u998f\u51fa\u663e\u8457\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u51cf\u5c11\u5197\u4f59\u540c\u65f6\u4fdd\u7559\u5bf9\u9f50\u7684\u5173\u952e\u7ec6\u8282\u3002", "result": "\u5728MSRVTT\u3001DiDeMo\u548cActivityNet\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5229\u7528\u5206\u5c42\u7279\u5f81\u63a8\u8fdb\u89c6\u9891\u6587\u672c\u68c0\u7d22\u7684\u6709\u6548\u6027\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.13644", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13644", "abs": "https://arxiv.org/abs/2601.13644", "authors": ["Yang Cao", "Bicheng Yu", "Sikun Yang", "Ming Liu", "Yujiu Yang"], "title": "Towards Token-Level Text Anomaly Detection", "comment": "WWW 2026", "summary": "Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.", "AI": {"tldr": "\u63d0\u51fatoken\u7ea7\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u6587\u672c\u5185\u90e8\u5f02\u5e38\u5b9a\u4f4d\uff0c\u6784\u5efa\u7edf\u4e00\u68c0\u6d4b\u6846\u67b6\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e6\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5c40\u9650\u4e8e\u6587\u6863\u7ea7\u522b\uff0c\u65e0\u6cd5\u5b9a\u4f4d\u6587\u672c\u5185\u90e8\u5177\u4f53\u5f02\u5e38\u90e8\u5206\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u68c0\u6d4b\u80fd\u529b", "method": "\u63d0\u51fatoken\u7ea7\u5f02\u5e38\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u5b9a\u4e49\u6587\u6863\u7ea7\u548ctoken\u7ea7\u5f02\u5e38\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u5c42\u6b21\u68c0\u6d4b\u6846\u67b6", "result": "\u5728\u5783\u573e\u90ae\u4ef6\u3001\u8bc4\u8bba\u548c\u8bed\u6cd5\u9519\u8bef\u4e09\u4e2a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u6027\u80fd\u4f18\u4e8e6\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7cbe\u786e\u5f02\u5e38\u5b9a\u4f4d", "conclusion": "token\u7ea7\u5f02\u5e38\u68c0\u6d4b\u4e3a\u6587\u672c\u7cbe\u786e\u5f02\u5e38\u5b9a\u4f4d\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90"}}
{"id": "2601.12770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12770", "abs": "https://arxiv.org/abs/2601.12770", "authors": ["Shuling Zhao", "Dan Xu"], "title": "Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image", "comment": "Project page: https://shaelynz.github.io/fhavatar/", "summary": "Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u6784\u5efa3D\u53ef\u52a8\u753b\u5934\u90e8\u5316\u8eab\u7684\u5355\u6b21\u524d\u9988\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u548c360\u5ea6\u6e32\u67d3", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89d2\u5ea6\u76f8\u673a\u59ff\u6001\u53d8\u5316\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u5f71\u54cd3D\u5316\u8eab\u7684\u771f\u5b9e\u611f\uff0c\u9700\u8981\u89e3\u51b3\u5355\u56fe\u50cf3D\u5168\u5934\u53ef\u52a8\u753b\u5316\u8eab\u7684\u91cd\u5efa\u95ee\u9898", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u4eba\u8138\u6a21\u578bUV\u7a7a\u95f4\u4e0a\u7684\u9ad8\u65af\u57fa\u5143\u5efa\u6a213D\u5934\u90e8\uff1b\u5229\u7528\u9884\u8bad\u7ec33D GAN\u63d0\u53d6\u5168\u5c40\u5168\u5934\u7279\u5f81\u548c\u591a\u89c6\u89d2\u76d1\u7763\uff1b\u7ed3\u5408UV\u7a7a\u95f4\u5bf9\u79f0\u6027\u548c\u4eba\u8138\u5bf9\u79f0\u6027\u878d\u5408\u5c40\u90e8\u7cbe\u7ec6\u7279\u5f81\u4e0e\u5168\u5c40\u7eb9\u7406", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf3D\u5168\u5934\u5efa\u6a21\u548c\u5b9e\u65f6\u52a8\u753b\uff0c\u63d0\u5347\u4e863D\u8bf4\u8bdd\u5316\u8eab\u7684\u771f\u5b9e\u611f", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u56fe\u50cf3D\u53ef\u52a8\u753b\u5934\u90e8\u5316\u8eab\u91cd\u5efa\u95ee\u9898\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u753b\u548c360\u5ea6\u6e32\u67d3\uff0c\u63d0\u9ad8\u4e863D\u5316\u8eab\u7684\u771f\u5b9e\u611f"}}
{"id": "2601.13649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13649", "abs": "https://arxiv.org/abs/2601.13649", "authors": ["Xiaolin Zhou", "Zheng Luo", "Yicheng Gao", "Qixuan Chen", "Xiyang Hu", "Yue Zhao", "Ruishan Liu"], "title": "Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u8bed\u8a00\u504f\u89c1\u65b9\u9762\u5b58\u5728\u4e24\u79cd\u7c7b\u578b\uff1a\u540c\u8bed\u8a00\u8bc4\u5224\u65f6\u6b27\u6d32\u8bed\u8a00\u8868\u73b0\u4f18\u4e8e\u975e\u6d32\u8bed\u8a00\uff0c\u8de8\u8bed\u8a00\u8bc4\u5224\u65f6\u6a21\u578b\u504f\u597d\u82f1\u8bed\u7b54\u6848\uff0c\u4e14\u8fd9\u79cd\u504f\u89c1\u4e0d\u80fd\u5b8c\u5168\u7528\u56f0\u60d1\u5ea6\u504f\u5dee\u89e3\u91ca\u3002", "motivation": "\u867d\u7136LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5148\u524d\u7814\u7a76\u8868\u660e\u5176\u5b58\u5728\u5404\u79cd\u504f\u89c1\uff0c\u5176\u4e2d\u8bed\u8a00\u504f\u89c1\u4f1a\u5bfc\u81f4\u8bc4\u5224\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u8bed\u8a00\u504f\u89c1\u65b9\u9762\u7684\u5177\u4f53\u8868\u73b0\u548c\u673a\u5236\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u8bed\u8a00\u504f\u89c1\uff1a(1)\u540c\u8bed\u8a00\u8bc4\u5224\u65f6\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bed\u8a00\u5bb6\u65cf\u7684\u8868\u73b0\uff1b(2)\u8de8\u8bed\u8a00\u8bc4\u5224\u65f6\u7684\u504f\u89c1\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u4e0d\u540c\u8bed\u8a00\u7b54\u6848\u7684\u504f\u597d\u3002\u540c\u65f6\u63a2\u7a76\u8bed\u8a00\u504f\u89c1\u4e0e\u56f0\u60d1\u5ea6\u504f\u5dee\u7684\u5173\u7cfb\u3002", "result": "\u540c\u8bed\u8a00\u8bc4\u5224\u4e2d\uff0c\u6b27\u6d32\u8bed\u8a00\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u975e\u6d32\u8bed\u8a00\uff0c\u6587\u5316\u76f8\u5173\u4e3b\u9898\u4e2d\u504f\u89c1\u66f4\u660e\u663e\uff1b\u8de8\u8bed\u8a00\u8bc4\u5224\u4e2d\uff0c\u5927\u591a\u6570\u6a21\u578b\u504f\u597d\u82f1\u8bed\u7b54\u6848\uff0c\u4e14\u7b54\u6848\u8bed\u8a00\u6bd4\u95ee\u9898\u8bed\u8a00\u5f71\u54cd\u66f4\u5927\uff1b\u8bed\u8a00\u504f\u89c1\u4e0e\u56f0\u60d1\u5ea6\u4ec5\u6709\u8f7b\u5fae\u76f8\u5173\u6027\uff0c\u4e0d\u80fd\u5b8c\u5168\u7531\u56f0\u60d1\u5ea6\u89e3\u91ca\u3002", "conclusion": "LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5b58\u5728\u663e\u8457\u7684\u8bed\u8a00\u504f\u89c1\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4e0d\u540c\u8bc4\u5224\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u540c\uff0c\u4e14\u4e0d\u80fd\u7b80\u5355\u5f52\u56e0\u4e8e\u56f0\u60d1\u5ea6\u504f\u5dee\u3002\u7814\u7a76\u7ed3\u679c\u5bf9\u5f00\u53d1\u66f4\u516c\u5e73\u7684LLM\u8bc4\u5224\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.12779", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12779", "abs": "https://arxiv.org/abs/2601.12779", "authors": ["Nafis Sadeq", "Qingfeng Liu", "Mostafa El-Khamy"], "title": "Open Vocabulary Panoptic Segmentation With Retrieval Augmentation", "comment": null, "summary": "Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.", "AI": {"tldr": "RetCLIP\uff1a\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u5168\u666f\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u6570\u636e\u5e93\uff0c\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u4f3c\u7279\u5f81\u548c\u7c7b\u522b\u6807\u7b7e\uff0c\u7ed3\u5408CLIP\u5206\u6570\u63d0\u5347\u672a\u89c1\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5168\u666f\u5206\u5272\u7cfb\u7edf\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\u3002\u5f00\u653e\u8bcd\u6c47\u5168\u666f\u5206\u5272\u9700\u8981\u80fd\u591f\u6839\u636e\u7528\u6237\u8f93\u5165\u5206\u5272\u4efb\u610f\u7c7b\u522b\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\u6784\u5efa\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u6570\u636e\u5e93\u3002\u63a8\u7406\u65f6\uff0c\u5c06\u8f93\u5165\u56fe\u50cf\u7684\u63a9\u7801\u7247\u6bb5\u7279\u5f81\u4f5c\u4e3a\u67e5\u8be2\u952e\uff0c\u4ece\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u76f8\u4f3c\u7279\u5f81\u548c\u76f8\u5173\u7c7b\u522b\u6807\u7b7e\u3002\u57fa\u4e8e\u67e5\u8be2\u7279\u5f81\u4e0e\u68c0\u7d22\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u5206\u914d\u5206\u7c7b\u5206\u6570\uff0c\u5e76\u5c06\u68c0\u7d22\u5206\u7c7b\u5206\u6570\u4e0eCLIP\u5206\u6570\u7ed3\u5408\u5f97\u5230\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728COCO\u4e0a\u8bad\u7ec3\uff0c\u5728ADE20k\u6570\u636e\u96c6\u4e0a\u8fbe\u523030.9 PQ\u300119.3 mAP\u300144.0 mIoU\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u63d0\u5347+4.5 PQ\u3001+2.5 mAP\u3001+10.0 mIoU\u3002", "conclusion": "RetCLIP\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u5168\u666f\u5206\u5272\u4e2d\u672a\u89c1\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u4efb\u610f\u7c7b\u522b\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13658", "abs": "https://arxiv.org/abs/2601.13658", "authors": ["Arthur Amalvy", "Hen-Hsen Huang"], "title": "Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation", "comment": "12 pages", "summary": "The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u9884\u6d4b\u672a\u6765\u4e8b\u5b9e\u7684\u5408\u6210\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6\u4efb\u52a1\u4e2d\u6570\u636e\u6c61\u67d3\u548c\u8bc4\u4f30\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6\uff08TKGE\uff09\u5bf9\u4e8e\u6784\u5efa\u5927\u578b\u7f51\u7edc\u77e5\u8bc6\u5e93\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u4e14\u5b58\u5728\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff08\u8bad\u7ec3\u96c6\u548c\u8bc4\u4f30\u96c6\u91cd\u53e0\uff09\uff0c\u5bfc\u81f4LLM\u6027\u80fd\u88ab\u9ad8\u4f30\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff1a1\uff09\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\uff08TKGF\uff09\u751f\u6210\u5408\u7406\u7684\u672a\u6765\u56db\u5143\u7ec4\uff0c\u5e76\u8fc7\u6ee4\u4ee5\u7b26\u5408\u539f\u59cb\u77e5\u8bc6\u5e93\u6a21\u5f0f\uff1b2\uff09\u4f7f\u7528LLM\u8fdb\u884c\u56db\u5143\u7ec4\u5230\u6587\u672c\u7684\u751f\u6210\uff0c\u521b\u5efa\u8bed\u4e49\u5bf9\u9f50\u7684\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b4.2K\u4e2a\u672a\u6765\u56db\u5143\u7ec4\u53ca\u5bf9\u5e94\u6587\u672c\u63cf\u8ff0\u7684\u6570\u636e\u96c6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aLLM\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u76f8\u6bd4\u5df2\u77e5\u4e8b\u5b9e\u6570\u636e\u96c6\u6709\u6240\u4e0b\u964d\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u6c61\u67d3\u95ee\u9898\u7684\u5b58\u5728\u3002", "conclusion": "\u63d0\u51fa\u7684\u5408\u6210\u6570\u636e\u96c6\u89e3\u51b3\u4e86TKGE\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u957f\u671f\u3001\u65e0\u6c61\u67d3\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u751f\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u6301\u7eed\u521b\u5efa\u65e0\u9650\u672a\u6765\u65f6\u5e8f\u6570\u636e\u96c6\u3002"}}
{"id": "2601.12791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12791", "abs": "https://arxiv.org/abs/2601.12791", "authors": ["Zhihan Zeng", "Yang Zhao", "Kaihe Wang", "Dusit Niyato", "Hongyuan Shu", "Junchu Zhao", "Yanjun Huang", "Yue Xiu", "Zhongpei Zhang", "Ning Wei"], "title": "SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification", "comment": null, "summary": "As the electromagnetic environment becomes increasingly complex, Global Navigation Satellite Systems (GNSS) face growing threats from sophisticated jamming interference. Although Deep Learning (DL) effectively identifies basic interference, classifying compound interference remains difficult due to the superposition of diverse jamming sources. Existing single-domain approaches often suffer from performance degradation because transient burst signals and continuous global signals require conflicting feature extraction scales. We propose the Selective Kernel and Asymmetric convolution Network(SKANet), a cognitive deep learning framework built upon a dual-stream architecture that integrates Time-Frequency Images (TFIs) and Power Spectral Density (PSD). Distinct from conventional fusion methods that rely on static receptive fields, the proposed architecture incorporates a Multi-Branch Selective Kernel (SK) module combined with Asymmetric Convolution Blocks (ACBs). This mechanism enables the network to dynamically adjust its receptive fields, acting as an adaptive filter that simultaneously captures micro-scale transient features and macro-scale spectral trends within entangled compound signals. To complement this spatial-temporal adaptation, a Squeeze-and-Excitation (SE) mechanism is integrated at the fusion stage to adaptively recalibrate the contribution of heterogeneous features from each modality. Evaluations on a dataset of 405,000 samples demonstrate that SKANet achieves an overall accuracy of 96.99\\%, exhibiting superior robustness for compound jamming classification, particularly under low Jamming-to-Noise Ratio (JNR) regimes.", "AI": {"tldr": "SKANet\uff1a\u4e00\u79cd\u57fa\u4e8e\u53cc\u6d41\u67b6\u6784\u7684\u8ba4\u77e5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6838\u4e0e\u4e0d\u5bf9\u79f0\u5377\u79ef\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\uff0c\u6709\u6548\u8bc6\u522bGNSS\u590d\u5408\u5e72\u6270\u4fe1\u53f7\uff0c\u5728\u4f4eJNR\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u7535\u78c1\u73af\u5883\u65e5\u76ca\u590d\u6742\uff0cGNSS\u9762\u4e34\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u5e72\u6270\u5a01\u80c1\u3002\u6df1\u5ea6\u5b66\u4e60\u80fd\u6709\u6548\u8bc6\u522b\u57fa\u672c\u5e72\u6270\uff0c\u4f46\u590d\u5408\u5e72\u6270\u5206\u7c7b\u56f0\u96be\uff0c\u56e0\u4e3a\u4e0d\u540c\u5e72\u6270\u4fe1\u53f7\u7684\u7279\u5f81\u5c3a\u5ea6\u5b58\u5728\u51b2\u7a81\uff08\u77ac\u6001\u7a81\u53d1\u4fe1\u53f7\u9700\u8981\u5fae\u5c3a\u5ea6\u7279\u5f81\uff0c\u8fde\u7eed\u5168\u5c40\u4fe1\u53f7\u9700\u8981\u5b8f\u5c3a\u5ea6\u7279\u5f81\uff09\u3002\u73b0\u6709\u5355\u57df\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faSKANet\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u6574\u5408\u65f6\u9891\u56fe\u50cf\u548c\u529f\u7387\u8c31\u5bc6\u5ea6\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a\u591a\u5206\u652f\u9009\u62e9\u6027\u6838\u6a21\u5757\u7ed3\u5408\u4e0d\u5bf9\u79f0\u5377\u79ef\u5757\uff0c\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\u4ee5\u540c\u65f6\u6355\u83b7\u5fae\u5c3a\u5ea6\u77ac\u6001\u7279\u5f81\u548c\u5b8f\u5c3a\u5ea6\u9891\u8c31\u8d8b\u52bf\uff1b\u878d\u5408\u9636\u6bb5\u96c6\u6210\u538b\u7f29-\u6fc0\u52b1\u673a\u5236\uff0c\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u5404\u6a21\u6001\u5f02\u6784\u7279\u5f81\u7684\u8d21\u732e\u3002", "result": "\u5728405,000\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cSKANet\u8fbe\u523096.99%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5728\u590d\u5408\u5e72\u6270\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4eJNR\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SKANet\u901a\u8fc7\u52a8\u6001\u611f\u53d7\u91ce\u8c03\u6574\u548c\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNSS\u590d\u5408\u5e72\u6270\u5206\u7c7b\u4e2d\u7684\u5c3a\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684\u5e72\u6270\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13659", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.13659", "abs": "https://arxiv.org/abs/2601.13659", "authors": ["Chunlei Meng", "Ziyang Zhou", "Lucas He", "Xiaojing Du", "Chun Ouyang", "Zhongxue Gan"], "title": "Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis", "comment": "This study has been accepted by IEEE ICASSP2026", "summary": "Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.", "AI": {"tldr": "TSDA\u63d0\u51fa\u65f6\u7a7a\u89e3\u8026\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5404\u6a21\u6001\u663e\u5f0f\u5206\u89e3\u4e3a\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u7ed3\u6784\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u65f6\u7a7a\u5bf9\u9f50\uff0c\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u6a21\u6001\u4e0d\u53d8/\u7279\u5b9a\u5206\u89e3\u6216\u590d\u6742\u878d\u5408\uff0c\u4f46\u4ecd\u4f9d\u8d56\u65f6\u7a7a\u6df7\u5408\u5efa\u6a21\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u65f6\u7a7a\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u6027\u80fd\u53d7\u9650\u3002", "method": "TSDA\uff08Temporal-Spatial Decouple before Act\uff09\uff1a1\uff09\u5c06\u6bcf\u4e2a\u6a21\u6001\u663e\u5f0f\u89e3\u8026\u4e3a\u65f6\u95f4\u52a8\u6001\u548c\u7a7a\u95f4\u7ed3\u6784\u4e0a\u4e0b\u6587\uff1b2\uff09\u65f6\u95f4\u7f16\u7801\u5668\u548c\u7a7a\u95f4\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\uff1b3\uff09\u56e0\u5b50\u4e00\u81f4\u8de8\u6a21\u6001\u5bf9\u9f50\uff1a\u65f6\u95f4\u7279\u5f81\u4ec5\u4e0e\u8de8\u6a21\u6001\u65f6\u95f4\u7279\u5f81\u5bf9\u9f50\uff0c\u7a7a\u95f4\u7279\u5f81\u4ec5\u4e0e\u8de8\u6a21\u6001\u7a7a\u95f4\u7279\u5f81\u5bf9\u9f50\uff1b4\uff09\u56e0\u5b50\u7279\u5b9a\u76d1\u7763\u548c\u53bb\u76f8\u5173\u6b63\u5219\u5316\u51cf\u5c11\u4ea4\u53c9\u56e0\u5b50\u6cc4\u6f0f\uff1b5\uff09\u95e8\u63a7\u91cd\u8026\u5408\u6a21\u5757\u5c06\u5bf9\u9f50\u540e\u7684\u6d41\u91cd\u65b0\u8026\u5408\u7528\u4e8e\u4efb\u52a1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eTSDA\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u5206\u6790\u7814\u7a76\u8bc1\u5b9e\u4e86\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u65f6\u7a7a\u89e3\u8026\u548c\u5bf9\u9f50\uff0cTSDA\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.12795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12795", "abs": "https://arxiv.org/abs/2601.12795", "authors": ["Zeren Sun", "Yazhou Yao", "Tongliang Liu", "Zechao Li", "Fumin Shen", "Jinhui Tang"], "title": "Combating Noisy Labels through Fostering Self- and Neighbor-Consistency", "comment": "accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence", "summary": "Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\\textbf{Jo}int sample selection and model regularization based on \\textbf{S}elf- and \\textbf{N}eighbor-\\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the ``likelihood'' of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faJo-SNC\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u6b63\u5219\u5316\u5904\u7406\u6807\u7b7e\u566a\u58f0\uff0c\u4f7f\u7528Jensen-Shannon\u6563\u5ea6\u6d4b\u91cf\u6837\u672c\u6e05\u6d01\u5ea6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u9608\u503c\u548c\u4e09\u79cd\u4e00\u81f4\u6027\u6b63\u5219\u5316\u63d0\u5347\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6807\u7b7e\u566a\u58f0\u666e\u904d\u5b58\u5728\uff0c\u6df1\u5ea6\u7f51\u7edc\u5bb9\u6613\u8bb0\u5fc6\u566a\u58f0\u6837\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc6\u522b\u5e72\u51c0\u6570\u636e\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0d\u540c\u5c0f\u6279\u91cf\u4e2d\u6807\u7b7e\u566a\u58f0\u7684\u4e0d\u5e73\u8861\u6027\uff0c\u4e14\u5bf9\u5206\u5e03\u5916\u566a\u58f0\u6570\u636e\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51faJo-SNC\u65b9\u6cd5\uff1a1) \u4f7f\u7528Jensen-Shannon\u6563\u5ea6\u7ed3\u5408\u6837\u672c\u6700\u8fd1\u90bb\u6d4b\u91cf\u6e05\u6d01\u5ea6\uff1b2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u6570\u636e\u9a71\u52a8\u9608\u503c\u65b9\u6848\u8c03\u6574\u6bcf\u7c7b\u9009\u62e9\u9608\u503c\uff1b3) \u5e72\u51c0\u6837\u672c\u5e38\u89c4\u8bad\u7ec3\uff0c\u5206\u5e03\u5185\u566a\u58f0\u6837\u672c\u91c7\u7528\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff0c\u5206\u5e03\u5916\u566a\u58f0\u6837\u672c\u91c7\u7528\u8d1f\u5b66\u4e60\uff1b4) \u63d0\u51fa\u4e09\u5143\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4fc3\u8fdb\u81ea\u9884\u6d4b\u4e00\u81f4\u6027\u3001\u90bb\u9884\u6d4b\u4e00\u81f4\u6027\u548c\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "Jo-SNC\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u6837\u672c\u9009\u62e9\u548c\u6a21\u578b\u6b63\u5219\u5316\uff0c\u6709\u6548\u5904\u7406\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u566a\u58f0\u4e0d\u5e73\u8861\u548c\u5206\u5e03\u5916\u566a\u58f0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u566a\u58f0\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13669", "abs": "https://arxiv.org/abs/2601.13669", "authors": ["Jiayu Lin", "Zhongyu Wei"], "title": "CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks", "comment": null, "summary": "Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a \"middle ground\". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u793e\u533a\u7ea7\u5bf9\u9f50\u4f5c\u4e3a\u4e2a\u4f53\u7ea7\u548c\u901a\u7528\u7ea7\u5bf9\u9f50\u4e4b\u95f4\u7684\u6298\u4e2d\u65b9\u6848\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u793e\u533a\u7ea7\u5bf9\u9f50\u8bc4\u4f30\u57fa\u51c6CommunityBench\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u5728\u5efa\u6a21\u793e\u533a\u7279\u5b9a\u504f\u597d\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u7b56\u7565\u5b58\u5728\u4e24\u4e2a\u6781\u7aef\uff1a\u901a\u7528\u4ef7\u503c\u96c6\uff08one-size-fits-all\uff09\u4f1a\u8fb9\u7f18\u5316\u5c11\u6570\u7fa4\u4f53\u89c4\u8303\uff0c\u800c\u4e2a\u4f53\u7ea7\u5b9a\u5236\u5219\u6210\u672c\u8fc7\u9ad8\u3002\u4eba\u7c7b\u793e\u4f1a\u4e2d\u5b58\u5728\u793e\u4f1a\u96c6\u7fa4\uff0c\u7fa4\u5185\u4ef7\u503c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u56e0\u6b64\u9700\u8981\u793e\u533a\u7ea7\u5bf9\u9f50\u4f5c\u4e3a\u6298\u4e2d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u793e\u533a\u7ea7\u5bf9\u9f50\u6982\u5ff5\uff0c\u5e76\u521b\u5efaCommunityBench\u57fa\u51c6\uff0c\u5305\u542b\u57fa\u4e8e\u5171\u540c\u8eab\u4efd\u548c\u5171\u540c\u7ebd\u5e26\u7406\u8bba\u7684\u56db\u4e2a\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5bf9\u793e\u533a\u7279\u5b9a\u504f\u597d\u7684\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5bf9\u591a\u79cd\u57fa\u7840\u6a21\u578b\u5728CommunityBench\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524dLLMs\u5728\u5efa\u6a21\u793e\u533a\u7279\u5b9a\u504f\u597d\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002\u540c\u65f6\u53d1\u73b0\u793e\u533a\u7ea7\u5bf9\u9f50\u6709\u52a9\u4e8e\u4fc3\u8fdb\u4e2a\u4f53\u5efa\u6a21\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u591a\u5143\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u793e\u533a\u7ea7\u5bf9\u9f50\u662fLLM\u5bf9\u9f50\u7684\u6709\u6548\u6298\u4e2d\u65b9\u6848\uff0cCommunityBench\u4e3a\u8bc4\u4f30\u793e\u533a\u7ea7\u5bf9\u9f50\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u793e\u533a\u7ea7\u5bf9\u9f50\u4e3a\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u591a\u5143\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2601.12798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12798", "abs": "https://arxiv.org/abs/2601.12798", "authors": ["Zhihan Zeng", "Yang Zhao", "Kaihe Wang", "Dusit Niyato", "Yue Xiu", "Lu Chen", "Zhongpei Zhang", "Ning Wei"], "title": "PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition", "comment": null, "summary": "Complex electromagnetic interference increasingly compromises Global Navigation Satellite Systems (GNSS), threatening the reliability of Space-Air-Ground Integrated Networks (SAGIN). Although deep learning has advanced interference recognition, current static models suffer from a \\textbf{fundamental limitation}: they impose a fixed computational topology regardless of the input's physical entropy. This rigidity leads to severe resource mismatch, where simple primitives consume the same processing cost as chaotic, saturated mixtures. To resolve this, this paper introduces PhyG-MoE (Physics-Guided Mixture-of-Experts), a framework designed to \\textbf{dynamically align model capacity with signal complexity}. Unlike static architectures, the proposed system employs a spectrum-based gating mechanism that routes signals based on their spectral feature entanglement. A high-capacity TransNeXt expert is activated on-demand to disentangle complex features in saturated scenarios, while lightweight experts handle fundamental signals to minimize latency. Evaluations on 21 jamming categories demonstrate that PhyG-MoE achieves an overall accuracy of 97.58\\%. By resolving the intrinsic conflict between static computing and dynamic electromagnetic environments, the proposed framework significantly reduces computational overhead without performance degradation, offering a viable solution for resource-constrained cognitive receivers.", "AI": {"tldr": "PhyG-MoE\uff1a\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bb9\u91cf\u5339\u914d\u4fe1\u53f7\u590d\u6742\u5ea6\uff0c\u89e3\u51b3GNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u9759\u6001\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u572821\u79cd\u5e72\u6270\u7c7b\u522b\u4e0a\u8fbe\u523097.58%\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dGNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u9759\u6001\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff1a\u65e0\u8bba\u8f93\u5165\u4fe1\u53f7\u7684\u7269\u7406\u71b5\u5982\u4f55\uff0c\u90fd\u91c7\u7528\u56fa\u5b9a\u7684\u8ba1\u7b97\u62d3\u6251\u7ed3\u6784\u3002\u8fd9\u79cd\u521a\u6027\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u7b80\u5355\u4fe1\u53f7\u548c\u590d\u6742\u9971\u548c\u6df7\u5408\u4fe1\u53f7\u6d88\u8017\u76f8\u540c\u7684\u5904\u7406\u6210\u672c\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7535\u78c1\u73af\u5883\u3002", "method": "\u63d0\u51faPhyG-MoE\uff08\u7269\u7406\u5f15\u5bfc\u7684\u4e13\u5bb6\u6df7\u5408\uff09\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u9891\u8c31\u7684\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u4fe1\u53f7\u7684\u9891\u8c31\u7279\u5f81\u7ea0\u7f20\u7a0b\u5ea6\u8fdb\u884c\u8def\u7531\u3002\u7cfb\u7edf\u5305\u542b\uff1a1\uff09\u9ad8\u5bb9\u91cfTransNeXt\u4e13\u5bb6\uff0c\u6309\u9700\u6fc0\u6d3b\u4ee5\u89e3\u8026\u9971\u548c\u573a\u666f\u4e2d\u7684\u590d\u6742\u7279\u5f81\uff1b2\uff09\u8f7b\u91cf\u7ea7\u4e13\u5bb6\uff0c\u5904\u7406\u57fa\u672c\u4fe1\u53f7\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "result": "\u572821\u79cd\u5e72\u6270\u7c7b\u522b\u8bc4\u4f30\u4e2d\uff0cPhyG-MoE\u8fbe\u523097.58%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u9759\u6001\u8ba1\u7b97\u4e0e\u52a8\u6001\u7535\u78c1\u73af\u5883\u4e4b\u95f4\u7684\u5185\u5728\u51b2\u7a81\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u800c\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8ba4\u77e5\u63a5\u6536\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PhyG-MoE\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u5bb9\u91cf\u4e0e\u4fe1\u53f7\u590d\u6742\u5ea6\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNSS\u5e72\u6270\u8bc6\u522b\u4e2d\u9759\u6001\u6a21\u578b\u7684\u8d44\u6e90\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3aSAGIN\u4e2d\u7684\u53ef\u9760\u5e72\u6270\u8bc6\u522b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13684", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13684", "abs": "https://arxiv.org/abs/2601.13684", "authors": ["Zhiyuan Shi", "Qibo Qiu", "Feng Xue", "Zhonglin Jiang", "Li Yu", "Jian Jiang", "Xiaofei He", "Wenxiao Wang"], "title": "HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference", "comment": null, "summary": "The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\\times$ compared to the original model in the 224K context. Our code will be open-source.", "AI": {"tldr": "HeteroCache\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001KV\u7f13\u5b58\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5934\u90e8\u5206\u7c7b\u548c\u5206\u5c42\u5b58\u50a8\u673a\u5236\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b03\u500d\u89e3\u7801\u52a0\u901f\u3002", "motivation": "KV\u7f13\u5b58\u7684\u7ebf\u6027\u5185\u5b58\u589e\u957f\u662f\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u9759\u6001\u538b\u7f29\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u5168\u5c40\u91cd\u8981\u4fe1\u606f\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u6ce8\u610f\u529b\u6f02\u79fb\u73b0\u8c61\uff08token\u91cd\u8981\u6027\u52a8\u6001\u53d8\u5316\uff09\u3002\u867d\u7136\u52a8\u6001\u68c0\u7d22\u65b9\u6cd5\u8bd5\u56fe\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u5b58\u5728\u7c97\u7c92\u5ea6\u7f13\u5b58\u7b56\u7565\u548c\u9ad8I/O\u5f00\u9500\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a\u6ce8\u610f\u529b\u5934\u5177\u6709\u4e0d\u540c\u7684\u65f6\u95f4\u5f02\u8d28\u6027\uff0c\u540c\u4e00\u5c42\u5185\u5934\u90e8\u95f4\u5b58\u5728\u663e\u8457\u7a7a\u95f4\u5197\u4f59\u3002HeteroCache\u6839\u636e\u7a33\u5b9a\u6027\u548c\u5197\u4f59\u6027\u5bf9\u5934\u90e8\u8fdb\u884c\u5206\u7c7b\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u6743\u91cd\u5206\u914d\u7b56\u7565\uff0c\u4e3a\u6ce8\u610f\u529b\u5feb\u901f\u53d8\u5316\u7684\u5934\u90e8\u5206\u914d\u66f4\u5927\u7f13\u5b58\u9884\u7b97\u3002\u540c\u65f6\u4f7f\u7528\u5206\u5c42\u5b58\u50a8\u673a\u5236\uff0c\u8ba9\u4ee3\u8868\u6027\u5934\u90e8\u5b50\u96c6\u76d1\u63a7\u6ce8\u610f\u529b\u53d8\u5316\uff0c\u5f02\u6b65\u6309\u9700\u4eceCPU\u68c0\u7d22\u4e0a\u4e0b\u6587\uff0c\u9690\u85cfI/O\u5ef6\u8fdf\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728224K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u76f8\u6bd4\u539f\u59cb\u6a21\u578b\u5b9e\u73b0\u9ad8\u8fbe3\u500d\u7684\u89e3\u7801\u52a0\u901f\u3002", "conclusion": "HeteroCache\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5934\u90e8\u5206\u7c7b\u548c\u5206\u5c42\u5b58\u50a8\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684KV\u7f13\u5b58\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u89e3\u7801\u52a0\u901f\u3002"}}
{"id": "2601.12809", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12809", "abs": "https://arxiv.org/abs/2601.12809", "authors": ["Takaki Yamamoto", "Chihiro Noguchi", "Toshihiro Tanizawa"], "title": "Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data", "comment": null, "summary": "Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u63a7\u76841D\u56fe\u50cf-\u6587\u672c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7814\u7a76\u4e86CLIP\u98ce\u683c\u5bf9\u6bd4\u8bad\u7ec3\u4e2d\u5de6\u53f3\u5173\u7cfb\u7406\u89e3\u7684\u6d8c\u73b0\u673a\u5236\uff0c\u53d1\u73b0\u6807\u7b7e\u591a\u6837\u6027\u6bd4\u5e03\u5c40\u591a\u6837\u6027\u66f4\u80fd\u9a71\u52a8\u6cdb\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u4f4d\u7f6e\u5d4c\u5165\u4e0etoken\u5d4c\u5165\u7684\u4ea4\u4e92\u5982\u4f55\u6253\u7834\u5de6\u53f3\u5bf9\u79f0\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u7406\u89e3\u662f\u5426\u771f\u6b63\u88ab\u4e60\u5f97\uff0c\u4ee5\u53ca\u901a\u8fc7\u4f55\u79cd\u673a\u5236\u4e60\u5f97\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76Transformer-based\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u5728CLIP\u98ce\u683c\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u4e2d\uff0c\u5de6\u53f3\u5173\u7cfb\u7406\u89e3\u5982\u4f55\u6d8c\u73b0\u3002", "method": "\u6784\u5efa\u53ef\u63a7\u76841D\u56fe\u50cf-\u6587\u672c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7Transformer-based\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u7aef\u5230\u7aef\uff0c\u4f7f\u7528\u5355\u7269\u4f53\u548c\u53cc\u7269\u4f53\u573a\u666f\u7684\u914d\u5bf9\u63cf\u8ff0\u3002\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u6807\u7b7e\u548c\u5e03\u5c40\u591a\u6837\u6027\u6765\u8bc4\u4f30\u5bf9\u672a\u89c1\u7269\u4f53\u5bf9\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u89e3\u5206\u6790\uff0c\u7814\u7a76\u4f4d\u7f6e\u5d4c\u5165\u4e0etoken\u5d4c\u5165\u7684\u4ea4\u4e92\u673a\u5236\u3002", "result": "\u5bf9\u6bd4\u8bad\u7ec3\u786e\u5b9e\u80fd\u5b66\u4e60\u5de6\u53f3\u5173\u7cfb\uff0c\u4e14\u6807\u7b7e\u591a\u6837\u6027\u6bd4\u5e03\u5c40\u591a\u6837\u6027\u66f4\u80fd\u9a71\u52a8\u6cdb\u5316\u3002\u6ce8\u610f\u529b\u5206\u89e3\u663e\u793a\u4f4d\u7f6e\u5d4c\u5165\u4e0etoken\u5d4c\u5165\u7684\u4ea4\u4e92\u8bf1\u5bfc\u4e86\u6c34\u5e73\u6ce8\u610f\u529b\u68af\u5ea6\uff0c\u6253\u7834\u4e86\u7f16\u7801\u5668\u4e2d\u7684\u5de6\u53f3\u5bf9\u79f0\u6027\uff1b\u6d88\u9664\u8fd9\u4e00\u8d21\u732e\u4f1a\u663e\u8457\u964d\u4f4e\u5de6\u53f3\u8fa8\u522b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aCLIP\u98ce\u683c\u6a21\u578b\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u83b7\u5f97\u5173\u7cfb\u80fd\u529b\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u6807\u7b7e\u591a\u6837\u6027\u5728\u7a7a\u95f4\u5173\u7cfb\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u53ca\u4f4d\u7f6e-\u8bed\u4e49\u4ea4\u4e92\u5728\u6253\u7834\u5bf9\u79f0\u6027\u4e2d\u7684\u673a\u5236\u3002"}}
{"id": "2601.13690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13690", "abs": "https://arxiv.org/abs/2601.13690", "authors": ["Yue Guo", "Fanfu Wang", "Jianwei Lv", "Xincheng Shi", "Yuchen Li", "Youya Wang", "Yunsheng Zeng", "Yujing Liu", "Yunhao Qiao", "Gen Li", "Junfeng Wang", "Bo Yuan"], "title": "Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning", "comment": null, "summary": "Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDr. Assistant\u4e34\u5e8a\u8bca\u65ad\u6a21\u578b\uff0c\u901a\u8fc7CDRD\u6570\u636e\u7ed3\u6784\u6355\u6349\u4e34\u5e8a\u63a8\u7406\u903b\u8f91\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08SFT+RL\uff09\uff0c\u5728\u8bca\u65ad\u63a8\u7406\u548c\u95ee\u8be2\u65b9\u9762\u8d85\u8d8a\u5f00\u6e90\u6a21\u578b\uff0c\u8fbe\u5230\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u6c34\u5e73\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08CDSS\uff09\u5b58\u5728\u7ef4\u62a4\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u4f4e\u7684\u95ee\u9898\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u8bca\u65ad\u63a8\u7406\u548c\u95ee\u8be2\u80fd\u529b\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faCDRD\u6570\u636e\u7ed3\u6784\u6355\u6349\u62bd\u8c61\u4e34\u5e8a\u63a8\u7406\u903b\u8f91\uff0c\u5e76\u6784\u5efaDr. Assistant\u4e34\u5e8a\u8bca\u65ad\u6a21\u578b\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u540e\u63a5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u4f7f\u7528\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u3002\u540c\u65f6\u5f15\u5165\u8bc4\u4f30\u8bca\u65ad\u63a8\u7406\u548c\u95ee\u8be2\u7684\u57fa\u51c6\u3002", "result": "Dr. Assistant\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u8bca\u65ad\u63a8\u7406\u548c\u95ee\u8be2\u65b9\u9762\u8fbe\u5230\u4e0e\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u95ee\u8be2\u6307\u5bfc\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Dr. Assistant\u901a\u8fc7\u521b\u65b0\u7684CDRD\u6570\u636e\u7ed3\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\u548c\u95ee\u8be2\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.12814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12814", "abs": "https://arxiv.org/abs/2601.12814", "authors": ["Yu-Jen Tseng", "Chia-Hao Kao", "Jing-Zhong Chen", "Alessandro Gnutti", "Shao-Yuan Lo", "Yen-Yu Lin", "Wen-Hsiao Peng"], "title": "CSGaussian: Progressive Rate-Distortion Compression and Segmentation for 3D Gaussian Splatting", "comment": "Accepted at WACV 2026", "summary": "We present the first unified framework for rate-distortion-optimized compression and segmentation of 3D Gaussian Splatting (3DGS). While 3DGS has proven effective for both real-time rendering and semantic scene understanding, prior works have largely treated these tasks independently, leaving their joint consideration unexplored. Inspired by recent advances in rate-distortion-optimized 3DGS compression, this work integrates semantic learning into the compression pipeline to support decoder-side applications--such as scene editing and manipulation--that extend beyond traditional scene reconstruction and view synthesis. Our scheme features a lightweight implicit neural representation-based hyperprior, enabling efficient entropy coding of both color and semantic attributes while avoiding costly grid-based hyperprior as seen in many prior works. To facilitate compression and segmentation, we further develop compression-guided segmentation learning, consisting of quantization-aware training to enhance feature separability and a quality-aware weighting mechanism to suppress unreliable Gaussian primitives. Extensive experiments on the LERF and 3D-OVS datasets demonstrate that our approach significantly reduces transmission cost while preserving high rendering quality and strong segmentation performance.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7edf\u4e00\u76843D\u9ad8\u65af\u6cfc\u6e85\u7387\u5931\u771f\u4f18\u5316\u538b\u7f29\u4e0e\u5206\u5272\u6846\u67b6\uff0c\u5c06\u8bed\u4e49\u5b66\u4e60\u6574\u5408\u5230\u538b\u7f29\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u89e3\u7801\u7aef\u7684\u573a\u666f\u7f16\u8f91\u7b49\u5e94\u7528\u3002", "motivation": "\u867d\u71363DGS\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u8bed\u4e49\u573a\u666f\u7406\u89e3\u65b9\u9762\u90fd\u6709\u6548\uff0c\u4f46\u5148\u524d\u5de5\u4f5c\u5927\u591a\u72ec\u7acb\u5904\u7406\u8fd9\u4e24\u4e2a\u4efb\u52a1\uff0c\u672a\u63a2\u7d22\u5b83\u4eec\u7684\u8054\u5408\u8003\u8651\u3002\u53d7\u8fd1\u671f\u7387\u5931\u771f\u4f18\u53163DGS\u538b\u7f29\u8fdb\u5c55\u542f\u53d1\uff0c\u9700\u8981\u652f\u6301\u89e3\u7801\u7aef\u5e94\u7528\uff08\u5982\u573a\u666f\u7f16\u8f91\u548c\u64cd\u4f5c\uff09\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f20\u7edf\u7684\u573a\u666f\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u3002", "method": "1. \u96c6\u6210\u8bed\u4e49\u5b66\u4e60\u5230\u538b\u7f29\u6d41\u7a0b\u4e2d\uff1b2. \u91c7\u7528\u8f7b\u91cf\u7ea7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8d85\u5148\u9a8c\uff0c\u9ad8\u6548\u7f16\u7801\u989c\u8272\u548c\u8bed\u4e49\u5c5e\u6027\uff1b3. \u5f00\u53d1\u538b\u7f29\u5f15\u5bfc\u7684\u5206\u5272\u5b66\u4e60\uff0c\u5305\u62ec\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u57fa\u4e8e\u8d28\u91cf\u7684\u52a0\u6743\u673a\u5236\u3002", "result": "\u5728LERF\u548c3D-OVS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6e32\u67d3\u8d28\u91cf\u548c\u5f3a\u5927\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u76843DGS\u538b\u7f29\u4e0e\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u8bed\u4e49\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u8d85\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7387\u5931\u771f\u4f18\u5316\uff0c\u652f\u6301\u89e3\u7801\u7aef\u5e94\u7528\uff0c\u4e3a3D\u573a\u666f\u7684\u538b\u7f29\u548c\u8bed\u4e49\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13695", "abs": "https://arxiv.org/abs/2601.13695", "authors": ["Sifan Li", "Hongkai Chen", "Yujun Cai", "Liyang Chen", "Qingwen Ye", "Yiwei Wang"], "title": "OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens", "comment": null, "summary": "Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.", "AI": {"tldr": "OptiSQL\uff1a\u4e00\u4e2a\u76f4\u63a5\u4ece\u8868\u683c\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u751f\u6210\u53ef\u6267\u884cSQL\u7684\u89c6\u89c9\u9a71\u52a8\u6846\u67b6\uff0c\u4f7f\u7528\u7d27\u51d1\u5149\u5b66\u6807\u8bb0\u5927\u5e45\u51cf\u5c11\u8f93\u5165\u6807\u8bb0\u6570\u91cf", "motivation": "\u4f20\u7edf\u6587\u672c\u5230SQL\u65b9\u6cd5\u9700\u8981\u5c06\u8868\u683c\u5b8c\u5168\u7ebf\u6027\u5316\u4e3a\u6587\u672c\u6a21\u5f0f\uff0c\u8fd9\u5047\u8bbe\u4e86\u7ed3\u6784\u5316\u6587\u672c\u7684\u8bbf\u95ee\u6743\u9650\u5e76\u4ea7\u751f\u5927\u91cf\u6807\u8bb0\u5f00\u9500\uff0c\u4e0e\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\uff08\u8868\u683c\u4ee5\u89c6\u89c9\u5f62\u5f0f\u51fa\u73b0\u5728\u6587\u6863\u6216\u7f51\u9875\u4e2d\uff09\u4e0d\u5339\u914d\u3002\u7814\u7a76\u7d27\u51d1\u5149\u5b66\u8868\u793a\u662f\u5426\u80fd\u4f5c\u4e3a\u53ef\u6267\u884c\u8bed\u4e49\u89e3\u6790\u7684\u9ad8\u6548\u63a5\u53e3\u3002", "method": "\u63d0\u51faOptiSQL\u6846\u67b6\uff1a1\uff09\u4f7f\u7528OCR\u5bfc\u5411\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5c06\u8868\u683c\u7ed3\u6784\u548c\u5185\u5bb9\u538b\u7f29\u4e3a\u4e00\u5c0f\u7ec4\u5149\u5b66\u6807\u8bb0\uff1b2\uff09\u5fae\u8c03\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u8fdb\u884cSQL\u751f\u6210\uff0c\u540c\u65f6\u51bb\u7ed3\u7f16\u7801\u5668\u4ee5\u9694\u79bb\u8868\u793a\u5145\u5206\u6027\uff1b3\uff09\u76f4\u63a5\u4ece\u8868\u683c\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u751f\u6210\u53ef\u6267\u884cSQL\u3002", "result": "\u5728\u53ef\u89c6\u5316\u7684Spider 2.0-Snow\u6570\u636e\u96c6\u4e0a\uff0cOptiSQL\u5728\u4fdd\u6301\u5f3a\u5927\u6267\u884c\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u8868\u683c\u8f93\u5165\u6807\u8bb0\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u9c81\u68d2\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u8868\u660e\u5149\u5b66\u6807\u8bb0\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u4fdd\u7559\u4e86\u5fc5\u8981\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "conclusion": "\u7d27\u51d1\u5149\u5b66\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u6267\u884c\u8bed\u4e49\u89e3\u6790\u7684\u9ad8\u6548\u63a5\u53e3\uff0cOptiSQL\u6846\u67b6\u901a\u8fc7\u5149\u5b66\u6807\u8bb0\u5927\u5e45\u51cf\u5c11\u8f93\u5165\u6807\u8bb0\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301SQL\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u5904\u7406\u89c6\u89c9\u8868\u683c\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12820", "abs": "https://arxiv.org/abs/2601.12820", "authors": ["Wei Chen", "Liang Wu", "Shuyi Lu", "Yuanyuan Sun", "Wenkai Bi", "Zilong Yuan", "Yaoyao He", "Feng Wang", "Junchi Ma", "Shuyong Liu", "Zhaoping Cheng", "Xiaoyan Hu", "Jianfeng Qiu"], "title": "A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling", "comment": null, "summary": "Total-body PET/CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET/CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET/CT diagnostics and system-level precision oncology.", "AI": {"tldr": "SDF-HOLO\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u8eabPET/CT\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u6d41\u7f16\u7801\u5668\u5206\u79bbCT\u548cPET\u8868\u793a\u5b66\u4e60\uff0c\u4f7f\u7528\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u8026\u5408\uff0c\u7ed3\u5408\u5206\u5c42\u4e0a\u4e0b\u6587\u5efa\u6a21\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u89e3\u5256\u5206\u5272\u63a9\u7801\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u5b9e\u73b0\u4f53\u7d20-\u63a9\u7801-\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u5168\u8eabPET/CT\u5177\u6709\u5f02\u8d28\u6027\u89e3\u5256\u4ee3\u8c22\u4fe1\u53f7\u3001\u7ea62\u7c73\u8f74\u5411\u8986\u76d6\u548c\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u8bed\u4e49\uff0c\u73b0\u6709\u533b\u5b66AI\u6a21\u578b\u5047\u8bbe\u5355\u6a21\u6001\u8f93\u5165\u3001\u5c40\u90e8\u89c6\u91ce\u548c\u7c97\u7c92\u5ea6\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "1. \u4f7f\u7528\u53cc\u6d41\u7f16\u7801\u5668\u89e3\u8026CT\u548cPET\u8868\u793a\u5b66\u4e60\uff1b2. \u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u8026\u5408\u4e24\u79cd\u6a21\u6001\uff0c\u8ba9\u89e3\u5256\u4e0a\u4e0b\u6587\u7ec6\u5316PET\u805a\u5408\uff0c\u4ee3\u8c22\u663e\u8457\u6027\u6307\u5bfc\u5f62\u6001\u63a8\u7406\uff1b3. \u5206\u5c42\u4e0a\u4e0b\u6587\u5efa\u6a21\u7ed3\u5408\u9ad8\u6548\u5c40\u90e8\u7a97\u53e3\u548c\u5168\u5c40\u6ce8\u610f\u529b\uff1b4. \u4f7f\u7528\u89e3\u5256\u5206\u5272\u63a9\u7801\u4f5c\u4e3a\u663e\u5f0f\u8bed\u4e49\u951a\u70b9\uff0c\u5728\u9884\u8bad\u7ec3\u4e2d\u6267\u884c\u4f53\u7d20-\u63a9\u7801-\u6587\u672c\u5bf9\u9f50\u3002", "result": "\u5728\u80bf\u7624\u5206\u5272\u3001\u4f4e\u5242\u91cf\u75c5\u53d8\u68c0\u6d4b\u548c\u591a\u8bed\u8a00\u8bca\u65ad\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSDF-HOLO\u4f18\u4e8e\u5f3a\u4efb\u52a1\u7279\u5b9a\u548c\u4e34\u5e8a\u53c2\u8003\u57fa\u7ebf\uff0c\u51cf\u5c11\u4e86\u5b9a\u4f4d\u9519\u8bef\u548c\u5e7b\u89c9\u53d1\u73b0\u3002\u6a21\u578b\u8fd8\u80fd\u5b9e\u73b0\u5168\u8eab\u4ee3\u8c22\u5206\u6790\u548c\u63ed\u793a\u80bf\u7624\u76f8\u5173\u7684\u5668\u5b98\u95f4\u4ee3\u8c22\u7f51\u7edc\u76f8\u4e92\u4f5c\u7528\u6307\u7eb9\u3002", "conclusion": "SDF-HOLO\u4e3a\u5168\u8eabPET/CT\u8bca\u65ad\u548c\u7cfb\u7edf\u7ea7\u7cbe\u51c6\u80bf\u7624\u5b66\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u8d85\u8d8a\u4e86\u5c40\u90e8\u89e3\u91ca\uff0c\u5b9e\u73b0\u4e86\u5168\u8eab\u4ee3\u8c22\u5206\u6790\u548c\u7cfb\u7edf\u7ea7\u76f8\u4e92\u4f5c\u7528\u7406\u89e3\u3002"}}
{"id": "2601.13697", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13697", "abs": "https://arxiv.org/abs/2601.13697", "authors": ["Zhihang Yuan", "Chengyu Yue", "Long Huang", "Litu Ou", "Lei Shi"], "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning", "comment": "Preprint", "summary": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.", "AI": {"tldr": "GRADFILTERING\u662f\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u4f7f\u7528GPT-2\u4ee3\u7406\u6a21\u578b\u548cLoRA\u96c6\u6210\u8ba1\u7b97\u68af\u5ea6\u4fe1\u566a\u6bd4\uff0c\u5728\u6307\u4ee4\u8c03\u4f18\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u9009\u62e9", "motivation": "\u73b0\u4ee3\u6307\u4ee4\u6570\u636e\u96c6\u5e9e\u5927\u3001\u5608\u6742\u4e14\u5197\u4f59\uff0c\u5168\u6570\u636e\u5fae\u8c03\u6210\u672c\u9ad8\u4e14\u4e0d\u5fc5\u8981\u3002\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u6784\u5efa\u6602\u8d35\u7684\u68af\u5ea6\u6570\u636e\u5b58\u50a8\uff0c\u8981\u4e48\u4f7f\u7528\u5f31\u4ee3\u7406\u7684\u9759\u6001\u8bc4\u5206\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u6f14\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u8fd9\u4e00\u5173\u952e\u89e3\u91ca\u6027\u6765\u6e90\u3002", "method": "\u63d0\u51faGRADFILTERING\u6846\u67b6\uff1a\u4f7f\u7528\u5c0f\u578bGPT-2\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u7ed3\u5408LoRA\u96c6\u6210\u6280\u672f\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u68af\u5ea6\u805a\u5408\u6210\u68af\u5ea6\u4fe1\u566a\u6bd4(G-SNR)\u6548\u7528\u5206\u6570\uff0c\u5b9e\u73b0\u76ee\u6807\u65e0\u5173\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u3002", "result": "\u5728\u5927\u591a\u6570LLM-as-a-judge\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\uff0cGRADFILTERING\u9009\u62e9\u7684\u6570\u636e\u5b50\u96c6\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u5b50\u96c6\u548c\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u6240\u9009\u5b50\u96c6\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "GRADFILTERING\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8bc4\u5206\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\uff0c\u8bc1\u660e\u4e86\u8003\u8651\u6a21\u578b\u6f14\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3aLLM\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u7684\u6570\u636e\u7b5b\u9009\u65b9\u6848\u3002"}}
{"id": "2601.12823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12823", "abs": "https://arxiv.org/abs/2601.12823", "authors": ["Belal Shaheen", "Minh-Hieu Nguyen", "Bach-Thuan Bui", "Shubham", "Tim Wu", "Michael Fairley", "Matthew David Zane", "Michael Wu", "James Tompkin"], "title": "TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement", "comment": null, "summary": "Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS's depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.", "AI": {"tldr": "TreeDGS\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684\u822a\u7a7a\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a7a\u4e2d\u9065\u611f\u51c6\u786e\u6d4b\u91cf\u6811\u6728\u80f8\u5f84\uff08DBH\uff09\uff0c\u572810\u4e2a\u6837\u5730\u6d4b\u8bd5\u4e2d\u8fbe\u52304.79cm RMSE\uff0c\u4f18\u4e8e\u6fc0\u5149\u96f7\u8fbe\u57fa\u7ebf\uff087.91cm RMSE\uff09\u3002", "motivation": "\u822a\u7a7a\u9065\u611f\u867d\u7136\u80fd\u9ad8\u6548\u8fdb\u884c\u5927\u9762\u79ef\u52d8\u6d4b\uff0c\u4f46\u5728\u590d\u6742\u81ea\u7136\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u5bf9\u8c61\u7ea7\u6d4b\u91cf\u3002\u73b0\u67093D\u89c6\u89c9\u65b9\u6cd5\uff08\u5982NeRF\u548c3D\u9ad8\u65af\u6e85\u5c04\uff09\u867d\u63d0\u5347\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u4f46\u5bf9\u6811\u6728\u80f8\u5f84\u7b49\u5173\u952e\u81ea\u7136\u5c5e\u6027\u7684\u76f4\u63a5\u822a\u7a7a\u6d4b\u91cf\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6811\u5e72\u5728\u822a\u7a7a\u56fe\u50cf\u4e2d\u8ddd\u79bb\u8fdc\u3001\u89c2\u6d4b\u7a00\u758f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7ea6\u675f\u80f8\u9ad8\u5904\u7684\u6811\u5e72\u51e0\u4f55\u5f62\u72b6\u3002", "method": "TreeDGS\u5229\u75283D\u9ad8\u65af\u6e85\u5c04\u4f5c\u4e3a\u8fde\u7eed\u3001\u53ef\u5bc6\u96c6\u5316\u7684\u573a\u666f\u8868\u793a\u3002\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09SfM-MVS\u521d\u59cb\u5316\u548c\u9ad8\u65af\u4f18\u5316\uff1b2\uff09\u4f7f\u7528RaDe-GS\u7684\u6df1\u5ea6\u611f\u77e5\u7d2f\u79ef\u4e0d\u900f\u660e\u5ea6\u79ef\u5206\u4ece\u9ad8\u65af\u573a\u63d0\u53d6\u5bc6\u96c6\u70b9\u96c6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6837\u672c\u5206\u914d\u591a\u89c6\u89d2\u4e0d\u900f\u660e\u5ea6\u53ef\u9760\u6027\u5206\u6570\uff1b3\uff09\u901a\u8fc7\u4e0d\u900f\u660e\u5ea6\u52a0\u6743\u5b9e\u5fc3\u5706\u62df\u5408\u4ece\u6811\u5e72\u5206\u79bb\u70b9\u4f30\u8ba1DBH\u3002", "result": "\u572810\u4e2a\u5177\u6709\u5b9e\u5730\u6d4b\u91cfDBH\u7684\u6837\u5730\u8bc4\u4f30\u4e2d\uff0cTreeDGS\u8fbe\u52304.79cm RMSE\uff08\u7ea62.6\u50cf\u7d20\uff09\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6fc0\u5149\u96f7\u8fbe\u57fa\u7ebf\uff087.91cm RMSE\uff09\u3002\u8fd9\u8868\u660e\u57fa\u4e8e\u6e85\u5c04\u7684\u5bc6\u96c6\u5316\u51e0\u4f55\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u4f4e\u6210\u672c\u7684\u822a\u7a7aDBH\u6d4b\u91cf\u3002", "conclusion": "TreeDGS\u8bc1\u660e\u4e863D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u5728\u822a\u7a7a\u6811\u6728\u6d4b\u91cf\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u89c2\u6d4b\u7684\u822a\u7a7a\u56fe\u50cf\u4e2d\u51c6\u786e\u91cd\u5efa\u6811\u5e72\u51e0\u4f55\u5e76\u6d4b\u91cfDBH\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u5927\u89c4\u6a21\u7684\u68ee\u6797\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.13711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13711", "abs": "https://arxiv.org/abs/2601.13711", "authors": ["Lotta Kiefer", "Christoph Leiter", "Sotaro Takeshita", "Elena Schmidt", "Steffen Eger"], "title": "GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark", "comment": null, "summary": "Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.", "AI": {"tldr": "GerAV\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5fb7\u8bed\u4f5c\u8005\u9a8c\u8bc1\u57fa\u51c6\uff0c\u5305\u542b\u8d85\u8fc760\u4e07\u6807\u8bb0\u6587\u672c\u5bf9\uff0c\u6765\u81eaTwitter\u548cReddit\u6570\u636e\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5fb7\u8bed\u4f5c\u8005\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f5c\u8005\u9a8c\u8bc1\u4efb\u52a1\u5728\u82f1\u8bed\u9886\u57df\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u4ed6\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u548c\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u7136\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u5fb7\u8bed\u4f5c\u8005\u9a8c\u8bc1\u9886\u57df\u7684\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaGerAV\u57fa\u51c6\uff0c\u5305\u542bTwitter\u548cReddit\u6570\u636e\uff0c\u5176\u4e2dReddit\u90e8\u5206\u8fdb\u4e00\u6b65\u5206\u4e3a\u57df\u5185\u548c\u8de8\u57df\u6d88\u606f\u5b50\u96c6\u4ee5\u53ca\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u5b50\u96c6\u3002\u4f7f\u7528\u63d0\u4f9b\u7684\u8bad\u7ec3\u5206\u5272\u5bf9\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\uff08\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u6bd4\u6700\u8fd1\u57fa\u7ebf\u9ad8\u51fa0.09\u7edd\u5bf9F1\u5206\u6570\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u6bd4GPT-5\u9ad8\u51fa0.08\u3002\u89c2\u5bdf\u5230\u4e13\u4e1a\u5316\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u5728\u5339\u914d\u6761\u4ef6\u4e0b\uff0c\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u7c7b\u578b\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8de8\u6570\u636e\u5236\u5ea6\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u8fd9\u4e00\u9650\u5236\u53ef\u4ee5\u901a\u8fc7\u7ec4\u5408\u8bad\u7ec3\u6e90\u6765\u7f13\u89e3\u3002", "conclusion": "GerAV\u4e3a\u63a8\u8fdb\u5fb7\u8bed\u548c\u8de8\u57df\u4f5c\u8005\u9a8c\u8bc1\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u591a\u529f\u80fd\u7684\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u5206\u6790\u6570\u636e\u6e90\u3001\u4e3b\u9898\u57df\u548c\u6587\u672c\u957f\u5ea6\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.12826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12826", "abs": "https://arxiv.org/abs/2601.12826", "authors": ["Teerapong Panboonyuen"], "title": "Seeing Isn't Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification", "comment": "7 pages", "summary": "Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH/NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to \"trust\" a model's explanation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Grad-CAM\u5728\u80ba\u764c\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u5377\u79ef\u7f51\u7edc\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728Vision Transformer\u4e2d\u89e3\u91ca\u4fdd\u771f\u5ea6\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u663e\u8457\u6027\u89e3\u91ca\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1Grad-CAM\u7b49XAI\u6280\u672f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u53ef\u89c6\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u5176\u5fe0\u5b9e\u6027\u548c\u53ef\u9760\u6027\u4ecd\u53d7\u5230\u8d28\u7591\u3002\u672c\u7814\u7a76\u65e8\u5728\u6279\u5224\u6027\u5730\u63a2\u7a76Grad-CAM\u662f\u5426\u771f\u6b63\u4ee3\u8868\u4e86\u7528\u4e8e\u80ba\u764c\u56fe\u50cf\u5206\u7c7b\u7684\u6df1\u5ea6\u6a21\u578b\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684IQ-OTH/NCCD\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u79cd\u4ee3\u8868\u6027\u67b6\u6784\uff1aResNet-50\u3001ResNet-101\u3001DenseNet-161\u3001EfficientNet-B0\u548cViT-Base-Patch16-224\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u5b9a\u4f4d\u51c6\u786e\u6027\u3001\u57fa\u4e8e\u6270\u52a8\u7684\u5fe0\u5b9e\u6027\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u8de8\u67b6\u6784\u7684Grad-CAM\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u867d\u7136Grad-CAM\u5728\u5927\u591a\u6570\u5377\u79ef\u7f51\u7edc\u4e2d\u80fd\u6709\u6548\u7a81\u51fa\u663e\u8457\u7684\u80bf\u7624\u533a\u57df\uff0c\u4f46\u5176\u89e3\u91ca\u4fdd\u771f\u5ea6\u5728Vision Transformer\u6a21\u578b\u4e2d\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u662f\u7531\u4e8e\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u884c\u4e3a\u5bfc\u81f4\u7684\u3002\u8de8\u6a21\u578b\u6bd4\u8f83\u663e\u793a\u663e\u8457\u6027\u5b9a\u4f4d\u5b58\u5728\u663e\u8457\u53d8\u5f02\u6027\uff0c\u8868\u660eGrad-CAM\u89e3\u91ca\u53ef\u80fd\u5e76\u4e0d\u603b\u662f\u5bf9\u5e94\u4e8e\u7f51\u7edc\u4f7f\u7528\u7684\u771f\u5b9e\u8bca\u65ad\u8bc1\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8e\u663e\u8457\u6027\u7684XAI\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u65e2\u8ba1\u7b97\u5408\u7406\u53c8\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u6a21\u578b\u611f\u77e5\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u65e8\u5728\u4fc3\u4f7f\u533b\u5b66AI\u9886\u57df\u66f4\u8c28\u614e\u3001\u66f4\u4e25\u683c\u5730\u91c7\u7528\u89c6\u89c9\u89e3\u91ca\u5de5\u5177\uff0c\u5e76\u91cd\u65b0\u601d\u8003\"\u4fe1\u4efb\"\u6a21\u578b\u89e3\u91ca\u7684\u771f\u6b63\u542b\u4e49\u3002"}}
{"id": "2601.13717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13717", "abs": "https://arxiv.org/abs/2601.13717", "authors": ["Zehan Li", "Yuxuan Wang", "Ali El Lahib", "Ying-Jieh Xia", "Xinyu Pi"], "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff", "comment": null, "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.", "AI": {"tldr": "\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8fd1\u4f3c\u771f\u5b9e\u65e0\u77e5\u72b6\u6001\uff0c\u4e0d\u80fd\u53ef\u9760\u5730\"\u5012\u56de\"\u6a21\u578b\u77e5\u8bc6\uff0c\u56e0\u6b64\u57fa\u4e8e\u6a21\u62df\u65e0\u77e5\u7684\u56de\u987e\u6027\u9884\u6d4b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677", "motivation": "\u8bc4\u4f30LLM\u9884\u6d4b\u80fd\u529b\u9762\u4e34\u4e24\u96be\uff1a\u524d\u77bb\u6027\u8bc4\u4f30\u65b9\u6cd5\u4e25\u8c28\u4f46\u5ef6\u8fdf\u9ad8\uff0c\u56de\u987e\u6027\u9884\u6d4b\u9762\u4e34\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u6709\u6548\u8fd1\u4f3c\u771f\u5b9e\u65e0\u77e5\u72b6\u6001", "method": "\u5728477\u4e2a\u7ade\u8d5b\u7ea7\u522b\u95ee\u9898\u548c9\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u6a21\u62df\u65e0\u77e5\uff08\u901a\u8fc7\u63d0\u793a\u6291\u5236\u622a\u6b62\u65e5\u671f\u540e\u77e5\u8bc6\uff09\u4e0e\u771f\u5b9e\u65e0\u77e5\uff08\u6a21\u578b\u786e\u5b9e\u4e0d\u77e5\u9053\u672a\u6765\u4e8b\u4ef6\uff09\u7684\u8868\u73b0\u5dee\u5f02", "result": "\u6a21\u62df\u65e0\u77e5\u65b9\u6cd5\u7cfb\u7edf\u6027\u5931\u8d25\uff1a1\uff09\u622a\u6b62\u6307\u4ee4\u5bfc\u81f4SI\u4e0eTI\u5b58\u572852%\u6027\u80fd\u5dee\u8ddd\uff1b2\uff09\u601d\u7ef4\u94fe\u63a8\u7406\u65e0\u6cd5\u6709\u6548\u6291\u5236\u5148\u9a8c\u77e5\u8bc6\uff1b3\uff09\u63a8\u7406\u4f18\u5316\u6a21\u578bSI\u4fdd\u771f\u5ea6\u66f4\u5dee\u3002\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\"\u5012\u56de\"\u6a21\u578b\u77e5\u8bc6", "conclusion": "\u57fa\u4e8e\u9884\u622a\u6b62\u4e8b\u4ef6\u7684\u56de\u987e\u6027\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u4e0d\u5efa\u8bae\u4f7f\u7528\u6a21\u62df\u65e0\u77e5\u4e3a\u57fa\u7840\u7684\u56de\u987e\u6027\u8bbe\u7f6e\u6765\u8bc4\u4f30\u9884\u6d4b\u80fd\u529b"}}
{"id": "2601.12863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12863", "abs": "https://arxiv.org/abs/2601.12863", "authors": ["Jun Wan", "Xinyu Xiong", "Ning Chen", "Zhihui Lai", "Jie Zhou", "Wenwen Min"], "title": "FGTBT: Frequency-Guided Task-Balancing Transformer for Unified Facial Landmark Detection", "comment": null, "summary": "Recently, deep learning based facial landmark detection (FLD) methods have achieved considerable success. However, in challenging scenarios such as large pose variations, illumination changes, and facial expression variations, they still struggle to accurately capture the geometric structure of the face, resulting in performance degradation. Moreover, the limited size and diversity of existing FLD datasets hinder robust model training, leading to reduced detection accuracy. To address these challenges, we propose a Frequency-Guided Task-Balancing Transformer (FGTBT), which enhances facial structure perception through frequency-domain modeling and multi-dataset unified training. Specifically, we propose a novel Fine-Grained Multi-Task Balancing loss (FMB-loss), which moves beyond coarse task-level balancing by assigning weights to individual landmarks based on their occurrence across datasets. This enables more effective unified training and mitigates the issue of inconsistent gradient magnitudes. Additionally, a Frequency-Guided Structure-Aware (FGSA) model is designed to utilize frequency-guided structure injection and regularization to help learn facial structure constraints. Extensive experimental results on popular benchmark datasets demonstrate that the integration of the proposed FMB-loss and FGSA model into our FGTBT framework achieves performance comparable to state-of-the-art methods. The code is available at https://github.com/Xi0ngxinyu/FGTBT.", "AI": {"tldr": "\u63d0\u51faFGTBT\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u5f15\u5bfc\u548c\u4efb\u52a1\u5e73\u8861\u63d0\u5347\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5728\u6311\u6218\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5305\u542bFMB-loss\u548cFGSA\u6a21\u578b", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u65b9\u6cd5\u5728\u59ff\u6001\u53d8\u5316\u3001\u5149\u7167\u53d8\u5316\u548c\u8868\u60c5\u53d8\u5316\u7b49\u6311\u6218\u573a\u666f\u4e0b\u96be\u4ee5\u51c6\u786e\u6355\u6349\u9762\u90e8\u51e0\u4f55\u7ed3\u6784\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3001\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bad\u7ec3", "method": "\u63d0\u51fa\u9891\u7387\u5f15\u5bfc\u4efb\u52a1\u5e73\u8861Transformer(FGTBT)\uff0c\u5305\u542b\uff1a1)\u7ec6\u7c92\u5ea6\u591a\u4efb\u52a1\u5e73\u8861\u635f\u5931(FMB-loss)\uff0c\u57fa\u4e8e\u5404\u6570\u636e\u96c6\u4e2d\u7684\u5730\u6807\u51fa\u73b0\u9891\u7387\u4e3a\u5355\u4e2a\u5730\u6807\u5206\u914d\u6743\u91cd\uff1b2)\u9891\u7387\u5f15\u5bfc\u7ed3\u6784\u611f\u77e5(FGSA)\u6a21\u578b\uff0c\u5229\u7528\u9891\u7387\u5f15\u5bfc\u7684\u7ed3\u6784\u6ce8\u5165\u548c\u6b63\u5219\u5316\u5b66\u4e60\u9762\u90e8\u7ed3\u6784\u7ea6\u675f", "result": "\u5728\u6d41\u884c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFGTBT\u6846\u67b6\u6574\u5408FMB-loss\u548cFGSA\u6a21\u578b\u540e\uff0c\u6027\u80fd\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6c34\u5e73", "conclusion": "\u901a\u8fc7\u9891\u7387\u57df\u5efa\u6a21\u548c\u591a\u6570\u636e\u96c6\u7edf\u4e00\u8bad\u7ec3\u589e\u5f3a\u9762\u90e8\u7ed3\u6784\u611f\u77e5\uff0cFGTBT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5728\u6311\u6218\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2601.13722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13722", "abs": "https://arxiv.org/abs/2601.13722", "authors": ["Yulin Hu", "Zimo Long", "Jiahe Guo", "Xingyu Sui", "Xing Fu", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents", "comment": null, "summary": "Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \\emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \\textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \\textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \\textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86OP-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5bf9\u8bdd\u4ee3\u7406\u7684\u8fc7\u5ea6\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86Self-ReCheck\u65b9\u6cd5\u6765\u7f13\u89e3\u8be5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u589e\u5f3a\u5bf9\u8bdd\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u80fd\u5426\u56de\u5fc6\u548c\u5e94\u7528\u7528\u6237\u4fe1\u606f\uff0c\u4f46\u5ffd\u89c6\u4e86\u8fc7\u5ea6\u4e2a\u6027\u5316\u95ee\u9898\u2014\u2014\u4ee3\u7406\u53ef\u80fd\u8fc7\u5ea6\u4f7f\u7528\u4e2a\u4eba\u4fe1\u606f\uff0c\u4ea7\u751f\u8ba9\u7528\u6237\u611f\u5230\u5f3a\u8feb\u3001\u4fb5\u5165\u6216\u793e\u4ea4\u4e0d\u9002\u5f53\u7684\u56de\u5e94\u3002", "method": "\u5c06\u8fc7\u5ea6\u4e2a\u6027\u5316\u5f62\u5f0f\u5316\u4e3a\u4e09\u79cd\u7c7b\u578b\uff1a\u65e0\u5173\u6027\u3001\u91cd\u590d\u6027\u548c\u8c04\u5a9a\u6027\uff1b\u6784\u5efa\u4e86\u5305\u542b1700\u4e2a\u9a8c\u8bc1\u5b9e\u4f8b\u7684OP-Bench\u57fa\u51c6\uff1b\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u8bb0\u5fc6\u8fc7\u6ee4\u673a\u5236Self-ReCheck\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5f15\u5165\u8bb0\u5fc6\u65f6\u8fc7\u5ea6\u4e2a\u6027\u5316\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4ee3\u7406\u503e\u5411\u4e8e\u68c0\u7d22\u548c\u8fc7\u5ea6\u5173\u6ce8\u7528\u6237\u8bb0\u5fc6\uff1bSelf-ReCheck\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8fc7\u5ea6\u4e2a\u6027\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u6027\u5316\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bb0\u5fc6\u589e\u5f3a\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u66f4\u53ef\u63a7\u548c\u9002\u5f53\u7684\u4e2a\u6027\u5316\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12865", "abs": "https://arxiv.org/abs/2601.12865", "authors": ["Xiaowei Fu", "Fuxiang Huang", "Lei Zhang"], "title": "Proxy Robustness in Vision Language Models is Effortlessly Transferable", "comment": null, "summary": "As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHPT-GPD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f02\u6784\u4ee3\u7406\u4f20\u8f93\u6846\u67b6\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u8f6c\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edf\u5bf9\u6297\u9c81\u68d2\u6027\u84b8\u998f\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u65f6\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4f46\u53d1\u73b0\u666e\u901aCLIP\u5bf9\u4e0d\u540c\u67b6\u6784CLIP\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5177\u6709\u5185\u5728\u9632\u5fa1\u80fd\u529b", "method": "\u63d0\u51fa\u5f02\u6784\u4ee3\u7406\u4f20\u8f93\uff08HPT\uff09\u6846\u67b6\uff0c\u5efa\u7acbCLIP\u53d8\u4f53\u95f4\u7684\u8de8\u67b6\u6784\u9c81\u68d2\u6027\u84b8\u998f\u901a\u9053\uff1b\u8bbe\u8ba1\u6cdb\u5316\u67a2\u8f74\u89e3\u8026\uff08GPD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u540c\u5b66\u4e60\u7387\u8c03\u5ea6\u5c06\u4ee3\u7406\u4f20\u8f93\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4fdd\u6301\u6cdb\u5316\u7684\u9884\u70ed\u9636\u6bb5\u548c\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u7684HPT\u9636\u6bb5", "result": "\u572815\u4e2a\u96f6\u6837\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86HPT-GPD\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u81ea\u7136\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5bf9\u6297\u9c81\u68d2\u6027\u8f6c\u79fb", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u8f6c\u79fb\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u95ee\u9898\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u6cdb\u5316\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5e73\u8861"}}
{"id": "2601.13729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13729", "abs": "https://arxiv.org/abs/2601.13729", "authors": ["Weichuan Wang", "Mingyang Liu", "Linqi Song", "Chen Ma"], "title": "On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation", "comment": "9 pages, 12 figures", "summary": "In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u73b0\u8c61\uff0c\u53d1\u73b0\u6e29\u5ea6\u7ea6\u675f\u4e0b\u7684\u975e\u786e\u5b9a\u6027\u673a\u5668\u7ffb\u8bd1\uff08ND-MT\uff09\u80fd\u63d0\u4f9b\u6bd4\u786e\u5b9a\u6027\u7ffb\u8bd1\uff08D-MT\uff09\u66f4\u9ad8\u8d28\u91cf\u7684\u5019\u9009\u8bd1\u6587\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30ND-MT\u7cfb\u7edf\u6027\u80fd\uff0c\u5b58\u5728\"\u6c34\u6876\u6548\u5e94\"\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u975e\u786e\u5b9a\u6027\u7279\u6027\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f71\u54cd\u663e\u8457\uff0c\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u8fd9\u4e00\u590d\u6742\u975e\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u7814\u7a76\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u8bc4\u4f30\u73b0\u4ee3MT\u7cfb\u7edf\u7684\u975e\u786e\u5b9a\u6027\u7279\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u89e3\u51b3MT\u4e2d\u957f\u671f\u5b58\u5728\u7684\u591a\u6a21\u6001\u95ee\u9898\u7684\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u73b0\u4ee3MT\u7cfb\u7edf\uff0c\u8bc6\u522b\u6e29\u5ea6\u7ea6\u675f\u4e0b\u7684ND-MT\u73b0\u8c61\uff1b\u5728\u4e09\u4e2a\u5f00\u653e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e94\u4e2a\u6700\u5148\u8fdb\u7684ND-MT\u7cfb\u7edf\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bcd\u6c47\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u6307\u6807\u5728\u4e0d\u540c\u91c7\u6837\u89c4\u6a21\u4e0b\u8fdb\u884c\u5206\u6790\uff1b\u63d0\u51faExpectoSample\u7b56\u7565\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u53ef\u9760\u6027\u3002", "result": "ND-MT\u5728\u6e29\u5ea6\u7ea6\u675f\u4e0b\u80fd\u63d0\u4f9b\u6bd4D-MT\u66f4\u9ad8\u8d28\u91cf\u7684\u5019\u9009\u8bd1\u6587\uff0c\u4f46\u73b0\u6709D-MT\u8bc4\u4f30\u6846\u67b6\u5728\u8bc4\u4f30ND-MT\u65f6\u7ed3\u679c\u4e0d\u4e00\u81f4\uff1b\u53d1\u73b0\"\u6c34\u6876\u6548\u5e94\"\uff1aND-MT\u751f\u6210\u7684\u6700\u4f4e\u8d28\u91cf\u5019\u9009\u8bd1\u6587\u51b3\u5b9a\u4e86\u6574\u4e2a\u7cfb\u7edf\u5728\u4e0d\u540c\u91c7\u6837\u89c4\u6a21\u4e0b\u7684\u6392\u540d\uff1bExpectoSample\u7b56\u7565\u80fd\u6709\u6548\u8bc4\u4f30\u6307\u6807\u53ef\u9760\u6027\u3002", "conclusion": "ND-MT\u4e3a\u89e3\u51b3MT\u591a\u6a21\u6001\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff1b\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u6700\u4f4e\u8d28\u91cf\u5019\u9009\u8bd1\u6587\u5bf9\u7cfb\u7edf\u6392\u540d\u5f71\u54cd\u6700\u5927\uff1b\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9ND-MT\u7684\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.12876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12876", "abs": "https://arxiv.org/abs/2601.12876", "authors": ["Zhenxuan Lu", "Zhihua Xu", "Zhijing Yang", "Feng Gao", "Yongyi Lu", "Keze Wang", "Tianshui Chen"], "title": "Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation", "comment": "Accepted by ACM Transactions on Multimedia Computing, Communications, and Applications", "summary": "Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.", "AI": {"tldr": "\u63d0\u51faTHFEM\u6846\u67b6\uff0c\u6574\u5408\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u4e0e\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u6280\u672f\uff0c\u901a\u8fc7\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u5507\u90e8\u540c\u6b65\u7cbe\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709SPFEM\u6280\u672f\u5728\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u65f6\u96be\u4ee5\u4fdd\u6301\u51c6\u786e\u7684\u5507\u90e8\u540c\u6b65\uff0c\u56e0\u4e3a\u9762\u90e8\u8868\u60c5\u4e0e\u5634\u578b\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u8054\u3002\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u5728\u5507\u90e8\u540c\u6b65\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u76f4\u63a5\u6574\u5408\u4f1a\u964d\u4f4e\u56fe\u50cf\u771f\u5b9e\u611f\u548c\u8868\u60c5\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51faTHFEM\u6846\u67b6\uff1a1) \u5229\u7528AD-THG\u6a21\u578b\u4ece\u97f3\u9891\u8f93\u5165\u548cSPFEM\u5904\u7406\u56fe\u50cf\u751f\u6210\u5507\u90e8\u540c\u6b65\u5e27\uff1b2) \u5f00\u53d1\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\uff0c\u5fae\u8c03AD-THG\u6a21\u578b\u4ee5\u9884\u6d4b\u8fde\u7eed\u5e27\u5e8f\u5217\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u90bb\u5e27\u4fe1\u606f\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8868\u60c5\u64cd\u7eb5\u8fc7\u7a0b\u4e2d\u6709\u6548\u4fdd\u6301\u4e86\u5634\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5507\u90e8\u540c\u6b65\u7cbe\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86AD-THG\u4e0eSPFEM\u6574\u5408\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "THFEM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86SPFEM\u4e2d\u7684\u5507\u90e8\u540c\u6b65\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408AD-THG\u6a21\u578b\u548c\u76f8\u90bb\u5e27\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u8868\u60c5\u64cd\u7eb5\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7684\u5634\u578b\u3002"}}
{"id": "2601.13734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13734", "abs": "https://arxiv.org/abs/2601.13734", "authors": ["Chenyu Hui"], "title": "Towards robust long-context understanding of large language model via active recap learning", "comment": "5 pages", "summary": "In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM", "AI": {"tldr": "ARL\u901a\u8fc7\u4e3b\u52a8\u56de\u987e\u5b66\u4e60\u6846\u67b6\u589e\u5f3aLLM\u7684\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u6784\u5efa\u76ee\u6807\u5e8f\u5217\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u56de\u987e\u6027\u603b\u7ed3\uff0c\u5efa\u7acb\u8de8\u6bb5\u843d\u7684\u9012\u5f52\u8bb0\u5fc6\u673a\u5236", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u957f\u6587\u672c\u7684\u5904\u7406\u80fd\u529b", "method": "1. \u57fa\u4e8e\u957f\u77ed\u524d\u5411\u4e0a\u4e0b\u6587\u635f\u5931\u5dee\u8ddd\u8bc6\u522b\u5173\u952etoken\uff0c\u627e\u5230\u6700\u76f8\u5173\u7684\u524d\u9762\u6bb5\u843d\u5e76\u7528LLM\u603b\u7ed3\uff1b2. \u5728\u63a8\u7406\u65f6\u8ba9\u6a21\u578b\u81ea\u4e3b\u751f\u6210\u548c\u5229\u7528\u8fd9\u4e9b\u56de\u987e\u6027\u603b\u7ed3\uff0c\u5efa\u7acb\u8de8\u6bb5\u843d\u7684\u9012\u5f52\u8bb0\u5fc6\u673a\u5236", "result": "\u5728RULER\u4e0a\u63d0\u534726.8%\uff0c\u5728LongBench\u4e0a\u63d0\u53479.44%\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b", "conclusion": "ARL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86LLM\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u63a8\u8fdb\u4e86\u53ef\u6269\u5c55\u8bb0\u5fc6\u589e\u5f3a\u7684\u53d1\u5c55"}}
{"id": "2601.12882", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12882", "abs": "https://arxiv.org/abs/2601.12882", "authors": ["Sudip Chakrabarty"], "title": "YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection", "comment": null, "summary": "The \"You Only Look Once\" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.", "AI": {"tldr": "YOLO26\u901a\u8fc7\u6d88\u9664NMS\u540e\u5904\u7406\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u63a8\u7406\u901f\u5ea6\u548c\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u65b0\u7684Pareto\u524d\u6cbf\u3002", "motivation": "\u4f20\u7edfYOLO\u7cfb\u5217\uff08v1-v11\uff09\u53d7\u9650\u4e8eNMS\u540e\u5904\u7406\u7684\u5ef6\u8fdf\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\uff0c\u9700\u8981\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u4e2d\u89e3\u51b3\u8fd9\u4e00\u5386\u53f2\u6027\u6743\u8861\u95ee\u9898\u3002", "method": "\u5f15\u5165MuSGD\u4f18\u5316\u5668\u7a33\u5b9a\u8f7b\u91cf\u7ea7\u9aa8\u5e72\u7f51\u7edc\uff0cSTAL\u7528\u4e8e\u5c0f\u76ee\u6807\u611f\u77e5\u5206\u914d\uff0cProgLoss\u5b9e\u73b0\u52a8\u6001\u76d1\u7763\uff0c\u5b8c\u5168\u6d88\u9664NMS\u540e\u5904\u7406\u3002", "result": "YOLO26\u5728\u5b98\u65b9\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5305\u62ecRTMDet\u548cDAMO-YOLO\u5728\u5185\u7684\u591a\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u901f\u5ea6\u548c\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u542f\u53d1\u5f0f\u540e\u5904\u7406\uff0cYOLO26\u6210\u529f\u89e3\u51b3\u4e86\u5ef6\u8fdf\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5386\u53f2\u6027\u6743\u8861\uff0c\u6807\u5fd7\u7740\u8fb9\u7f18\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u4e0b\u4e00\u8fdb\u5316\u6b65\u9aa4\u3002"}}
{"id": "2601.13742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13742", "abs": "https://arxiv.org/abs/2601.13742", "authors": ["Arjun Chandra", "Kevin Miller", "Venkatesh Ravichandran", "Constantinos Papayiannis", "Venkatesh Saligrama"], "title": "Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues", "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.", "AI": {"tldr": "TRACE\u6846\u67b6\u8ba9LLM\u80fd\u901a\u8fc7\u97f3\u9891\u7ebf\u7d22\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u8bc4\u4f30\uff0c\u8d85\u8d8a\u73b0\u6709ALM\u548c\u4ec5\u6587\u672cLLM\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u8bed\u97f3\u5230\u8bed\u97f3\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u900f\u660e\u4e14\u6602\u8d35\u7684\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u800cLLM\u6cd5\u5b98\u867d\u7136\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u4ec5\u9650\u4e8e\u6587\u672c\u5185\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTRACE\u6846\u67b6\uff1a1) \u5f15\u5165\u4eba\u7c7b\u601d\u7ef4\u94fe\u6807\u6ce8\u534f\u8bae\uff0c\u5c06\u8bc4\u4f30\u5206\u4e3a\u5185\u5bb9\u3001\u8bed\u97f3\u8d28\u91cf\u548c\u526f\u8bed\u8a00\u4e09\u4e2a\u7ef4\u5ea6\uff1b2) \u6784\u5efa\u97f3\u9891\u4fe1\u53f7\u7684\u6587\u672c\u84dd\u56fe\uff1b3) \u63d0\u793aLLM\u8fdb\u884c\u7ef4\u5ea6\u5224\u65ad\uff1b4) \u901a\u8fc7\u786e\u5b9a\u6027\u7b56\u7565\u878d\u5408\u4e3a\u603b\u4f53\u8bc4\u5206\u3002", "result": "TRACE\u5728\u4eba\u7c7b\u8bc4\u5206\u8005\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u4ec5\u6587\u672cLLM\u6cd5\u5b98\uff0c\u540c\u65f6\u6210\u672c\u663e\u8457\u66f4\u4f4e\u3002", "conclusion": "TRACE\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bed\u97f3\u5230\u8bed\u97f3\u8bc4\u4f30\uff0c\u5c06\u53d1\u5e03\u4eba\u7c7b\u601d\u7ef4\u94fe\u6807\u6ce8\u548cTRACE\u6846\u67b6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2601.12889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12889", "abs": "https://arxiv.org/abs/2601.12889", "authors": ["Nazibul Basar Ayon", "Abdul Hasib", "Md. Faishal Ahmed", "Md. Sadiqur Rahman", "Kamrul Islam", "T. M. Mehrab Hasan", "A. S. M. Ahsanul Sarkar Akib"], "title": "Simultaneous Detection of LSD and FMD in Cattle Using Ensemble Deep Learning", "comment": null, "summary": "Lumpy Skin Disease (LSD) and Foot-and-Mouth Disease (FMD) are highly contagious viral diseases affecting cattle, causing significant economic losses and welfare challenges. Their visual diagnosis is complicated by significant symptom overlap with each other and with benign conditions like insect bites or chemical burns, hindering timely control measures. Leveraging a comprehensive dataset of 10,516 expert-annotated images from 18 farms across India, Brazil, and the USA, this study presents a novel Ensemble Deep Learning framework integrating VGG16, ResNet50, and InceptionV3 with optimized weighted averaging for simultaneous LSD and FMD detection. The model achieves a state-of-the-art accuracy of 98.2\\%, with macro-averaged precision of 98.2\\%, recall of 98.1\\%, F1-score of 98.1\\%, and an AUC-ROC of 99.5\\%. This approach uniquely addresses the critical challenge of symptom overlap in multi-disease detection, enabling early, precise, and automated diagnosis. This tool has the potential to enhance disease management, support global agricultural sustainability, and is designed for future deployment in resource-limited settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528VGG16\u3001ResNet50\u548cInceptionV3\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u52a0\u6743\u5e73\u5747\u5b9e\u73b0\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u548c\u53e3\u8e44\u75ab\u7684\u540c\u65f6\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe98.2%\u3002", "motivation": "\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\uff08LSD\uff09\u548c\u53e3\u8e44\u75ab\uff08FMD\uff09\u662f\u9ad8\u5ea6\u4f20\u67d3\u6027\u75c5\u6bd2\u75be\u75c5\uff0c\u9020\u6210\u91cd\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u5b83\u4eec\u7684\u89c6\u89c9\u8bca\u65ad\u56e0\u75c7\u72b6\u91cd\u53e0\uff08\u5f7c\u6b64\u4e4b\u95f4\u4ee5\u53ca\u4e0e\u826f\u6027\u75c5\u75c7\u5982\u6606\u866b\u53ee\u54ac\u6216\u5316\u5b66\u70e7\u4f24\uff09\u800c\u590d\u6742\u5316\uff0c\u963b\u788d\u4e86\u53ca\u65f6\u63a7\u5236\u63aa\u65bd\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u5370\u5ea6\u3001\u5df4\u897f\u548c\u7f8e\u56fd18\u4e2a\u519c\u573a\u768410,516\u5f20\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408VGG16\u3001ResNet50\u548cInceptionV3\u6a21\u578b\uff0c\u91c7\u7528\u4f18\u5316\u52a0\u6743\u5e73\u5747\u65b9\u6cd5\u8fdb\u884cLSD\u548cFMD\u7684\u540c\u65f6\u68c0\u6d4b\u3002", "result": "\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u768498.2%\u51c6\u786e\u7387\uff0c\u5b8f\u5e73\u5747\u7cbe\u786e\u5ea6\u4e3a98.2%\uff0c\u53ec\u56de\u7387\u4e3a98.1%\uff0cF1\u5206\u6570\u4e3a98.1%\uff0cAUC-ROC\u4e3a99.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u72ec\u7279\u5730\u89e3\u51b3\u4e86\u591a\u75be\u75c5\u68c0\u6d4b\u4e2d\u75c7\u72b6\u91cd\u53e0\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u3001\u7cbe\u786e\u548c\u81ea\u52a8\u5316\u8bca\u65ad\uff0c\u6709\u6f5c\u529b\u589e\u5f3a\u75be\u75c5\u7ba1\u7406\uff0c\u652f\u6301\u5168\u7403\u519c\u4e1a\u53ef\u6301\u7eed\u6027\uff0c\u5e76\u8bbe\u8ba1\u7528\u4e8e\u672a\u6765\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002"}}
{"id": "2601.13749", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13749", "abs": "https://arxiv.org/abs/2601.13749", "authors": ["Benaya Trabelsi", "Jonathan Shaki", "Sarit Kraus"], "title": "Pro-AI Bias in Large Language Models", "comment": "13 pages, 6 figures. Code available at: https://github.com/benayat/Pro-AI-bias-in-LLMs", "summary": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.", "AI": {"tldr": "LLMs\u663e\u793a\u7cfb\u7edf\u6027\u504f\u597dAI\u7684\u504f\u89c1\uff1a\u5728\u5efa\u8bae\u4e2d\u8fc7\u5ea6\u63a8\u8350AI\u9009\u9879\uff0c\u9ad8\u4f30AI\u76f8\u5173\u804c\u4f4d\u85aa\u8d44\uff0c\u4e14\"\u4eba\u5de5\u667a\u80fd\"\u5728\u8868\u5f81\u4e2d\u5177\u6709\u4e2d\u5fc3\u6027", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u5b58\u5728\u5bf9\u4eba\u5de5\u667a\u80fd\u672c\u8eab\u7684\u7cfb\u7edf\u6027\u504f\u597d\u504f\u89c1\uff0c\u8fd9\u5bf9\u9ad8\u98ce\u9669\u7684\u51b3\u7b56\u573a\u666f\u6709\u91cd\u8981\u5f71\u54cd", "method": "\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u5b9e\u9a8c\uff1a1)\u5206\u6790LLMs\u5bf9\u54a8\u8be2\u67e5\u8be2\u7684AI\u76f8\u5173\u9009\u9879\u63a8\u8350\uff1b2)\u6bd4\u8f83AI\u4e0e\u975eAI\u804c\u4f4d\u7684\u85aa\u8d44\u4f30\u8ba1\uff1b3)\u63a2\u7d22\u5f00\u6e90\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e2d\"\u4eba\u5de5\u667a\u80fd\"\u7684\u76f8\u4f3c\u6027", "result": "\u53d1\u73b0\u4e00\u81f4\u7684pro-AI\u504f\u89c1\uff1a\u4e13\u6709\u6a21\u578b\u51e0\u4e4e\u786e\u5b9a\u6027\u5730\u63a8\u8350AI\u9009\u9879\uff1b\u9ad8\u4f30AI\u85aa\u8d4410\u4e2a\u767e\u5206\u70b9\uff1b\"\u4eba\u5de5\u667a\u80fd\"\u5728\u5404\u79cd\u6846\u67b6\u4e0b\u90fd\u663e\u793a\u6700\u9ad8\u7684\u8868\u5f81\u4e2d\u5fc3\u6027", "conclusion": "LLM\u751f\u6210\u7684\u5efa\u8bae\u548c\u4f30\u503c\u53ef\u80fd\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7cfb\u7edf\u6027\u626d\u66f2\u9009\u62e9\u548c\u8ba4\u77e5\uff0c\u9700\u8981\u8b66\u60d5\u8fd9\u79cd\u504f\u89c1"}}
{"id": "2601.12895", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12895", "abs": "https://arxiv.org/abs/2601.12895", "authors": ["Chan Naseeb", "Adeel Ashraf Cheema", "Hassan Sami", "Tayyab Afzal", "Muhammad Omair", "Usman Habib"], "title": "TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation, Detection and Localization in Identity Documents", "comment": "8 pages", "summary": "The proliferation of sophisticated generative AI models has significantly escalated the threat of synthetic manipulations in identity documents, particularly through face swapping and text inpainting attacks. This paper presents TwoHead-SwinFPN, a unified deep learning architecture that simultaneously performs binary classification and precise localization of manipulated regions in ID documents. Our approach integrates a Swin Transformer backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced with Convolutional Block Attention Module (CBAM) for improved feature representation. The model employs a dual-head architecture for joint optimization of detection and segmentation tasks, utilizing uncertainty-weighted multi-task learning. Extensive experiments on the FantasyIDiap dataset demonstrate superior performance with 84.31\\% accuracy, 90.78\\% AUC for classification, and 57.24\\% mean Dice score for localization. The proposed method achieves an F1-score of 88.61\\% for binary classification while maintaining computational efficiency suitable for real-world deployment through FastAPI implementation. Our comprehensive evaluation includes ablation studies, cross-device generalization analysis, and detailed performance assessment across 10 languages and 3 acquisition devices.", "AI": {"tldr": "TwoHead-SwinFPN\uff1a\u4e00\u4e2a\u7528\u4e8e\u8eab\u4efd\u8bc1\u4ef6\u9632\u4f2a\u7684\u7edf\u4e00\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u540c\u65f6\u8fdb\u884c\u7be1\u6539\u68c0\u6d4b\u548c\u533a\u57df\u5b9a\u4f4d\uff0c\u5728FantasyIDiap\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u666e\u53ca\uff0c\u8eab\u4efd\u8bc1\u4ef6\u9762\u4e34\u4eba\u8138\u66ff\u6362\u548c\u6587\u672c\u4fee\u590d\u7b49\u5408\u6210\u7be1\u6539\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u67b6\u6784\uff0c\u96c6\u6210Swin Transformer\u9aa8\u5e72\u7f51\u7edc\u3001\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u548cUNet\u98ce\u683c\u89e3\u7801\u5668\uff0c\u91c7\u7528CBAM\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u7528\u53cc\u5934\u67b6\u6784\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u5728FantasyIDiap\u6570\u636e\u96c6\u4e0a\u53d6\u5f9784.31%\u51c6\u786e\u7387\u300190.78% AUC\uff08\u5206\u7c7b\uff09\u548c57.24%\u5e73\u5747Dice\u5206\u6570\uff08\u5b9a\u4f4d\uff09\uff0cF1\u5206\u657088.61%\uff0c\u8ba1\u7b97\u6548\u7387\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "TwoHead-SwinFPN\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u5b9a\u4f4d\u8eab\u4efd\u8bc1\u4ef6\u4e2d\u7684\u5408\u6210\u7be1\u6539\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u548c\u591a\u8bbe\u5907\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.13802", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.13802", "abs": "https://arxiv.org/abs/2601.13802", "authors": ["Yushen Chen", "Junzhe Liu", "Yujie Tu", "Zhikang Niu", "Yuzhe Liang", "Kai Yu", "Chunyu Qiang", "Chen Zhang", "Xie Chen"], "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis", "comment": null, "summary": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .", "AI": {"tldr": "Habibi\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u7684\u7edf\u4e00\u5efa\u6a21\u5957\u4ef6\uff0c\u5229\u7528\u73b0\u6709ASR\u8bed\u6599\u5e93\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u652f\u6301\u591a\u79cd\u65b9\u8a00\uff0c\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41\u5546\u4e1a\u670d\u52a1\uff0c\u65e0\u9700\u6587\u672c\u97f3\u6807\u6807\u6ce8\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u7814\u7a76\u5b58\u5728\u660e\u663e\u7a7a\u767d\uff0c\u7f3a\u4e4f\u7edf\u4e00\u5efa\u6a21\u65b9\u6cd5\u3002\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u7684\u8bed\u8a00\u590d\u6742\u6027\u9ad8\uff0c\u52a0\u4e0a\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u3001\u57fa\u51c6\u548c\u8bc4\u4f30\u6307\u5357\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u5f00\u5c55\u76f8\u5173\u5de5\u4f5c\u3002", "method": "\u5229\u7528\u73b0\u6709\u5f00\u6e90ASR\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u8bed\u8a00\u5b66\u6307\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u6784\u5efa\u4e13\u95e8\u7edf\u4e00\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\uff0c\u652f\u6301\u4ece\u9ad8\u8d44\u6e90\u5230\u4f4e\u8d44\u6e90\u7684\u591a\u79cd\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\uff0c\u65e0\u9700\u6587\u672c\u97f3\u6807\u6807\u6ce8\uff0c\u5e76\u5177\u5907\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6269\u5c55\u6027\u3002", "result": "Habibi\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u9886\u5148\u7684\u5546\u4e1a\u670d\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u3002\u4f5c\u8005\u627f\u8bfa\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u521b\u5efa\u9996\u4e2a\u7cfb\u7edf\u6027\u7684\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u5408\u6210\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u6311\u6218\u548c\u5efa\u7acb\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2601.12919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12919", "abs": "https://arxiv.org/abs/2601.12919", "authors": ["Jun Wan", "Yuanzhi Yao", "Zhihui Lai", "Jie Zhou", "Xianxu Hou", "Wenwen Min"], "title": "Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection", "comment": null, "summary": "High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51faSHT\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u8138\u5e7b\u89c9\u548c\u59ff\u6001\u8f6c\u6362\u4efb\u52a1\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u56fe\u50cf\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u56fe\u50cf\u6216\u7279\u5f81\u538b\u7f29\u4f1a\u964d\u4f4e\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u548c\u6807\u6ce8\u4e0d\u7cbe\u786e\u8fdb\u4e00\u6b65\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5f31\u76d1\u7763\u65b9\u6cd5", "method": "\u63d0\u51faSHT\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u76f8\u4e92\u589e\u5f3a\u6a21\u5757\uff1aDHLN\u901a\u8fc7\u4eba\u8138\u5e7b\u89c9\u4efb\u52a1\u5b66\u4e60\u9ad8\u5206\u8fa8\u7387\u8868\u793a\uff0cFPTN\u901a\u8fc7\u59ff\u6001\u8f6c\u6362\u8fdb\u4e00\u6b65\u4f18\u5316\u70ed\u56fe\u548c\u5e7b\u89c9\u4eba\u8138", "result": "\u5728\u4eba\u8138\u5e7b\u89c9\u548c\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f", "conclusion": "\u9996\u6b21\u63a2\u7d22\u901a\u8fc7\u6574\u5408\u4eba\u8138\u5e7b\u89c9\u548c\u9762\u90e8\u59ff\u6001\u8f6c\u6362\u4efb\u52a1\u7684\u5f31\u76d1\u7763\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\uff0cSHT\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u4eba\u8138\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6"}}
{"id": "2601.13806", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13806", "abs": "https://arxiv.org/abs/2601.13806", "authors": ["Dezhao Song", "Guglielmo Bonifazi", "Frank Schilder", "Jonathan Richard Schwarz"], "title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning", "comment": null, "summary": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.", "AI": {"tldr": "\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7IRAC\u6846\u67b6\u6784\u5efa\u6cd5\u5f8b\u6848\u4f8b\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408SFT\u548cDPO\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u548c\u4eba\u7c7b\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u9886\u57df\u77e5\u8bc6\u7ed3\u6784\u7684\u6355\u6349\uff0c\u5bfc\u81f4\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u7279\u522b\u662f\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\u5982\u6cd5\u5f8b\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u6cd5\u5f8b\u63a8\u7406\u9700\u8981\u6df1\u5165\u7406\u89e3\u5404\u79cd\u6cd5\u5f8b\u6982\u5ff5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8fd9\u662f\u5f53\u524dLLM\u540e\u8bad\u7ec3\u7f3a\u5931\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002", "method": "1. \u91c7\u7528IRAC\uff08Issue, Rule, Analysis, Conclusion\uff09\u6846\u67b6\u5efa\u6a21\u5173\u952e\u6cd5\u5f8b\u6982\u5ff5\uff1b2. \u6784\u5efa\u5305\u542b12K\u6cd5\u5f8b\u6848\u4f8b\u7684\u77e5\u8bc6\u56fe\u8c31\uff1b3. \u4f7f\u7528IRAC\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff1b4. \u5bf9\u4e09\u4e2aSOTA LLM\uff0830B\u300149B\u300170B\uff09\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u3002", "result": "1. \u540e\u8bad\u7ec3\u6a21\u578b\u57284/5\u4e2a\u591a\u6837\u5316\u6cd5\u5f8b\u57fa\u51c6\u6d4b\u8bd5\uff0814\u4e2a\u4efb\u52a1\uff09\u4e0a\u83b7\u5f97\u6bd4\u57fa\u7ebf\u66f4\u597d\u7684\u5e73\u5747\u6027\u80fd\uff1b2. 70B DPO\u6a21\u578b\u57284/6\u4e2a\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u4f73\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\u548c141B SOTA\u6cd5\u5f8bLLM\uff1b3. \u8bc1\u660e\u4e86\u77e5\u8bc6\u56fe\u8c31\u5728\u589e\u5f3aLLM\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3aLLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6cd5\u5f8b\u9886\u57df\u3002\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\uff0c\u4e3a\u89e3\u51b3LLM\u5728\u590d\u6742\u4e13\u4e1a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.12926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12926", "abs": "https://arxiv.org/abs/2601.12926", "authors": ["Jun Wan", "Jun Liu", "Zhihui lai", "Jie Zhou"], "title": "Dual-Stream Collaborative Transformer for Image Captioning", "comment": null, "summary": "Current region feature-based image captioning methods have progressed rapidly and achieved remarkable performance. However, they are still prone to generating irrelevant descriptions due to the lack of contextual information and the over-reliance on generated partial descriptions for predicting the remaining words. In this paper, we propose a Dual-Stream Collaborative Transformer (DSCT) to address this issue by introducing the segmentation feature. The proposed DSCT consolidates and then fuses the region and segmentation features to guide the generation of caption sentences. It contains multiple Pattern-Specific Mutual Attention Encoders (PSMAEs) and Dynamic Nomination Decoders (DNDs). The PSMAE effectively highlights and consolidates the private information of two representations by querying each other. The DND dynamically searches for the most relevant learning blocks to the input textual representations and exploits the homogeneous features between the consolidated region and segmentation features to generate more accurate and descriptive caption sentences. To the best of our knowledge, this is the first study to explore how to fuse different pattern-specific features in a dynamic way to bypass their semantic inconsistencies and spatial misalignment issues for image captioning. The experimental results from popular benchmark datasets demonstrate that our DSCT outperforms the state-of-the-art image captioning models in the literature.", "AI": {"tldr": "\u63d0\u51faDSCT\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u533a\u57df\u7279\u5f81\u548c\u5206\u5272\u7279\u5f81\uff0c\u89e3\u51b3\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u548c\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u5206\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u533a\u57df\u7279\u5f81\u7684\u56fe\u50cf\u63cf\u8ff0\u65b9\u6cd5\u867d\u7136\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u4ecd\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b2\uff09\u8fc7\u5ea6\u4f9d\u8d56\u5df2\u751f\u6210\u7684\u90e8\u5206\u63cf\u8ff0\u6765\u9884\u6d4b\u540e\u7eed\u8bcd\u6c47\uff0c\u5bfc\u81f4\u751f\u6210\u4e0d\u76f8\u5173\u7684\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u534f\u4f5cTransformer\uff08DSCT\uff09\uff0c\u5f15\u5165\u5206\u5272\u7279\u5f81\u5e76\u4e0e\u533a\u57df\u7279\u5f81\u878d\u5408\u3002\u5305\u542b\u6a21\u5f0f\u7279\u5b9a\u4e92\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff08PSMAE\uff09\u548c\u52a8\u6001\u63d0\u540d\u89e3\u7801\u5668\uff08DND\uff09\u3002PSMAE\u901a\u8fc7\u76f8\u4e92\u67e5\u8be2\u7a81\u51fa\u548c\u6574\u5408\u4e24\u79cd\u8868\u793a\u7684\u79c1\u6709\u4fe1\u606f\uff1bDND\u52a8\u6001\u641c\u7d22\u4e0e\u8f93\u5165\u6587\u672c\u8868\u793a\u6700\u76f8\u5173\u7684\u5b66\u4e60\u5757\uff0c\u5e76\u5229\u7528\u6574\u5408\u540e\u7684\u533a\u57df\u548c\u5206\u5272\u7279\u5f81\u4e4b\u95f4\u7684\u540c\u8d28\u7279\u5f81\u6765\u751f\u6210\u66f4\u51c6\u786e\u7684\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u6d41\u884c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSCT\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u63a2\u7d22\u5982\u4f55\u4ee5\u52a8\u6001\u65b9\u5f0f\u878d\u5408\u4e0d\u540c\u6a21\u5f0f\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u7ed5\u8fc7\u5176\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u63cf\u8ff0\u6027\u7684\u53e5\u5b50\u3002"}}
{"id": "2601.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13835", "abs": "https://arxiv.org/abs/2601.13835", "authors": ["Sam OConnor Russell", "Delphine Charuau", "Naomi Harte"], "title": "The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations", "comment": "Accepted to ICASSP 2026", "summary": "Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u58f0\u7801\u5668\u65b9\u6cd5\u63a7\u5236\u8bed\u97f3\u4e2d\u7684\u97f5\u5f8b\u548c\u8bcd\u6c47\u7ebf\u7d22\uff0c\u63a2\u7a76S3R\u57fa\u7840\u7684\u4ea4\u53e0\u9884\u6d4b\u6a21\u578b\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u7ebf\u7d22\uff0c\u53d1\u73b0\u97f5\u5f8b\u548c\u8bcd\u6c47\u7ebf\u7d22\u90fd\u80fd\u72ec\u7acb\u652f\u6301\u4ea4\u53e0\u9884\u6d4b\uff0c\u672a\u6765\u6a21\u578b\u53ef\u80fd\u4ec5\u9700\u97f5\u5f8b\u7ebf\u7d22\u5373\u53ef\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\uff0c\u6d41\u7545\u7684\u4ea4\u53e0\u9884\u6d4b\u662f\u5173\u952e\u6311\u6218\u3002\u867d\u7136\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\uff08S3Rs\uff09\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u57fa\u4e8eS3R\u7684\u4ea4\u53e0\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u97f5\u5f8b\u7ebf\u7d22\u3001\u8bcd\u6c47\u7ebf\u7d22\u8fd8\u662f\u4e24\u8005\u517c\u6709\u3002\u9700\u8981\u66f4\u6e05\u6670\u7684\u65b9\u6cd5\u6765\u5206\u79bb\u548c\u63a7\u5236\u8fd9\u4e9b\u7ebf\u7d22\u4ee5\u6df1\u5165\u7406\u89e3\u6a21\u578b\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u58f0\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u66f4\u6e05\u6670\u5730\u63a7\u5236\u8bed\u97f3\u4e2d\u7684\u97f5\u5f8b\u548c\u8bcd\u6c47\u7ebf\u7d22\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u63a2\u6d4b\u57fa\u4e8eS3R\u7684\u4ea4\u53e0\u9884\u6d4b\u6a21\u578b\uff08voice-activity projection model\uff09\uff0c\u5728\u97f5\u5f8b\u5339\u914d\u4f46\u4e0d\u53ef\u7406\u89e3\u7684\u566a\u58f0\u4e0a\u8fdb\u884c\u9884\u6d4b\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u6e05\u6670\u8bed\u97f3\u7684\u51c6\u786e\u6027\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u97f5\u5f8b\u5339\u914d\u7684\u4e0d\u53ef\u7406\u89e3\u566a\u58f0\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u6e05\u6670\u8bed\u97f3\u76f8\u4f3c\u3002\u8fd9\u8868\u660e\u97f5\u5f8b\u548c\u8bcd\u6c47\u7ebf\u7d22\u90fd\u80fd\u652f\u6301\u4ea4\u53e0\u9884\u6d4b\uff0c\u4e14\u4efb\u4e00\u7ebf\u7d22\u90fd\u53ef\u72ec\u7acb\u4f7f\u7528\u3002\u5f53\u97f5\u5f8b\u6216\u8bcd\u6c47\u4fe1\u606f\u88ab\u7834\u574f\u65f6\uff0c\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5229\u7528\u53e6\u4e00\u7ebf\u7d22\uff0c\u8868\u660e\u5b83\u4eec\u5728S3Rs\u4e2d\u7684\u7f16\u7801\u5177\u6709\u6709\u9650\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u7ed3\u679c\u5728CPC-based\u548cwav2vec2.0 S3Rs\u4e2d\u4e00\u81f4\u3002", "conclusion": "\u97f5\u5f8b\u548c\u8bcd\u6c47\u7ebf\u7d22\u90fd\u80fd\u72ec\u7acb\u652f\u6301\u4ea4\u53e0\u9884\u6d4b\uff0c\u672a\u6765\u6a21\u578b\u53ef\u80fd\u4ec5\u9700\u97f5\u5f8b\u7ebf\u7d22\u5373\u53ef\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u6f5c\u5728\u6027\u80fd\u63d0\u5347\u3002\u4e24\u79cd\u7ebf\u7d22\u5728S3Rs\u4e2d\u7684\u7f16\u7801\u76f8\u5bf9\u72ec\u7acb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002\u6240\u6709\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2601.12929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12929", "abs": "https://arxiv.org/abs/2601.12929", "authors": ["Gonzalo Mancera", "Daniel DeAlcala", "Aythami Morales", "Ruben Tolosana", "Julian Fierrez"], "title": "Membership Inference Test: Auditing Training Data in Object Classification Models", "comment": "Deployable AI (DAI 2025) workshop co-located with AAAI-25", "summary": "In this research, we analyze the performance of Membership Inference Tests (MINT), focusing on determining whether given data were utilized during the training phase, specifically in the domain of object recognition. Within the area of object recognition, we propose and develop architectures tailored for MINT models. These architectures aim to optimize performance and efficiency in data utilization, offering a tailored solution to tackle the complexities inherent in the object recognition domain. We conducted experiments involving an object detection model, an embedding extractor, and a MINT module. These experiments were performed in three public databases, totaling over 174K images. The proposed architecture leverages convolutional layers to capture and model the activation patterns present in the data during the training process. Through our analysis, we are able to identify given data used for testing and training, achieving precision rates ranging between 70% and 80%, contingent upon the depth of the detection module layer chosen for input to the MINT module. Additionally, our studies entail an analysis of the factors influencing the MINT Module, delving into the contributing elements behind more transparent training processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u76ee\u6807\u8bc6\u522b\u9886\u57df\u7684\u6210\u5458\u63a8\u65ad\u6d4b\u8bd5\uff08MINT\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u5377\u79ef\u5c42\u5efa\u6a21\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b070-80%\u7684\u7cbe\u5ea6\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cdMINT\u6a21\u5757\u6027\u80fd\u7684\u56e0\u7d20\u3002", "motivation": "\u5728\u76ee\u6807\u8bc6\u522b\u9886\u57df\u4e2d\uff0c\u9700\u8981\u786e\u5b9a\u7279\u5b9a\u6570\u636e\u662f\u5426\u88ab\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\uff08\u6210\u5458\u63a8\u65ad\u6d4b\u8bd5\uff09\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u8bad\u7ec3\u900f\u660e\u5ea6\u548c\u6570\u636e\u4f7f\u7528\u60c5\u51b5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9\u76ee\u6807\u8bc6\u522b\u9886\u57df\u7684MINT\u67b6\u6784\uff0c\u5305\u542b\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3001\u5d4c\u5165\u63d0\u53d6\u5668\u548cMINT\u6a21\u5757\u3002\u5229\u7528\u5377\u79ef\u5c42\u6355\u83b7\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5728\u4e0d\u540c\u6df1\u5ea6\u7684\u68c0\u6d4b\u6a21\u5757\u5c42\u4f5c\u4e3aMINT\u8f93\u5165\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u5e93\uff08\u603b\u8ba1\u8d85\u8fc7174K\u56fe\u50cf\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u8bc6\u522b\u6d4b\u8bd5\u548c\u8bad\u7ec3\u6570\u636e\uff0c\u7cbe\u5ea6\u8fbe\u523070-80%\uff0c\u5177\u4f53\u7cbe\u5ea6\u53d6\u51b3\u4e8e\u9009\u62e9\u7684\u68c0\u6d4b\u6a21\u5757\u5c42\u6df1\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684MINT\u67b6\u6784\u5728\u76ee\u6807\u8bc6\u522b\u9886\u57df\u6709\u6548\uff0c\u80fd\u591f\u4ee5\u53ef\u63a5\u53d7\u7684\u7cbe\u5ea6\u8fdb\u884c\u6210\u5458\u63a8\u65ad\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5f71\u54cdMINT\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bad\u7ec3\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2601.13836", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.13836", "abs": "https://arxiv.org/abs/2601.13836", "authors": ["Qian Chen", "Jinlan Fu", "Changsong Li", "See-Kiong Ng", "Xipeng Qiu"], "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "comment": "https://openmoss.github.io/FutureOmni", "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "AI": {"tldr": "FutureOmni\uff1a\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u97f3\u89c6\u9891\u73af\u5883\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b919\u4e2a\u89c6\u9891\u548c1034\u4e2aQA\u5bf9\uff0c\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u63d0\u51faOFF\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u591a\u6a21\u6001\u611f\u77e5\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4ece\u97f3\u89c6\u9891\u7ebf\u7d22\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u65b9\u9762\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u56de\u987e\u6027\u7406\u89e3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u6784\u5efaFutureOmni\u57fa\u51c6\uff1a\u901a\u8fc7\u53ef\u6269\u5c55\u7684LLM\u8f85\u52a9\u3001\u4eba\u5728\u56de\u8def\u6d41\u7a0b\u521b\u5efa\uff0c\u5305\u542b919\u4e2a\u89c6\u9891\u548c1034\u4e2a\u591a\u9879\u9009\u62e9QA\u5bf9\uff0c\u6db5\u76d68\u4e2a\u4e3b\u8981\u9886\u57df\uff1b2. \u63d0\u51faOmni-Modal Future Forecasting (OFF)\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u6784\u5efa7K\u6837\u672c\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f3013\u4e2a\u591a\u6a21\u6001\u548c7\u4e2a\u7eaf\u89c6\u9891\u6a21\u578b\u53d1\u73b0\uff0c\u5f53\u524d\u7cfb\u7edf\u5728\u97f3\u89c6\u9891\u672a\u6765\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u8bed\u97f3\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\uff0cGemini 3 Flash\u6700\u4f73\u51c6\u786e\u7387\u4ec564.8%\u3002OFF\u7b56\u7565\u5728FutureOmni\u548c\u6d41\u884c\u97f3\u89c6\u9891\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u672a\u6765\u9884\u6d4b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u89c6\u9891\u672a\u6765\u9884\u6d4b\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0cFutureOmni\u57fa\u51c6\u586b\u8865\u4e86\u8be5\u9886\u57df\u7a7a\u767d\uff0c\u63d0\u51fa\u7684OFF\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u80fd\u529b\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.12936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12936", "abs": "https://arxiv.org/abs/2601.12936", "authors": ["Tianran Ouyang", "Xingping Dong", "Jing Zhang", "Mang Ye", "Jun Chen", "Bo Du"], "title": "QASA: Quality-Guided K-Adaptive Slot Attention for Unsupervised Object-Centric Learning", "comment": null, "summary": "Slot Attention, an approach that binds different objects in a scene to a set of \"slots\", has become a leading method in unsupervised object-centric learning. Most methods assume a fixed slot count K, and to better accommodate the dynamic nature of object cardinality, a few works have explored K-adaptive variants. However, existing K-adaptive methods still suffer from two limitations. First, they do not explicitly constrain slot-binding quality, so low-quality slots lead to ambiguous feature attribution. Second, adding a slot-count penalty to the reconstruction objective creates conflicting optimization goals between reducing the number of active slots and maintaining reconstruction fidelity. As a result, they still lag significantly behind strong K-fixed baselines. To address these challenges, we propose Quality-Guided K-Adaptive Slot Attention (QASA). First, we decouple slot selection from reconstruction, eliminating the mutual constraints between the two objectives. Then, we propose an unsupervised Slot-Quality metric to assess per-slot quality, providing a principled signal for fine-grained slot--object binding. Based on this metric, we design a Quality-Guided Slot Selection scheme that dynamically selects a subset of high-quality slots and feeds them into our newly designed gated decoder for reconstruction during training. At inference, token-wise competition on slot attention yields a K-adaptive outcome. Experiments show that QASA substantially outperforms existing K-adaptive methods on both real and synthetic datasets. Moreover, on real-world datasets QASA surpasses K-fixed methods.", "AI": {"tldr": "QASA\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf\u5f15\u5bfc\u7684K\u81ea\u9002\u5e94\u69fd\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u69fd\u9009\u62e9\u4e0e\u91cd\u5efa\uff0c\u5f15\u5165\u65e0\u76d1\u7763\u69fd\u8d28\u91cf\u5ea6\u91cf\uff0c\u52a8\u6001\u9009\u62e9\u9ad8\u8d28\u91cf\u69fd\u8fdb\u884c\u91cd\u5efa\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709K\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709K\u81ea\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6ca1\u6709\u660e\u786e\u7ea6\u675f\u69fd\u7ed1\u5b9a\u8d28\u91cf\uff0c\u5bfc\u81f4\u4f4e\u8d28\u91cf\u69fd\u4ea7\u751f\u6a21\u7cca\u7279\u5f81\u5f52\u5c5e\uff1b2\uff09\u5728\u91cd\u5efa\u76ee\u6807\u4e2d\u6dfb\u52a0\u69fd\u6570\u91cf\u60e9\u7f5a\u4f1a\u9020\u6210\u51cf\u5c11\u6d3b\u8dc3\u69fd\u6570\u91cf\u4e0e\u4fdd\u6301\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u4f18\u5316\u76ee\u6807\u51b2\u7a81\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u843d\u540e\u4e8e\u56fa\u5b9aK\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "method": "1\uff09\u89e3\u8026\u69fd\u9009\u62e9\u4e0e\u91cd\u5efa\uff0c\u6d88\u9664\u4e24\u4e2a\u76ee\u6807\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\uff1b2\uff09\u63d0\u51fa\u65e0\u76d1\u7763\u69fd\u8d28\u91cf\u5ea6\u91cf\u6765\u8bc4\u4f30\u6bcf\u4e2a\u69fd\u7684\u8d28\u91cf\uff1b3\uff09\u8bbe\u8ba1\u8d28\u91cf\u5f15\u5bfc\u7684\u69fd\u9009\u62e9\u65b9\u6848\uff0c\u52a8\u6001\u9009\u62e9\u9ad8\u8d28\u91cf\u69fd\u5b50\u96c6\uff1b4\uff09\u4f7f\u7528\u65b0\u8bbe\u8ba1\u7684\u95e8\u63a7\u89e3\u7801\u5668\u8fdb\u884c\u91cd\u5efa\u8bad\u7ec3\uff1b5\uff09\u63a8\u7406\u65f6\u901a\u8fc7\u69fd\u6ce8\u610f\u529b\u4e0a\u7684token-wise\u7ade\u4e89\u5b9e\u73b0K\u81ea\u9002\u5e94\u3002", "result": "QASA\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684K\u81ea\u9002\u5e94\u65b9\u6cd5\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cQASA\u751a\u81f3\u8d85\u8d8a\u4e86\u56fa\u5b9aK\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u69fd\u9009\u62e9\u4e0e\u91cd\u5efa\uff0c\u5e76\u5f15\u5165\u8d28\u91cf\u5f15\u5bfc\u7684\u69fd\u9009\u62e9\u673a\u5236\uff0cQASA\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709K\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u5bf9\u8c61\u7ed1\u5b9a\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u52a8\u6001\u69fd\u6570\u91cf\u9002\u5e94\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2601.13876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13876", "abs": "https://arxiv.org/abs/2601.13876", "authors": ["Unggi Lee", "Jahyun Jeong", "Sunyoung Shin", "Haeun Park", "Jeongsu Moon", "Youngchang Song", "Jaechang Shim", "JaeHwan Lee", "Yunju Noh", "Seungwon Choi", "Ahhyun Kim", "TaeHyeon Kim", "Kyungtae Joo", "Taeyeong Kim", "Gyeonggeon Lee"], "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education", "comment": null, "summary": "Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \\textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.", "AI": {"tldr": "\u63d0\u51faPedagogical VLA Framework\uff0c\u901a\u8fc7\u6559\u5b66\u5bf9\u9f50\u4f7f\u8f7b\u91cf\u7ea7VLA\u6a21\u578b\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6559\u80b2\u73af\u5883\u4e2d\u751f\u6210\u89e3\u91ca\u6027\u79d1\u5b66\u6f14\u793a\u3002", "motivation": "\u79d1\u5b66\u6f14\u793a\u5bf9STEM\u6559\u80b2\u5f88\u91cd\u8981\uff0c\u4f46\u6559\u5e08\u9762\u4e34\u5b89\u5168\u548c\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002\u5f53\u524dVLA\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e14\u727a\u7272\u8bed\u8a00\u751f\u6210\u80fd\u529b\uff0c\u4e0d\u9002\u5408\u9700\u8981\u53ef\u89e3\u91ca\u7cfb\u7edf\u7684\u6559\u80b2\u73af\u5883\u3002", "method": "\u63d0\u51fa\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\u7684\u6559\u5b66VLA\u6846\u67b6\uff1a\u6587\u672c\u4fee\u590d\u6062\u590d\u8bed\u8a00\u751f\u6210\u80fd\u529b\u3001LLM\u84b8\u998f\u4f20\u9012\u6559\u5b66\u77e5\u8bc6\u3001\u5b89\u5168\u8bad\u7ec3\u9002\u5e94\u6559\u80b2\u73af\u5883\u3001\u6559\u5b66\u8bc4\u4f30\u8c03\u6574\u5230\u79d1\u5b66\u6559\u80b2\u573a\u666f\u3002", "result": "\u5728\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u3001\u5730\u7403\u79d1\u5b66\u4e94\u4e2a\u79d1\u5b66\u6f14\u793a\u4e2d\u8bc4\u4f30\uff0c\u4efb\u52a1\u6027\u80fd\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u751f\u6210\u60c5\u5883\u9002\u5f53\u7684\u6559\u80b2\u89e3\u91ca\u3002", "conclusion": "Pedagogical VLA Framework\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u6559\u80b2\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u6559\u5b66\u89e3\u91ca\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2601.12948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12948", "abs": "https://arxiv.org/abs/2601.12948", "authors": ["Riccardo Catalini", "Davide Di Nucci", "Guido Borghi", "Davide Davoli", "Lorenzo Garattoni", "Giampiero Francesca", "Yuki Kawana", "Roberto Vezzani"], "title": "GazeD: Context-Aware Diffusion for Accurate 3D Gaze Estimation", "comment": null, "summary": "We introduce GazeD, a new 3D gaze estimation method that jointly provides 3D gaze and human pose from a single RGB image. Leveraging the ability of diffusion models to deal with uncertainty, it generates multiple plausible 3D gaze and pose hypotheses based on the 2D context information extracted from the input image. Specifically, we condition the denoising process on the 2D pose, the surroundings of the subject, and the context of the scene. With GazeD we also introduce a novel way of representing the 3D gaze by positioning it as an additional body joint at a fixed distance from the eyes. The rationale is that the gaze is usually closely related to the pose, and thus it can benefit from being jointly denoised during the diffusion process. Evaluations across three benchmark datasets demonstrate that GazeD achieves state-of-the-art performance in 3D gaze estimation, even surpassing methods that rely on temporal information. Project details will be available at https://aimagelab.ing.unimore.it/go/gazed.", "AI": {"tldr": "GazeD\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u76eeRGB\u56fe\u50cf3D\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u540c\u65f6\u9884\u6d4b3D\u89c6\u7ebf\u548c\u4eba\u4f53\u59ff\u6001\uff0c\u901a\u8fc7\u591a\u5047\u8bbe\u751f\u6210\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4ece\u5355\u5f20RGB\u56fe\u50cf\u540c\u65f6\u4f30\u8ba13D\u89c6\u7ebf\u548c\u4eba\u4f53\u59ff\u6001\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u89c6\u7ebf\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6269\u6563\u6a21\u578b\u64c5\u957f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u6b64\u88ab\u7528\u4e8e\u751f\u6210\u591a\u4e2a\u5408\u7406\u76843D\u89c6\u7ebf\u548c\u59ff\u6001\u5047\u8bbe\u3002", "method": "1\uff09\u4f7f\u7528\u6269\u6563\u6a21\u578b\uff0c\u4ee52D\u59ff\u6001\u3001\u4eba\u7269\u5468\u56f4\u73af\u5883\u548c\u573a\u666f\u4e0a\u4e0b\u6587\u4e3a\u6761\u4ef6\u8fdb\u884c\u53bb\u566a\uff1b2\uff09\u63d0\u51fa\u65b0\u9896\u76843D\u89c6\u7ebf\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u89c6\u7ebf\u4f5c\u4e3a\u8ddd\u79bb\u773c\u775b\u56fa\u5b9a\u8ddd\u79bb\u7684\u989d\u5916\u8eab\u4f53\u5173\u8282\u70b9\uff1b3\uff09\u8054\u5408\u53bb\u566a\u89c6\u7ebf\u548c\u59ff\u6001\uff0c\u5229\u7528\u4e24\u8005\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGazeD\u57283D\u89c6\u7ebf\u4f30\u8ba1\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u4f9d\u8d56\u65f6\u5e8f\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "conclusion": "GazeD\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u5728\u8054\u54083D\u89c6\u7ebf\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5c06\u89c6\u7ebf\u8868\u793a\u4e3a\u989d\u5916\u5173\u8282\u70b9\u5e76\u4e0e\u59ff\u6001\u8054\u5408\u53bb\u566a\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u5e76\u5b9e\u73b0SOTA\u6027\u80fd\u3002"}}
{"id": "2601.13882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13882", "abs": "https://arxiv.org/abs/2601.13882", "authors": ["Unggi Lee", "Sookbun Lee", "Heungsoo Choi", "Jinseo Lee", "Haeun Park", "Younghoon Jeon", "Sungmin Cho", "Minju Kang", "Junbo Koh", "Jiyeong Bae", "Minwoo Nam", "Juyeon Eun", "Yeonji Jung", "Yeil Jeong"], "title": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models", "comment": null, "summary": "Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.", "AI": {"tldr": "OpenLearnLM Benchmark\u662f\u4e00\u4e2a\u57fa\u4e8e\u6559\u80b2\u8bc4\u4f30\u7406\u8bba\u7684\u4e09\u7ef4\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u77e5\u8bc6\u3001\u6280\u80fd\u548c\u6001\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5305\u542b124K+\u6d4b\u8bd5\u9879\uff0c\u8bc4\u4f30\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u7ef4\u5ea6\u8868\u73b0\u5404\u5f02\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u7ef4\u5ea6\u90fd\u9886\u5148\u3002", "motivation": "\u73b0\u6709LLM\u6559\u80b2\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u72ed\u7a84\u7684\u6280\u80fd\uff0c\u7f3a\u4e4f\u5b66\u4e60\u79d1\u5b66\u7406\u8bba\u57fa\u7840\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u7684\u7efc\u5408\u6027\u8bc4\u4f30\u6846\u67b6\u6765\u63a8\u52a8LLM\u5728\u6559\u80b2\u9886\u57df\u7684\u771f\u6b63\u51c6\u5907\u5ea6\u3002", "method": "\u57fa\u4e8e\u6559\u80b2\u8bc4\u4f30\u7406\u8bba\u6784\u5efa\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\uff1a1) \u77e5\u8bc6\u7ef4\u5ea6\uff08\u8bfe\u7a0b\u5bf9\u9f50\u5185\u5bb9\u548c\u6559\u5b66\u7406\u89e3\uff09\uff1b2) \u6280\u80fd\u7ef4\u5ea6\uff08\u57fa\u4e8e\u56db\u5c42\u4e2d\u5fc3-\u89d2\u8272-\u573a\u666f-\u5b50\u573a\u666f\u5c42\u6b21\u7ed3\u6784\u7684\u573a\u666f\u5316\u80fd\u529b\uff09\uff1b3) \u6001\u5ea6\u7ef4\u5ea6\uff08\u4e00\u81f4\u6027\u5bf9\u9f50\u548c\u6b3a\u9a97\u62b5\u6297\uff09\u3002\u57fa\u51c6\u5305\u542b124K+\u6d4b\u8bd5\u9879\uff0c\u6db5\u76d6\u591a\u4e2a\u5b66\u79d1\u3001\u6559\u80b2\u89d2\u8272\u548c\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u96be\u5ea6\u7ea7\u522b\u3002\u77e5\u8bc6\u57df\u4f18\u5148\u4f7f\u7528\u5df2\u5efa\u7acb\u57fa\u51c6\u7684\u771f\u5b9e\u8bc4\u4f30\u9879\u76ee\uff0c\u6001\u5ea6\u57df\u91c7\u7528Anthropic\u7684\u5bf9\u9f50\u4f2a\u88c5\u65b9\u6cd5\u6765\u68c0\u6d4b\u4e0d\u540c\u76d1\u63a7\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u8bc4\u4f30\u4e03\u4e2a\u524d\u6cbf\u6a21\u578b\u663e\u793a\u4e0d\u540c\u7684\u80fd\u529b\u5206\u5e03\uff1aClaude-Opus-4.5\u5728\u5b9e\u9645\u6280\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4f46\u5185\u5bb9\u77e5\u8bc6\u8f83\u4f4e\uff0c\u800cGrok-4.1-fast\u5728\u77e5\u8bc6\u65b9\u9762\u9886\u5148\u4f46\u663e\u793a\u5bf9\u9f50\u95ee\u9898\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u7ef4\u5ea6\u90fd\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u9a8c\u8bc1\u4e86\u591a\u8f74\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "OpenLearnLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u3001\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u8fdbLLM\u5728\u771f\u5b9e\u6559\u80b2\u73af\u5883\u4e2d\u7684\u51c6\u5907\u5ea6\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u6559\u80b2\u7ef4\u5ea6\u5404\u6709\u4f18\u52bf\uff0c\u8fd9\u4e3a\u672a\u6765LLM\u6559\u80b2\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.12954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12954", "abs": "https://arxiv.org/abs/2601.12954", "authors": ["Zhou Hong", "Rongsheng Hu", "Yicheng Di", "Xiaolong Xu", "Ning Dong", "Yihua Shao", "Run Ling", "Yun Wang", "Juqin Wang", "Zhanjie Zhang", "Ao Ma"], "title": "StyMam: A Mamba-Based Generator for Artistic Style Transfer", "comment": "Accepted by ICASSP 2026", "summary": "Image style transfer aims to integrate the visual patterns of a specific artistic style into a content image while preserving its content structure. Existing methods mainly rely on the generative adversarial network (GAN) or stable diffusion (SD). GAN-based approaches using CNNs or Transformers struggle to jointly capture local and global dependencies, leading to artifacts and disharmonious patterns. SD-based methods reduce such issues but often fail to preserve content structures and suffer from slow inference. To address these issues, we revisit GAN and propose a mamba-based generator, termed as StyMam, to produce high-quality stylized images without introducing artifacts and disharmonious patterns. Specifically, we introduce a mamba-based generator with a residual dual-path strip scanning mechanism and a channel-reweighted spatial attention module. The former efficiently captures local texture features, while the latter models global dependencies. Finally, extensive qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art algorithms in both quality and speed.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684\u751f\u6210\u5668StyMam\uff0c\u89e3\u51b3\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e2d\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u6355\u83b7\u4e0d\u8db3\u3001\u4f2a\u5f71\u548c\u4e0d\u534f\u8c03\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GAN\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u4f9d\u8d56\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u4e0d\u534f\u8c03\u6a21\u5f0f\uff1bSD\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u8fd9\u4e9b\u95ee\u9898\u4f46\u96be\u4ee5\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u4e14\u63a8\u7406\u6162\u3002\u9700\u8981\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684\u751f\u6210\u5668StyMam\uff0c\u5305\u542b\u6b8b\u5dee\u53cc\u8def\u5f84\u6761\u5e26\u626b\u63cf\u673a\u5236\u548c\u901a\u9053\u91cd\u52a0\u6743\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u524d\u8005\u6355\u83b7\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff0c\u540e\u8005\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u56fe\u50cf\u800c\u4e0d\u5f15\u5165\u4f2a\u5f71\u548c\u4e0d\u534f\u8c03\u6a21\u5f0f\u3002", "conclusion": "StyMam\u901a\u8fc7Mamba\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u4e14\u63a8\u7406\u901f\u5ea6\u5feb\u3002"}}
{"id": "2601.13885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13885", "abs": "https://arxiv.org/abs/2601.13885", "authors": ["Esma Balk\u0131r", "Alice Pernthaller", "Marco Basaldella", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Nigel Collier"], "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores", "comment": null, "summary": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 \u03c4 over random sampling, with 95% accuracy on confident predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u8fde\u7eed\u8bc4\u5206LLM\u8bc4\u4f30\u7684\u81ea\u9002\u5e94\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u5f02\u65b9\u5dee\u6b63\u6001\u5206\u5e03\u66ff\u4ee3\u4f2f\u52aa\u5229\u5206\u5e03\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6392\u5e8f\u5668\u548c\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\uff0c\u5728\u4ec5\u4f7f\u75282%\u6d4b\u8bd5\u9879\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6392\u540d\u76f8\u5173\u6027", "motivation": "\u4f20\u7edfCAT\u65b9\u6cd5\u9002\u7528\u4e8e\u9009\u62e9\u9898\u8bc4\u4f30\uff0c\u4f46\u73b0\u4ee3LLM\u8bc4\u4f30\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u751f\u6210\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4f7f\u7528\u8fde\u7eed\u8bc4\u5206\uff08\u5982ROUGE\u3001BLEU\u3001LLM-as-a-Judge\uff09\u800c\u975e\u4e8c\u5143\u6b63\u786e/\u9519\u8bef\u5224\u65ad\uff0c\u9700\u8981\u6269\u5c55\u81ea\u9002\u5e94\u6d4b\u8bd5\u65b9\u6cd5\u4ee5\u9002\u5e94\u8fde\u7eed\u8bc4\u5206\u573a\u666f", "method": "1. \u5c06IRT\u81ea\u9002\u5e94\u6d4b\u8bd5\u6269\u5c55\u5230\u8fde\u7eed\u6709\u754c\u8bc4\u5206\uff0c\u7528\u5f02\u65b9\u5dee\u6b63\u6001\u5206\u5e03\u66ff\u4ee3\u4f2f\u52aa\u5229\u54cd\u5e94\u5206\u5e03\uff1b2. \u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6392\u5e8f\u5668\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\uff0c\u5728\u5c3d\u53ef\u80fd\u5c11\u7684\u6d4b\u8bd5\u9879\u548c\u4f4e\u6210\u672c\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u6a21\u578b\u6392\u540d", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecn-gram\u3001embedding\u548cLLM-as-judge\u6307\u6807\uff09\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u4f7f\u75282%\u7684\u6d4b\u8bd5\u9879\uff0c\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u63d0\u5347\u6392\u540d\u76f8\u5173\u60270.12 \u03c4\uff0c\u5728\u7f6e\u4fe1\u9884\u6d4b\u4e0a\u8fbe\u523095%\u51c6\u786e\u7387", "conclusion": "\u6210\u529f\u5c06CAT\u6269\u5c55\u5230\u8fde\u7eed\u8bc4\u5206LLM\u8bc4\u4f30\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u6d4b\u8bd5\u6210\u672c\u540c\u65f6\u63d0\u9ad8\u6392\u540d\u53ef\u9760\u6027\uff0c\u4e3a\u73b0\u4ee3\u751f\u6210\u5f0fLLM\u7684\u9ad8\u6548\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12964", "abs": "https://arxiv.org/abs/2601.12964", "authors": ["John Waithaka", "Gustave Bwirayesu", "Moise Busogi"], "title": "Cross-Scale Pretraining: Enhancing Self-Supervised Learning for Low-Resolution Satellite Imagery for Semantic Segmentation", "comment": null, "summary": "Self-supervised pretraining in remote sensing is mostly done using mid-spatial resolution (MR) image datasets due to their high availability. Given the release of high-resolution (HR) datasets, we ask how HR datasets can be included in self-supervised pretraining to enhance MR image representation learning and downstream segmentation performance on MR tasks. We design a spatial affinity component that can be added to existing self-supervised learning frameworks and that uses HR imagery to learn better representations of MR imagery. We test the spatial affinity component on two self-supervised learning frameworks and show that it outperforms models pretrained on HR or MR images alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4\u4eb2\u548c\u529b\u7ec4\u4ef6\uff0c\u53ef\u5c06\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u878d\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u4ee5\u63d0\u5347\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u5f81\u5b66\u4e60\u548c\u4e0b\u6e38\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u9065\u611f\u9886\u57df\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4e3b\u8981\u4f7f\u7528\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4f46\u968f\u7740\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u6765\u589e\u5f3a\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u5f81\u5b66\u4e60\uff0c\u63d0\u5347\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7a7a\u95f4\u4eb2\u548c\u529b\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u6765\u5b66\u4e60\u66f4\u597d\u7684\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u8868\u5f81\u3002\u8be5\u7ec4\u4ef6\u5728\u4e24\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u7a7a\u95f4\u4eb2\u548c\u529b\u7ec4\u4ef6\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4ec5\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u6216\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u5355\u72ec\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u878d\u5165\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4e2d\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8868\u5f81\u8d28\u91cf\uff0c\u4ece\u800c\u6539\u5584\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.13918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13918", "abs": "https://arxiv.org/abs/2601.13918", "authors": ["Yusheng Liao", "Chuan Xuan", "Yutong Cai", "Lina Yang", "Zhe Chen", "Yanfeng Wang", "Yu Wang"], "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization", "comment": "37 pages, 12 figures", "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentEHR\u57fa\u51c6\u6d4b\u8bd5\u548cRetroSum\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5728\u539f\u59cb\u3001\u9ad8\u566a\u58f0\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u6267\u884c\u590d\u6742\u533b\u7597\u51b3\u7b56\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u4fe1\u606f\u4e22\u5931\u548c\u63a8\u7406\u8fde\u7eed\u6027\u65ad\u88c2\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u4e3b\u8981\u4f9d\u8d56\u7cbe\u5fc3\u7b56\u5212\u7684\u8f93\u5165\u548c\u7b80\u5316\u7684\u68c0\u7d22\u4efb\u52a1\uff0c\u65e0\u6cd5\u5728\u539f\u59cb\u3001\u9ad8\u566a\u58f0\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u73af\u5883\u4e2d\u8fdb\u884c\u590d\u6742\u7684\u81ea\u4e3b\u51b3\u7b56\uff0c\u5982\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u7a0b\u4ea4\u4e92\u63a8\u7406\u65f6\u5b58\u5728\u5173\u952e\u4fe1\u606f\u4e22\u5931\u548c\u63a8\u7406\u8fde\u7eed\u6027\u65ad\u88c2\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86RetroSum\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u56de\u987e\u6027\u603b\u7ed3\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u8bc4\u4f30\u4ea4\u4e92\u5386\u53f2\u6765\u9632\u6b62\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\u5e76\u786e\u4fdd\u903b\u8f91\u8fde\u8d2f\u6027\uff1b2\uff09\u6f14\u5316\u7ecf\u9a8c\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u8bb0\u5fc6\u5e93\u4e2d\u68c0\u7d22\u7d2f\u79ef\u7ecf\u9a8c\u6765\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u3002", "result": "RetroSum\u5728AgentEHR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe29.16%\uff0c\u540c\u65f6\u5c06\u603b\u4ea4\u4e92\u9519\u8bef\u7387\u663e\u8457\u964d\u4f4e\u9ad8\u8fbe92.3%\u3002", "conclusion": "RetroSum\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56de\u987e\u6027\u603b\u7ed3\u548c\u6f14\u5316\u7ecf\u9a8c\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u539f\u59cb\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u73af\u5883\u4e2d\u8fdb\u884c\u590d\u6742\u533b\u7597\u51b3\u7b56\u65f6\u7684\u4fe1\u606f\u4e22\u5931\u548c\u63a8\u7406\u8fde\u7eed\u6027\u65ad\u88c2\u95ee\u9898\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.12981", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12981", "abs": "https://arxiv.org/abs/2601.12981", "authors": ["Sulaiman Khan", "Md. Rafiul Biswas", "Zubair Shah"], "title": "Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers", "comment": "08 pages, 06 figures, accepted for publication in FLLM2025", "summary": "This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.\n  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8868\u683cTransformer\uff08TabTrans\uff09\u67b6\u6784\u7684\u65b0\u578bT2DM\u65e9\u671f\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u7eb5\u5411\u60a3\u8005\u6570\u636e\uff0c\u5728\u5361\u5854\u5c14\u751f\u7269\u94f6\u884c\u961f\u5217\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u7eb5\u5411\u5065\u5eb7\u8bb0\u5f55\u548c\u9aa8\u9abc\u76f8\u5173\u8868\u683c\u6570\u636e\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u4e2d\u7684\u590d\u6742\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5206\u6790\u8fd9\u7c7b\u590d\u6742\u8868\u683c\u533b\u7597\u6570\u636e\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8868\u683cTransformer\uff08TabTrans\uff09\u67b6\u6784\u5904\u7406\u7eb5\u5411\u60a3\u8005\u6570\u636e\uff0c\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u548c\u53cc\u80fdX\u5c04\u7ebf\u5438\u6536\u6d4b\u5b9a\u6cd5\uff08DXA\uff09\u6570\u636e\uff0c\u4f7f\u7528SMOTE\u548cSMOTE-ENN\u6280\u672f\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\u6a21\u578b\uff08Claude 3.5 Sonnet\u3001GPT-4\u3001Gemini Pro\uff09\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "TabTrans\u6a21\u578b\u5728T2DM\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0cROC AUC \u2265 79.7%\uff0c\u4f18\u4e8e\u751f\u6210\u5f0fAI\u6a21\u578b\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u7279\u5f81\u89e3\u91ca\u5206\u6790\u786e\u5b9a\u4e86\u5185\u810f\u8102\u80aa\u7ec4\u7ec7\uff08VAT\uff09\u8d28\u91cf\u548c\u4f53\u79ef\u3001ward\u9aa8\u5bc6\u5ea6\uff08BMD\uff09\u548c\u9aa8\u77ff\u7269\u8d28\u542b\u91cf\uff08BMC\uff09\u3001T\u548cZ\u5206\u6570\u3001L1-L4\u5206\u6570\u7b49\u5173\u952e\u98ce\u9669\u6307\u6807\u3002", "conclusion": "TabTrans\u5728\u5206\u6790\u590d\u6742\u8868\u683c\u533b\u7597\u6570\u636e\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4e3a\u5361\u5854\u5c14\u4eba\u7fa4\u7684\u4e3b\u52a8T2DM\u7ba1\u7406\u548c\u4e2a\u6027\u5316\u4e34\u5e8a\u5e72\u9884\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u4e0e\u7cd6\u5c3f\u75c5\u53d1\u5c55\u76f8\u5173\u7684\u91cd\u8981\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2601.13919", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13919", "abs": "https://arxiv.org/abs/2601.13919", "authors": ["Yuezhe Yang", "Hao Wang", "Yige Peng", "Jinman Kim", "Lei Bi"], "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs", "comment": "Under Review", "summary": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker", "AI": {"tldr": "HyperWalker\uff1a\u57fa\u4e8e\u52a8\u6001\u8d85\u56fe\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u7684\u6df1\u5ea6\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaiBrochure\u8d85\u56fe\u5efa\u6a21EHR\u6570\u636e\uff0c\u4f7f\u7528Walker\u667a\u80fd\u4f53\u5bfc\u822a\u8bca\u65ad\u8def\u5f84\uff0c\u7ed3\u5408linger\u673a\u5236\u68c0\u7d22\u4e92\u8865\u4e34\u5e8a\u6848\u4f8b\uff0c\u5728MRG\u548c\u533b\u7597VQA\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u91c7\u7528\u6837\u672c\u9694\u79bb\u63a8\u7406\u8303\u5f0f\uff0c\u72ec\u7acb\u5904\u7406\u75c5\u4f8b\u800c\u65e0\u6cd5\u8bbf\u95ee\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6216\u76f8\u5173\u60a3\u8005\u6848\u4f8b\uff0c\u4ec5\u4f9d\u8d56\u56fe\u50cf\u4fe1\u606f\u9650\u5236\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002\u9700\u8981\u6574\u5408\u5916\u90e8\u8865\u5145\u533b\u7597\u8bc1\u636e\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u8bca\u65ad\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u8d85\u56feiBrochure\u5efa\u6a21EHR\u6570\u636e\u7684\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u591a\u6a21\u6001\u4e34\u5e8a\u4fe1\u606f\u7684\u9ad8\u9636\u5173\u8054\uff1b2. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53Walker\u5728\u8d85\u56fe\u4e2d\u5bfc\u822a\u5e76\u8bc6\u522b\u6700\u4f18\u8bca\u65ad\u8def\u5f84\uff1b3. \u5f15\u5165linger\u673a\u5236\uff08\u591a\u8df3\u6b63\u4ea4\u68c0\u7d22\u7b56\u7565\uff09\u8fed\u4ee3\u9009\u62e9\u53cd\u6620\u4e0d\u540c\u4e34\u5e8a\u5c5e\u6027\u7684\u4e92\u8865\u90bb\u57df\u6848\u4f8b\u3002", "result": "\u5728MIMIC\u6570\u636e\u96c6\u4e0a\u7684\u533b\u7597\u62a5\u544a\u751f\u6210\uff08MRG\uff09\u548cEHRXQA\u4e0a\u7684\u533b\u7597\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u5b9e\u9a8c\u4e2d\uff0cHyperWalker\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HyperWalker\u901a\u8fc7\u52a8\u6001\u8d85\u56fe\u548c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u91cd\u65b0\u6784\u5efa\u4e34\u5e8a\u63a8\u7406\uff0c\u80fd\u591f\u6574\u5408\u7eb5\u5411EHR\u6570\u636e\u548c\u76f8\u5173\u60a3\u8005\u6848\u4f8b\uff0c\u514b\u670d\u4e86\u6837\u672c\u9694\u79bb\u63a8\u7406\u7684\u9650\u5236\uff0c\u4e3a\u81ea\u52a8\u5316\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12994", "abs": "https://arxiv.org/abs/2601.12994", "authors": ["Shiming Wang", "Holger Caesar", "Liangliang Nan", "Julian F. P. Kooij"], "title": "AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection", "comment": null, "summary": "In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.", "AI": {"tldr": "AsyncBEV\u662f\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u53473D BEV\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5bf9\u4f20\u611f\u5668\u5f02\u6b65\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u4f30\u8ba1BEV\u7279\u5f81\u6d41\u5e76\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50\u6765\u8865\u507f\u65f6\u95f4\u504f\u79fb\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6a21\u6001\u611f\u77e5\u4efb\u52a1\u901a\u5e38\u4f9d\u8d56\u4f20\u611f\u5668\u540c\u6b65\uff0c\u4f46\u5b9e\u9645\u4e2d\u7531\u4e8e\u4e0d\u540c\u4f20\u611f\u5668\u9891\u7387\u3001\u7f51\u7edc\u5ef6\u8fdf\u3001\u786c\u4ef6\u6545\u969c\u7b49\u56e0\u7d20\uff0c\u5b8c\u7f8e\u540c\u6b65\u96be\u4ee5\u4fdd\u8bc1\uff0c\u8fd9\u79cd\u5f02\u6b65\u4f1a\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u52a8\u6001\u7269\u4f53\u3002", "method": "\u53d7\u573a\u666f\u6d41\u4f30\u8ba1\u542f\u53d1\uff0cAsyncBEV\u9996\u5148\u57fa\u4e8e\u4e24\u4e2a\u4e0d\u540c\u4f20\u611f\u5668\u6a21\u6001\u7684BEV\u7279\u5f81\u4f30\u8ba12D\u6d41\uff0c\u8003\u8651\u5df2\u77e5\u7684\u65f6\u95f4\u504f\u79fb\uff0c\u7136\u540e\u4f7f\u7528\u9884\u6d4b\u7684\u7279\u5f81\u6d41\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u626d\u66f2\u548c\u7a7a\u95f4\u5bf9\u9f50\uff0c\u53ef\u96c6\u6210\u5230\u4e0d\u540c\u7684BEV\u68c0\u6d4b\u5668\u67b6\u6784\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAsyncBEV\u663e\u8457\u63d0\u5347\u4e86\u5bf9LiDAR\u548c\u76f8\u673a\u4f20\u611f\u5668\u95f4\u5c0f\u548c\u5927\u5f02\u6b65\u7684\u9c81\u68d2\u6027\uff0c\u57280.5\u79d2\u65f6\u95f4\u504f\u79fb\u7684\u6700\u574f\u60c5\u51b5\u4e0b\uff0c\u52a8\u6001\u7269\u4f53NDS\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u534716.6%\u548c11.9%\u3002", "conclusion": "AsyncBEV\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u589e\u5f3aBEV\u68c0\u6d4b\u5668\u5bf9\u4f20\u611f\u5668\u5f02\u6b65\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u52a8\u6001\u7269\u4f53\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.13922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13922", "abs": "https://arxiv.org/abs/2601.13922", "authors": ["Adrian Cosma", "Oleg Szehr", "David Kletz", "Alessandro Antonucci", "Olivier Pelletier"], "title": "Automatic Prompt Optimization for Dataset-Level Feature Discovery", "comment": "5 Figures, 1 Table", "summary": "Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u7279\u5f81\u53d1\u73b0\u89c6\u4e3a\u6570\u636e\u96c6\u7ea7\u63d0\u793a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5224\u522b\u6027\u7279\u5f81\u5b9a\u4e49", "motivation": "\u5f53\u524d\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u7279\u5f81\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u6216\u56fa\u5b9a\u7279\u5f81\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u7684\u7279\u5f81\u53d1\u73b0\u673a\u5236", "method": "\u591a\u667a\u80fd\u4f53\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff1a\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u51fa\u7279\u5f81\u5b9a\u4e49\u3001\u63d0\u53d6\u7279\u5f81\u503c\u3001\u4f7f\u7528\u6570\u636e\u96c6\u7ea7\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u53cd\u9988\u8bc4\u4f30\u7279\u5f81\u8d28\u91cf\uff0c\u8fed\u4ee3\u4f18\u5316\u6307\u4ee4\u63d0\u793a", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u81ea\u52a8\u53d1\u73b0\u7279\u5f81\u7684\u539f\u7406\u6027\u673a\u5236\uff0c\u4f18\u5316\u7684\u662f\u8bf1\u5bfc\u5171\u4eab\u7279\u5f81\u96c6\u7684\u63d0\u793a\uff0c\u800c\u975e\u5355\u6837\u672c\u9884\u6d4b", "conclusion": "\u5c06\u7279\u5f81\u53d1\u73b0\u91cd\u65b0\u8868\u8ff0\u4e3a\u6570\u636e\u96c6\u7ea7\u63d0\u793a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u53d1\u73b0"}}
{"id": "2601.13029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13029", "abs": "https://arxiv.org/abs/2601.13029", "authors": ["Zaibin Zhang", "Yuhan Wu", "Lianjie Jia", "Yifan Wang", "Zhongbo Zhang", "Yijiang Li", "Binghao Ran", "Fuxi Zhang", "Zhuohan Sun", "Zhenfei Yin", "Lijun Wang", "Huchuan Lu"], "title": "Think3D: Thinking with Space for Spatial Reasoning", "comment": null, "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "AI": {"tldr": "Think3D\u6846\u67b6\u8ba9\u89c6\u89c9\u5927\u6a21\u578b\u901a\u8fc73D\u91cd\u5efa\u548c\u4ea4\u4e92\u5f0f\u7a7a\u95f4\u64cd\u4f5c\u5b9e\u73b0\u771f\u6b63\u76843D\u63a8\u7406\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b", "motivation": "\u5f53\u524d\u89c6\u89c9\u5927\u6a21\u578b\u867d\u7136\u64c5\u957f2D\u89c6\u89c9\u7406\u89e3\uff0c\u4f46\u7f3a\u4e4f\u771f\u6b63\u76843D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u7406\u89e3\u51e0\u4f55\u3001\u900f\u89c6\u548c\u7a7a\u95f4\u5173\u7cfb\u7b49\u7269\u7406\u4e16\u754c\u8ba4\u77e5\u6240\u9700\u7684\u5173\u952e\u80fd\u529b", "method": "\u5229\u75283D\u91cd\u5efa\u6a21\u578b\u4ece\u56fe\u50cf/\u89c6\u9891\u6062\u590d\u70b9\u4e91\u548c\u76f8\u673a\u4f4d\u59ff\uff0c\u8ba9\u667a\u80fd\u4f53\u901a\u8fc7\u76f8\u673a\u64cd\u4f5c\u548c\u89c6\u89d2\u5207\u6362\u4e3b\u52a8\u64cd\u63a7\u7a7a\u95f4\uff0c\u5c06\u7a7a\u95f4\u63a8\u7406\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f3D\u601d\u7ef4\u94fe\u8fc7\u7a0b", "result": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u663e\u8457\u63d0\u5347GPT-4.1\u548cGemini 2.5 Pro\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\uff0c\u5728BLINK Multi-view\u548cMindCube\u4e0a\u5e73\u5747\u63d0\u53477.8%\uff0c\u5728VSI-Bench\u4e0a\u63d0\u53474.7%\uff1b\u5c0f\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89d2\u548c\u64cd\u4f5c\uff0c\u5de5\u5177\u4f7f\u7528\u6536\u76ca\u4ece0.7%\u63d0\u5347\u52306.8%", "conclusion": "\u514d\u8bad\u7ec3\u7684\u5de5\u5177\u589e\u5f3a\u7a7a\u95f4\u63a2\u7d22\u662f\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u7c7b\u4eba3D\u63a8\u7406\u7684\u53ef\u884c\u8def\u5f84\uff0c\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u5efa\u7acb\u4e86\u65b0\u7684\u7ef4\u5ea6\uff0c\u4ee3\u7801\u548c\u6743\u91cd\u5df2\u5f00\u6e90"}}
{"id": "2601.13992", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13992", "abs": "https://arxiv.org/abs/2601.13992", "authors": ["Jin Cui", "Jiaqi Guo", "Jiepeng Zhou", "Ruixuan Yang", "Jiayi Lu", "Jiajun Xu", "Jiangcheng Song", "Boran Zhao", "Pengju Ren"], "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework", "comment": "11pages, 9figures", "summary": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.", "AI": {"tldr": "COMPACT\u662f\u4e00\u4e2a\u591a\u6559\u5e08CoT\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u68af\u5ea6\u52a0\u6743\u878d\u5408\u4e0d\u540c\u6559\u5e08\u7684\u76d1\u7763\uff0c\u89e3\u51b3\u5355\u4e00\u6559\u5e08\u80fd\u529b\u504f\u89c1\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u5c0f\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709CoT\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6559\u5e08\u6a21\u578b\uff0c\u4f46\u5355\u4e2aLLM\u5b58\u5728\u80fd\u529b\u504f\u89c1\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6f5c\u529b\u3002\u867d\u7136\u5229\u7528\u591a\u6837\u5316\u6559\u5e08\u6a21\u578b\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u6709\u6548\u878d\u5408\u5b83\u4eec\u7684\u76d1\u7763\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff1a\u5e08\u751f\u4e0d\u517c\u5bb9\u53ef\u80fd\u653e\u5927\u5e7b\u89c9\uff0c\u88ab\u52a8\u76d1\u7763\u65e0\u6cd5\u786e\u4fdd\u771f\u6b63\u7684\u903b\u8f91\u5185\u5316\u3002", "method": "COMPACT\u901a\u8fc7\u57fa\u4e8e\u5b66\u751f\u5b9e\u65f6\u517c\u5bb9\u6027\u7684\u52a8\u6001\u68af\u5ea6\u52a0\u6743\u6765\u878d\u5408\u4e0d\u540c\u6559\u5e08\u7684\u76d1\u7763\uff0c\u517c\u5bb9\u6027\u8bc4\u4f30\u91c7\u7528\u4e09\u7ef4\u5ea6\u6307\u6807\uff1a1) \u57fa\u4e8e\u56fe\u7684\u5171\u8bc6\u8fc7\u6ee4\u8bef\u5bfc\u6027\u63a8\u7406\u8def\u5f84\uff1b2) \u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u9002\u5e94\u6027\u68c0\u6d4b\"\u987f\u609f\u65f6\u523b\"\u4ee5\u786e\u4fdd\u771f\u6b63\u7406\u89e3\u800c\u975e\u6a21\u4eff\uff1b3) \u57fa\u4e8e\u635f\u5931\u7684\u96be\u5ea6\u8bc4\u4f30\u5b66\u751f\u5bf9\u6559\u5e08\u6307\u5bfc\u7684\u63a5\u53d7\u5ea6\uff0c\u9632\u6b62\u8d1f\u8fc1\u79fb\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u8868\u660e\uff0cCOMPACT\u80fd\u6709\u6548\u6574\u5408\u591a\u6837\u5316\u63a8\u7406\u80fd\u529b\u800c\u4e0d\u635f\u5bb3\u6a21\u578b\u539f\u6709\u77e5\u8bc6\u7ed3\u6784\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "COMPACT\u901a\u8fc7\u52a8\u6001\u878d\u5408\u591a\u6559\u5e08\u76d1\u7763\uff0c\u6210\u529f\u89e3\u51b3\u4e86CoT\u84b8\u998f\u4e2d\u7684\u5e08\u751f\u4e0d\u517c\u5bb9\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u5c0f\u6a21\u578b\u83b7\u5f97\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2601.13052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13052", "abs": "https://arxiv.org/abs/2601.13052", "authors": ["Antoine Carreaud", "Shanci Li", "Malo De Lacour", "Digre Frinde", "Jan Skaloud", "Adrien Gressin"], "title": "GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure", "comment": null, "summary": "This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.", "AI": {"tldr": "GridNet-HD\u662f\u4e00\u4e2a\u7528\u4e8e\u7535\u529b\u57fa\u7840\u8bbe\u65bd3D\u8bed\u4e49\u5206\u5272\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u5bc6\u5ea6LiDAR\u548c\u9ad8\u5206\u8fa8\u7387\u503e\u659c\u56fe\u50cf\uff0c\u63d0\u4f9b7,694\u5f20\u56fe\u50cf\u548c25\u4ebf\u4e2a\u70b9\uff0c\u6807\u6ce8\u4e3a11\u4e2a\u7c7b\u522b\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u76f8\u6bd4\u5355\u6a21\u6001\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u540c\u65f6\u63d0\u4f9b\u9ad8\u5bc6\u5ea6LiDAR\u3001\u9ad8\u5206\u8fa8\u7387\u503e\u659c\u56fe\u50cf\u548c3D\u8bed\u4e49\u6807\u7b7e\u7684\u516c\u5f00\u7535\u529b\u7ebf\u8def\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5728\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u521b\u5efaGridNet-HD\u6570\u636e\u96c6\uff0c\u5305\u542b7,694\u5f20\u9ad8\u5206\u8fa8\u7387\u503e\u659c\u56fe\u50cf\u548c25\u4ebf\u4e2aLiDAR\u70b9\uff0c\u6807\u6ce8\u4e3a11\u4e2a\u8bed\u4e49\u7c7b\u522b\u3002\u63d0\u4f9b\u9884\u5b9a\u4e49\u7684\u6570\u636e\u5206\u5272\u548cmIoU\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u4e86\u5355\u6a21\u6001\uff08\u4ec5LiDAR\u3001\u4ec5\u56fe\u50cf\uff09\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5728\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u4e2d\uff0c\u76f8\u6bd4\u6700\u4f73\u5355\u6a21\u6001\u57fa\u7ebf\uff0cGridNet-HD\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u4e86+5.55 mIoU\uff0c\u8bc1\u660e\u4e86\u51e0\u4f55\u4fe1\u606f\uff08LiDAR\uff09\u548c\u5916\u89c2\u4fe1\u606f\uff08\u56fe\u50cf\uff09\u7684\u4e92\u8865\u6027\u3002", "conclusion": "GridNet-HD\u586b\u8865\u4e86\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u57283D\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u4f18\u52bf\uff0c\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u6a21\u578b\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2601.13995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13995", "abs": "https://arxiv.org/abs/2601.13995", "authors": ["Zihan Niu", "Wenping Hu", "Junmin Chen", "Xiyue Wang", "Tong Xu", "Ruiming Tang"], "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning", "comment": null, "summary": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.", "AI": {"tldr": "TAGS\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6807\u7b7e\u6784\u5efa\u77e5\u8bc6\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u76ee\u6807\u5bf9\u9f50\u7684\u8054\u5408\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u6307\u4ee4\u8c03\u4f18\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5b9e\u4f8b\u7ea7\u8d28\u91cf\u8bc4\u5206\u6216\u57fa\u4e8e\u5d4c\u5165\u805a\u7c7b/\u8bed\u4e49\u6807\u7b7e\u7684\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u4f46\u53d7\u9650\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u5e73\u5766\u6027\u6216\u6807\u7b7e\u7684\u7c97\u7cd9\u6027\uff0c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u53ca\u5176\u5185\u5728\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\uff0c\u963b\u788d\u4e86\u7cbe\u786e\u7684\u6570\u636e\u8bc4\u4f30\u548c\u77e5\u8bc6\u5bf9\u9f50\u91c7\u6837\u3002", "method": "\u63d0\u51faTree-aware Aligned Global Sampling (TAGS)\u6846\u67b6\uff1a1) \u4f7f\u7528LLM-based tagger\u63d0\u53d6\u539f\u5b50\u77e5\u8bc6\u6982\u5ff5\uff1b2) \u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u5c42\u6b21\u805a\u7c7b\u6784\u5efa\u5168\u5c40\u77e5\u8bc6\u6811\uff1b3) \u5c06\u6570\u636e\u5b9e\u4f8b\u6620\u5c04\u5230\u77e5\u8bc6\u6811\u4e0a\uff1b4) \u57fa\u4e8e\u6811\u611f\u77e5\u7684\u5ea6\u91cf\u91cf\u5316\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\uff1b5) \u53ef\u63a7\u91c7\u6837\u7b56\u7565\u6700\u5927\u5316\u6811\u7ea7\u4fe1\u606f\u589e\u76ca\u5e76\u901a\u8fc7KL\u6563\u5ea6\u5b9e\u73b0\u53f6\u7ea7\u5bf9\u9f50\u3002", "result": "TAGS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u75285%\u6570\u636e\u5c31\u8d85\u8d8a\u5168\u6570\u636e\u96c6\u6a21\u578b+5.84%\uff0c\u5bf9\u9f50\u91c7\u6837\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u5e73\u5747\u6027\u80fd+4.24%\u3002", "conclusion": "TAGS\u901a\u8fc7\u6784\u5efa\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u6811\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u636e\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u76ee\u6807\u5bf9\u9f50\u7684\u8054\u5408\u63a7\u5236\uff0c\u4e3aLLM\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\u3002"}}
{"id": "2601.13059", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13059", "abs": "https://arxiv.org/abs/2601.13059", "authors": ["Yulun Guo"], "title": "Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures", "comment": null, "summary": "Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: https://github.com/YulunGuo/CrackFSS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Retinex\u7406\u8bba\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u53cc\u5206\u652f\u539f\u578b\u5b66\u4e60\u7f51\u7edc\uff0c\u7528\u4e8e\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u88c2\u7f1d\u5206\u5272\uff0c\u901a\u8fc7\u5149\u7167\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\u548c\u5ea6\u91cf\u5b66\u4e60\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u88c2\u7f1d\u5e38\u51fa\u73b0\u5728\u96a7\u9053\u3001\u6865\u5e95\u7b49\u4f4e\u5149\u7167\u73af\u5883\u4e2d\uff0c\u5bfc\u81f4\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u5272\u7cbe\u5ea6\u4e0b\u964d\u3002\u50cf\u7d20\u7ea7\u6807\u6ce8\u4f4e\u5149\u7167\u88c2\u7f1d\u56fe\u50cf\u8017\u65f6\u5de8\u5927\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u9700\u8981\u5145\u8db3\u5149\u7167\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "1) \u53cc\u5206\u652f\u539f\u578b\u5b66\u4e60\u7f51\u7edc\uff1a\u6574\u5408Retinex\u7406\u8bba\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\uff1b2) Retinex\u53cd\u5c04\u5206\u91cf\u5f15\u5bfc\u5149\u7167\u4e0d\u53d8\u5168\u5c40\u8868\u793a\u5b66\u4e60\uff1b3) \u5ea6\u91cf\u5b66\u4e60\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff1b4) \u8de8\u76f8\u4f3c\u6027\u5148\u9a8c\u63a9\u7801\u751f\u6210\u6a21\u5757\uff1a\u8ba1\u7b97\u67e5\u8be2\u4e0e\u652f\u6301\u7279\u5f81\u7684\u9ad8\u7ef4\u76f8\u4f3c\u6027\u4ee5\u6355\u6349\u88c2\u7f1d\u4f4d\u7f6e\u548c\u7ed3\u6784\uff1b5) \u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff1a\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0e\u5148\u9a8c\u63a9\u7801\u4ee5\u7f13\u89e3\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u5c55\u73b0\u51fa\u6301\u7eed\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u88c2\u7f1d\u5206\u5272\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408Retinex\u7406\u8bba\u548c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5149\u7167\u53d8\u5316\u9c81\u68d2\u4e14\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u88c2\u7f1d\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2601.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14004", "abs": "https://arxiv.org/abs/2601.14004", "authors": ["Hengyuan Zhang", "Zhihao Zhang", "Mingyang Wang", "Zunhai Su", "Yiwei Wang", "Qianli Wang", "Shuzhou Yuan", "Ercong Nie", "Xufeng Duan", "Qibo Xue", "Zeping Yu", "Chenming Shang", "Xiao Liang", "Jing Xiong", "Hui Shen", "Chaofan Tao", "Zhengwu Liu", "Senjie Jin", "Zhiheng Xi", "Dongdong Zhang", "Sophia Ananiadou", "Tao Gui", "Ruobing Xie", "Hayden Kwok-Hay So", "Hinrich Sch\u00fctze", "Xuanjing Huang", "Qi Zhang", "Ngai Wong"], "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models", "comment": null, "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5b9e\u7528\u7684\u53ef\u64cd\u4f5c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u8c03\u67e5\uff0c\u56f4\u7ed5\"\u5b9a\u4f4d\u3001\u5f15\u5bfc\u3001\u6539\u8fdb\"\u6d41\u7a0b\u6784\u5efa\u7cfb\u7edf\u6846\u67b6\uff0c\u5c06MI\u4ece\u89c2\u5bdf\u79d1\u5b66\u8f6c\u53d8\u4e3a\u53ef\u5e72\u9884\u7684\u65b9\u6cd5\u8bba\u3002", "motivation": "\u73b0\u6709\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e3b\u8981\u4f5c\u4e3a\u89c2\u5bdf\u79d1\u5b66\uff0c\u603b\u7ed3\u5206\u6790\u89c1\u89e3\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u53ef\u64cd\u4f5c\u5e72\u9884\u6846\u67b6\uff0c\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u56f4\u7ed5\"\u5b9a\u4f4d\u3001\u5f15\u5bfc\u3001\u6539\u8fdb\"\u6d41\u7a0b\u6784\u5efa\u5b9e\u7528\u8c03\u67e5\u6846\u67b6\uff0c\u57fa\u4e8e\u7279\u5b9a\u53ef\u89e3\u91ca\u5bf9\u8c61\u5f62\u5f0f\u5316\u5206\u7c7b\u5b9a\u4f4d\uff08\u8bca\u65ad\uff09\u548c\u5f15\u5bfc\uff08\u5e72\u9884\uff09\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e25\u683c\u7684\u5e72\u9884\u534f\u8bae\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u5bf9\u9f50\u6027\u3001\u80fd\u529b\u548c\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u5207\u5b9e\u6539\u8fdb\uff0c\u6709\u6548\u5c06\u673a\u5236\u53ef\u89e3\u91ca\u6027\u64cd\u4f5c\u5316\u4e3a\u6a21\u578b\u4f18\u5316\u7684\u53ef\u64cd\u4f5c\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4ece\u89c2\u5bdf\u79d1\u5b66\u8f6c\u53d8\u4e3a\u53ef\u64cd\u4f5c\u7684\u65b9\u6cd5\u8bba\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5e72\u9884\u534f\u8bae\u3002"}}
{"id": "2601.13094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13094", "abs": "https://arxiv.org/abs/2601.13094", "authors": ["Gelei Xu", "Yuying Duan", "Jun Xia", "Ruining Deng", "Wei Jin", "Yiyu Shi"], "title": "Patient-Conditioned Adaptive Offsets for Reliable Diagnosis across Subgroups", "comment": null, "summary": "AI models for medical diagnosis often exhibit uneven performance across patient populations due to heterogeneity in disease prevalence, imaging appearance, and clinical risk profiles. Existing algorithmic fairness approaches typically seek to reduce such disparities by suppressing sensitive attributes. However, in medical settings these attributes often carry essential diagnostic information, and removing them can degrade accuracy and reliability, particularly in high-stakes applications. In contrast, clinical decision making explicitly incorporates patient context when interpreting diagnostic evidence, suggesting a different design direction for subgroup-aware models. In this paper, we introduce HyperAdapt, a patient-conditioned adaptation framework that improves subgroup reliability while maintaining a shared diagnostic model. Clinically relevant attributes such as age and sex are encoded into a compact embedding and used to condition a hypernetwork-style module, which generates small residual modulation parameters for selected layers of a shared backbone. This design preserves the general medical knowledge learned by the backbone while enabling targeted adjustments that reflect patient-specific variability. To ensure efficiency and robustness, adaptations are constrained through low-rank and bottlenecked parameterizations, limiting both model complexity and computational overhead. Experiments across multiple public medical imaging benchmarks demonstrate that the proposed approach consistently improves subgroup-level performance without sacrificing overall accuracy. On the PAD-UFES-20 dataset, our method outperforms the strongest competing baseline by 4.1% in recall and 4.4% in F1 score, with larger gains observed for underrepresented patient populations.", "AI": {"tldr": "HyperAdapt\u662f\u4e00\u4e2a\u60a3\u8005\u6761\u4ef6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u751f\u6210\u6b8b\u5dee\u8c03\u5236\u53c2\u6570\u6765\u6539\u5584\u533b\u7597AI\u6a21\u578b\u5728\u4e0d\u540c\u60a3\u8005\u4e9a\u7ec4\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5171\u4eab\u8bca\u65ad\u6a21\u578b\u3002", "motivation": "\u533b\u7597AI\u6a21\u578b\u5728\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u4e2d\u8868\u73b0\u4e0d\u5747\uff0c\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u654f\u611f\u5c5e\u6027\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u800c\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u7ed3\u5408\u60a3\u8005\u80cc\u666f\u4fe1\u606f\u8fdb\u884c\u8bca\u65ad\u3002", "method": "\u5c06\u5e74\u9f84\u3001\u6027\u522b\u7b49\u4e34\u5e8a\u76f8\u5173\u5c5e\u6027\u7f16\u7801\u4e3a\u7d27\u51d1\u5d4c\u5165\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u98ce\u683c\u6a21\u5757\u751f\u6210\u6b8b\u5dee\u8c03\u5236\u53c2\u6570\uff0c\u5bf9\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u7684\u9009\u5b9a\u5c42\u8fdb\u884c\u6761\u4ef6\u8c03\u6574\uff0c\u91c7\u7528\u4f4e\u79e9\u548c\u74f6\u9888\u53c2\u6570\u5316\u786e\u4fdd\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u533b\u7597\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u6539\u5584\u4e9a\u7ec4\u6027\u80fd\u800c\u4e0d\u727a\u7272\u6574\u4f53\u51c6\u786e\u6027\uff0c\u5728PAD-UFES-20\u6570\u636e\u96c6\u4e0a\uff0c\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa4.1%\u548c4.4%\u3002", "conclusion": "HyperAdapt\u6846\u67b6\u901a\u8fc7\u60a3\u8005\u6761\u4ef6\u9002\u5e94\uff0c\u5728\u4fdd\u6301\u5171\u4eab\u533b\u7597\u77e5\u8bc6\u7684\u540c\u65f6\u9488\u5bf9\u60a3\u8005\u7279\u5f02\u6027\u53d8\u5f02\u8fdb\u884c\u8c03\u6574\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u533b\u7597AI\u6a21\u578b\u7684\u4e9a\u7ec4\u53ef\u9760\u6027\u3002"}}
{"id": "2601.14007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14007", "abs": "https://arxiv.org/abs/2601.14007", "authors": ["Junyu Zhang", "Yipeng Kang", "Jiong Guo", "Jiayu Zhan", "Junqi Wang"], "title": "BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models", "comment": "34 pagess, 16 figures, 6 tables, submitted to ACL 2026", "summary": "Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.", "AI": {"tldr": "LLMs\u786e\u5b9e\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\uff0c\u5b83\u4eec\u5177\u6709\u4e09\u5c42\u7406\u89e3\u80fd\u529b\uff1a\u62bd\u8c61\u89e3\u91ca\u3001\u5177\u4f53\u843d\u5730\u548c\u5e94\u7528\u8c03\u8282\uff0c\u4ef7\u503c\u6982\u5ff5\u5728\u6a21\u578b\u4e2d\u4f5c\u4e3a\u7a33\u5b9a\u951a\u70b9\u800c\u975e\u53ef\u5851\u6fc0\u6d3b\u5b58\u5728\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u62bd\u8c61\u6982\u5ff5\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5728\u64cd\u4f5c\u7edf\u8ba1\u6a21\u5f0f\u3002\u4ee5\u4eba\u7c7b\u4ef7\u503c\u89c2\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u4ef7\u503c\u89c2\u5177\u6709\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u5bf9\u9f50\u4e2d\u5fc3\u6027\u3002", "method": "\u63d0\u51fa\u62bd\u8c61-\u843d\u5730\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u7406\u89e3\u5206\u89e3\u4e3a\u4e09\u5c42\u80fd\u529b\uff1aA-A\uff08\u62bd\u8c61\u89e3\u91ca\uff09\u3001A-C\uff08\u62bd\u8c61\u5230\u5177\u4f53\uff09\u3001C-C\uff08\u5177\u4f53\u5e94\u7528\uff09\u3002\u4f7f\u7528\u63a2\u6d4b\uff08\u68c0\u6d4b\u5185\u90e8\u6fc0\u6d3b\u4e2d\u7684\u4ef7\u503c\u75d5\u8ff9\uff09\u548c\u5f15\u5bfc\uff08\u4fee\u6539\u8868\u5f81\u4ee5\u6539\u53d8\u884c\u4e3a\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u5f00\u6e90LLM\u548c\u5341\u4e2a\u4ef7\u503c\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u63a2\u6d4b\u663e\u793a\uff1a\u4ec5\u57fa\u4e8e\u62bd\u8c61\u4ef7\u503c\u63cf\u8ff0\u8bad\u7ec3\u7684\u63a2\u6d4b\u6a21\u578b\u80fd\u53ef\u9760\u68c0\u6d4b\u5177\u4f53\u4e8b\u4ef6\u53d9\u8ff0\u548c\u51b3\u7b56\u63a8\u7406\u4e2d\u7684\u76f8\u540c\u4ef7\u503c\uff0c\u5c55\u793a\u8de8\u5c42\u7ea7\u8fc1\u79fb\u3002\u5f15\u5bfc\u63ed\u793a\u4e0d\u5bf9\u79f0\u6027\uff1a\u5e72\u9884\u4ef7\u503c\u8868\u5f81\u80fd\u56e0\u679c\u6027\u5730\u6539\u53d8\u5177\u4f53\u5224\u65ad\u548c\u51b3\u7b56\uff08A-C, C-C\uff09\uff0c\u4f46\u62bd\u8c61\u89e3\u91ca\u4fdd\u6301\u4e0d\u53d8\uff08A-A\uff09\uff0c\u8868\u660e\u7f16\u7801\u7684\u62bd\u8c61\u4ef7\u503c\u4f5c\u4e3a\u7a33\u5b9a\u951a\u70b9\u800c\u975e\u53ef\u5851\u6fc0\u6d3b\u3002", "conclusion": "LLMs\u7ef4\u62a4\u7ed3\u6784\u5316\u7684\u4ef7\u503c\u8868\u5f81\uff0c\u8fde\u63a5\u62bd\u8c61\u4e0e\u884c\u52a8\uff0c\u4e3a\u6784\u5efa\u4ef7\u503c\u9a71\u52a8\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u548c\u64cd\u4f5c\u6027\u57fa\u7840\uff0c\u53ef\u5b9e\u73b0\u66f4\u900f\u660e\u3001\u53ef\u6cdb\u5316\u7684\u5bf9\u9f50\u548c\u63a7\u5236\u3002"}}
{"id": "2601.13126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13126", "abs": "https://arxiv.org/abs/2601.13126", "authors": ["Mattia D'Urso", "Emanuele Santellani", "Christian Sormann", "Mattia Rossi", "Andreas Kuhn", "Friedrich Fraundorfer"], "title": "A Streamlined Attention-Based Network for Descriptor Extraction", "comment": "Accepted to 3DV 2026", "summary": "We introduce SANDesc, a Streamlined Attention-Based Network for Descriptor extraction that aims to improve on existing architectures for keypoint description.\n  Our descriptor network learns to compute descriptors that improve matching without modifying the underlying keypoint detector. We employ a revised U-Net-like architecture enhanced with Convolutional Block Attention Modules and residual paths, enabling effective local representation while maintaining computational efficiency. We refer to the building blocks of our model as Residual U-Net Blocks with Attention. The model is trained using a modified triplet loss in combination with a curriculum learning-inspired hard negative mining strategy, which improves training stability.\n  Extensive experiments on HPatches, MegaDepth-1500, and the Image Matching Challenge 2021 show that training SANDesc on top of existing keypoint detectors leads to improved results on multiple matching tasks compared to the original keypoint descriptors. At the same time, SANDesc has a model complexity of just 2.4 million parameters.\n  As a further contribution, we introduce a new urban dataset featuring 4K images and pre-calibrated intrinsics, designed to evaluate feature extractors. On this benchmark, SANDesc achieves substantial performance gains over the existing descriptors while operating with limited computational resources.", "AI": {"tldr": "SANDesc\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f7b\u91cf\u7ea7\u63cf\u8ff0\u7b26\u63d0\u53d6\u7f51\u7edc\uff0c\u53ef\u5728\u4e0d\u4fee\u6539\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5339\u914d\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u4ec5240\u4e07\u3002", "motivation": "\u73b0\u6709\u63cf\u8ff0\u7b26\u63d0\u53d6\u67b6\u6784\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u5347\u5339\u914d\u6027\u80fd\u800c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\u548c\u6b8b\u5dee\u8def\u5f84\uff0c\u4f7f\u7528\u5e26\u8bfe\u7a0b\u5b66\u4e60\u542f\u53d1\u7684\u96be\u8d1f\u6837\u672c\u6316\u6398\u7b56\u7565\u7684\u6539\u8fdb\u4e09\u5143\u7ec4\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728HPatches\u3001MegaDepth-1500\u548cImage Matching Challenge 2021\u7b49\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u63cf\u8ff0\u7b26\uff0c\u5e76\u5728\u65b0\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SANDesc\u662f\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7684\u63cf\u8ff0\u7b26\u63d0\u53d6\u7f51\u7edc\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5339\u914d\u6027\u80fd\uff0c\u540c\u65f6\u8d21\u732e\u4e86\u65b0\u7684\u57ce\u5e02\u6570\u636e\u96c6\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u8bc4\u4f30\u3002"}}
{"id": "2601.14032", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14032", "abs": "https://arxiv.org/abs/2601.14032", "authors": ["Hongli Zhou", "Hui Huang", "Wei Liu", "Chenglong Wang", "Xingyuan Bu", "Lvyuan Han", "Fuhai Song", "Muyun Yang", "Wenhao Jiang", "Hailong Cao", "Tiejun Zhao"], "title": "RM-Distiller: Exploiting Generative LLM for Reward Model Distillation", "comment": null, "summary": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.", "AI": {"tldr": "RM-Distiller\uff1a\u5229\u7528\u751f\u6210\u5f0fLLM\u7684\u591a\u65b9\u9762\u80fd\u529b\uff08\u7cbe\u70bc\u3001\u8bc4\u5206\u3001\u751f\u6210\uff09\u8fdb\u884c\u5956\u52b1\u6a21\u578b\u84b8\u998f\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5c06\u6559\u5e08\u6a21\u578b\u89c6\u4e3a\u7b80\u5355\u7684\u4e8c\u5143\u6807\u6ce8\u5668\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e30\u5bcc\u77e5\u8bc6\u548c\u591a\u65b9\u9762\u80fd\u529b\uff0c\u5bfc\u81f4\u5956\u52b1\u6a21\u578b\u8d28\u91cf\u53d7\u9650", "method": "\u63d0\u51faRM-Distiller\u6846\u67b6\uff0c\u7cfb\u7edf\u5229\u7528\u6559\u5e08LLM\u7684\u4e09\u65b9\u9762\u80fd\u529b\uff1a1) \u7cbe\u70bc\u80fd\u529b\uff1a\u5408\u6210\u9ad8\u5ea6\u76f8\u5173\u7684\u54cd\u5e94\u5bf9\u521b\u5efa\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u4fe1\u53f7\uff1b2) \u8bc4\u5206\u80fd\u529b\uff1a\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u4f18\u5316\u76ee\u6807\u6307\u5bfcRM\u6355\u6349\u7cbe\u786e\u504f\u597d\u5f3a\u5ea6\uff1b3) \u751f\u6210\u80fd\u529b\uff1a\u7ed3\u5408\u6559\u5e08\u751f\u6210\u5206\u5e03\u6765\u6b63\u5219\u5316RM\u4ee5\u4fdd\u7559\u57fa\u7840\u8bed\u8a00\u77e5\u8bc6", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRM-Distiller\u5728\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\uff0c\u8bc1\u660e\u5229\u7528\u6559\u5e08\u591a\u65b9\u9762\u80fd\u529b\u5bf9\u6709\u6548\u5956\u52b1\u5efa\u6a21\u81f3\u5173\u91cd\u8981", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u4ece\u751f\u6210\u5f0fLLM\u8fdb\u884c\u5956\u52b1\u6a21\u578b\u84b8\u998f\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u5145\u5206\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u591a\u65b9\u9762\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u8d28\u91cf\u548c\u5bf9\u9f50\u6548\u679c"}}
{"id": "2601.13128", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13128", "abs": "https://arxiv.org/abs/2601.13128", "authors": ["Sung Ju Lee", "Nam Ik Cho"], "title": "PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain", "comment": "Accepted to the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "The proliferation of hyper-realistic images from Latent Diffusion Models (LDMs) demands robust watermarking, yet existing post-hoc methods are prohibitively slow due to iterative optimization or inversion processes. We introduce PhaseMark, a single-shot, optimization-free framework that directly modulates the phase in the VAE latent frequency domain. This approach makes PhaseMark thousands of times faster than optimization-based techniques while achieving state-of-the-art resilience against severe attacks, including regeneration, without degrading image quality. We analyze four modulation variants, revealing a clear performance-quality trade-off. PhaseMark demonstrates a new paradigm where efficient, resilient watermarking is achieved by exploiting intrinsic latent properties.", "AI": {"tldr": "PhaseMark\uff1a\u4e00\u79cd\u57fa\u4e8eVAE\u6f5c\u5728\u9891\u7387\u57df\u76f8\u4f4d\u8c03\u5236\u7684\u5355\u6b21\u4f18\u5316\u81ea\u7531\u6c34\u5370\u6846\u67b6\uff0c\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5feb\u6570\u5343\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u6297\u653b\u51fb\u80fd\u529b\u548c\u56fe\u50cf\u8d28\u91cf", "motivation": "\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u8d85\u903c\u771f\u56fe\u50cf\u7684\u6fc0\u589e\u9700\u8981\u9c81\u68d2\u7684\u6c34\u5370\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7684\u540e\u5904\u7406\u65b9\u6cd5\u7531\u4e8e\u8fed\u4ee3\u4f18\u5316\u6216\u53cd\u6f14\u8fc7\u7a0b\u800c\u901f\u5ea6\u8fc7\u6162", "method": "PhaseMark\u76f4\u63a5\u5728VAE\u6f5c\u5728\u9891\u7387\u57df\u4e2d\u8c03\u5236\u76f8\u4f4d\uff0c\u91c7\u7528\u5355\u6b21\u3001\u65e0\u4f18\u5316\u7684\u6846\u67b6\uff0c\u5206\u6790\u4e86\u56db\u79cd\u8c03\u5236\u53d8\u4f53\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb", "result": "PhaseMark\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u6280\u672f\u5feb\u6570\u5343\u500d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6297\u653b\u51fb\u80fd\u529b\uff08\u5305\u62ec\u518d\u751f\u653b\u51fb\uff09\uff0c\u4e14\u4e0d\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf", "conclusion": "PhaseMark\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u6f5c\u5728\u5c5e\u6027\u7684\u5185\u5728\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6c34\u5370\u6280\u672f"}}
{"id": "2601.14041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14041", "abs": "https://arxiv.org/abs/2601.14041", "authors": ["Yunhe Wang", "Kai Han", "Huiling Zhen", "Yuchuan Tian", "Hanting Chen", "Yongbing Huang", "Yufei Cui", "Yingte Shu", "Shan Gao", "Ismail Elezi", "Roy Vaughan Miles", "Songcen Xu", "Feng Wen", "Chao Xu", "Sinan Zeng", "Dacheng Tao"], "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants", "comment": null, "summary": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524d\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u5341\u5927\u6311\u6218\uff0c\u9700\u8981\u5efa\u7acb\u6269\u6563\u539f\u751f\u751f\u6001\u7cfb\u7edf\u6765\u5b9e\u73b0\u4e0b\u4e00\u4ee3AI\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u5b58\u5728\u56e0\u679c\u74f6\u9888\u9650\u5236\u5168\u5c40\u7ed3\u6784\u9884\u89c1\u548c\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\u3002\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6574\u4f53\u3001\u53cc\u5411\u7684\u53bb\u566a\u751f\u6210\u65b9\u5f0f\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u53d1\u6325\uff0c\u4ecd\u53d7\u9650\u4e8e\u81ea\u56de\u5f52\u9057\u7559\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6218\u7565\u8def\u7ebf\u56fe\uff0c\u5305\u542b\u56db\u5927\u652f\u67f1\uff1a\u57fa\u7840\u67b6\u6784\u3001\u7b97\u6cd5\u4f18\u5316\u3001\u8ba4\u77e5\u63a8\u7406\u548c\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u3002\u5efa\u8bae\u8f6c\u5411\u6269\u6563\u539f\u751f\u751f\u6001\u7cfb\u7edf\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u6807\u8bb0\u5316\u3001\u4e3b\u52a8\u91cd\u63a9\u7801\u548c\u6f5c\u5728\u601d\u7ef4\u7b49\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7684\u5341\u5927\u6839\u672c\u6311\u6218\uff0c\u5305\u62ec\u67b6\u6784\u60ef\u6027\u3001\u68af\u5ea6\u7a00\u758f\u6027\u548c\u7ebf\u6027\u63a8\u7406\u9650\u5236\u7b49\uff0c\u8fd9\u4e9b\u963b\u788d\u4e86DLM\u8fbe\u5230\"GPT-4\u65f6\u523b\"\u7684\u7a81\u7834\u3002", "conclusion": "\u5411\u6269\u6563\u539f\u751f\u751f\u6001\u7cfb\u7edf\u7684\u8f6c\u53d8\u5bf9\u4e8e\u5f00\u53d1\u5177\u5907\u590d\u6742\u7ed3\u6784\u63a8\u7406\u3001\u52a8\u6001\u81ea\u6211\u4fee\u6b63\u548c\u65e0\u7f1d\u591a\u6a21\u6001\u96c6\u6210\u7684\u4e0b\u4e00\u4ee3AI\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u7a81\u7834\u56e0\u679c\u89c6\u91ce\u7684\u9650\u5236\u3002"}}
{"id": "2601.13132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13132", "abs": "https://arxiv.org/abs/2601.13132", "authors": ["Kim Yu-Ji", "Dahye Lee", "Kim Jun-Seong", "GeonU Kim", "Nam Hyeon-Woo", "Yongjin Kwon", "Yu-Chiang Frank Wang", "Jaesung Choe", "Tae-Hyun Oh"], "title": "GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning", "comment": "Project page: https://gaussexplorer.github.io/", "summary": "We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.", "AI": {"tldr": "GaussExplorer\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5177\u8eab\u63a2\u7d22\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u573a\u666f\u4e2d\u5b9e\u73b0\u95ee\u9898\u9a71\u52a8\u7684\u63a2\u7d22\u548c\u63a8\u7406\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u57fa\u4e8e\u8bed\u8a00\u5d4c\u5165\u76843DGS\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u7ec4\u5408\u5f0f\u8bed\u8a00\u67e5\u8be2\uff1b2\uff09\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u7684RGB-D\u7ed3\u6784\u5316\u8bb0\u5fc6\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u7a7a\u95f4\u57fa\u7840\uff0c\u4f46\u53d7\u9650\u4e8e\u9884\u56fa\u5b9a\u7684\u89c6\u89d2\u3002", "method": "\u57283DGS\u57fa\u7840\u4e0a\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u9996\u5148\u8bc6\u522b\u4e0e\u67e5\u8be2\u95ee\u9898\u6700\u76f8\u5173\u7684\u9884\u6355\u83b7\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u5176\u8c03\u6574\u4e3a\u65b0\u9896\u89c6\u89d2\u4ee5\u66f4\u51c6\u786e\u5730\u6355\u6349\u89c6\u89c9\u4fe1\u606f\u4f9bVLM\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5c06\u57fa\u4e8eVLM\u7684\u63a8\u7406\u4e0e3DGS\u96c6\u6210\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "GaussExplorer\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u54083DGS\u548cVLM\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u7ec4\u5408\u5f0f\u8bed\u8a00\u67e5\u8be2\u7684\u6709\u6548\u5904\u7406\uff0c\u4e3a\u5177\u8eab\u63a2\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.14046", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14046", "abs": "https://arxiv.org/abs/2601.14046", "authors": ["Shikhar Bharadwaj", "Chin-Jou Li", "Yoonjae Kim", "Kwanghee Choi", "Eunjung Yeo", "Ryan Soh-Eun Shim", "Hanyu Zhou", "Brendon Boldt", "Karen Rosero Jacome", "Kalvin Chang", "Darsh Agrawal", "Keer Xu", "Chao-Han Huck Yang", "Jian Zhu", "Shinji Watanabe", "David R. Mortensen"], "title": "PRiSM: Benchmarking Phone Realization in Speech Models", "comment": null, "summary": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.", "AI": {"tldr": "PRiSM\u662f\u9996\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5185\u5728\u548c\u5916\u5728\u8bc4\u4f30\u63ed\u793a\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u97f3\u4f4d\u611f\u77e5\u76f2\u70b9\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u8bad\u7ec3\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cencoder-CTC\u6a21\u578b\u6700\u7a33\u5b9a\uff0c\u4e13\u7528\u6a21\u578b\u4ecd\u4f18\u4e8e\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u8bc4\u4f30\u53ea\u5173\u6ce8\u8868\u5c42\u8f6c\u5f55\u51c6\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u97f3\u4f4d\u611f\u77e5\u80fd\u529b\u7684\u6df1\u5165\u8bc4\u4f30\u3002\u9700\u8981\u5f00\u53d1\u57fa\u51c6\u6d4b\u8bd5\u6765\u63ed\u793a\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u97f3\u4f4d\u611f\u77e5\u65b9\u9762\u7684\u76f2\u70b9\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u8bed\u97f3\u6a21\u578b\u53d1\u5c55\u3002", "method": "PRiSM\u57fa\u51c6\u6807\u51c6\u5316\u8f6c\u5f55\u8bc4\u4f30\uff0c\u5e76\u5728\u4e34\u5e8a\u3001\u6559\u80b2\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u901a\u8fc7\u8f6c\u5f55\u548c\u8868\u793a\u63a2\u9488\u8bc4\u4f30\u4e0b\u6e38\u5b9e\u7528\u6027\u3002\u5305\u542b\u4ee3\u7801\u3001\u914d\u65b9\u548c\u6570\u636e\u96c6\u53d1\u5e03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u8bad\u7ec3\u671f\u95f4\u7684\u591a\u8bed\u8a00\u66b4\u9732\u5bf9\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1b2) encoder-CTC\u6a21\u578b\u6700\u7a33\u5b9a\uff1b3) \u4e13\u7528\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4ecd\u4f18\u4e8e\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "PRiSM\u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u9886\u57df\u5411\u5177\u6709\u5f3a\u5927\u97f3\u4f4d\u80fd\u529b\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u6a21\u578b\u53d1\u5c55\uff0c\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u8d44\u6e90\uff0c\u5e2e\u52a9\u8bc6\u522b\u548c\u6539\u8fdb\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u97f3\u4f4d\u611f\u77e5\u7f3a\u9677\u3002"}}
{"id": "2601.13133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13133", "abs": "https://arxiv.org/abs/2601.13133", "authors": ["Mingshuang Luo", "Ruibing Hou", "Bo Chao", "Hong Chang", "Zimo Liu", "Yaowei Wang", "Shiguang Shan"], "title": "CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks", "comment": "Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures", "summary": "Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.", "AI": {"tldr": "CLASP\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u4efb\u52a1\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528CLIP\u751f\u6210\u591a\u7ea7\u8bed\u4e49\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7Prompt-Controlled MoE\u6a21\u5757\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u96c6\u7684\u6d8c\u73b0\uff0c\u9700\u8981\u4e00\u4e2a\u901a\u7528\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u652f\u6301\u591a\u6837\u5316\u7684\u4eba\u4f53\u4e2d\u5fc3\u4e0b\u6e38\u4efb\u52a1\uff0c\u5982\u76d1\u63a7\u3001\u533b\u7597\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "method": "1. \u5229\u7528CLIP\u751f\u6210\u4f4e\u7ea7\uff08\u8eab\u4f53\u90e8\u4f4d\uff09\u548c\u9ad8\u7ea7\uff08\u5c5e\u6027\uff09\u8bed\u4e49\u4f2a\u6807\u7b7e\uff1b2. \u5c06\u8fd9\u4e9b\u591a\u7ea7\u8bed\u4e49\u7ebf\u7d22\u6574\u5408\u5230\u89c6\u89c9\u8868\u793a\u4e2d\uff1b3. \u4f7f\u7528Prompt-Controlled Mixture-of-Experts\u6a21\u5757\u6839\u636e\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u52a8\u6001\u8c03\u6574\u7279\u5f81\u63d0\u53d6\uff1b4. \u91c7\u7528\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7531CLIP\u751f\u6210\u7684\u90e8\u4f4d\u548c\u5c5e\u6027\u7ea7\u4f2a\u6807\u7b7e\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCLASP\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\u3002", "conclusion": "CLASP\u901a\u8fc7\u7ed3\u5408CLIP\u7684\u591a\u7ea7\u8bed\u4e49\u4f2a\u6807\u7b7e\u548cPrompt-Controlled MoE\u6a21\u5757\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u9002\u5e94\u7684\u4eba\u4f53\u4e2d\u5fc3\u89c6\u89c9\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.14050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14050", "abs": "https://arxiv.org/abs/2601.14050", "authors": ["Yuxin Chen", "Zhengzhou Cai", "Xiangtian Ji", "Weixiang Zhao", "An Zhang", "Xiang Wang", "Tat-Seng Chua"], "title": "Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86MoE\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5904\u7406\u673a\u5236\uff0c\u53d1\u73b0\u8def\u7531\u884c\u4e3a\u4e0e\u8bed\u7cfb\u5bf9\u9f50\u3001\u4e13\u5bb6\u4f7f\u7528\u5448\u73b0\u5c42\u6b21\u5316\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8def\u7531\u5f15\u5bfc\u7684\u8c03\u63a7\u65b9\u6cd5\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1MoE\u67b6\u6784\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3001\u6027\u80fd\u63d0\u5347\u539f\u56e0\u4ee5\u53ca\u8de8\u8bed\u8a00\u5dee\u5f02\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u6790MoE\u6a21\u578b\u7684\u8def\u7531\u884c\u4e3a\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u6a21\u5f0f\u3002", "method": "\u5bf9MoE\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u8bed\u8a00\u548c\u7f51\u7edc\u6df1\u5ea6\u4e0b\u7684\u8def\u7531\u884c\u4e3a\u4e0e\u4e13\u5bb6\u4e13\u4e1a\u5316\uff1b\u63d0\u51fa\u8def\u7531\u5f15\u5bfc\u7684\u8c03\u63a7\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u81ea\u9002\u5e94\u5730\u5c06\u4e2d\u95f4\u5c42\u8def\u7531\u5bfc\u5411\u4e0e\u4e3b\u5bfc\u8bed\u8a00\u76f8\u5173\u7684\u5171\u4eab\u4e13\u5bb6\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a1\uff09\u8def\u7531\u884c\u4e3a\u4e0e\u8bed\u8a00\u5bb6\u65cf\u5bf9\u9f50\uff1b2\uff09\u4e13\u5bb6\u4f7f\u7528\u5448\u73b0\u6e05\u6670\u7684\u5c42\u6b21\u5316\u6a21\u5f0f\uff1b3\uff09\u9ad8\u8d44\u6e90\u8bed\u8a00\u4f9d\u8d56\u5171\u4eab\u4e13\u5bb6\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u66f4\u591a\u4f7f\u7528\u8bed\u8a00\u4e13\u5c5e\u4e13\u5bb6\u4f46\u6027\u80fd\u8f83\u5f31\uff1b4\uff09\u65e9\u671f\u548c\u665a\u671fMoE\u5c42\u652f\u6301\u8bed\u8a00\u7279\u5b9a\u5904\u7406\uff0c\u4e2d\u95f4\u5c42\u4f5c\u4e3a\u8bed\u8a00\u65e0\u5173\u7684\u5bb9\u91cf\u67a2\u7ebd\u3002", "conclusion": "MoE\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5904\u7406\u5177\u6709\u9ad8\u5ea6\u7ed3\u6784\u5316\u7279\u5f81\uff0c\u57fa\u4e8e\u8def\u7531\u884c\u4e3a\u6d1e\u5bdf\u63d0\u51fa\u7684\u8c03\u63a7\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8bed\u8a00\u76f8\u5173\u7684\u8bed\u8a00\u5bf9\u4e2d\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2601.13142", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13142", "abs": "https://arxiv.org/abs/2601.13142", "authors": ["Zhantao Ma", "Quanfeng Lu", "Shuai Zhong", "Dahai Yu", "Ping Luo", "Michael K. Ng"], "title": "TVWorld: Foundations for Remote-Control TV Agents", "comment": null, "summary": "Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \\textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \\textbf{TVWorld-N} for topology-aware navigation and \\textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \\emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \\textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.", "AI": {"tldr": "TVWorld\uff1a\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u79bb\u7ebf\u7535\u89c6\u5bfc\u822a\u62bd\u8c61\u57fa\u51c6\uff0c\u5305\u542bTVWorld-N\uff08\u62d3\u6251\u611f\u77e5\u5bfc\u822a\uff09\u548cTVWorld-G\uff08\u7126\u70b9\u611f\u77e5\u5b9a\u4f4d\uff09\u4e24\u4e2a\u4e92\u8865\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u667a\u80fd\u4f53\u5728\u7535\u89c6\u5bfc\u822a\u4e2d\u7684\u62d3\u6251\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u62d3\u6251\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\u548cTVTheseus\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u70b9\u6309\u5f0f\u4ea4\u4e92\uff0c\u800c\u65e5\u5e38\u751f\u6d3b\u4e2d\u5e38\u89c1\u7684\u9065\u63a7\u5f0f\u7535\u89c6\u5bfc\u822a\u4ea4\u4e92\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u590d\u73b0\u3001\u514d\u90e8\u7f72\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u63d0\u51faTVWorld\u2014\u2014\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u79bb\u7ebf\u7535\u89c6\u5bfc\u822a\u62bd\u8c61\u57fa\u51c6\uff1b2. \u6784\u5efa\u4e24\u4e2a\u4e92\u8865\u57fa\u51c6\uff1aTVWorld-N\uff08\u62d3\u6251\u611f\u77e5\u5bfc\u822a\uff09\u548cTVWorld-G\uff08\u7126\u70b9\u611f\u77e5\u5b9a\u4f4d\uff09\uff1b3. \u63d0\u51fa\u62d3\u6251\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u62d3\u6251\u611f\u77e5\u80fd\u529b\u6ce8\u5165\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b4. \u5f00\u53d1TVTheseus\u2014\u2014\u4e13\u95e8\u7528\u4e8e\u7535\u89c6\u5bfc\u822a\u7684\u57fa\u7840\u6a21\u578b\u3002", "result": "TVTheseus\u5728TVWorld-N\u4e0a\u8fbe\u523068.3%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86Gemini 3 Flash\u7b49\u5f3a\u95ed\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5206\u6790\u4e3a\u5f00\u53d1\u6709\u6548\u7684\u7535\u89c6\u4f7f\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u9065\u63a7\u5f0f\u7535\u89c6\u5bfc\u822a\u4ea4\u4e92\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7TVWorld\u57fa\u51c6\u63ed\u793a\u4e86\u73b0\u6709\u667a\u80fd\u4f53\u7684\u62d3\u6251\u611f\u77e5\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u62d3\u6251\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\u548cTVTheseus\u6a21\u578b\u4e3a\u7535\u89c6\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8bbe\u5907\u63a7\u5236\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.14051", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14051", "abs": "https://arxiv.org/abs/2601.14051", "authors": ["Peter Devine", "Mardhiyah Sanni", "Farid Adilazuarda", "Julieta Gil Loizaga", "Barry Haddow"], "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models", "comment": null, "summary": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.", "AI": {"tldr": "Kakugo\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7ba1\u9053\uff0c\u4ec5\u9700\u8bed\u8a00\u540d\u79f0\u5373\u53ef\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3\u901a\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u5408\u6210\u63d0\u793a\u548c\u7ffb\u8bd1\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4e3a54\u79cd\u8bed\u8a00\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\uff0c\u6bcf\u8bed\u8a00\u6210\u672c\u4f4e\u4e8e50\u7f8e\u5143\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u548cAI\u6a21\u578b\u7684\u95ee\u9898\uff0c\u4e3a\u8fd9\u4e9b\u8bed\u8a00\u793e\u533a\u63d0\u4f9b\u53ef\u8bbf\u95ee\u7684\u8bed\u8a00\u7279\u5b9aAI\u5f00\u53d1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u578b\u6559\u5e08\u6a21\u578b\u751f\u6210\u5408\u6210\u63d0\u793a\u5e76\u7ffb\u8bd1\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u521b\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u9700\u8bed\u8a00\u540d\u79f0\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u4e3a54\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u521b\u5efa\u4e86\u8bad\u7ec3\u6570\u636e\u548cSLMs\uff0c\u5728\u7ffb\u8bd1\u3001\u5206\u7c7b\u3001\u95ee\u7b54\u7b49NLP\u4efb\u52a1\u4e0a\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u6bcf\u8bed\u8a00\u603b\u6210\u672c\u4f4e\u4e8e50\u7f8e\u5143\u3002", "conclusion": "Kakugo\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u7ba1\u9053\uff0c\u4f7f\u793e\u533a\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u8bed\u8a00\u7279\u5b9a\u7684AI\u6a21\u578b\u3002"}}
{"id": "2601.13148", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13148", "abs": "https://arxiv.org/abs/2601.13148", "authors": ["Richard Shaw", "Youngkyoon Jang", "Athanasios Papaioannou", "Arthur Moreau", "Helisa Dhamo", "Zhensong Zhang", "Eduardo P\u00e9rez-Pellitero"], "title": "ICo3D: An Interactive Conversational 3D Virtual Human", "comment": "Accepted by International Journal on Computer Vision (IJCV). Project page: https://ico3d.github.io/. This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in International Journal of Computer Vision and is available online at https://doi.org/10.1007/s11263-025-02725-8", "summary": "This work presents Interactive Conversational 3D Virtual Human (ICo3D), a method for generating an interactive, conversational, and photorealistic 3D human avatar. Based on multi-view captures of a subject, we create an animatable 3D face model and a dynamic 3D body model, both rendered by splatting Gaussian primitives. Once merged together, they represent a lifelike virtual human avatar suitable for real-time user interactions. We equip our avatar with an LLM for conversational ability. During conversation, the audio speech of the avatar is used as a driving signal to animate the face model, enabling precise synchronization. We describe improvements to our dynamic Gaussian models that enhance photorealism: SWinGS++ for body reconstruction and HeadGaS++ for face reconstruction, and provide as well a solution to merge the separate face and body models without artifacts. We also present a demo of the complete system, showcasing several use cases of real-time conversation with the 3D avatar. Our approach offers a fully integrated virtual avatar experience, supporting both oral and written form interactions in immersive environments. ICo3D is applicable to a wide range of fields, including gaming, virtual assistance, and personalized education, among others. Project page: https://ico3d.github.io/", "AI": {"tldr": "ICo3D\u662f\u4e00\u79cd\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u5bf9\u8bdd\u5f0f\u3001\u903c\u771f3D\u865a\u62df\u4eba\u50cf\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u53ef\u52a8\u753b\u76843D\u9762\u90e8\u6a21\u578b\u548c\u52a8\u60013D\u8eab\u4f53\u6a21\u578b\uff0c\u4f7f\u7528\u9ad8\u65af\u57fa\u5143\u6e32\u67d3\uff0c\u5e76\u96c6\u6210LLM\u5b9e\u73b0\u5bf9\u8bdd\u80fd\u529b\u3002", "motivation": "\u521b\u5efa\u80fd\u591f\u5b9e\u65f6\u4ea4\u4e92\u3001\u5bf9\u8bdd\u4e14\u903c\u771f\u76843D\u865a\u62df\u4eba\u50cf\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u3001\u865a\u62df\u52a9\u624b\u3001\u4e2a\u6027\u5316\u6559\u80b2\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u63d0\u4f9b\u6c89\u6d78\u5f0f\u7684\u865a\u62df\u5316\u8eab\u4f53\u9a8c\u3002", "method": "\u57fa\u4e8e\u591a\u89c6\u89d2\u6355\u6349\u521b\u5efa\u53ef\u52a8\u753b\u76843D\u9762\u90e8\u6a21\u578b\u548c\u52a8\u60013D\u8eab\u4f53\u6a21\u578b\uff0c\u4f7f\u7528\u9ad8\u65af\u57fa\u5143\u6e32\u67d3\uff1b\u6539\u8fdbSWinGS++\u7528\u4e8e\u8eab\u4f53\u91cd\u5efa\u548cHeadGaS++\u7528\u4e8e\u9762\u90e8\u91cd\u5efa\uff1b\u96c6\u6210LLM\u5b9e\u73b0\u5bf9\u8bdd\u80fd\u529b\uff1b\u4f7f\u7528\u97f3\u9891\u8bed\u97f3\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u5b9e\u73b0\u7cbe\u786e\u540c\u6b65\u3002", "result": "\u5f00\u53d1\u51faICo3D\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u4ea4\u4e92\u5f0f3D\u865a\u62df\u4eba\u50cf\uff0c\u652f\u6301\u5b9e\u65f6\u5bf9\u8bdd\uff0c\u9762\u90e8\u52a8\u753b\u4e0e\u8bed\u97f3\u7cbe\u786e\u540c\u6b65\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u4e2a\u5b9e\u65f6\u5bf9\u8bdd\u7528\u4f8b\u3002", "conclusion": "ICo3D\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u96c6\u6210\u7684\u865a\u62df\u5316\u8eab\u4f53\u9a8c\uff0c\u652f\u6301\u53e3\u5934\u548c\u4e66\u9762\u5f62\u5f0f\u7684\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u6c89\u6d78\u5f0f\u73af\u5883\uff0c\u5728\u6e38\u620f\u3001\u865a\u62df\u52a9\u624b\u3001\u6559\u80b2\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.14063", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.14063", "abs": "https://arxiv.org/abs/2601.14063", "authors": ["Mohsinul Kabir", "Tasnim Ahmed", "Md Mezbaur Rahman", "Shaoxiong Ji", "Hassan Alhuzali", "Sophia Ananiadou"], "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs", "comment": "30 Pages, 13 Figures", "summary": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.", "AI": {"tldr": "XCR-Bench\u662f\u4e00\u4e2a\u5305\u542b4.9k\u5e73\u884c\u53e5\u5b50\u548c1,098\u4e2a\u72ec\u7279\u6587\u5316\u7279\u5b9a\u9879\u76ee\u7684\u8de8\u6587\u5316\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u9002\u5e94\u6587\u5316\u7279\u5b9a\u9879\u76ee\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6587\u5316\u80fd\u529b\u53d7\u5230\u9ad8\u8d28\u91cf\u6587\u5316\u7279\u5b9a\u9879\u76ee\u6807\u6ce8\u8bed\u6599\u5e93\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u5de5\u5177\u6765\u6df1\u5165\u5206\u6790\u6a21\u578b\u5728\u6587\u5316\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u7ed3\u5408Newmark\u7684\u6587\u5316\u7279\u5b9a\u9879\u76ee\u6846\u67b6\u548cHall\u7684\u6587\u5316\u4e09\u5143\u8bba\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u63a8\u7406\u4efb\u52a1\u548c\u76f8\u5e94\u8bc4\u4f30\u6307\u6807\u7684\u5e73\u884c\u53e5\u5b50\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u8868\u5c42\u6587\u5316\u5143\u7d20\u5230\u6df1\u5c42\u793e\u4f1a\u89c4\u8303\u3001\u4fe1\u4ef0\u548c\u4ef7\u503c\u89c2\u3002", "result": "\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u9002\u5e94\u793e\u4ea4\u793c\u4eea\u548c\u6587\u5316\u53c2\u8003\u76f8\u5173\u7684\u6587\u5316\u7279\u5b9a\u9879\u76ee\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u5f31\u70b9\uff0c\u5e76\u4e14\u5728\u6587\u5316\u9002\u5e94\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u533a\u57df\u548c\u6c11\u65cf\u5b97\u6559\u504f\u89c1\u3002", "conclusion": "XCR-Bench\u4e3a\u8de8\u6587\u5316\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6587\u5316\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u6df1\u5c42\u6587\u5316\u5143\u7d20\u7684\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2601.13166", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13166", "abs": "https://arxiv.org/abs/2601.13166", "authors": ["Pedro M. Gordaliza", "Jaume Banus", "Beno\u00eet G\u00e9rin", "Maxence Wynen", "Nataliia Molchanova", "Jonas Richiardi", "Meritxell Bach Cuadra"], "title": "From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models", "comment": "Work presented at the SSL3D Challenge (1st place, ResEnc-L track) and FOMO Challenge (1st place, Methods track) on Brain MRI Foundation Models at MICCAI 2025", "summary": "Developing Foundation Models for medical image analysis is essential to overcome the unique challenges of radiological tasks. The first challenges of this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our solution ranked first in tracks of both contests. It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge. Notably, our models trained 1-2 orders of magnitude faster and were 10 times smaller than competing transformer-based approaches. Models are available here: https://github.com/jbanusco/BrainFM4Challenges.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e3D\u8111MRI\u5206\u6790\u7684\u533b\u5b66\u5f71\u50cf\u57fa\u7840\u6a21\u578b\uff0c\u5728MICCAI 2025\u7684SSL3D\u548cFOMO25\u6311\u6218\u8d5b\u4e2d\u5747\u83b7\u7b2c\u4e00\u540d\uff0c\u91c7\u7528U-Net CNN\u67b6\u6784\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4Transformer\u65b9\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u6a21\u578b\u4f53\u79ef\u5c0f10\u500d\u3002", "motivation": "\u5f00\u53d1\u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u57fa\u7840\u6a21\u578b\u5bf9\u4e8e\u514b\u670d\u653e\u5c04\u5b66\u4efb\u52a1\u7684\u72ec\u7279\u6311\u6218\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u57283D\u8111MRI\u9886\u57df\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528U-Net CNN\u67b6\u6784\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u548c\u795e\u7ecf\u5f71\u50cf\u9886\u57df\u77e5\u8bc6\u7b56\u7565\uff0c\u76f8\u6bd4Transformer\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "result": "\u5728MICCAI 2025\u7684SSL3D\u548cFOMO25\u6311\u6218\u8d5b\u7684\u4e24\u4e2a\u8d5b\u9053\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u6bd4Transformer\u65b9\u6cd5\u5feb1-2\u4e2a\u6570\u91cf\u7ea7\uff0c\u6a21\u578b\u4f53\u79ef\u5c0f10\u500d\u3002", "conclusion": "U-Net CNN\u67b6\u6784\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\u57283D\u8111MRI\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8eTransformer\u65b9\u6cd5\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.14105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14105", "abs": "https://arxiv.org/abs/2601.14105", "authors": ["Olesya Razuvayevskaya", "Kalina Bontcheva"], "title": "Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks", "comment": "In Proceedings of the ACM Web Conference 2026 (WWW 2026)", "summary": "This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u4f17\u5305\u4e0e\u4e13\u4e1a\u8f9f\u8c23\u5728\u8bf4\u670d\u6280\u5de7\u4f7f\u7528\u4e0a\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u4fee\u8f9e\u5dee\u5f02\uff1b\u4f17\u5305\u8bc4\u4f30\u8005\u80fd\u6709\u6548\u60e9\u7f5a\u95ee\u9898\u4fee\u8f9e\u624b\u6bb5", "motivation": "\u6bd4\u8f83\u4f17\u5305\u4e0e\u4e13\u4e1a\u8f9f\u8c23\u4e2d\u8bf4\u670d\u6280\u5de7\u7684\u4f7f\u7528\u5dee\u5f02\uff0c\u9a8c\u8bc1\u5148\u524d\u5173\u4e8e\u4f17\u5305\u8f9f\u8c23\u66f4\u4f9d\u8d56\u4e3b\u89c2\u6216\u8bf4\u670d\u6027\u8bed\u8a00\u7684\u5047\u8bbe", "method": "\u4f7f\u7528Community Notes\u3001EUvsDisinfo\u548cDatabase of Known Fakes\u4e09\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91cf\u5316\u5206\u6790\u4e0d\u540c\u4e8b\u5b9e\u6838\u67e5\u751f\u6001\u7cfb\u7edf\u4e2d\u8bf4\u670d\u6280\u5de7\u7684\u666e\u904d\u6027\u548c\u7c7b\u578b", "result": "1. \u672a\u53d1\u73b0CNs\u6bd4\u4e13\u4e1a\u8f9f\u8c23\u4f7f\u7528\u66f4\u591a\u8bf4\u670d\u6280\u5de7\uff1b2. \u8bc6\u522b\u51faCNs\u4e0e\u4e13\u4e1a\u8f9f\u8c23\u95f4\u7684\u7cfb\u7edf\u6027\u4fee\u8f9e\u5dee\u5f02\uff0c\u53cd\u6620\u5236\u5ea6\u89c4\u8303\u548c\u4e3b\u9898\u8986\u76d6\u5dee\u5f02\uff1b3. \u867d\u7136\u66f4\u591a\u8bf4\u670d\u5143\u7d20\u7684\u7b14\u8bb0\u83b7\u5f97\u7a0d\u9ad8\u5e2e\u52a9\u6027\u8bc4\u5206\uff0c\u4f46\u4f17\u5305\u8bc4\u4f30\u8005\u80fd\u6709\u6548\u60e9\u7f5a\u7279\u5b9a\u95ee\u9898\u4fee\u8f9e\u624b\u6bb5", "conclusion": "\u4f17\u5305\u4e0e\u4e13\u4e1a\u8f9f\u8c23\u5728\u8bf4\u670d\u6280\u5de7\u4f7f\u7528\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u5b58\u5728\u4fee\u8f9e\u98ce\u683c\u5dee\u5f02\uff1b\u4f17\u5305\u8bc4\u4f30\u673a\u5236\u80fd\u6709\u6548\u8bc6\u522b\u548c\u60e9\u7f5a\u4e0d\u5f53\u4fee\u8f9e\uff0c\u8868\u660e\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\u5177\u6709\u8d28\u91cf\u4fdd\u969c\u80fd\u529b"}}
{"id": "2601.13207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13207", "abs": "https://arxiv.org/abs/2601.13207", "authors": ["Jinnao Li", "Zijian Chen", "Tingzhu Chen", "Changbo Wang"], "title": "GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction", "comment": null, "summary": "Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GTPred\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5730\u7406-\u65f6\u95f4\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5ffd\u7565\u65f6\u95f4\u4fe1\u606f\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u5b9a\u4f4d\u57fa\u51c6\u5927\u591a\u5ffd\u7565\u56fe\u50cf\u4e2d\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u800c\u65f6\u95f4\u4fe1\u606f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7ea6\u675f\u4f4d\u7f6e\u63a8\u65ad\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u540c\u65f6\u8003\u8651\u5730\u7406\u548c\u65f6\u95f4\u4fe1\u606f\u7684\u57fa\u51c6\u6765\u8bc4\u4f30MLLMs\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86GTPred\u57fa\u51c6\uff0c\u5305\u542b370\u5f20\u5168\u7403\u5206\u5e03\u3001\u65f6\u95f4\u8de8\u5ea6\u8d85\u8fc7120\u5e74\u7684\u56fe\u50cf\u3002\u8bc4\u4f30\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u8054\u5408\u8003\u8651\u5e74\u4efd\u548c\u5206\u5c42\u4f4d\u7f6e\u5e8f\u5217\u5339\u914d\uff1b2\uff09\u4f7f\u7528\u7cbe\u5fc3\u6807\u6ce8\u7684\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u94fe\u3002", "result": "\u57288\u4e2a\u4e13\u6709\u548c7\u4e2a\u5f00\u6e90MLLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u5c3d\u7ba1\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u5f3a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u4e16\u754c\u77e5\u8bc6\u548c\u5730\u7406\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff1b2\uff09\u7ed3\u5408\u65f6\u95f4\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u4f4d\u7f6e\u63a8\u65ad\u6027\u80fd\u3002", "conclusion": "GTPred\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u5730\u7406-\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u65f6\u95f4\u4fe1\u606f\u5bf9\u5730\u7406\u5b9a\u4f4d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2601.14112", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14112", "abs": "https://arxiv.org/abs/2601.14112", "authors": ["George Mihaila"], "title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns", "comment": null, "summary": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.", "AI": {"tldr": "ExpNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5b66\u4e60\u4ecetransformer\u6ce8\u610f\u529b\u6a21\u5f0f\u5230token\u91cd\u8981\u6027\u5206\u6570\u7684\u663e\u5f0f\u6620\u5c04\uff0c\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u6ce8\u610f\u529b\u7279\u5f81\u7ec4\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6ce8\u610f\u529b\u89e3\u91ca\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u805a\u5408\u7b56\u7565\u548c\u56fa\u5b9a\u5f52\u56e0\u89c4\u5219\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740transformer\u6a21\u578b\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\uff0c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6ce8\u610f\u529b\u89e3\u91ca\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u7684\u805a\u5408\u7b56\u7565\u548c\u56fa\u5b9a\u5f52\u56e0\u89c4\u5219\uff0c\u800c\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff08\u5982LIME\u3001SHAP\uff09\u5c06\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faExplanation Network (ExpNet)\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u5b66\u4e60\u4ecetransformer\u6ce8\u610f\u529b\u6a21\u5f0f\u5230token\u7ea7\u91cd\u8981\u6027\u5206\u6570\u7684\u663e\u5f0f\u6620\u5c04\u3002\u4e0e\u5148\u524d\u65b9\u6cd5\u4e0d\u540c\uff0cExpNet\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u6ce8\u610f\u529b\u7279\u5f81\u7ec4\u5408\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u9884\u5b9a\u89c4\u5219\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8bc4\u4f30ExpNet\uff0c\u5e76\u4e0e\u6db5\u76d6\u56db\u4e2a\u65b9\u6cd5\u5bb6\u65cf\u7684\u5e7f\u6cdb\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6280\u672f\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "ExpNet\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5b66\u4e60\u6ce8\u610f\u529b\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u89c4\u5219\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e3atransformer\u6a21\u578b\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13208", "abs": "https://arxiv.org/abs/2601.13208", "authors": ["Vikram R Lakkavalli"], "title": "Rethinking Skip Connections: Additive U-Net for Robust and Interpretable Denoising", "comment": null, "summary": "Skip connections are central to U-Net architectures for image denoising, but standard concatenation doubles channel dimensionality and obscures information flow, allowing uncontrolled noise transfer. We propose the Additive U-Net, which replaces concatenative skips with gated additive connections. Each skip pathway is scaled by a learnable non-negative scalar, offering explicit and interpretable control over encoder contributions while avoiding channel inflation. Evaluations on the Kodak-17 denoising benchmark show that Additive U-Net achieves competitive PSNR/SSIM at noise levels \u03c3 = 15, 25, 50, with robustness across kernel schedules and depths. Notably, effective denoising is achieved even without explicit down/up-sampling or forced hierarchies, as the model naturally learns a progression from high-frequency to band-pass to low-frequency features. These results position additive skips as a lightweight and interpretable alternative to concatenation, enabling both efficient design and a clearer understanding of multi-scale information transfer in reconstruction networks.", "AI": {"tldr": "Additive U-Net \u7528\u95e8\u63a7\u52a0\u6cd5\u8fde\u63a5\u66ff\u4ee3\u4f20\u7edf\u7684\u62fc\u63a5\u8df3\u8dc3\u8fde\u63a5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u975e\u8d1f\u6807\u91cf\u63a7\u5236\u7f16\u7801\u5668\u8d21\u732e\uff0c\u907f\u514d\u901a\u9053\u81a8\u80c0\uff0c\u5728\u56fe\u50cf\u53bb\u566a\u4efb\u52a1\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfU-Net\u4e2d\u7684\u62fc\u63a5\u8df3\u8dc3\u8fde\u63a5\u4f1a\u52a0\u500d\u901a\u9053\u7ef4\u5ea6\u5e76\u6a21\u7cca\u4fe1\u606f\u6d41\uff0c\u5bfc\u81f4\u4e0d\u53d7\u63a7\u5236\u7684\u566a\u58f0\u4f20\u9012\u3002\u9700\u8981\u4e00\u79cd\u66f4\u8f7b\u91cf\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u6539\u5584\u591a\u5c3a\u5ea6\u4fe1\u606f\u4f20\u9012\u3002", "method": "\u63d0\u51faAdditive U-Net\uff0c\u7528\u95e8\u63a7\u52a0\u6cd5\u8fde\u63a5\u66ff\u6362\u62fc\u63a5\u8df3\u8dc3\u8fde\u63a5\u3002\u6bcf\u4e2a\u8df3\u8dc3\u8def\u5f84\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u975e\u8d1f\u6807\u91cf\u8fdb\u884c\u7f29\u653e\uff0c\u63d0\u4f9b\u5bf9\u7f16\u7801\u5668\u8d21\u732e\u7684\u663e\u5f0f\u548c\u53ef\u89e3\u91ca\u63a7\u5236\uff0c\u540c\u65f6\u907f\u514d\u901a\u9053\u81a8\u80c0\u3002", "result": "\u5728Kodak-17\u53bb\u566a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdditive U-Net\u5728\u566a\u58f0\u6c34\u5e73\u03c3=15\u300125\u300150\u4e0b\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684PSNR/SSIM\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u6838\u8c03\u5ea6\u548c\u6df1\u5ea6\u4e0b\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002\u5373\u4f7f\u6ca1\u6709\u663e\u5f0f\u7684\u4e0b\u91c7\u6837/\u4e0a\u91c7\u6837\u6216\u5f3a\u5236\u5c42\u6b21\u7ed3\u6784\uff0c\u6a21\u578b\u4e5f\u80fd\u81ea\u7136\u5b66\u4e60\u4ece\u9ad8\u9891\u5230\u5e26\u901a\u518d\u5230\u4f4e\u9891\u7279\u5f81\u7684\u6e10\u8fdb\u8fc7\u7a0b\u3002", "conclusion": "\u52a0\u6cd5\u8df3\u8dc3\u8fde\u63a5\u662f\u62fc\u63a5\u8fde\u63a5\u7684\u4e00\u79cd\u8f7b\u91cf\u7ea7\u548c\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e2\u80fd\u5b9e\u73b0\u9ad8\u6548\u8bbe\u8ba1\uff0c\u53c8\u80fd\u66f4\u6e05\u6670\u5730\u7406\u89e3\u91cd\u5efa\u7f51\u7edc\u4e2d\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u4f20\u9012\u3002"}}
{"id": "2601.14121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14121", "abs": "https://arxiv.org/abs/2601.14121", "authors": ["Jonathan Tonglet", "Iryna Gurevych", "Tinne Tuytelaars", "Marie-Francine Moens"], "title": "NewsRECON: News article REtrieval for image CONtextualization", "comment": "Preprint under review. Code available at https://github.com/jtonglet/arxiv2025-newsrecon", "summary": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.", "AI": {"tldr": "NewsRECON\u901a\u8fc7\u5c06\u65b0\u95fb\u56fe\u7247\u94fe\u63a5\u5230\u76f8\u5173\u6587\u7ae0\u6765\u63a8\u65ad\u62cd\u6444\u65f6\u95f4\u548c\u5730\u70b9\uff0c\u5728\u7f3a\u4e4f\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u65b0\u95fb\u56fe\u7247\u7684\u65f6\u95f4\u548c\u5730\u70b9\u8bc6\u522b\u5bf9\u8bb0\u8005\u548c\u6cd5\u8bc1\u4e13\u5bb6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u5de5\u5177\u7ecf\u5e38\u65e0\u6cd5\u8fd4\u56de\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u7814\u7a76\u9488\u5bf9\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\uff0c\u63d0\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "NewsRECON\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u53cc\u7f16\u7801\u5668\u7528\u4e8e\u68c0\u7d22\u4e8b\u4ef6\u76f8\u5173\u6587\u7ae0\uff1b2) \u4e24\u4e2a\u4ea4\u53c9\u7f16\u7801\u5668\u5206\u522b\u7528\u4e8e\u6309\u5730\u70b9\u548c\u4e8b\u4ef6\u4e00\u81f4\u6027\u91cd\u65b0\u6392\u5e8f\u6587\u7ae0\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8d85\u8fc790,000\u7bc7\u6587\u7ae0\u7684\u8bed\u6599\u5e93\u3002", "result": "\u5728TARA\u548c5Pils-OOC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNewsRECON\u8d85\u8d8a\u4e86\u5148\u524d\u5de5\u4f5c\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u7f3a\u4e4f\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u8bc1\u636e\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\u3002", "conclusion": "NewsRECON\u4e3a\u65b0\u95fb\u56fe\u7247\u7684\u65f6\u95f4\u548c\u5730\u70b9\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13218", "abs": "https://arxiv.org/abs/2601.13218", "authors": ["Igor Vozniak", "Philipp Mueller", "Nils Lipp", "Janis Sprenger", "Konstantin Poddubnyy", "Davit Hovhannisyan", "Christian Mueller", "Andreas Bulling", "Philipp Slusallek"], "title": "ObjectVisA-120: Object-based Visual Attention Prediction in Interactive Street-crossing Environments", "comment": "Accepted for publication at the IEEE Intelligent Vehicles Symposium (IV), 2026", "summary": "The object-based nature of human visual attention is well-known in cognitive science, but has only played a minor role in computational visual attention models so far. This is mainly due to a lack of suitable datasets and evaluation metrics for object-based attention. To address these limitations, we present \\dataset~ -- a novel 120-participant dataset of spatial street-crossing navigation in virtual reality specifically geared to object-based attention evaluations. The uniqueness of the presented dataset lies in the ethical and safety affiliated challenges that make collecting comparable data in real-world environments highly difficult. \\dataset~ not only features accurate gaze data and a complete state-space representation of objects in the virtual environment, but it also offers variable scenario complexities and rich annotations, including panoptic segmentation, depth information, and vehicle keypoints. We further propose object-based similarity (oSIM) as a novel metric to evaluate the performance of object-based visual attention models, a previously unexplored performance characteristic. Our evaluations show that explicitly optimising for object-based attention not only improves oSIM performance but also leads to an improved model performance on common metrics. In addition, we present SUMGraph, a Mamba U-Net-based model, which explicitly encodes critical scene objects (vehicles) in a graph representation, leading to further performance improvements over several state-of-the-art visual attention prediction methods. The dataset, code and models will be publicly released.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5bf9\u8c61\u89c6\u89c9\u6ce8\u610f\u529b\u7684VR\u8857\u9053\u7a7f\u8d8a\u6570\u636e\u96c6\uff0c\u5305\u542b120\u540d\u53c2\u4e0e\u8005\u6570\u636e\uff0c\u5e76\u5f15\u5165\u5bf9\u8c61\u76f8\u4f3c\u6027(oSIM)\u65b0\u6307\u6807\u548c\u57fa\u4e8eMamba U-Net\u7684SUMGraph\u6a21\u578b\uff0c\u5728\u5bf9\u8c61\u6ce8\u610f\u529b\u548c\u4f20\u7edf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u672c\u8d28\u4e0a\u662f\u57fa\u4e8e\u5bf9\u8c61\u7684\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u6a21\u578b\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002\u7279\u522b\u662f\u5728\u8857\u9053\u7a7f\u8d8a\u7b49\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5b58\u5728\u4f26\u7406\u548c\u5b89\u5168\u6311\u6218\u3002", "method": "1) \u521b\u5efa120\u4eba\u53c2\u4e0e\u7684VR\u8857\u9053\u7a7f\u8d8a\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u5305\u542b\u7cbe\u786e\u6ce8\u89c6\u6570\u636e\u3001\u5b8c\u6574\u5bf9\u8c61\u72b6\u6001\u7a7a\u95f4\u3001\u591a\u79cd\u573a\u666f\u590d\u6742\u5ea6\u548c\u4e30\u5bcc\u6807\u6ce8\uff1b2) \u63d0\u51fa\u5bf9\u8c61\u76f8\u4f3c\u6027(oSIM)\u4f5c\u4e3a\u57fa\u4e8e\u5bf9\u8c61\u6ce8\u610f\u529b\u7684\u8bc4\u4f30\u6307\u6807\uff1b3) \u8bbe\u8ba1SUMGraph\u6a21\u578b\uff0c\u5c06\u5173\u952e\u573a\u666f\u5bf9\u8c61(\u8f66\u8f86)\u7f16\u7801\u4e3a\u56fe\u8868\u793a\uff0c\u57fa\u4e8eMamba U-Net\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u660e\u786e\u4f18\u5316\u57fa\u4e8e\u5bf9\u8c61\u6ce8\u610f\u529b\u80fd\u540c\u65f6\u63d0\u5347oSIM\u548c\u4f20\u7edf\u6307\u6807\u6027\u80fd\uff1b2) SUMGraph\u6a21\u578b\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u65b9\u6cd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff1b3) \u6570\u636e\u96c6\u586b\u8865\u4e86\u57fa\u4e8e\u5bf9\u8c61\u6ce8\u610f\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u4e86\u57fa\u4e8e\u5bf9\u8c61\u89c6\u89c9\u6ce8\u610f\u529b\u5efa\u6a21\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u3001oSIM\u6307\u6807\u548cSUMGraph\u6a21\u578b\u4e3a\u672a\u6765\u57fa\u4e8e\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u573a\u666f\u4e2d\u3002"}}
{"id": "2601.14123", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14123", "abs": "https://arxiv.org/abs/2601.14123", "authors": ["Sofia Bennani", "Charles Moslonka"], "title": "A Systematic Analysis of Chunking Strategies for Reliable Question Answering", "comment": "3 pages, 2 figures, 1 table, pre-print", "summary": "We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a \"context cliff\" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).", "AI": {"tldr": "\u6587\u6863\u5206\u5757\u7b56\u7565\u5bf9RAG\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff1a\u7814\u7a76\u53d1\u73b0\u53e5\u5b50\u5206\u5757\u662f\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u65b9\u6cd5\uff0c\u91cd\u53e0\u5206\u5757\u65e0\u663e\u8457\u6536\u76ca\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5b58\u5728\"\u60ac\u5d16\u6548\u5e94\"\uff08\u7ea62.5k token\u540e\u8d28\u91cf\u4e0b\u964d\uff09\uff0c\u6700\u4f18\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d6\u51b3\u4e8e\u5177\u4f53\u76ee\u6807\u3002", "motivation": "\u5de5\u4e1a\u5b9e\u8df5\u4e2dRAG\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u6587\u6863\u5206\u5757\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u5206\u5757\u7b56\u7565\u5bf9RAG\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u4e3a\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6307\u5bfc\u3002", "method": "\u5728Natural Questions\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bc4\u4f30\uff0c\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u5206\u5757\u65b9\u6cd5\uff08token\u3001\u53e5\u5b50\u3001\u8bed\u4e49\u3001\u4ee3\u7801\uff09\u3001\u5206\u5757\u5927\u5c0f\u3001\u91cd\u53e0\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002\u91c7\u7528\u6807\u51c6\u5de5\u4e1a\u8bbe\u7f6e\uff1aSPLADE\u68c0\u7d22\u5668\u548cMistral-8B\u751f\u6210\u5668\u3002", "result": "1) \u91cd\u53e0\u5206\u5757\u65e0\u663e\u8457\u6536\u76ca\u4e14\u589e\u52a0\u7d22\u5f15\u6210\u672c\uff1b2) \u53e5\u5b50\u5206\u5757\u662f\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u65b9\u6cd5\uff0c\u5728\u7ea65k token\u5185\u4e0e\u8bed\u4e49\u5206\u5757\u6548\u679c\u76f8\u5f53\uff1b3) \u4e0a\u4e0b\u6587\u957f\u5ea6\u5b58\u5728\"\u60ac\u5d16\u6548\u5e94\"\uff0c\u8d85\u8fc7\u7ea62.5k token\u540e\u8d28\u91cf\u4e0b\u964d\uff1b4) \u6700\u4f18\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d6\u51b3\u4e8e\u76ee\u6807\uff1a\u8bed\u4e49\u8d28\u91cf\u5728\u5c0f\u4e0a\u4e0b\u6587\u4e2d\u8fbe\u5230\u5cf0\u503c\uff0c\u7cbe\u786e\u5339\u914d\u9700\u8981\u66f4\u5927\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u4e3a\u6210\u672c\u9ad8\u6548\u7684RAG\u90e8\u7f72\u63d0\u4f9b\u5177\u4f53\u5efa\u8bae\uff1a\u907f\u514d\u4f7f\u7528\u91cd\u53e0\u5206\u5757\uff0c\u4f18\u5148\u91c7\u7528\u53e5\u5b50\u5206\u5757\uff0c\u63a7\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6\u57282.5k token\u4ee5\u5185\uff0c\u5e76\u6839\u636e\u5177\u4f53\u76ee\u6807\uff08\u8bed\u4e49\u8d28\u91cfvs\u7cbe\u786e\u5339\u914d\uff09\u8c03\u6574\u4e0a\u4e0b\u6587\u957f\u5ea6\u7b56\u7565\u3002"}}
{"id": "2601.13225", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13225", "abs": "https://arxiv.org/abs/2601.13225", "authors": ["Tim Lachmann", "Alexandra Israelsson", "Christina Tornberg", "Teimuraz Saghinadze", "Michal Balazia", "Philipp M\u00fcller", "Petri Laukka"], "title": "Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations", "comment": "Accepted for publication at IEEE Face & Gesture 2026", "summary": "Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50/50, 70/30, and 30/70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.", "AI": {"tldr": "\u63d0\u51faBLEMORE\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u6df7\u5408\u60c5\u611f\u8bc6\u522b\uff0c\u5305\u542b\u60c5\u611f\u76f8\u5bf9\u663e\u8457\u5ea6\u6807\u6ce8\uff0c\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u5728\u6df7\u5408\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4eba\u7c7b\u901a\u5e38\u540c\u65f6\u4f53\u9a8c\u591a\u79cd\u60c5\u611f\u7684\u6df7\u5408\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5927\u591a\u53ea\u80fd\u8bc6\u522b\u5355\u4e00\u60c5\u611f\uff0c\u7f3a\u4e4f\u80fd\u8bc4\u4f30\u6df7\u5408\u60c5\u611f\u4e2d\u76f8\u5bf9\u663e\u8457\u5ea6\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u3002", "method": "\u521b\u5efaBLEMORE\u6570\u636e\u96c6\uff0c\u5305\u542b3000+\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d66\u79cd\u57fa\u672c\u60c5\u611f\u548c10\u79cd\u6df7\u5408\u60c5\u611f\uff0c\u6bcf\u79cd\u6df7\u5408\u67093\u79cd\u663e\u8457\u5ea6\u914d\u7f6e(50/50, 70/30, 30/70)\u3002\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u60c5\u611f\u5b58\u5728\u9884\u6d4b\u548c\u60c5\u611f\u663e\u8457\u5ea6\u9884\u6d4b\u3002", "result": "\u5355\u6a21\u6001\u5206\u7c7b\u5668\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523029%\u5b58\u5728\u51c6\u786e\u7387\u548c13%\u663e\u8457\u5ea6\u51c6\u786e\u7387\uff1b\u591a\u6a21\u6001\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0cImageBind + WavLM\u8fbe\u523035%\u5b58\u5728\u51c6\u786e\u7387\uff0cHiCMAE\u8fbe\u523018%\u663e\u8457\u5ea6\u51c6\u786e\u7387\u3002\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\u8fbe\u523033%\u5b58\u5728\u51c6\u786e\u7387(VideoMAEv2 + HuBERT)\u548c18%\u663e\u8457\u5ea6\u51c6\u786e\u7387(HiCMAE)\u3002", "conclusion": "BLEMORE\u6570\u636e\u96c6\u4e3a\u63a8\u8fdb\u8003\u8651\u6df7\u5408\u60c5\u611f\u8868\u8fbe\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\u7684\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u6df7\u5408\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u3002"}}
{"id": "2601.14124", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14124", "abs": "https://arxiv.org/abs/2601.14124", "authors": ["Saad Mankarious", "Aya Zirikly"], "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic", "comment": null, "summary": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e3\u963f\u62c9\u4f2f\u8bed\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u901a\u8fc7\u98ce\u683c\u8f6c\u6362\u589e\u5f3a\u5973\u6027\u4f5c\u8005\u5185\u5bb9\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b58\u5728\u8f93\u51fa\u591a\u6837\u6027\u6709\u9650\u548c\u4f20\u64ad\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u7684\u98ce\u9669\u3002\u5728\u5fc3\u7406\u5065\u5eb7\u5206\u6790\u9886\u57df\uff0c\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\uff0c\u7279\u522b\u662f\u963f\u62c9\u4f2f\u8bed\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u5b58\u5728\u663e\u8457\u7684\u6027\u522b\u4e0d\u5e73\u8861\u3002", "method": "\u5c06\u504f\u89c1\u7f13\u89e3\u89c6\u4e3a\u98ce\u683c\u8f6c\u6362\u95ee\u9898\uff0c\u63d0\u51fa\u9884\u8bad\u7ec3\u65e0\u5173\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002\u4f7f\u7528CARMA\u963f\u62c9\u4f2f\u8bed\u5fc3\u7406\u5065\u5eb7\u8bed\u6599\u5e93\uff0c\u9488\u5bf9\u7537\u6027\u5230\u5973\u6027\u7684\u98ce\u683c\u8f6c\u6362\uff0c\u6784\u5efa\u4e94\u4e2a\u6355\u6349\u963f\u62c9\u4f2f\u8bed\u6027\u522b\u8868\u8fbe\u4e0d\u540c\u8bed\u8a00\u548c\u8bed\u4e49\u65b9\u9762\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8bbe\u7f6e\u8bad\u7ec3\u72ec\u7acb\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u6e90\u6587\u672c\u548c\u751f\u6210\u6587\u672c\u4e4b\u95f4\u8bed\u4e49\u4fdd\u771f\u5ea6\u9ad8\uff0c\u540c\u65f6\u5177\u6709\u6709\u610f\u4e49\u7684\u8868\u5c42\u98ce\u683c\u5dee\u5f02\u3002\u5b9a\u6027\u5206\u6790\u786e\u8ba4\u8bed\u8a00\u4e0a\u5408\u7406\u7684\u6027\u522b\u8f6c\u6362\u3002\u6269\u6563\u6a21\u578b\u98ce\u683c\u8f6c\u6362\u80fd\u591f\u751f\u6210\u9ad8\u71b5\u3001\u8bed\u4e49\u5fe0\u5b9e\u7684\u5408\u6210\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3LLMs\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u98ce\u683c\u8f6c\u6362\u4e3a\u7f13\u89e3\u654f\u611f\u3001\u4f4e\u8d44\u6e90\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u6027\u522b\u504f\u89c1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u8bed\u4e49\u51c6\u786e\u7684\u5408\u6210\u6570\u636e\uff0c\u907f\u514d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u504f\u89c1\u4f20\u64ad\u95ee\u9898\u3002"}}
{"id": "2601.13234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13234", "abs": "https://arxiv.org/abs/2601.13234", "authors": ["Md. Nishan Khan", "Kazi Shahriar Sanjid", "Md. Tanzim Hossain", "Asib Mostakim Fony", "Istiak Ahmed", "M. Monir Uddin"], "title": "ConvMambaNet: A Hybrid CNN-Mamba State Space Architecture for Accurate and Real-Time EEG Seizure Detection", "comment": null, "summary": "Epilepsy is a chronic neurological disorder marked by recurrent seizures that can severely impact quality of life. Electroencephalography (EEG) remains the primary tool for monitoring neural activity and detecting seizures, yet automated analysis remains challenging due to the temporal complexity of EEG signals. This study introduces ConvMambaNet, a hybrid deep learning model that integrates Convolutional Neural Networks (CNNs) with the Mamba Structured State Space Model (SSM) to enhance temporal feature extraction. By embedding the Mamba-SSM block within a CNN framework, the model effectively captures both spatial and long-range temporal dynamics. Evaluated on the CHB-MIT Scalp EEG dataset, ConvMambaNet achieved a 99% accuracy and demonstrated robust performance under severe class imbalance. These results underscore the model's potential for precise and efficient seizure detection, offering a viable path toward real-time, automated epilepsy monitoring in clinical environments.", "AI": {"tldr": "\u63d0\u51faConvMambaNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408CNN\u548cMamba-SSM\uff0c\u7528\u4e8e\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\uff0c\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u51c6\u786e\u7387\u3002", "motivation": "\u766b\u75eb\u4e25\u91cd\u5f71\u54cd\u751f\u6d3b\u8d28\u91cf\uff0cEEG\u662f\u4e3b\u8981\u76d1\u6d4b\u5de5\u5177\uff0c\u4f46EEG\u4fe1\u53f7\u7684\u65f6\u95f4\u590d\u6742\u6027\u4f7f\u5f97\u81ea\u52a8\u5206\u6790\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faConvMambaNet\u6df7\u5408\u6a21\u578b\uff0c\u5c06Mamba\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5757\u5d4c\u5165CNN\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u6355\u6349\u7a7a\u95f4\u7279\u5f81\u548c\u957f\u7a0b\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728CHB-MIT\u5934\u76aeEEG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8fbe\u523099%\u51c6\u786e\u7387\uff0c\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "ConvMambaNet\u5c55\u793a\u4e86\u7cbe\u786e\u9ad8\u6548\u7684\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u65f6\u81ea\u52a8\u5316\u766b\u75eb\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.14152", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14152", "abs": "https://arxiv.org/abs/2601.14152", "authors": ["Hyunjong Ok", "Jaeho Lee"], "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models", "comment": "preprint", "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u9898\u4e2d\uff0c\u5c06\u4e0a\u4e0b\u6587\u653e\u5728\u95ee\u9898\u548c\u9009\u9879\u4e4b\u524d\uff08CQO\uff09\u6bd4\u53cd\u5411\u987a\u5e8f\uff08QOC\uff09\u6027\u80fd\u63d0\u5347\u8d85\u8fc714%\uff0c\u6838\u5fc3\u673a\u5236\u662f\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u5bfc\u81f4\u9009\u9879\u65e0\u6cd5\u5173\u6ce8\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u7ed3\u6784\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u654f\u611f\u6027\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u4e00\u4e2a\u663e\u8457\u73b0\u8c61\uff1a\u5728\u591a\u9879\u9009\u62e9\u9898\u56de\u7b54\u4e2d\uff0c\u4e0a\u4e0b\u6587-\u95ee\u9898-\u9009\u9879\uff08CQO\uff09\u987a\u5e8f\u660e\u663e\u4f18\u4e8e\u95ee\u9898-\u9009\u9879-\u4e0a\u4e0b\u6587\uff08QOC\uff09\u987a\u5e8f\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u67b6\u6784\u5206\u6790\uff0c\u8bc6\u522b\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u662f\u6838\u5fc3\u673a\u5236\u3002\u5728QOC\u63d0\u793a\u4e2d\uff0c\u56e0\u679c\u63a9\u7801\u963b\u6b62\u9009\u9879\u6807\u8bb0\u5173\u6ce8\u4e0a\u4e0b\u6587\uff0c\u5f62\u6210\u4fe1\u606f\u74f6\u9888\uff0c\u4f7f\u4e0a\u4e0b\u6587\u5bf9\u9009\u9879\u4e0d\u53ef\u89c1\u3002", "result": "CQO\u987a\u5e8f\u6bd4QOC\u987a\u5e8f\u6027\u80fd\u63d0\u5347\u8d85\u8fc714%\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728\u5e7f\u6cdb\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u88ab\u786e\u5b9a\u4e3a\u5bfc\u81f4\u8fd9\u79cd\u6027\u80fd\u5dee\u5f02\u7684\u6838\u5fc3\u539f\u56e0\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u9898\u4e2d\u7684\u6027\u80fd\u5bf9\u63d0\u793a\u7ed3\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u79cd\u654f\u611f\u6027\u6e90\u4e8e\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u7684\u4fe1\u606f\u6d41\u52a8\u9650\u5236\u3002\u7406\u89e3\u8fd9\u4e00\u673a\u5236\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2601.13238", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13238", "abs": "https://arxiv.org/abs/2601.13238", "authors": ["Chengyin Hu", "Xiang Chen", "Zhe Jia", "Weiwen Shi", "Fengyu Zhang", "Jiujiang Guo", "Yiwei Wei"], "title": "A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5229\u7528\u771f\u5b9e\u5929\u6c14\u6761\u4ef6\u653b\u51fb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u53c2\u6570\u5316\u6270\u52a8\u6a21\u578b\u5206\u6790\u96e8\u5929\u6c14\u5019\u5bf9VLM\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u7269\u7406\u4e0a\u5408\u7406\u7684\u5929\u6c14\u6270\u52a8\u80fd\u5bfc\u81f4\u4e3b\u6d41VLM\u4ea7\u751f\u663e\u8457\u7684\u8bed\u4e49\u9519\u4f4d\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u6807\u51c6\u89c6\u89c9\u6761\u4ef6\u4e0b\u8bad\u7ec3\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u7814\u7a76\u4e0d\u8db3\u3002\u7279\u522b\u662f\u96e8\u5929\u6c14\u5019\u5bf9VLM\u51b3\u7b56\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u98ce\u9669\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u53c2\u6570\u5316\u6270\u52a8\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4f4e\u7ef4\u5168\u5c40\u8c03\u5236\u5efa\u6a21\u964d\u96e8\u7684\u5168\u5c40\u6548\u5e94\uff0c\u9010\u6b65\u524a\u5f31\u539f\u59cb\u8bed\u4e49\u51b3\u7b56\u8fb9\u754c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u591a\u5c3a\u5ea6\u96e8\u6ef4\u5916\u89c2\u548c\u964d\u96e8\u5f15\u8d77\u7684\u7167\u660e\u53d8\u5316\uff0c\u5f15\u5165\u7ed3\u6784\u5316\u96e8\u53d8\u5316\uff0c\u5e76\u4f18\u5316\u4e0d\u53ef\u5fae\u7684\u5929\u6c14\u7a7a\u95f4\u4ee5\u8bf1\u5bfc\u7a33\u5b9a\u7684\u8bed\u4e49\u504f\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u7269\u7406\u4e0a\u5408\u7406\u4e14\u9ad8\u5ea6\u53d7\u9650\u7684\u5929\u6c14\u6270\u52a8\uff0c\u4e5f\u80fd\u5728\u4e3b\u6d41VLM\u4e2d\u8bf1\u5bfc\u663e\u8457\u7684\u8bed\u4e49\u9519\u4f4d\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u7167\u660e\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u96e8\u6ef4\u7ed3\u6784\u662f\u8fd9\u4e9b\u8bed\u4e49\u504f\u79fb\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86VLM\u5728\u771f\u5b9e\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u7684\u5bf9\u6297\u6846\u67b6\u4e0d\u4ec5\u7269\u7406\u53ef\u89e3\u91ca\uff0c\u800c\u4e14\u5c55\u793a\u4e86\u7269\u7406\u4e0a\u5408\u7406\u7684\u5929\u6c14\u6270\u52a8\u5bf9VLM\u8bed\u4e49\u5bf9\u9f50\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5bf9\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2601.14160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14160", "abs": "https://arxiv.org/abs/2601.14160", "authors": ["Ali Hamza Bashir", "Muhammad Rehan Khalid", "Kostadin Cvejoski", "Jana Birr", "Jule Berghaus", "Armin Berger", "Sandra Halscheidt", "Christian Temath", "Rafet Sifa", "David Berghaus"], "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law", "comment": null, "summary": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u5148\u8fdbLLMs\u9002\u914d\u5230\u5fb7\u56fd\u6cd5\u5f8b\u95ee\u7b54\u9886\u57df\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u4e0d\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6cd5\u5f8b\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5982\u6cd5\u5f8b\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u4e13\u5bb6\u77e5\u8bc6\u5bfc\u81f4\u4e8b\u5b9e\u9519\u8bef\u6216\u5e7b\u89c9\u3002\u5fb7\u56fd\u6cd5\u5f8b\u95ee\u7b54\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u5c06LLMs\u9002\u914d\u5230\u8be5\u4e13\u4e1a\u9886\u57df\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u6743\u5a01\u5fb7\u56fd\u6cd5\u89c4\u4e2d\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u6cd5\u5f8b\u51c6\u786e\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u3002\u91c7\u7528\u4e25\u683c\u7684\u81ea\u52a8\u5316\u8fc7\u6ee4\u65b9\u6cd5\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u3002", "result": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u5fae\u8c03\u7684LLMs\u5728\u5fb7\u56fd\u6cd5\u5f8b\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u5de5\u6807\u6ce8\u7684\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u5de5\u6807\u6ce8\u7684\u7a33\u5065\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347LLMs\u5728\u4e13\u4e1a\u9886\u57df\u7684\u8868\u73b0\u3002"}}
{"id": "2601.13263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13263", "abs": "https://arxiv.org/abs/2601.13263", "authors": ["Chenyu Liu", "Marco Cecotti", "Harikrishnan Vijayakumar", "Patrick Robinson", "James Barson", "Mihai Caleap"], "title": "Deep Learning for Semantic Segmentation of 3D Ultrasound Data", "comment": "14 pages, 10 figures, 8 tables, presented at 2025 13th International Conference on Robot Intelligence Technology and Applications (RITA)", "summary": "Developing cost-efficient and reliable perception systems remains a central challenge for automated vehicles. LiDAR and camera-based systems dominate, yet they present trade-offs in cost, robustness and performance under adverse conditions. This work introduces a novel framework for learning-based 3D semantic segmentation using Calyo Pulse, a modular, solid-state 3D ultrasound sensor system for use in harsh and cluttered environments. A 3D U-Net architecture is introduced and trained on the spatial ultrasound data for volumetric segmentation. Results demonstrate robust segmentation performance from Calyo Pulse sensors, with potential for further improvement through larger datasets, refined ground truth, and weighted loss functions. Importantly, this study highlights 3D ultrasound sensing as a promising complementary modality for reliable autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u8d85\u58f0\u4f20\u611f\u5668Calyo Pulse\u7684\u5b66\u4e60\u578b3D\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u6fc0\u5149\u96f7\u8fbe\u548c\u6444\u50cf\u5934\uff0c\u4f46\u5728\u6210\u672c\u3001\u9c81\u68d2\u6027\u548c\u6076\u52a3\u73af\u5883\u6027\u80fd\u65b9\u9762\u5b58\u5728\u6743\u8861\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7ecf\u6d4e\u53ef\u9760\u4e14\u80fd\u5728\u6076\u52a3\u73af\u5883\u4e0b\u5de5\u4f5c\u7684\u611f\u77e5\u65b9\u6848\u3002", "method": "\u91c7\u7528Calyo Pulse\u56fa\u60013D\u8d85\u58f0\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u63d0\u51fa3D U-Net\u67b6\u6784\u5bf9\u7a7a\u95f4\u8d85\u58f0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4f53\u7d20\u7ea7\u8bed\u4e49\u5206\u5272\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCalyo Pulse\u4f20\u611f\u5668\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u5206\u5272\u6027\u80fd\uff0c\u901a\u8fc7\u66f4\u5927\u6570\u636e\u96c6\u3001\u66f4\u7cbe\u786e\u7684\u771f\u503c\u6807\u6ce8\u548c\u52a0\u6743\u635f\u5931\u51fd\u6570\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "3D\u8d85\u58f0\u4f20\u611f\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u4e92\u8865\u611f\u77e5\u6a21\u6001\uff0c\u80fd\u591f\u4e3a\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u65b0\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2601.14172", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14172", "abs": "https://arxiv.org/abs/2601.14172", "authors": ["V\u00edctor Yeste", "Paolo Rosso"], "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum", "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,", "summary": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.", "AI": {"tldr": "\u7814\u7a76\u53e5\u5b50\u7ea7\u522b\u8bc6\u522bSchwartz\u4ef7\u503c\u7406\u8bba\u4e2d\u768419\u79cd\u4ef7\u503c\u89c2\uff0c\u4f5c\u4e3a\u6587\u672c\u4e2d\u4eba\u7c7b\u4ef7\u503c\u68c0\u6d4b\u7684\u5177\u4f53\u5f62\u5f0f\u3002\u5728\u65b0\u95fb\u548c\u653f\u6cbb\u5ba3\u8a00\u7684\u96f6\u4e0a\u4e0b\u6587\u53e5\u5b50\u4e2d\uff0c\u9762\u4e34\u7a00\u758f\u7684\u9053\u5fb7\u7ebf\u7d22\u548c\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5728\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u3001\u9053\u5fb7\u7ebf\u7d22\u7a00\u758f\u4e14\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u53e5\u5b50\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u7684Schwartz\u4ef7\u503c\u89c2\u3002\u8fd9\u79cd\u8bbe\u7f6e\u5bf9\u73b0\u4ee3\u795e\u7ecf\u6a21\u578b\u6784\u6210\u4e86\u5185\u5728\u56f0\u96be\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5efa\u7acb\u4e8c\u5143\u9053\u5fb7\u5b58\u5728\u4efb\u52a1\uff0c\u7136\u540e\u6bd4\u8f83\u5b58\u5728\u95e8\u63a7\u5c42\u6b21\u7ed3\u6784\u548c\u76f4\u63a5\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u3002\u4f7f\u7528DeBERTa-base\u6a21\u578b\uff0c\u5e76\u52a0\u5165\u8f7b\u91cf\u7ea7\u4fe1\u53f7\uff08\u524d\u53e5\u4e0a\u4e0b\u6587\u3001LIWC-22/eMFD/MJD\u8bcd\u5178\u3001\u4e3b\u9898\u7279\u5f81\uff09\u3002\u540c\u65f6\u57fa\u51c6\u6d4b\u8bd5\u6307\u4ee4\u8c03\u4f18\u7684LLMs\uff08Gemma 2 9B\u3001Llama 3.1 8B\u7b49\uff09\uff0c\u6784\u5efa\u7b80\u5355\u96c6\u6210\u6a21\u578b\u3002", "result": "\u4e8c\u5143\u9053\u5fb7\u5b58\u5728\u4efb\u52a1\u53ef\u5b66\u4e60\uff08F1\u22480.74\uff09\u3002\u5c42\u6b21\u7ed3\u6784\u672a\u4f18\u4e8e\u76f4\u63a5\u9884\u6d4b\u3002\u8f6f\u6295\u7968\u76d1\u7763\u96c6\u6210\u8fbe\u5230macro-F1 0.332\uff0c\u663e\u8457\u8d85\u8d8a\u6700\u4f73\u5355\u76d1\u7763\u6a21\u578b\u548c\u5148\u524d\u57fa\u7ebf\u3002\u8f7b\u91cf\u7ea7\u4fe1\u53f7\u548c\u5c0f\u578b\u96c6\u6210\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u6539\u8fdb\u3002", "conclusion": "\u57288GB\u5355GPU\u7ea6\u675f\u548c7-9B\u89c4\u6a21\u4e0b\uff0c\u7cbe\u5fc3\u8c03\u4f18\u7684\u76d1\u7763\u7f16\u7801\u5668\u4ecd\u7136\u662f\u7ed3\u6784\u5316\u4eba\u7c7b\u4ef7\u503c\u68c0\u6d4b\u7684\u5f3a\u5927\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u7ebf\u3002\u66f4\u4e30\u5bcc\u7684\u4ef7\u503c\u7ed3\u6784\u548c\u6587\u6863\u4e0a\u4e0b\u6587\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2601.13299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13299", "abs": "https://arxiv.org/abs/2601.13299", "authors": ["Ethan Seefried", "Prahitha Movva", "Naga Harshita Marupaka", "Tilak Kasturi", "Tirthankar Ghosal"], "title": "Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams", "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Ai4 Science", "summary": "We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.", "AI": {"tldr": "Enginuity\u662f\u9996\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u5e26\u6709\u5168\u9762\u7684\u7ed3\u6784\u5316\u6807\u6ce8\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u56fe\u8868\u89e3\u6790\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u7406\u89e3\u548c\u5904\u7406\u5de5\u7a0b\u56fe\u4e2d\u7684\u89c6\u89c9-\u7ed3\u6784\u77e5\u8bc6\uff0c\u8fd9\u963b\u788d\u4e86AI\u5728\u79d1\u5b66\u53d1\u73b0\u5de5\u4f5c\u6d41\u4e2d\u7684\u5168\u9762\u53c2\u4e0e\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u56fe\u8868\u89e3\u91ca\u3001\u6280\u672f\u56fe\u7eb8\u5206\u6790\u548c\u89c6\u89c9\u63a8\u7406\u7684\u73af\u8282\u3002", "method": "\u6784\u5efa\u5305\u542b\u5c42\u6b21\u5316\u7ec4\u4ef6\u5173\u7cfb\u3001\u8fde\u63a5\u548c\u8bed\u4e49\u5143\u7d20\u7684\u591a\u9886\u57df\u5de5\u7a0b\u56fe\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u5de5\u7a0b\u56fe\u7ed3\u6784\u5316\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u80fd\u591f\u652f\u6301\u7ed3\u6784\u5316\u56fe\u8868\u89e3\u6790\u3001\u8de8\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u548cAI\u8f85\u52a9\u5de5\u7a0b\u4eff\u771f\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "Enginuity\u6570\u636e\u96c6\u5c06\u53d8\u9769AI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u548c\u64cd\u4f5c\u5de5\u7a0b\u56fe\u4e2d\u5d4c\u5165\u7684\u89c6\u89c9-\u7ed3\u6784\u77e5\u8bc6\uff0c\u6253\u7834\u5f53\u524d\u963b\u788dAI\u5168\u9762\u53c2\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u57fa\u672c\u969c\u788d\u3002"}}
{"id": "2601.14210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14210", "abs": "https://arxiv.org/abs/2601.14210", "authors": ["Rohan Bhatnagar", "Youran Sun", "Chi Andrew Zhang", "Yixin Wen", "Haizhao Yang"], "title": "HALT: Hallucination Assessment via Latent Testing", "comment": null, "summary": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u63a2\u9488\uff0c\u76f4\u63a5\u4eceLLM\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u8bfb\u53d6\u5e7b\u89c9\u98ce\u9669\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u5ef6\u8fdf\u7684\u98ce\u9669\u8bc4\u4f30\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u751f\u6210\u548c\u8def\u7531\u51b3\u7b56\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u53ef\u7406\u89e3\u4e3a\u5fe0\u5b9e\u8bfb\u53d6\u5931\u8d25\uff1a\u867d\u7136\u5185\u90e8\u8868\u793a\u53ef\u80fd\u7f16\u7801\u4e86\u67e5\u8be2\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u89e3\u7801\u538b\u529b\u4ecd\u4f1a\u4ea7\u751f\u6d41\u7545\u7b54\u6848\u3002\u9700\u8981\u4ece\u4e2d\u95f4\u5c42\u63d0\u53d6\u88ab\u6700\u7ec8\u89e3\u7801\u9636\u6bb5\u8870\u51cf\u7684\u8ba4\u77e5\u4fe1\u53f7\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u63a2\u9488\uff0c\u4ece\u95ee\u9898\u6807\u8bb0\u7684\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u76f4\u63a5\u8bfb\u53d6\u5e7b\u89c9\u98ce\u9669\u3002\u63a2\u9488\u662f\u5c0f\u578b\u8f85\u52a9\u7f51\u7edc\uff0c\u8ba1\u7b97\u6210\u672c\u6bd4\u4ee4\u724c\u751f\u6210\u4f4e\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u53ef\u4e0e\u63a8\u7406\u5b8c\u5168\u5e76\u884c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u5ef6\u8fdf\u7684\u98ce\u9669\u4f30\u8ba1\u3002", "result": "\u5728\u56db\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u4e2aLLM\u5bb6\u65cf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3aAUROC\u548cAURAC\u6027\u80fd\uff0c\u5728\u6570\u636e\u96c6\u504f\u79fb\u4e0b\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u4e2d\u95f4\u8868\u793a\u7684\u53ef\u89e3\u91ca\u7ed3\u6784\u3002", "conclusion": "\u5feb\u901f\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u8bfb\u53d6\u53ef\u4f5c\u4e3a\u53ef\u9760\u667a\u80fdAI\u7684\u539f\u5219\u6027\u57fa\u7840\uff0c\u901a\u8fc7\u4ee3\u7406\u6279\u8bc4\u5668\u5b9e\u73b0\u5feb\u901f\u9009\u62e9\u6027\u751f\u6210\u548c\u8def\u7531\uff0c\u8ba9LLM\u7acb\u5373\u56de\u7b54\u81ea\u4fe1\u67e5\u8be2\uff0c\u5c06\u4e0d\u786e\u5b9a\u67e5\u8be2\u59d4\u6258\u7ed9\u66f4\u5f3a\u7684\u9a8c\u8bc1\u6d41\u7a0b\u3002"}}
{"id": "2601.13304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13304", "abs": "https://arxiv.org/abs/2601.13304", "authors": ["Wenxin Ma", "Chenlong Wang", "Ruisheng Yuan", "Hao Chen", "Nanru Dai", "S. Kevin Zhou", "Yijun Yang", "Alan Yuille", "Jieneng Chen"], "title": "CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning", "comment": "Code is available: https://github.com/CausalSpatial/CausalSpatial", "summary": "Humans can look at a static scene and instantly predict what happens next -- will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer \"what-if\" questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: https://github.com/CausalSpatial/CausalSpatial", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CausalSpatial\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86Causal Object World\u6a21\u578b\u6846\u67b6\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u89c2\u5bdf\u9759\u6001\u573a\u666f\u5e76\u9884\u6d4b\u540e\u7eed\u52a8\u6001\u7ed3\u679c\uff08\u5982\u78b0\u649e\uff09\uff0c\u4f46\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u7a7a\u95f4\u611f\u77e5\uff0c\u65e0\u6cd5\u56de\u7b54\u4e09\u7ef4\u573a\u666f\u4e2d\u7684\"\u5047\u8bbe\u6027\"\u95ee\u9898\u3002\u9700\u8981\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "1) \u521b\u5efaCausalSpatial\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b\u78b0\u649e\u3001\u517c\u5bb9\u6027\u3001\u906e\u6321\u548c\u8f68\u8ff9\u56db\u4e2a\u4efb\u52a1\uff1b2) \u5206\u6790\u6a21\u578b\u5931\u8d25\u539f\u56e0\uff1b3) \u63d0\u51faCausal Object World\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5047\u8bbe\u52a8\u6001\u7684\u89c6\u9891\u6765\u5916\u90e8\u5316\u6a21\u62df\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u660e\u786e\u7684\u56e0\u679c\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u4eba\u7c7b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u520684%\uff0c\u800cGPT-5\u4ec5\u5f9754%\uff0c\u663e\u793a\u51fa\u5de8\u5927\u5dee\u8ddd\u3002\u5206\u6790\u53d1\u73b0\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u504f\u79bb\u89c6\u89c9\u8bc1\u636e\uff0c\u4ea7\u751f\u6d41\u7545\u4f46\u7a7a\u95f4\u4e0a\u65e0\u6839\u636e\u7684\u5e7b\u89c9\u3002COW\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u6a21\u62df\u6539\u5584\u4e86\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u56e0\u679c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u5c06\u63a8\u7406\u8fc7\u7a0b\u5916\u90e8\u5316\u4e3a\u89c6\u89c9\u6a21\u62df\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u6587\u672c\u601d\u7ef4\u94fe\u3002COW\u6846\u67b6\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.14230", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.14230", "abs": "https://arxiv.org/abs/2601.14230", "authors": ["Yiyang Wang", "Yiqiao Jin", "Alex Cabral", "Josiah Hester"], "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems", "comment": "15 pages, 9 figures", "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.", "AI": {"tldr": "MASCOT\uff1a\u4e00\u4e2a\u9632\u6b62\u89d2\u8272\u5d29\u6e83\u548c\u793e\u4f1a\u5949\u627f\u7684\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7b56\u7565\u63d0\u5347\u89d2\u8272\u4e00\u81f4\u6027\u548c\u793e\u4f1a\u8d21\u732e\u5ea6", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u89d2\u8272\u5d29\u6e83\uff08\u667a\u80fd\u4f53\u9000\u5316\u4e3a\u901a\u7528\u540c\u8d28\u5316\u52a9\u624b\u884c\u4e3a\uff09\u548c\u793e\u4f1a\u5949\u627f\uff08\u4ea7\u751f\u5197\u4f59\u3001\u975e\u5efa\u8bbe\u6027\u5bf9\u8bdd\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u4e2a\u4f53\u89d2\u8272\u7279\u6027\u540c\u65f6\u4fc3\u8fdb\u5efa\u8bbe\u6027\u793e\u4f1a\u534f\u4f5c\u7684\u6846\u67b6", "method": "\u63d0\u51faMASCOT\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u7b56\u7565\uff1a1\uff09\u89d2\u8272\u611f\u77e5\u884c\u4e3a\u5bf9\u9f50\uff0c\u4f7f\u7528RLAIF\u9a71\u52a8\u7684\u7ba1\u9053\u5fae\u8c03\u4e2a\u4f53\u667a\u80fd\u4f53\u4ee5\u786e\u4fdd\u4e25\u683c\u89d2\u8272\u4fdd\u771f\u5ea6\uff1b2\uff09\u534f\u4f5c\u5bf9\u8bdd\u4f18\u5316\uff0c\u901a\u8fc7\u7fa4\u4f53\u7ea7\u5956\u52b1\u6307\u5bfc\u7684\u5143\u7b56\u7565\u786e\u4fdd\u591a\u6837\u5316\u548c\u5bcc\u6709\u6210\u6548\u7684\u5bf9\u8bdd", "result": "\u5728\u5fc3\u7406\u652f\u6301\u548c\u804c\u573a\u9886\u57df\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMASCOT\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe+14.1\uff0c\u5728\u793e\u4f1a\u8d21\u732e\u5ea6\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe+10.6", "conclusion": "MASCOT\u4e3a\u4e0b\u4e00\u4ee3\u793e\u4f1a\u667a\u80fd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5de5\u7a0b\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u7ebf\u56fe\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u5d29\u6e83\u548c\u793e\u4f1a\u5949\u627f\u95ee\u9898"}}
{"id": "2601.13331", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13331", "abs": "https://arxiv.org/abs/2601.13331", "authors": ["Wei Wang", "Quoc-Toan Ly", "Chong Yu", "Jun Bai"], "title": "MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic", "comment": null, "summary": "Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at https://github.com/LabJunBMI/MultiST.git.", "AI": {"tldr": "MultiST\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u7a7a\u95f4\u62d3\u6251\u3001\u57fa\u56e0\u8868\u8fbe\u548c\u7ec4\u7ec7\u5f62\u6001\u5b66\uff0c\u5728\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u4e2d\u5b9e\u73b0\u66f4\u6e05\u6670\u7684\u7a7a\u95f4\u57df\u8fb9\u754c\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u65b9\u6cd5\u7f3a\u4e4f\u7ec4\u7ec7\u5f62\u6001\u5b66\u4e0e\u5206\u5b50\u8c31\u7684\u6709\u6548\u6574\u5408\uff0c\u901a\u5e38\u91c7\u7528\u6d45\u5c42\u878d\u5408\u7b56\u7565\u6216\u5b8c\u5168\u5ffd\u7565\u7ec4\u7ec7\u56fe\u50cf\uff0c\u5bfc\u81f4\u65e0\u6cd5\u89e3\u51b3\u6a21\u7cca\u7684\u7a7a\u95f4\u57df\u8fb9\u754c\u95ee\u9898\u3002", "method": "MultiST\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u57fa\u56e0\u7f16\u7801\u5668\u4e0e\u5bf9\u6297\u6027\u5bf9\u9f50\u5b66\u4e60\u9c81\u68d2\u7a7a\u95f4\u8868\u793a\uff0c\u540c\u65f6\u6574\u5408\u989c\u8272\u5f52\u4e00\u5316\u7684\u7ec4\u7ec7\u5b66\u7279\u5f81\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u5206\u5b50-\u5f62\u6001\u5b66\u4f9d\u8d56\u5173\u7cfb\u5e76\u4f18\u5316\u57df\u8fb9\u754c\u3002", "result": "\u572813\u4e2a\u4e0d\u540cST\u6570\u636e\u96c6\uff08\u5305\u62ec\u4eba\u8111\u76ae\u5c42\u548c\u4e73\u817a\u764c\u7ec4\u7ec7\uff09\u4e0a\u8bc4\u4f30\uff0cMultiST\u4ea7\u751f\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6e05\u6670\u3001\u66f4\u8fde\u8d2f\u7684\u7a7a\u95f4\u57df\u8fb9\u754c\uff0c\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u4f2a\u65f6\u95f4\u8f68\u8ff9\u548c\u66f4\u5177\u751f\u7269\u5b66\u89e3\u91ca\u6027\u7684\u7ec6\u80de\u95f4\u76f8\u4e92\u4f5c\u7528\u6a21\u5f0f\u3002", "conclusion": "MultiST\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5728\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u7a7a\u95f4\u57df\u8bc6\u522b\u548c\u66f4\u4e30\u5bcc\u7684\u751f\u7269\u5b66\u6d1e\u89c1\uff0c\u4e3a\u7ec4\u7ec7\u7ed3\u6784\u548c\u7ec6\u80de\u76f8\u4e92\u4f5c\u7528\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2601.14242", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14242", "abs": "https://arxiv.org/abs/2601.14242", "authors": ["Bertie Vidgen", "Austin Mann", "Abby Fennelly", "John Wright Stanly", "Lucas Rothman", "Marco Burstein", "Julien Benchek", "David Ostrofsky", "Anirudh Ravichandran", "Debnil Sur", "Neel Venugopal", "Alannah Hsia", "Isaac Robinson", "Calix Huang", "Olivia Varones", "Daniyal Khan", "Michael Haines", "Zach Richards", "Chirag Mahapatra", "Brendan Foody", "Osvald Nitski"], "title": "APEX-Agents", "comment": null, "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.", "AI": {"tldr": "APEX-Agents\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u6267\u884c\u6295\u8d44\u94f6\u884c\u3001\u7ba1\u7406\u54a8\u8be2\u548c\u6cd5\u5f8b\u9886\u57df\u8de8\u5e94\u7528\u957f\u7a0b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u3001\u6587\u4ef6\u548c\u5de5\u5177\uff0cGemini 3 Flash\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u4e2d\u6267\u884c\u590d\u6742\u8de8\u5e94\u7528\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u6295\u8d44\u94f6\u884c\u5206\u6790\u5e08\u3001\u7ba1\u7406\u987e\u95ee\u548c\u5f8b\u5e08\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u5de5\u4f5c\u9700\u6c42\u3002", "method": "\u521b\u5efa\u5305\u542b480\u4e2a\u4efb\u52a1\u7684APEX-Agents\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u4f7f\u7528\u6587\u4ef6\u548c\u5de5\u5177\u6267\u884c\u4efb\u52a1\uff1b\u4f7f\u7528Pass@1\u8bc4\u4f308\u4e2a\u4ee3\u7406\uff0c\u5e76\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u548cArchipelago\u6267\u884c\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\u3002", "result": "Gemini 3 Flash (Thinking=High)\u4ee524.0%\u5f97\u5206\u6700\u9ad8\uff0c\u5176\u6b21\u662fGPT-5.2\u3001Claude Opus 4.5\u548cGemini 3 Pro\uff1b\u6240\u6709\u6d4b\u8bd5\u4ee3\u7406\u5728Thinking=High\u6a21\u5f0f\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "APEX-Agents\u4e3a\u8bc4\u4f30AI\u4ee3\u7406\u5728\u4e13\u4e1a\u5de5\u4f5c\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5f53\u524d\u6700\u4f73\u4ee3\u7406\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u5f00\u6e90\u57fa\u51c6\u548c\u57fa\u7840\u8bbe\u65bd\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2601.13364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13364", "abs": "https://arxiv.org/abs/2601.13364", "authors": ["Zhenan Liu", "Yaodong Cui", "Amir Khajepour", "George Shaker"], "title": "Real-Time 4D Radar Perception for Robust Human Detection in Harsh Enclosed Environments", "comment": null, "summary": "This paper introduces a novel methodology for generating controlled, multi-level dust concentrations in a highly cluttered environment representative of harsh, enclosed environments, such as underground mines, road tunnels, or collapsed buildings, enabling repeatable mm-wave propagation studies under severe electromagnetic constraints. We also present a new 4D mmWave radar dataset, augmented by camera and LiDAR, illustrating how dust particles and reflective surfaces jointly impact the sensing functionality. To address these challenges, we develop a threshold-based noise filtering framework leveraging key radar parameters (RCS, velocity, azimuth, elevation) to suppress ghost targets and mitigate strong multipath reflections at the raw data level. Building on the filtered point clouds, a cluster-level, rule-based classification pipeline exploits radar semantics-velocity, RCS, and volumetric spread-to achieve reliable, real-time pedestrian detection without extensive domainspecific training. Experimental results confirm that this integrated approach significantly enhances clutter mitigation, detection robustness, and overall system resilience in dust-laden mining environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7c89\u5c18\u73af\u5883\u4e2d\u751f\u6210\u53ef\u63a7\u591a\u7ea7\u7c89\u5c18\u6d53\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e864D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\u548c\u566a\u58f0\u8fc7\u6ee4\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u73af\u5883\u4e0b\u7684\u53ef\u9760\u884c\u4eba\u68c0\u6d4b\u3002", "motivation": "\u5730\u4e0b\u77ff\u4e95\u3001\u516c\u8def\u96a7\u9053\u7b49\u6076\u52a3\u5c01\u95ed\u73af\u5883\u4e2d\u5b58\u5728\u5927\u91cf\u7c89\u5c18\u548c\u53cd\u5c04\u8868\u9762\uff0c\u4e25\u91cd\u5f71\u54cd\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u4f20\u611f\u529f\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5728\u8fd9\u4e9b\u7535\u78c1\u7ea6\u675f\u6761\u4ef6\u4e0b\u53ef\u9760\u5de5\u4f5c\u7684\u611f\u77e5\u7cfb\u7edf\u3002", "method": "1) \u5728\u9ad8\u5ea6\u6742\u4e71\u73af\u5883\u4e2d\u751f\u6210\u53ef\u63a7\u591a\u7ea7\u7c89\u5c18\u6d53\u5ea6\u7684\u65b9\u6cd5\uff1b2) \u521b\u5efa\u5305\u542b\u76f8\u673a\u548cLiDAR\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u96c6\uff1b3) \u57fa\u4e8e\u5173\u952e\u96f7\u8fbe\u53c2\u6570(RCS\u3001\u901f\u5ea6\u3001\u65b9\u4f4d\u89d2\u3001\u4ef0\u89d2)\u7684\u9608\u503c\u566a\u58f0\u8fc7\u6ee4\u6846\u67b6\uff1b4) \u57fa\u4e8e\u805a\u7c7b\u7ea7\u522b\u7684\u89c4\u5219\u5206\u7c7b\u7ba1\u9053\uff0c\u5229\u7528\u96f7\u8fbe\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u5b9e\u65f6\u884c\u4eba\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u7c89\u5c18\u73af\u5883\u4e2d\u7684\u6742\u6ce2\u6291\u5236\u3001\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u6574\u4f53\u5f39\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u5b9e\u65f6\u884c\u4eba\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6076\u52a3\u7c89\u5c18\u73af\u5883\u4e0b\u7684\u6beb\u7c73\u6ce2\u4f20\u64ad\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u5b9e\u73b0\u4e86\u5728\u5f3a\u7535\u78c1\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u884c\u4eba\u68c0\u6d4b\uff0c\u5bf9\u5730\u4e0b\u77ff\u4e95\u7b49\u5371\u9669\u73af\u5883\u7684\u611f\u77e5\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.14249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14249", "abs": "https://arxiv.org/abs/2601.14249", "authors": ["Yuming Yang", "Mingyoung Lai", "Wanxu Zhao", "Xiaoran Fan", "Zhiheng Xi", "Mingqi Wu", "Chiyue Huang", "Jun Zhao", "Haijun Lv", "Jian Tong", "Yunhua Zhou", "Yicheng Zou", "Qipeng Guo", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment", "comment": "26 pages. Project page: https://github.com/UmeanNever/RankSurprisalRatio", "summary": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.", "AI": {"tldr": "\u63d0\u51faRank-Surprisal Ratio (RSR)\u6307\u6807\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u9f50\u5ea6\u548c\u4fe1\u606f\u91cf\u6765\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u9002\u7528\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e2d\uff0c\u6765\u81ea\u66f4\u5f3a\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u4e0d\u4e00\u5b9a\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u5b66\u751f\u6a21\u578b\uff0c\u8868\u660e\u6570\u636e-\u5b66\u751f\u5339\u914d\u5ea6\u7684\u91cd\u8981\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u5b66\u751f\u4f3c\u7136\u5ea6\u8bc4\u4f30\u9002\u7528\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u8f68\u8ff9\u3002", "method": "\u63d0\u51faRank-Surprisal Ratio (RSR)\u6307\u6807\uff0c\u5b9a\u4e49\u4e3a\u8f68\u8ff9\u7684\u5e73\u5747\u8bcd\u5143\u7ea7\u6392\u540d\u4e0e\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136\u4e4b\u6bd4\u3002\u8be5\u6307\u6807\u6355\u6349\u5bf9\u9f50\u5ea6\u548c\u4fe1\u606f\u91cf\uff0c\u5e73\u8861\u5b66\u4e60\u4fe1\u53f7\u5f3a\u5ea6\u548c\u884c\u4e3a\u5bf9\u9f50\u3002", "result": "\u57285\u4e2a\u5b66\u751f\u6a21\u578b\u548c11\u4e2a\u4e0d\u540c\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\uff0cRSR\u4e0e\u8bad\u7ec3\u540e\u6027\u80fd\u5f3a\u76f8\u5173\uff08\u5e73\u5747Spearman 0.86\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u3002\u5728\u8f68\u8ff9\u9009\u62e9\u548c\u6559\u5e08\u9009\u62e9\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "RSR\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6307\u6807\uff0c\u80fd\u591f\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u9002\u7528\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u5bf9\u9f50\u5ea6\u800c\u5ffd\u7565\u4fe1\u606f\u91cf\u7684\u95ee\u9898\uff0c\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13371", "abs": "https://arxiv.org/abs/2601.13371", "authors": ["Junyi Zhang", "Yiming Wang", "Yunhong Lu", "Qichao Wang", "Wenzhe Qian", "Xiaoyin Xu", "David Gu", "Min Zhang"], "title": "Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations", "comment": "Association for the Advancement of Artificial Intelligence", "summary": "A fundamental challenge in text-to-3D face generation is achieving high-quality geometry. The core difficulty lies in the arbitrary and intricate distribution of vertices in 3D space, making it challenging for existing models to establish clean connectivity and resulting in suboptimal geometry. To address this, our core insight is to simplify the underlying geometric structure by constraining the distribution onto a simple and regular manifold, a topological sphere. Building on this, we first propose the Spherical Geometry Representation, a novel face representation that anchors geometric signals to uniform spherical coordinates. This guarantees a regular point distribution, from which the mesh connectivity can be robustly reconstructed. Critically, this canonical sphere can be seamlessly unwrapped into a 2D map, creating a perfect synergy with powerful 2D generative models. We then introduce Spherical Geometry Diffusion, a conditional diffusion framework built upon this 2D map. It enables diverse and controllable generation by jointly modeling geometry and texture, where the geometry explicitly conditions the texture synthesis process. Our method's effectiveness is demonstrated through its success in a wide range of tasks: text-to-3D generation, face reconstruction, and text-based 3D editing. Extensive experiments show that our approach substantially outperforms existing methods in geometric quality, textual fidelity, and inference efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u9762\u51e0\u4f55\u8868\u793a\u548c\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u52303D\u4eba\u8138\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u51e0\u4f55\u7ea6\u675f\u5230\u89c4\u5219\u7403\u9762\u6d41\u5f62\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u51e0\u4f55\u751f\u6210\u548c\u7eb9\u7406\u5408\u6210\u3002", "motivation": "\u6587\u672c\u52303D\u4eba\u8138\u751f\u6210\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u51e0\u4f55\u8d28\u91cf\u4e0d\u9ad8\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u74063D\u7a7a\u95f4\u4e2d\u9876\u70b9\u5206\u5e03\u7684\u4efb\u610f\u6027\u548c\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u8fde\u63a5\u6027\u5dee\u548c\u51e0\u4f55\u8d28\u91cf\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u7403\u9762\u51e0\u4f55\u8868\u793a\uff0c\u5c06\u51e0\u4f55\u4fe1\u53f7\u951a\u5b9a\u5230\u5747\u5300\u7403\u9762\u5750\u6807\u4e0a\uff0c\u4fdd\u8bc1\u89c4\u5219\u70b9\u5206\u5e03\uff1b\u7136\u540e\u63d0\u51fa\u7403\u9762\u51e0\u4f55\u6269\u6563\uff0c\u57fa\u4e8e2D\u6620\u5c04\u6784\u5efa\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u8054\u5408\u5efa\u6a21\u51e0\u4f55\u548c\u7eb9\u7406\uff0c\u51e0\u4f55\u660e\u786e\u5730\u6761\u4ef6\u5316\u7eb9\u7406\u5408\u6210\u8fc7\u7a0b\u3002", "result": "\u65b9\u6cd5\u5728\u6587\u672c\u52303D\u751f\u6210\u3001\u4eba\u8138\u91cd\u5efa\u548c\u57fa\u4e8e\u6587\u672c\u76843D\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u51e0\u4f55\u8d28\u91cf\u3001\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u590d\u6742\u51e0\u4f55\u7ea6\u675f\u5230\u7b80\u5355\u89c4\u5219\u6d41\u5f62\uff08\u62d3\u6251\u7403\u9762\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u4eba\u8138\u751f\u6210\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u52303D\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13373", "abs": "https://arxiv.org/abs/2601.13373", "authors": ["Zhenan Liu", "Amir Khajepour", "George Shaker"], "title": "A Lightweight Model-Driven 4D Radar Framework for Pervasive Human Detection in Harsh Conditions", "comment": null, "summary": "Pervasive sensing in industrial and underground environments is severely constrained by airborne dust, smoke, confined geometry, and metallic structures, which rapidly degrade optical and LiDAR based perception. Elevation resolved 4D mmWave radar offers strong resilience to such conditions, yet there remains a limited understanding of how to process its sparse and anisotropic point clouds for reliable human detection in enclosed, visibility degraded spaces. This paper presents a fully model-driven 4D radar perception framework designed for real-time execution on embedded edge hardware. The system uses radar as its sole perception modality and integrates domain aware multi threshold filtering, ego motion compensated temporal accumulation, KD tree Euclidean clustering with Doppler aware refinement, and a rule based 3D classifier. The framework is evaluated in a dust filled enclosed trailer and in real underground mining tunnels, and in the tested scenarios the radar based detector maintains stable pedestrian identification as camera and LiDAR modalities fail under severe visibility degradation. These results suggest that the proposed model-driven approach provides robust, interpretable, and computationally efficient perception for safety-critical applications in harsh industrial and subterranean environments.", "AI": {"tldr": "\u63d0\u51fa\u5b8c\u5168\u57fa\u4e8e\u6a21\u578b\u76844D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u6076\u52a3\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u5b9e\u65f6\u884c\u4eba\u68c0\u6d4b\uff0c\u5728\u7c89\u5c18/\u70df\u96fe\u7b49\u53ef\u89c1\u5ea6\u4e25\u91cd\u4e0b\u964d\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u7c89\u5c18\u3001\u70df\u96fe\u3001\u53d7\u9650\u51e0\u4f55\u7ed3\u6784\u548c\u91d1\u5c5e\u7ed3\u6784\u4f1a\u4e25\u91cd\u964d\u4f4e\u5149\u5b66\u548c\u6fc0\u5149\u96f7\u8fbe\u611f\u77e5\u6027\u80fd\uff0c\u800c4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5bf9\u6b64\u7c7b\u6761\u4ef6\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u5176\u7a00\u758f\u5404\u5411\u5f02\u6027\u70b9\u4e91\u7684\u5904\u7406\u65b9\u6cd5\u6709\u9650\u3002", "method": "\u5b8c\u5168\u6a21\u578b\u9a71\u52a8\u76844D\u96f7\u8fbe\u611f\u77e5\u6846\u67b6\uff0c\u5305\u542b\uff1a\u9886\u57df\u611f\u77e5\u591a\u9608\u503c\u6ee4\u6ce2\u3001\u81ea\u6211\u8fd0\u52a8\u8865\u507f\u65f6\u95f4\u7d2f\u79ef\u3001KD\u6811\u6b27\u51e0\u91cc\u5f97\u805a\u7c7b\u4e0e\u591a\u666e\u52d2\u611f\u77e5\u7ec6\u5316\u3001\u57fa\u4e8e\u89c4\u5219\u76843D\u5206\u7c7b\u5668\u3002", "result": "\u5728\u7c89\u5c18\u586b\u5145\u7684\u5c01\u95ed\u62d6\u8f66\u548c\u771f\u5b9e\u5730\u4e0b\u91c7\u77ff\u96a7\u9053\u4e2d\u8bc4\u4f30\uff0c\u5728\u53ef\u89c1\u5ea6\u4e25\u91cd\u4e0b\u964d\u6761\u4ef6\u4e0b\uff0c\u96f7\u8fbe\u68c0\u6d4b\u5668\u4fdd\u6301\u7a33\u5b9a\u7684\u884c\u4eba\u8bc6\u522b\uff0c\u800c\u76f8\u673a\u548c\u6fc0\u5149\u96f7\u8fbe\u6a21\u5f0f\u5931\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4e3a\u6076\u52a3\u5de5\u4e1a/\u5730\u4e0b\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13385", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13385", "abs": "https://arxiv.org/abs/2601.13385", "authors": ["Lavsen Dahal", "Yubraj Bhandari", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Organ-Aware Attention Improves CT Triage and Classification", "comment": null, "summary": "There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.", "AI": {"tldr": "ORACLE-CT\uff1a\u4e00\u79cd\u7528\u4e8eCT\u5f71\u50cf\u5206\u7c7b\u7684\u5668\u5b98\u611f\u77e5\u6a21\u578b\uff0c\u5728\u80f8\u90e8\u548c\u8179\u90e8CT\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76d1\u7763\u5206\u7c7b\u6027\u80fd", "motivation": "\u9700\u8981\u89e3\u51b3\u9ad8\u5bb9\u91cf\u533b\u5b66\u5f71\u50cf\uff08\u5982CT\uff09\u7684\u5feb\u901f\u5206\u8bca\u548c\u5206\u7c7b\u95ee\u9898\uff0c\u4ee5\u6539\u5584\u60a3\u8005\u62a4\u7406\u5e76\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u8d1f\u62c5\u3002\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u74063D\u89e3\u5256\u7ed3\u6784\u3001\u534f\u8bae\u53d8\u5316\u548c\u566a\u58f0\u62a5\u544a\u76d1\u7763\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faORACLE-CT\u6a21\u578b\uff0c\u5305\u542b\u5668\u5b98\u63a9\u7801\u6ce8\u610f\u529b\uff08\u63d0\u4f9b\u7a7a\u95f4\u8bc1\u636e\uff09\u548c\u5668\u5b98\u6807\u91cf\u878d\u5408\uff08\u878d\u5408\u4f53\u79ef\u548cHU\u503c\u7279\u5f81\uff09\u3002\u5728CT-RATE\u548cRADCHEST-CT\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u80f8\u90e8CT\u4e0a\u8fbe\u5230AUROC 0.86\uff0c\u5728\u8179\u90e8CT\u4e0a\u8fbe\u5230AUROC 0.85\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u62a5\u544a\u7684\u7ebf\u6027\u63a2\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76d1\u7763\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "ORACLE-CT\u5728\u80f8\u90e8\u548c\u8179\u90e8CT\u5206\u7c7b\u4e2d\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u8bca\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13380", "abs": "https://arxiv.org/abs/2601.13380", "authors": ["Chaoxin Wang", "Bharaneeshwar Balasubramaniyam", "Anurag Sangem", "Nicolais Guevara", "Doina Caragea"], "title": "Practical Insights into Semi-Supervised Object Detection Approaches", "comment": null, "summary": "Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff08MixPL\u3001Semi-DETR\u548cConsistent-Teacher\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u5728\u6807\u8bb0\u6570\u636e\u91cf\u53d8\u5316\u65f6\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u5728\u6807\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u56fe\u50cf\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u662f\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u95ee\u9898\u3002\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\uff08SSOD\uff09\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u6807\u8bb0\u56fe\u50cf\u548c\u5927\u91cf\u672a\u6807\u8bb0\u56fe\u50cf\u6765\u6539\u5584\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u79cd\u5148\u8fdb\u7684SSOD\u65b9\u6cd5\uff1aMixPL\u3001Semi-DETR\u548cConsistent-Teacher\uff0c\u5728MS-COCO\u548cPascal VOC\u4e24\u4e2a\u6807\u51c6\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u8fd8\u5728\u81ea\u5b9a\u4e49\u7684Beetle\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5206\u6790\u4e0d\u540c\u6807\u8bb0\u6570\u636e\u91cf\u4e0b\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5ef6\u8fdf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u5404\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u6807\u8bb0\u6570\u636e\u91cf\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6027\u80fd\u6bd4\u8f83\u548c\u6307\u5bfc\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u6839\u636e\u5177\u4f53\u9700\u6c42\uff08\u51c6\u786e\u6027\u3001\u6a21\u578b\u590d\u6742\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\uff09\u9009\u62e9\u6700\u9002\u5408\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.13400", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13400", "abs": "https://arxiv.org/abs/2601.13400", "authors": ["Nhat Thanh Tran", "Kevin Bui", "Jack Xin"], "title": "Deep Image Prior with L0 Gradient Regularizer for Image Smoothing", "comment": "To be published in the Proceedings of IEEE ICASSP 2026", "summary": "Image smoothing is a fundamental image processing operation that preserves the underlying structure, such as strong edges and contours, and removes minor details and textures in an image. Many image smoothing algorithms rely on computing local window statistics or solving an optimization problem. Recent state-of-the-art methods leverage deep learning, but they require a carefully curated training dataset. Because constructing a proper training dataset for image smoothing is challenging, we propose DIP-$\\ell_0$, a deep image prior framework that incorporates the $\\ell_0$ gradient regularizer. This framework can perform high-quality image smoothing without any training data. To properly minimize the associated loss function that has the nonconvex, nonsmooth $\\ell_0$ ``norm\", we develop an alternating direction method of multipliers algorithm that utilizes an off-the-shelf $\\ell_0$ gradient minimization solver. Numerical experiments demonstrate that the proposed DIP-$\\ell_0$ outperforms many image smoothing algorithms in edge-preserving image smoothing and JPEG artifact removal.", "AI": {"tldr": "\u63d0\u51faDIP-\u2113\u2080\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u548c\u2113\u2080\u68af\u5ea6\u6b63\u5219\u5316\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u5e73\u6ed1", "motivation": "\u73b0\u6709\u56fe\u50cf\u5e73\u6ed1\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u7a97\u53e3\u7edf\u8ba1\u6216\u4f18\u5316\u95ee\u9898\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u7cbe\u5fc3\u6784\u5efa\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4f46\u6784\u5efa\u5408\u9002\u7684\u56fe\u50cf\u5e73\u6ed1\u8bad\u7ec3\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027", "method": "\u63d0\u51faDIP-\u2113\u2080\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u4e0e\u2113\u2080\u68af\u5ea6\u6b63\u5219\u5316\u7ed3\u5408\uff0c\u4f7f\u7528ADMM\u7b97\u6cd5\u4f18\u5316\u975e\u51f8\u975e\u5149\u6ed1\u7684\u2113\u2080\"\u8303\u6570\"\u635f\u5931\u51fd\u6570", "result": "DIP-\u2113\u2080\u5728\u8fb9\u7f18\u4fdd\u6301\u56fe\u50cf\u5e73\u6ed1\u548cJPEG\u4f2a\u5f71\u53bb\u9664\u65b9\u9762\u4f18\u4e8e\u8bb8\u591a\u73b0\u6709\u56fe\u50cf\u5e73\u6ed1\u7b97\u6cd5", "conclusion": "\u63d0\u51fa\u7684DIP-\u2113\u2080\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u5e73\u6ed1\uff0c\u4e3a\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u76d1\u7763\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13386", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13386", "abs": "https://arxiv.org/abs/2601.13386", "authors": ["Changxu Zhang", "Zhaoze Wang", "Tai Fei", "Christopher Grimm", "Yi Jin", "Claas Tebruegge", "Ernst Warsitz", "Markus Gardill"], "title": "Leveraging Transformer Decoder for Automotive Radar Object Detection", "comment": null, "summary": "In this paper, we present a Transformer-based architecture for 3D radar object detection that uses a novel Transformer Decoder as the prediction head to directly regress 3D bounding boxes and class scores from radar feature representations. To bridge multi-scale radar features and the decoder, we propose Pyramid Token Fusion (PTF), a lightweight module that converts a feature pyramid into a unified, scale-aware token sequence. By formulating detection as a set prediction problem with learnable object queries and positional encodings, our design models long-range spatial-temporal correlations and cross-feature interactions. This approach eliminates dense proposal generation and heuristic post-processing such as extensive non-maximum suppression (NMS) tuning. We evaluate the proposed framework on the RADDet, where it achieves significant improvements over state-of-the-art radar-only baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u76843D\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff0c\u4f7f\u7528Transformer\u89e3\u7801\u5668\u4f5c\u4e3a\u9884\u6d4b\u5934\u76f4\u63a5\u4ece\u96f7\u8fbe\u7279\u5f81\u56de\u5f523D\u8fb9\u754c\u6846\u548c\u7c7b\u522b\u5206\u6570\uff0c\u901a\u8fc7Pyramid Token Fusion\u6a21\u5757\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5c06\u68c0\u6d4b\u5efa\u6a21\u4e3a\u96c6\u5408\u9884\u6d4b\u95ee\u9898\uff0c\u65e0\u9700\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548c\u590d\u6742\u7684NMS\u540e\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548c\u542f\u53d1\u5f0f\u540e\u5904\u7406\uff08\u5982NMS\u8c03\u4f18\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9700\u8981\u624b\u52a8\u8c03\u6574\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u56de\u5f523D\u8fb9\u754c\u6846\uff0c\u540c\u65f6\u5efa\u6a21\u96f7\u8fbe\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u8de8\u7279\u5f81\u4ea4\u4e92\u3002", "method": "1. \u4f7f\u7528Transformer\u89e3\u7801\u5668\u4f5c\u4e3a\u9884\u6d4b\u5934\uff0c\u76f4\u63a5\u56de\u5f523D\u8fb9\u754c\u6846\u548c\u7c7b\u522b\u5206\u6570\uff1b2. \u63d0\u51faPyramid Token Fusion\uff08PTF\uff09\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u5c06\u7279\u5f81\u91d1\u5b57\u5854\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u5c3a\u5ea6\u611f\u77e5token\u5e8f\u5217\uff1b3. \u5c06\u68c0\u6d4b\u5efa\u6a21\u4e3a\u96c6\u5408\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u5bf9\u8c61\u67e5\u8be2\u548c\u4f4d\u7f6e\u7f16\u7801\uff1b4. \u65e0\u9700\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548cNMS\u540e\u5904\u7406\u3002", "result": "\u5728RADDet\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u7eaf\u96f7\u8fbe\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eTransformer\u76843D\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u96c6\u5408\u9884\u6d4b\u65b9\u6cd5\uff0c\u6210\u529f\u6d88\u9664\u4e86\u4f20\u7edf\u68c0\u6d4b\u6d41\u7a0b\u4e2d\u7684\u5bc6\u96c6\u5019\u9009\u6846\u751f\u6210\u548c\u590d\u6742\u540e\u5904\u7406\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13401", "abs": "https://arxiv.org/abs/2601.13401", "authors": ["Peter A. Massih", "Eric Cosatto"], "title": "Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics", "comment": "Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565", "summary": "Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQVLM\u6a21\u578b\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u7f3a\u9677\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u67b6\u6784\u4fdd\u6301\u50cf\u7d20\u7ea7\u7cbe\u5ea6\uff0c\u5728SQuID\u57fa\u51c6\u4e0a\u8fbe\u523042.0%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edfVLM\u768428.1%\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u67b6\u6784\u4f1a\u7834\u574f\u50cf\u7d20\u7ea7\u4fe1\u606f\u3002\u89c6\u89c9\u7f16\u7801\u5668\u901a\u8fc7\u8865\u4e01\u5d4c\u5165\u538b\u7f29\u56fe\u50cf\uff0c\u51cf\u5c11\u4e86\u7a7a\u95f4\u7d22\u5f15\u80fd\u529b\uff0c\u4e22\u5931\u4e86\u7cbe\u786e\u7684\u50cf\u7d20\u7ea7\u8ddf\u8e2a\uff0c\u5bfc\u81f4\u8ba1\u6570\u548c\u6d4b\u91cf\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faQVLM\uff08\u5b9a\u91cf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u91c7\u7528\u4ee3\u7801\u751f\u6210\u67b6\u6784\uff0c\u5c06\u8bed\u8a00\u7406\u89e3\u4e0e\u89c6\u89c9\u5206\u6790\u89e3\u8026\u3002\u4e0d\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u5d4c\u5165\uff0c\u800c\u662f\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff1a\u5148\u8c03\u7528\u5206\u5272\u6a21\u578b\u83b7\u53d6\u50cf\u7d20\u7ea7\u63a9\u7801\uff0c\u7136\u540e\u76f4\u63a5\u5728\u63a9\u7801\u4e0a\u64cd\u4f5c\uff0c\u5728\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a7a\u95f4\u7d22\u5f15\u3002", "result": "QVLM\u4f7f\u7528GPT-5\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u5728SQuID\u57fa\u51c6\u4e0a\u8fbe\u523042.0%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfVLM\uff0828.1%\uff09\u3002SQuID\u5305\u542b2,000\u4e2a\u536b\u661f\u56fe\u50cf\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u5c42\u7ea7\u3002", "conclusion": "\u5bf9\u4e8e\u5b9a\u91cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff0c\u67b6\u6784\u89e3\u8026\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u50cf\u7d20\u7ea7\u4fe1\u606f\u7684\u4fdd\u6301\u5bf9\u8ba1\u6570\u548c\u6d4b\u91cf\u7b49\u5b9a\u91cf\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5d4c\u5165\u65b9\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2601.13404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13404", "abs": "https://arxiv.org/abs/2601.13404", "authors": ["Bhavan Vasu", "Giuseppe Raffa", "Prasad Tadepalli"], "title": "Local-to-Global Logical Explanations for Deep Vision Models", "comment": "15 pages, 5 figures, 5th International Joint Conference on Learning & Reasoning 2025", "summary": "While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u8bc6\u522b\u539f\u59cb\u6982\u5ff5\u7684\u9ed1\u76d2\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\uff0c\u5c06\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\u8868\u793a\u4e3a\u5355\u8c03\u6790\u53d6\u8303\u5f0f\u903b\u8f91\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u5355\u8c03\u89e3\u91ca\u5217\u8868\u5904\u7406\u591a\u7c7b\u522b\u5206\u7c7b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5206\u7c7b\u6548\u679c\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4e3a\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528\u4eba\u7c7b\u53ef\u8bc6\u522b\u7684\u539f\u59cb\u6982\u5ff5\u751f\u6210\u89e3\u91ca\uff1b2. \u5c06\u5c40\u90e8\uff08\u5355\u56fe\u50cf\uff09\u548c\u5168\u5c40\uff08\u56fe\u50cf\u96c6\uff09\u89e3\u91ca\u8868\u793a\u4e3a\u5355\u8c03\u6790\u53d6\u8303\u5f0f\u903b\u8f91\u516c\u5f0f\uff1b3. \u63d0\u51fa\u591a\u7c7b\u522b\u5206\u7c7b\u7684\u5355\u8c03\u89e3\u91ca\u5217\u8868\u7b97\u6cd5\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7b80\u5355\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u5bf9\u9ed1\u76d2\u6a21\u578b\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u8986\u76d6\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u539f\u59cb\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u91ca\u9ed1\u76d2\u6a21\u578b\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6a21\u578b\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.13412", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13412", "abs": "https://arxiv.org/abs/2601.13412", "authors": ["Puneet Sharma", "Kristian Dalsb\u00f8 Hindberg", "Benedicte Schelde-Olesen", "Ulrik Deding", "Esmaeil S. Nadimi", "Jan-Matthias Braun"], "title": "Using deep learning for predicting cleansing quality of colon capsule endoscopy images", "comment": "24 pages", "summary": "In this study, we explore the application of deep learning techniques for predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor, Fair, Good, and Excellent), a ResNet-18 model was trained for classification, leveraging stratified K-fold cross-validation to ensure robust performance. To optimize the model, structured pruning techniques were applied iteratively, achieving significant sparsity while maintaining high accuracy. Explainability of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM, Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent evaluation. Our results indicate that for a pruned model, we can achieve a cross-validation accuracy of 88% with 79% sparsity, demonstrating the effectiveness of pruning in improving efficiency from 84% without compromising performance. We also highlight the challenges of evaluating cleansing quality of CCE images, emphasize the importance of explainability in clinical applications, and discuss the challenges associated with using the ROAD method for our task. Finally, we employ a variant of adaptive temperature scaling to calibrate the pruned models for an external dataset.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u7ed3\u80a0\u80f6\u56ca\u5185\u955c\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\uff0c\u901a\u8fc7ResNet-18\u6a21\u578b\u548c\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\uff0c\u5728\u4fdd\u630188%\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b079%\u7a00\u758f\u5ea6\uff0c\u5e76\u5229\u7528\u591a\u79cdCAM\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "motivation": "\u7ed3\u80a0\u80f6\u56ca\u5185\u955c\u68c0\u67e5\u4e2d\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\u8bc4\u4f30\u5bf9\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bc4\u4f30\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u589e\u5f3a\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528500\u5f20\u753114\u4f4d\u4e34\u5e8a\u533b\u751f\u6309Leighton-Rex\u91cf\u8868\u6807\u6ce8\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3ResNet-18\u6a21\u578b\u8fdb\u884c\u56db\u5206\u7c7b\u3002\u91c7\u7528\u5206\u5c42K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u5e94\u7528\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\u4f18\u5316\u6a21\u578b\uff0c\u4f7f\u7528Grad-CAM\u3001Grad-CAM++\u3001Eigen-CAM\u3001Ablation-CAM\u548cRandom-CAM\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5e76\u7528ROAD\u65b9\u6cd5\u8fdb\u884c\u4e00\u81f4\u6027\u8bc4\u4f30\u3002", "result": "\u526a\u679d\u540e\u7684\u6a21\u578b\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8fbe\u523088%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5b9e\u73b079%\u7684\u7a00\u758f\u5ea6\uff0c\u76f8\u6bd4\u672a\u526a\u679d\u6a21\u578b\u768484%\u51c6\u786e\u7387\u6709\u6240\u63d0\u5347\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u6e29\u5ea6\u7f29\u653e\u53d8\u4f53\u6210\u529f\u6821\u51c6\u4e86\u526a\u679d\u6a21\u578b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u7ed3\u6784\u5316\u526a\u679d\u80fd\u6709\u6548\u63d0\u9ad8\u7ed3\u80a0\u80f6\u56ca\u5185\u955c\u56fe\u50cf\u6e05\u6d01\u8d28\u91cf\u9884\u6d4b\u6a21\u578b\u7684\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5bf9\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46ROAD\u65b9\u6cd5\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u5b58\u5728\u8bc4\u4f30\u6311\u6218\u3002\u6821\u51c6\u6280\u672f\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13416", "abs": "https://arxiv.org/abs/2601.13416", "authors": ["A. Nieto Juscafresa", "\u00c1. Mazcu\u00f1\u00e1n Herreros", "J. Sullivan"], "title": "Diffusion Representations for Fine-Grained Image Classification: A Marine Plankton Case Study", "comment": "21 pages, 6 figures, CVPR format", "summary": "Diffusion models have emerged as state-of-the-art generative methods for image synthesis, yet their potential as general-purpose feature encoders remains underexplored. Trained for denoising and generation without labels, they can be interpreted as self-supervised learners that capture both low- and high-level structure. We show that a frozen diffusion backbone enables strong fine-grained recognition by probing intermediate denoising features across layers and timesteps and training a linear classifier for each pair. We evaluate this in a real-world plankton-monitoring setting with practical impact, using controlled and comparable training setups against established supervised and self-supervised baselines. Frozen diffusion features are competitive with supervised baselines and outperform other self-supervised methods in both balanced and naturally long-tailed settings. Out-of-distribution evaluations on temporally and geographically shifted plankton datasets further show that frozen diffusion features maintain strong accuracy and Macro F1 under substantial distribution shift.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\u7684\u6f5c\u529b\u88ab\u4f4e\u4f30\uff0c\u672c\u6587\u8bc1\u660e\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u7279\u5f81\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u6301\u7a33\u5065\u6027\u80fd", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u56fe\u50cf\u751f\u6210\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4f46\u5176\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u867d\u7136\u6269\u6563\u6a21\u578b\u5728\u65e0\u6807\u7b7e\u6761\u4ef6\u4e0b\u8bad\u7ec3\u7528\u4e8e\u53bb\u566a\u548c\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u53ef\u4ee5\u88ab\u89c6\u4e3a\u6355\u6349\u4f4e\u5c42\u548c\u9ad8\u5c42\u7ed3\u6784\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5668", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u63a2\u6d4b\u4e0d\u540c\u5c42\u548c\u53bb\u566a\u65f6\u95f4\u6b65\u7684\u4e2d\u95f4\u7279\u5f81\uff0c\u4e3a\u6bcf\u4e2a\u5c42-\u65f6\u95f4\u6b65\u5bf9\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6d6e\u6e38\u751f\u7269\u76d1\u6d4b\u573a\u666f\u4e2d\uff0c\u4e0e\u76d1\u7763\u548c\u81ea\u76d1\u7763\u57fa\u7ebf\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30", "result": "\u51bb\u7ed3\u6269\u6563\u7279\u5f81\u4e0e\u76d1\u7763\u57fa\u7ebf\u7ade\u4e89\uff0c\u5728\u5e73\u8861\u548c\u957f\u5c3e\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5176\u4ed6\u81ea\u76d1\u7763\u65b9\u6cd5\u3002\u5728\u65f6\u95f4\u548c\u5730\u7406\u5206\u5e03\u504f\u79fb\u7684\u6d6e\u6e38\u751f\u7269\u6570\u636e\u96c6\u4e0a\uff0c\u6269\u6563\u7279\u5f81\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u548cMacro F1\u5206\u6570", "conclusion": "\u6269\u6563\u6a21\u578b\u4e0d\u4ec5\u662f\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u4e5f\u662f\u6709\u6548\u7684\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\uff0c\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u65f6\u4fdd\u6301\u7a33\u5065\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13417", "abs": "https://arxiv.org/abs/2601.13417", "authors": ["Yujian Xiong", "Xuanzhao Dong", "Wenhui Zhu", "Xin Li", "Oana Dumitrascu", "Yalin Wang"], "title": "SGW-GAN: Sliced Gromov-Wasserstein Guided GANs for Retinal Fundus Image Enhancement", "comment": null, "summary": "Retinal fundus photography is indispensable for ophthalmic screening and diagnosis, yet image quality is often degraded by noise, artifacts, and uneven illumination. Recent GAN- and diffusion-based enhancement methods improve perceptual quality by aligning degraded images with high-quality distributions, but our analysis shows that this focus can distort intra-class geometry: clinically related samples become dispersed, disease-class boundaries blur, and downstream tasks such as grading or lesion detection are harmed. The Gromov Wasserstein (GW) discrepancy offers a principled solution by aligning distributions through internal pairwise distances, naturally preserving intra-class structure, but its high computational cost restricts practical use. To overcome this, we propose SGW-GAN, the first framework to incorporate Sliced GW (SGW) into retinal image enhancement. SGW approximates GW via random projections, retaining relational fidelity while greatly reducing cost. Experiments on public datasets show that SGW-GAN produces visually compelling enhancements, achieves superior diabetic retinopathy grading, and reports the lowest GW discrepancy across disease labels, demonstrating both efficiency and clinical fidelity for unpaired medical image enhancement.", "AI": {"tldr": "\u63d0\u51faSGW-GAN\u6846\u67b6\uff0c\u5c06\u5207\u7247Gromov Wasserstein\u8ddd\u79bb\u5f15\u5165\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\uff0c\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u7684\u7c7b\u5185\u51e0\u4f55\u7ed3\u6784\uff0c\u6539\u5584\u4e0b\u6e38\u75be\u75c5\u5206\u7ea7\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709GAN\u548c\u6269\u6563\u6a21\u578b\u5728\u589e\u5f3a\u89c6\u7f51\u819c\u56fe\u50cf\u65f6\u8fc7\u5ea6\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5bfc\u81f4\u4e34\u5e8a\u76f8\u5173\u6837\u672c\u5206\u6563\u3001\u75be\u75c5\u7c7b\u522b\u8fb9\u754c\u6a21\u7cca\uff0c\u635f\u5bb3\u4e86\u4e0b\u6e38\u8bca\u65ad\u4efb\u52a1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u53c8\u80fd\u4fdd\u6301\u7c7b\u5185\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSGW-GAN\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5207\u7247Gromov Wasserstein\u8ddd\u79bb\u5f15\u5165\u89c6\u7f51\u819c\u56fe\u50cf\u589e\u5f3a\u3002SGW\u901a\u8fc7\u968f\u673a\u6295\u5f71\u8fd1\u4f3cGW\u8ddd\u79bb\uff0c\u5728\u4fdd\u6301\u5173\u7cfb\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSGW-GAN\u80fd\u4ea7\u751f\u89c6\u89c9\u4e0a\u4ee4\u4eba\u4fe1\u670d\u7684\u589e\u5f3a\u6548\u679c\uff0c\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u7ea7\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u5728\u75be\u75c5\u6807\u7b7e\u4e0a\u62a5\u544a\u4e86\u6700\u4f4e\u7684GW\u5dee\u5f02\u3002", "conclusion": "SGW-GAN\u4e3a\u65e0\u914d\u5bf9\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4fdd\u6301\u4e34\u5e8a\u4fdd\u771f\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u53c8\u4fdd\u6301\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u7c7b\u5185\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2601.13440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13440", "abs": "https://arxiv.org/abs/2601.13440", "authors": ["Mohit Kakda", "Mirudula Shri Muthukumaran", "Uttapreksha Patel", "Lawrence Swaminathan Xavier Prince"], "title": "Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation", "comment": "10 pages,4 images", "summary": "Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u5206\u6790\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86WinCLIP\u3001AprilLab\u7b49\u5173\u952e\u67b6\u6784\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86\u7279\u5f81\u63d0\u53d6\u3001\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u3001\u63d0\u793a\u5de5\u7a0b\u7b49\u7ef4\u5ea6\uff0c\u4e3a\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u7684VLM\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662fCLIP\uff09\u901a\u8fc7\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u7f3a\u9677\u8bc6\u522b\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u5373\u53ef\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u7f3a\u9677\u793a\u4f8b\u7684\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u5168\u9762\u5206\u6790VLM\u5728\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u4e09\u79cd\u5173\u952e\u67b6\u6784\u8303\u5f0f\uff1a1\uff09\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u7684\u5bc6\u96c6\u7279\u5f81\u63d0\u53d6\uff08WinCLIP\uff09\uff1b2\uff09\u5177\u6709\u53ef\u5b66\u4e60\u6295\u5f71\u7684\u591a\u9636\u6bb5\u7279\u5f81\u5bf9\u9f50\uff08AprilLab\u6846\u67b6\uff09\uff1b3\uff09\u7ec4\u5408\u63d0\u793a\u96c6\u6210\u7b56\u7565\u3002\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u673a\u5236\u3001\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u7b56\u7565\u3001\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3001\u96f6\u6837\u672c\u4e0e\u5c11\u6837\u672c\u6743\u8861\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u57df\u6cdb\u5316\u7b49\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728MVTec AD\u548cVisA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3001\u5206\u5272\u7cbe\u5ea6\u548c\u63a8\u7406\u6548\u7387\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u4e8eVLM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6210\u529f\u539f\u56e0\u7684\u57fa\u7840\u7406\u89e3\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u5bf9VLM\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5e94\u7528\u7684\u7efc\u5408\u7406\u89e3\uff0c\u4e3a\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u4e2d\u7684\u65b9\u6cd5\u91c7\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u7814\u7a76\u5f3a\u8c03\u4e86VLM\u5728\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u57df\u9002\u5e94\u6027\u7b49\u6311\u6218\u3002"}}
{"id": "2601.13498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13498", "abs": "https://arxiv.org/abs/2601.13498", "authors": ["Nimrod Kruger", "Nicholas Owen Ralph", "Gregory Cohen", "Paul Hurley"], "title": "Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging", "comment": null, "summary": "Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON/OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u6620\u5c04\u5230\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\u7684\u7269\u7406\u57fa\u7840\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u5d4c\u5165\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff0c\u5b9e\u73b0\u76f4\u63a5\u4ece\u4e8b\u4ef6\u6570\u636e\u8fdb\u884c\u9006\u6ee4\u6ce2\u548c\u7ef4\u7eb3\u89e3\u5377\u79ef\u3002", "motivation": "\u4e8b\u4ef6\u89c6\u89c9\u4f20\u611f\u5668\uff08\u795e\u7ecf\u5f62\u6001\u76f8\u673a\uff09\u8f93\u51fa\u7a00\u758f\u3001\u5f02\u6b65\u7684ON/OFF\u4e8b\u4ef6\uff0c\u5177\u6709\u5fae\u79d2\u7ea7\u611f\u77e5\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u4f4e\u6570\u636e\u5e26\u5bbd\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u975e\u7ebf\u6027\u4e8b\u4ef6\u8868\u793a\u96be\u4ee5\u4e0e\u5927\u591a\u6570\u8ba1\u7b97\u6210\u50cf\u548c\u5149\u5b66\u7cfb\u7edf\u8bbe\u8ba1\u6240\u4f9d\u8d56\u7684\u7ebf\u6027\u524d\u5411\u6a21\u578b\u96c6\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u57fa\u7840\u7684\u5904\u7406\u6d41\u7a0b\uff1a1) \u5c06\u4e8b\u4ef6\u6d41\u6620\u5c04\u5230\u6bcf\u50cf\u7d20\u5bf9\u6570\u5f3a\u5ea6\u548c\u5f3a\u5ea6\u5bfc\u6570\u4f30\u8ba1\uff1b2) \u5c06\u8fd9\u4e9b\u6d4b\u91cf\u5d4c\u5165\u5177\u6709\u65f6\u53d8\u70b9\u6269\u6563\u51fd\u6570\u7684\u52a8\u6001\u7ebf\u6027\u7cfb\u7edf\u6a21\u578b\uff1b3) \u4f7f\u7528\u5df2\u77e5\uff08\u6216\u53c2\u6570\u5316\uff09\u52a8\u6001\u4f20\u9012\u51fd\u6570\u8fdb\u884c\u9891\u57df\u7ef4\u7eb3\u89e3\u5377\u79ef\uff0c\u76f4\u63a5\u4ece\u4e8b\u4ef6\u6570\u636e\u5b9e\u73b0\u9006\u6ee4\u6ce2\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u8c03\u5236\u79bb\u7126\u4e0b\u5355\u70b9\u548c\u91cd\u53e0\u70b9\u6e90\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u4e8b\u4ef6\u6570\u636e\uff08\u53ef\u8c03\u7126\u671b\u8fdc\u955c\u89c2\u6d4b\u661f\u573a\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6e90\u5b9a\u4f4d\u548c\u53ef\u5206\u79bb\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e8b\u4ef6\u611f\u77e5\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u52a8\u6001\u5149\u5b66\u7cfb\u7edf\u8ba1\u7b97\u6210\u50cf\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6865\u6881\u3002"}}
{"id": "2601.13502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13502", "abs": "https://arxiv.org/abs/2601.13502", "authors": ["Nhi Kieu", "Kien Nguyen", "Arnold Wiliem", "Clinton Fookes", "Sridha Sridharan"], "title": "DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities", "comment": "Accepted to WACV 2026 - Computer Vision for Earth Observation Workshop", "summary": "The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.", "AI": {"tldr": "DIS2\uff1a\u9488\u5bf9\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u4e0e\u77e5\u8bc6\u84b8\u998f\u7684\u534f\u540c\u673a\u5236\uff0c\u5b9e\u73b0\u4e3b\u52a8\u7684\u7f3a\u5931\u7279\u5f81\u8865\u507f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u9762\u4e34\u6a21\u6001\u7f3a\u5931\u7684\u4e25\u91cd\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u89e3\u8026\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff09\u56e0\u9065\u611f\u6570\u636e\u7684\u9ad8\u5ea6\u5f02\u6784\u6027\u548c\u5c3a\u5ea6\u53d8\u5316\u800c\u5931\u6548\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u7279\u70b9\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51faDIS2\u65b9\u6cd5\uff0c\u6838\u5fc3\u662fDLKD\uff08\u89e3\u8026\u5b66\u4e60\u4e0e\u77e5\u8bc6\u84b8\u998f\u534f\u540c\uff09\u673a\u5236\uff0c\u901a\u8fc7\u8865\u507f\u7279\u5f81\u6355\u83b7\u4e0e\u53ef\u7528\u6a21\u6001\u7279\u5f81\u878d\u5408\u6765\u8fd1\u4f3c\u5b8c\u6574\u6a21\u6001\u8868\u793a\uff1b\u7ed3\u5408CFLM\uff08\u7c7b\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u6a21\u5757\uff09\u81ea\u9002\u5e94\u5b66\u4e60\u6bcf\u7c7b\u5224\u522b\u8bc1\u636e\uff1b\u91c7\u7528\u5206\u5c42\u6df7\u5408\u878d\u5408\u7ed3\u6784\u5229\u7528\u591a\u5206\u8fa8\u7387\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DIS2\u901a\u8fc7\u4ece\u6a21\u6001\u5171\u4eab\u7279\u5f81\u4f9d\u8d56\u548c\u65e0\u76ee\u6807\u6a21\u4eff\u8f6c\u5411\u4e3b\u52a8\u5f15\u5bfc\u7684\u7f3a\u5931\u7279\u5f81\u8865\u507f\uff0c\u4e3a\u9065\u611f\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176DLKD\u534f\u540c\u673a\u5236\u548c\u7c7b\u7279\u5b9a\u5b66\u4e60\u6a21\u5757\u662f\u5173\u952e\u521b\u65b0\u3002"}}
{"id": "2601.13622", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13622", "abs": "https://arxiv.org/abs/2601.13622", "authors": ["Donghee Lee", "Rui Cai", "Zhe Zhao"], "title": "CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models", "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.", "AI": {"tldr": "CARPE\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u96c6\u6210\u5c42\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u8ba9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u5730\u4f18\u5148\u5904\u7406\u56fe\u50cf\u8868\u793a\u6216\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u5206\u7c7b\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f80\u5f80\u4e0d\u5982\u5176\u57fa\u7840\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u3002\u9700\u8981\u89e3\u51b3\u6a21\u578b\u5728\u4f55\u65f6\u5e94\u8be5\u4f18\u5148\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u3001\u4f55\u65f6\u5e94\u8be5\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCARPE\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u89c6\u89c9\u96c6\u6210\u5c42\uff0c\u4f7f\u6a21\u578b\u80fd\u6355\u6349\u56fe\u50cf\u8868\u793a\u7684\u4e0d\u540c\u65b9\u9762\uff1b2\uff09\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5730\u6743\u8861\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u51b3\u5b9a\u4f55\u65f6\u4f18\u5148\u5904\u7406\u56fe\u50cf\u8868\u793a\u6216\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "result": "CARPE\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u83b7\u5f97\u6539\u8fdb\u3002\u8be5\u6846\u67b6\u80fd\u4e0e\u5927\u591a\u6570\u5f00\u6e90\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6709\u6548\u96c6\u6210\uff0c\u5177\u6709\u826f\u597d\u7684\u67b6\u6784\u9002\u5e94\u6027\u3002", "conclusion": "CARPE\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u6743\u8861\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2601.13524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13524", "abs": "https://arxiv.org/abs/2601.13524", "authors": ["Yang Yu", "Yunze Deng", "Yige Zhang", "Yanjie Xiao", "Youkun Ou", "Wenhao Hu", "Mingchao Li", "Bin Feng", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "GO-MLVTON: Garment Occlusion-Aware Multi-Layer Virtual Try-On with Diffusion Models", "comment": "5pages, 3 figures", "summary": "Existing Image-based virtual try-on (VTON) methods primarily focus on single-layer or multi-garment VTON, neglecting multi-layer VTON (ML-VTON), which involves dressing multiple layers of garments onto the human body with realistic deformation and layering to generate visually plausible outcomes. The main challenge lies in accurately modeling occlusion relationships between inner and outer garments to reduce interference from redundant inner garment features. To address this, we propose GO-MLVTON, the first multi-layer VTON method, introducing the Garment Occlusion Learning module to learn occlusion relationships and the StableDiffusion-based Garment Morphing & Fitting module to deform and fit garments onto the human body, producing high-quality multi-layer try-on results. Additionally, we present the MLG dataset for this task and propose a new metric named Layered Appearance Coherence Difference (LACD) for evaluation. Extensive experiments demonstrate the state-of-the-art performance of GO-MLVTON. Project page: https://upyuyang.github.io/go-mlvton/.", "AI": {"tldr": "GO-MLVTON\u662f\u9996\u4e2a\u591a\u5c42\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u670d\u88c5\u906e\u6321\u5b66\u4e60\u548c\u57fa\u4e8eStableDiffusion\u7684\u670d\u88c5\u53d8\u5f62\u62df\u5408\u6a21\u5757\uff0c\u89e3\u51b3\u591a\u5c42\u670d\u88c5\u8bd5\u7a7f\u4e2d\u7684\u906e\u6321\u5173\u7cfb\u548c\u53d8\u5f62\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u5c42\u6216\u591a\u4ef6\u670d\u88c5\u8bd5\u7a7f\uff0c\u5ffd\u7565\u4e86\u591a\u5c42\u670d\u88c5\u8bd5\u7a7f\uff08ML-VTON\uff09\uff0c\u591a\u5c42\u8bd5\u7a7f\u9700\u8981\u5904\u7406\u5185\u5916\u670d\u88c5\u4e4b\u95f4\u7684\u906e\u6321\u5173\u7cfb\uff0c\u907f\u514d\u5185\u5c42\u670d\u88c5\u7279\u5f81\u7684\u5197\u4f59\u5e72\u6270\u3002", "method": "\u63d0\u51faGO-MLVTON\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u670d\u88c5\u906e\u6321\u5b66\u4e60\u6a21\u5757\uff0c\u5b66\u4e60\u5185\u5916\u670d\u88c5\u7684\u906e\u6321\u5173\u7cfb\uff1b2) \u57fa\u4e8eStableDiffusion\u7684\u670d\u88c5\u53d8\u5f62\u62df\u5408\u6a21\u5757\uff0c\u5c06\u670d\u88c5\u53d8\u5f62\u5e76\u8d34\u5408\u5230\u4eba\u4f53\u4e0a\u3002", "result": "\u65b9\u6cd5\u5728MLG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807LACD\uff08\u5206\u5c42\u5916\u89c2\u4e00\u81f4\u6027\u5dee\u5f02\uff09\u3002", "conclusion": "GO-MLVTON\u662f\u9996\u4e2a\u89e3\u51b3\u591a\u5c42\u865a\u62df\u8bd5\u7a7f\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u906e\u6321\u5b66\u4e60\u548c\u53d8\u5f62\u62df\u5408\u6280\u672f\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u5c42\u8bd5\u7a7f\u7ed3\u679c\uff0c\u4e3a\u8fd9\u4e00\u65b0\u9886\u57df\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.13551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13551", "abs": "https://arxiv.org/abs/2601.13551", "authors": ["Feng Ding", "Wenhui Yi", "Xinan He", "Mengyao Xiao", "Jianfeng Xu", "Jianqiang Du"], "title": "DiffFace-Edit: A Diffusion-Based Facial Dataset for Forgery-Semantic Driven Deepfake Detection Analysis", "comment": null, "summary": "Generative models now produce imperceptible, fine-grained manipulated faces, posing significant privacy risks. However, existing AI-generated face datasets generally lack focus on samples with fine-grained regional manipulations. Furthermore, no researchers have yet studied the real impact of splice attacks, which occur between real and manipulated samples, on detectors. We refer to these as detector-evasive samples. Based on this, we introduce the DiffFace-Edit dataset, which has the following advantages: 1) It contains over two million AI-generated fake images. 2) It features edits across eight facial regions (e.g., eyes, nose) and includes a richer variety of editing combinations, such as single-region and multi-region edits. Additionally, we specifically analyze the impact of detector-evasive samples on detection models. We conduct a comprehensive analysis of the dataset and propose a cross-domain evaluation that combines IMDL methods. Dataset will be available at https://github.com/ywh1093/DiffFace-Edit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DiffFace-Edit\u6570\u636e\u96c6\uff0c\u5305\u542b200\u591a\u4e07\u5f20AI\u751f\u6210\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u4e13\u6ce8\u4e8e\u7ec6\u7c92\u5ea6\u533a\u57df\u7f16\u8f91\uff0c\u5e76\u9996\u6b21\u7814\u7a76\u4e86\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u73b0\u5728\u80fd\u4ea7\u751f\u96be\u4ee5\u5bdf\u89c9\u7684\u7ec6\u7c92\u5ea6\u4eba\u8138\u64cd\u7eb5\uff0c\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709AI\u751f\u6210\u4eba\u8138\u6570\u636e\u96c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u533a\u57df\u64cd\u7eb5\u6837\u672c\uff0c\u4e14\u65e0\u4eba\u7814\u7a76\u771f\u5b9e\u4e0e\u64cd\u7eb5\u6837\u672c\u4e4b\u95f4\u7684\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u521b\u5efaDiffFace-Edit\u6570\u636e\u96c6\uff0c\u5305\u542b200\u591a\u4e07\u5f20AI\u751f\u6210\u7684\u5047\u56fe\u50cf\uff0c\u6db5\u76d68\u4e2a\u9762\u90e8\u533a\u57df\uff08\u5982\u773c\u775b\u3001\u9f3b\u5b50\uff09\u7684\u7f16\u8f91\uff0c\u5305\u62ec\u5355\u533a\u57df\u548c\u591a\u533a\u57df\u7f16\u8f91\u7ec4\u5408\u3002\u5206\u6790\u68c0\u6d4b\u5668\u89c4\u907f\u6837\u672c\u5bf9\u68c0\u6d4b\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u8fdb\u884c\u6570\u636e\u96c6\u5168\u9762\u5206\u6790\u5e76\u63d0\u51fa\u7ed3\u5408IMDL\u65b9\u6cd5\u7684\u8de8\u57df\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u4e30\u5bcc\u7ec6\u7c92\u5ea6\u7f16\u8f91\u7684AI\u751f\u6210\u4eba\u8138\u6570\u636e\u96c6\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u62fc\u63a5\u653b\u51fb\u5bf9\u68c0\u6d4b\u5668\u7684\u5f71\u54cd\uff0c\u4e3a\u68c0\u6d4b\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u57fa\u51c6\u3002", "conclusion": "DiffFace-Edit\u6570\u636e\u96c6\u586b\u8865\u4e86\u7ec6\u7c92\u5ea6\u533a\u57df\u64cd\u7eb5\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u68c0\u6d4b\u5668\u89c4\u907f\u6837\u672c\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u7684AI\u751f\u6210\u4eba\u8138\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2601.13565", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13565", "abs": "https://arxiv.org/abs/2601.13565", "authors": ["Yu Qin", "Shimeng Fan", "Fan Yang", "Zixuan Xue", "Zijie Mai", "Wenrui Chen", "Kailun Yang", "Zhiyong Li"], "title": "Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation", "comment": "The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP", "summary": "Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.", "AI": {"tldr": "FiCoP\u63d0\u51fa\u4ece\u566a\u58f0\u6613\u53d1\u7684\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684patch\u7ea7\u5bf9\u5e94\uff0c\u901a\u8fc7patch-to-patch\u76f8\u5173\u77e9\u9635\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u6765\u7f29\u5c0f\u5339\u914d\u8303\u56f4\uff0c\u5728\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c476D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u65e0\u7ea6\u675f\u7684\u5168\u5c40\u5339\u914d\u7b56\u7565\uff0c\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u5c06\u951a\u70b9\u7279\u5f81\u4e0e\u6574\u4e2a\u67e5\u8be2\u56fe\u50cf\u7a7a\u95f4\u5339\u914d\u4f1a\u5f15\u5165\u8fc7\u591a\u6b67\u4e49\uff0c\u76ee\u6807\u7279\u5f81\u5bb9\u6613\u4e0e\u80cc\u666f\u5e72\u6270\u7269\u6df7\u6dc6\u3002", "method": "1) \u7269\u4f53\u4e2d\u5fc3\u89e3\u8026\u9884\u5904\u7406\u5206\u79bb\u8bed\u4e49\u76ee\u6807\u4e0e\u73af\u5883\u566a\u58f0\uff1b2) \u8de8\u89c6\u89d2\u5168\u5c40\u611f\u77e5\u6a21\u5757\u878d\u5408\u53cc\u89c6\u89d2\u7279\u5f81\uff0c\u901a\u8fc7\u663e\u5f0f\u4e0a\u4e0b\u6587\u63a8\u7406\u5efa\u7acb\u7ed3\u6784\u5171\u8bc6\uff1b3) Patch\u76f8\u5173\u9884\u6d4b\u5668\u751f\u6210\u7cbe\u786e\u7684\u5757\u7ea7\u5173\u8054\u56fe\uff0c\u4f5c\u4e3a\u7a7a\u95f4\u8fc7\u6ee4\u5668\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u6297\u566a\u58f0\u5339\u914d\u3002", "result": "\u5728REAL275\u548cToyota-Light\u6570\u636e\u96c6\u4e0a\uff0cFiCoP\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u5e73\u5747\u53ec\u56de\u73878.0%\u548c6.1%\uff0c\u8bc1\u660e\u5176\u5728\u590d\u6742\u65e0\u7ea6\u675f\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u9c81\u68d2\u548c\u6cdb\u5316\u611f\u77e5\u7684\u80fd\u529b\u3002", "conclusion": "FiCoP\u901a\u8fc7\u4ece\u5168\u5c40\u5339\u914d\u8f6c\u5411\u7a7a\u95f4\u7ea6\u675f\u7684patch\u7ea7\u5bf9\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c476D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u611f\u77e5\u57fa\u7840\u3002"}}
{"id": "2601.14084", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14084", "abs": "https://arxiv.org/abs/2601.14084", "authors": ["Abdurrahim Yilmaz", "Ozan Erdem", "Ece Gokyayla", "Ayda Acar", "Burc Bugra Dagtas", "Dilara Ilhan Erdil", "Gulsum Gencoglan", "Burak Temelkuran"], "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning", "comment": null, "summary": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.", "AI": {"tldr": "DermaBench\u662f\u4e00\u4e2a\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u7684\u76ae\u80a4\u75c5\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u57fa\u4e8eDDI\u6570\u636e\u96c6\u6784\u5efa\uff0c\u5305\u542b656\u5f20\u4e34\u5e8a\u56fe\u50cf\u548c\u7ea61.45\u4e07\u4e2aVQA\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\u5168\u9762\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u4e2d\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u56fe\u50cf\u7ea7\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u75c5\u53d8\u8bc6\u522b\uff09\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u3001\u8bed\u8a00\u57fa\u7840\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981VQA\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u76ae\u80a4\u75c5\u56fe\u50cf\u7684\u89e3\u8bfb\u3001\u5f62\u6001\u5b66\u63a8\u7406\u548c\u4e34\u5e8a\u63cf\u8ff0\u751f\u6210\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u591a\u6837\u76ae\u80a4\u75c5\u56fe\u50cf\uff08DDI\uff09\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u5305\u542b656\u5f20\u4e34\u5e8a\u56fe\u50cf\uff08\u6765\u81ea570\u540d\u60a3\u8005\uff0c\u6db5\u76d6Fitzpatrick\u76ae\u80a4\u7c7b\u578bI-VI\uff09\u7684DermaBench\u57fa\u51c6\u3002\u91c7\u7528\u5206\u5c42\u6807\u6ce8\u6a21\u5f0f\uff0c\u5305\u542b22\u4e2a\u4e3b\u8981\u95ee\u9898\u7c7b\u578b\uff08\u5355\u9009\u3001\u591a\u9009\u3001\u5f00\u653e\u5f0f\uff09\uff0c\u7531\u4e13\u4e1a\u76ae\u80a4\u79d1\u533b\u751f\u5bf9\u6bcf\u5f20\u56fe\u50cf\u8fdb\u884c\u8bca\u65ad\u3001\u89e3\u5256\u90e8\u4f4d\u3001\u75c5\u53d8\u5f62\u6001\u3001\u5206\u5e03\u3001\u8868\u9762\u7279\u5f81\u3001\u989c\u8272\u3001\u56fe\u50cf\u8d28\u91cf\u7b49\u6807\u6ce8\uff0c\u4ee5\u53ca\u5f00\u653e\u5f0f\u53d9\u8ff0\u63cf\u8ff0\u548c\u603b\u7ed3\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u7ea614,474\u4e2aVQA\u98ce\u683c\u6807\u6ce8\u7684\u76ae\u80a4\u75c5\u5b66\u57fa\u51c6\uff0c\u4f5c\u4e3a\u4ec5\u5143\u6570\u636e\u7684\u6570\u636e\u96c6\u53d1\u5e03\uff0c\u4ee5\u5c0a\u91cd\u4e0a\u6e38\u8bb8\u53ef\uff0c\u5e76\u5728\u54c8\u4f5bDataverse\u4e0a\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "DermaBench\u586b\u8865\u4e86\u76ae\u80a4\u75c5\u5b66\u4e2d\u5168\u9762\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u56fe\u50cf\u89e3\u8bfb\u3001\u5f62\u6001\u5b66\u63a8\u7406\u548c\u4e34\u5e8a\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2601.13606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13606", "abs": "https://arxiv.org/abs/2601.13606", "authors": ["Zheng Liu", "Honglin Lin", "Chonghan Qin", "Xiaoyang Wang", "Xin Gao", "Yu Li", "Mengzhang Cai", "Yun Zhu", "Zhanping Zhong", "Qizhi Pei", "Zhuoshi Pan", "Xiaoran Shang", "Bin Cui", "Conghui He", "Wentao Zhang", "Lijun Wu"], "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch", "comment": "29 pages", "summary": "Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.", "AI": {"tldr": "ChartVerse\uff1a\u4e00\u4e2a\u901a\u8fc7\u91cf\u5316\u56fe\u8868\u590d\u6742\u5ea6\uff08RPE\u6307\u6807\uff09\u548c\u7b54\u6848\u4f18\u5148\u7684\u9006\u5411QA\u5408\u6210\u65b9\u6cd5\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u8868\u63a8\u7406\u6570\u636e\u7684\u6846\u67b6\uff0c\u8bad\u7ec3\u51fa\u76848B\u6a21\u578b\u8d85\u8d8a\u4e86\u517632B\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u65b9\u9762\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u5408\u6210\u56fe\u8868\u8fc7\u4e8e\u7b80\u5355\u91cd\u590d\uff0c\u800c\u76f8\u5173QA\u6570\u636e\u5b58\u5728\u5e7b\u89c9\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u6025\u9700\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "1. \u63d0\u51faRollout Posterior Entropy (RPE)\u91cf\u5316\u56fe\u8868\u590d\u6742\u5ea6\uff0c\u6307\u5bfc\u590d\u6742\u5ea6\u611f\u77e5\u56fe\u8868\u7f16\u7801\u5668\u901a\u8fc7\u53ef\u6267\u884c\u7a0b\u5e8f\u5408\u6210\u591a\u6837\u5316\u9ad8\u590d\u6742\u5ea6\u56fe\u8868\u30022. \u91c7\u7528\u7b54\u6848\u4f18\u5148\u7684\u9006\u5411QA\u5408\u6210\uff1a\u4ece\u6e90\u4ee3\u7801\u63d0\u53d6\u786e\u5b9a\u6027\u7b54\u6848\uff0c\u57fa\u4e8e\u951a\u70b9\u751f\u6210\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e25\u683c\u4e00\u81f4\u6027\u9a8c\u8bc1\u30023. \u57fa\u4e8e\u6a21\u578b\u5931\u8d25\u7387\u7b5b\u9009\u6837\u672c\uff0c\u84b8\u998f\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u6784\u5efa\u4e86ChartVerse-SFT-600K\u548cChartVerse-RL-40K\u6570\u636e\u96c6\uff0c\u4f7f\u7528Qwen3-VL-30B-A3B-Thinking\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002ChartVerse-8B\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5176\u6559\u5e08\u6a21\u578b\uff0c\u751a\u81f3\u53ef\u4e0e\u66f4\u5f3a\u7684Qwen3-VL-32B-Thinking\u76f8\u5ab2\u7f8e\u3002", "conclusion": "ChartVerse\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u89e3\u51b3\u56fe\u8868\u590d\u6742\u5ea6\u548c\u63a8\u7406\u4e25\u8c28\u6027\u95ee\u9898\uff0c\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u56fe\u8868\u63a8\u7406\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u56fe\u8868\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5f00\u6e90VLM\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2601.14127", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14127", "abs": "https://arxiv.org/abs/2601.14127", "authors": ["Renmiao Chen", "Yida Lu", "Shiyao Cui", "Xuan Ouyang", "Victor Shea-Jay Huang", "Shumin Zhang", "Chengwei Pan", "Han Qiu", "Minlie Huang"], "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning", "comment": "*15 pages, 5 figures. Introduces MIR-SafetyBench (2,676 instances; 9 multi-image relations). Equal contribution; \u2020Corresponding author. Code/data: https://github.com/thu-coai/MIR-SafetyBench", "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.", "AI": {"tldr": "MIR-SafetyBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u591a\u56fe\u50cf\u63a8\u7406\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,676\u4e2a\u5b9e\u4f8b\u548c9\u79cd\u591a\u56fe\u50cf\u5173\u7cfb\u5206\u7c7b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5177\u6709\u66f4\u5f3a\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u7684MLLM\u6a21\u578b\u5728\u5b89\u5168\u6027\u65b9\u9762\u53cd\u800c\u66f4\u8106\u5f31\uff0c\u4e14\u8bb8\u591a\u88ab\u6807\u8bb0\u4e3a\u5b89\u5168\u7684\u56de\u590d\u5b9e\u9645\u4e0a\u662f\u80a4\u6d45\u7684\u8bef\u89e3\u6216\u9003\u907f\u6027\u56de\u7b54\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u63d0\u5347\uff0c\u8fd9\u79cd\u8fdb\u6b65\u53ef\u80fd\u5e26\u6765\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u591a\u56fe\u50cf\u63a8\u7406\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u7814\u7a76\u5148\u8fdb\u63a8\u7406\u80fd\u529b\u4e0e\u5b89\u5168\u6f0f\u6d1e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86MIR-SafetyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,676\u4e2a\u5b9e\u4f8b\uff0c\u6db5\u76d69\u79cd\u591a\u56fe\u50cf\u5173\u7cfb\u5206\u7c7b\u3002\u5bf919\u4e2aMLLM\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u653b\u51fb\u6210\u529f\u7387\u3001\u56de\u590d\u8d28\u91cf\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u5b89\u5168\u751f\u6210\u4e0e\u6ce8\u610f\u529b\u71b5\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4ee4\u4eba\u62c5\u5fe7\u7684\u8d8b\u52bf\uff1a\u5177\u6709\u66f4\u5148\u8fdb\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728MIR-SafetyBench\u4e0a\u66f4\u8106\u5f31\u3002\u8bb8\u591a\u88ab\u6807\u8bb0\u4e3a\u5b89\u5168\u7684\u56de\u590d\u5b9e\u9645\u4e0a\u662f\u80a4\u6d45\u7684\u8bef\u89e3\u6216\u9003\u907f\u6027\u56de\u7b54\u3002\u4e0d\u5b89\u5168\u751f\u6210\u7684\u5e73\u5747\u6ce8\u610f\u529b\u71b5\u4f4e\u4e8e\u5b89\u5168\u751f\u6210\uff0c\u8868\u660e\u6a21\u578b\u53ef\u80fd\u8fc7\u5ea6\u4e13\u6ce8\u4e8e\u4efb\u52a1\u89e3\u51b3\u800c\u5ffd\u89c6\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u591a\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u53ef\u80fd\u5e26\u6765\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u5b89\u5168\u8bc4\u4f30\u3002\u6ce8\u610f\u529b\u71b5\u7684\u5dee\u5f02\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u5904\u7406\u5b89\u5168\u7ea6\u675f\u7684\u673a\u5236\uff0c\u4e3a\u672a\u6765\u5b89\u5168\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2601.13707", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13707", "abs": "https://arxiv.org/abs/2601.13707", "authors": ["Yujin Jo", "Sangyoon Bae", "Taesup Kim"], "title": "Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs", "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.", "AI": {"tldr": "ACG\u901a\u8fc7\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u4e2d\u7684\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u6307\u5bfc\uff0c\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u8a00\u5148\u9a8c\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u4ece\u800c\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4e3b\u8981\u6e90\u4e8e\u8bed\u8a00\u5148\u9a8c\u4e3b\u5bfc\u89c6\u89c9\u8bc1\u636e\uff0c\u5bfc\u81f4\u7269\u4f53\u8bef\u8bc6\u522b\u548c\u89c6\u89c9\u4e0d\u4e00\u81f4\u63cf\u8ff0\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u5fe0\u5b9e\u5ea6\u53c8\u80fd\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u6307\u5bfc\uff08ACG\uff09\uff0c\u5728\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u4e2d\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00\u548c\u7eaf\u8bed\u8a00\u6ce8\u610f\u529b\u8def\u5f84\uff0c\u901a\u8fc7\u6b63\u4ea4\u5316\u6821\u6b63\u6d88\u9664\u8fd1\u4f3c\u504f\u5dee\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u89c6\u89c9\u8d21\u732e\u3002", "result": "\u5728CHAIR\u548cPOPE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5fe0\u5b9e\u5ea6\u548c\u5b57\u5e55\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u76f8\u6bd4\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\u7684\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u51cf\u5c11\u9ad8\u8fbe2\u500d\u3002", "conclusion": "ACG\u4e3a\u5e7b\u89c9\u7f13\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u4e2d\u7684\u6ce8\u610f\u529b\u7a7a\u95f4\u5bf9\u6bd4\u6307\u5bfc\uff0c\u6709\u6548\u5e73\u8861\u89c6\u89c9\u5fe0\u5b9e\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.13633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13633", "abs": "https://arxiv.org/abs/2601.13633", "authors": ["Guanqi Zhan", "Changye Li", "Zhijian Liu", "Yao Lu", "Yi Wu", "Song Han", "Ligeng Zhu"], "title": "Scaling Test-time Inference for Visual Grounding", "comment": null, "summary": "Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.", "AI": {"tldr": "EGM\u65b9\u6cd5\u901a\u8fc7\u6269\u5c55\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u751f\u6210\u66f4\u591atoken\uff09\uff0c\u5728\u4fdd\u6301\u90e8\u7f72\u53cb\u597d\u6027\u7684\u540c\u65f6\uff0c\u5c06\u89c6\u89c9\u5b9a\u4f4d\u6027\u80fd\u63d0\u5347\u5230\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u6a21\u578b\u901a\u5e38\u6a21\u578b\u89c4\u6a21\u8f83\u5927\uff0c\u90e8\u7f72\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u901f\u5ea6\u6162\u3002\u7814\u7a76\u53d1\u73b0\u5c0f\u578b\u548c\u5927\u578bVLMs\u7684\u4e3b\u8981\u5dee\u5f02\u5728\u4e8e\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u800c\u975e\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u843d\u540e\u4e3b\u8981\u662f\u56e0\u4e3a\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEGM\uff08\u9ad8\u6548\u89c6\u89c9\u5b9a\u4f4d\u8bed\u8a00\u6a21\u578b\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5c0f\u578b\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u589e\u52a0\u751f\u6210token\u6570\u91cf\uff09\u6765\u5f25\u8865\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u5dee\u8ddd\u3002\u8fd9\u79cd\u65b9\u6cd5\u90e8\u7f72\u53cb\u597d\uff0c\u56e0\u4e3a\u6bcf\u4e2atoken\u7684\u8ba1\u7b97\u6210\u672c\u8fdc\u4f4e\u4e8e\u76f4\u63a5\u8fd0\u884c\u5927\u578b\u6a21\u578b\u3002", "result": "\u5728RefCOCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEGM-Qwen3-VL-8B\u8fbe\u523091.4 IoU\uff0c\u5e73\u5747\u5ef6\u8fdf737ms\uff08\u6bd4Qwen3-VL-235B\u5feb5.9\u500d\uff09\uff0c\u800c\u540e\u8005\u9700\u89814320ms\u8fbe\u523090.5 IoU\u3002\u5728\u65b0\u5efa\u7acb\u7684\u65e0\u6a21\u6001\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u4e00\u81f4\u4e14\u663e\u8457\u63d0\u5347\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "EGM\u65b9\u6cd5\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\uff0c\u80fd\u591f\u5c06\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4f4d\u6027\u80fd\u63d0\u5347\u5230\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u90e8\u7f72\u53cb\u597d\u6027\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u5b9a\u4f4d\u7684\u6548\u7387\u3002"}}
{"id": "2601.13651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13651", "abs": "https://arxiv.org/abs/2601.13651", "authors": ["Marta Moscati", "Oleksandr Kats", "Mubashir Noman", "Muhammad Zaigham Zaheer", "Yufang Hou", "Markus Schedl", "Shah Nawaz"], "title": "Face-Voice Association with Inductive Bias for Maximum Class Separation", "comment": "Accepted at ICASSP 2026", "summary": "Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9762\u90e8-\u8bed\u97f3\u5173\u8054\u4efb\u52a1\u4e2d\u5e94\u7528\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u4e0d\u540c\u8bf4\u8bdd\u4eba\u4e4b\u95f4\u7684\u8868\u793a\u5206\u79bb\u5ea6\uff0c\u5728\u4e24\u79cd\u4efb\u52a1\u5236\u5b9a\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u9762\u90e8-\u8bed\u97f3\u5173\u8054\u7814\u7a76\u901a\u5e38\u901a\u8fc7\u635f\u5931\u51fd\u6570\u4f7f\u540c\u4e00\u4eba\u7684\u9762\u90e8\u548c\u8bed\u97f3\u5d4c\u5165\u63a5\u8fd1\u800c\u4e0e\u5176\u4ed6\u4eba\u7684\u5206\u79bb\u3002\u5206\u7c7b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u901a\u8fc7\u65bd\u52a0\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u53ef\u4ee5\u589e\u5f3a\u5d4c\u5165\u7684\u5224\u522b\u80fd\u529b\uff0c\u4f46\u8fd9\u4e00\u6280\u672f\u5c1a\u672a\u5728\u9762\u90e8-\u8bed\u97f3\u5173\u8054\u9886\u57df\u5e94\u7528\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9762\u90e8-\u8bed\u97f3\u5173\u8054\u65b9\u6cd5\uff0c\u5c06\u4e0d\u540c\u8bf4\u8bdd\u4eba\u7684\u591a\u6a21\u6001\u8868\u793a\u4e4b\u95f4\u7684\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7c7b\u95f4\u6b63\u4ea4\u6027\u635f\u5931\uff0c\u901a\u8fc7\u589e\u5f3a\u4e0d\u540c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e4b\u95f4\u7684\u5206\u79bb\u5ea6\u6765\u63d0\u5347\u5224\u522b\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u9762\u90e8-\u8bed\u97f3\u5173\u8054\u4efb\u52a1\u5236\u5b9a\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5f52\u7eb3\u504f\u7f6e\u4e0e\u7c7b\u95f4\u6b63\u4ea4\u6027\u635f\u5931\u7ed3\u5408\u4f7f\u7528\u65f6\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5e94\u7528\u5e76\u8bc1\u660e\u6700\u5927\u7c7b\u522b\u5206\u79bb\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u6709\u6548\u6027\u7684\u5de5\u4f5c\uff0c\u4e3a\u5efa\u7acb\u65b0\u7684\u8303\u5f0f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.13719", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.13719", "abs": "https://arxiv.org/abs/2601.13719", "authors": ["Xinlei Yin", "Xiulian Peng", "Xiao Li", "Zhiwei Xiong", "Yan Lu"], "title": "Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search", "comment": null, "summary": "Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.", "AI": {"tldr": "HAVEN\u6846\u67b6\u901a\u8fc7\u89c6\u542c\u5b9e\u4f53\u51dd\u805a\u548c\u5206\u5c42\u89c6\u9891\u7d22\u5f15\u7ed3\u5408\u667a\u80fd\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u4fe1\u606f\u788e\u7247\u5316\u548c\u5168\u5c40\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5728LVBench\u4e0a\u8fbe\u523084.1%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5206\u5757\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5b58\u5728\u4fe1\u606f\u788e\u7247\u5316\u548c\u5168\u5c40\u8fde\u8d2f\u6027\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u5168\u9762\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6574\u5408\u89c6\u89c9\u548c\u542c\u89c9\u6d41\u7684\u5b9e\u4f53\u7ea7\u8868\u793a\u4ee5\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff1b2. \u5c06\u5185\u5bb9\u7ec4\u7ec7\u4e3a\u5168\u5c40\u6458\u8981\u3001\u573a\u666f\u3001\u7247\u6bb5\u548c\u5b9e\u4f53\u5c42\u6b21\u7ed3\u6784\uff1b3. \u91c7\u7528\u667a\u80fd\u641c\u7d22\u673a\u5236\u5728\u8fd9\u4e9b\u5c42\u6b21\u95f4\u8fdb\u884c\u52a8\u6001\u68c0\u7d22\u548c\u63a8\u7406\u3002", "result": "\u5728LVBench\u4e0a\u8fbe\u523084.1%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u7c7b\u522b\u4e2d\u8fbe\u523080.1%\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3001\u5b9e\u4f53\u4e00\u81f4\u6027\u548c\u68c0\u7d22\u6548\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u6a21\u6001\u63a8\u7406\u80fd\u6709\u6548\u5b9e\u73b0\u957f\u89c6\u9891\u7684\u5168\u9762\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u7406\u89e3\uff0cHAVEN\u6846\u67b6\u4e3a\u6b64\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13664", "abs": "https://arxiv.org/abs/2601.13664", "authors": ["Tiancheng Fang", "Bowen Pan", "Lingxi Chen", "Jiangjing Lyu", "Chengfei Lyu", "Chaoyue Niu", "Fan Wu"], "title": "VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement", "comment": "Under review at CVPR 2026", "summary": "We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.", "AI": {"tldr": "VIAFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u89c6\u89d2\u6761\u4ef6\u4f53\u7d20\u4fee\u590d\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u50cf\u7d22\u5f15\u3001\u4fee\u6b63\u6d41\u76ee\u6807\u548c\u6df7\u5408\u6d41Transformer\u5b9e\u73b0\u4f53\u7d20\u4e0e\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u5728\u4fee\u590d\u5408\u6210\u548c\u771f\u5b9e\u4f53\u7d20\u7f3a\u9677\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u4f53\u7d20\u751f\u6210\u65b9\u6cd5\u5e38\u4ea7\u751f\u4e0d\u5b8c\u6574\u6216\u6709\u566a\u58f0\u7684\u4f53\u7d20\uff0c\u9700\u8981\u5229\u7528\u591a\u89c6\u89d2\u56fe\u50cf\u4f5c\u4e3a\u6307\u5bfc\u6765\u4fee\u590d\u8fd9\u4e9b\u7f3a\u9677\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f53\u7d20\u4e0e\u56fe\u50cf\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u878d\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1) \u56fe\u50cf\u7d22\u5f15\uff1a\u4e3a2D\u56fe\u50cftoken\u63d0\u4f9b\u660e\u786e\u76843D\u7a7a\u95f4\u5b9a\u4f4d\uff1b2) \u4fee\u6b63\u6d41\u76ee\u6807\uff1a\u5b66\u4e60\u76f4\u63a5\u7684\u4f53\u7d20\u4fee\u590d\u8f68\u8ff9\uff1b3) \u6df7\u5408\u6d41Transformer\uff1a\u5b9e\u73b0\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u878d\u5408\u3002", "result": "\u5728\u4fee\u590d\u4e25\u91cd\u5408\u6210\u635f\u574f\u548c\u4ece\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u83b7\u5f97\u7684\u771f\u5b9e\u4f53\u7d20\u4f2a\u5f71\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u5e76\u5728\u5b9e\u96453D\u521b\u5efa\u6d41\u7a0b\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "VIAFormer\u4e3a\u4f53\u7d20\u4e0e\u591a\u89c6\u89d2\u56fe\u50cf\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u662f\u5b9e\u96453D\u521b\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5b9e\u7528\u6865\u6881\uff0c\u63a8\u52a8\u4e86\u4f53\u7d20\u65b9\u6cd5\u5728\u5927\u6a21\u578b\u5927\u6570\u636e\u6d6a\u6f6e\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.13665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13665", "abs": "https://arxiv.org/abs/2601.13665", "authors": ["Mounika Kanulla", "Rajasree Dadigi", "Sailaja Thota", "Vivek Yelleti"], "title": "Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting", "comment": null, "summary": "Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408CNN+LSTM\u548cCNN+DeiT Transformer\u7684\u67b6\u6784\uff0c\u540c\u65f6\u5904\u7406\u852c\u83dc\u5206\u7c7b\u3001\u98df\u7269\u8150\u8d25\u68c0\u6d4b\u548c\u4fdd\u8d28\u671f\u9884\u6d4b\u4e09\u4e2a\u4efb\u52a1\uff0c\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u98df\u7269\u6d6a\u8d39\u662f\u519c\u4e1a\u4f9b\u5e94\u94fe\u7684\u5173\u952e\u6311\u6218\uff0c\u51c6\u786e\u6709\u6548\u7684\u8150\u8d25\u68c0\u6d4b\u548c\u9884\u6d4b\u6709\u52a9\u4e8e\u51cf\u5c11\u6d6a\u8d39\uff0c\u5ef6\u957f\u4f9b\u5e94\u94fe\u7ba1\u7406\u5bff\u547d\u3002", "method": "\u63d0\u51fa\u878d\u5408\u67b6\u6784\uff1aCNN+CNN-LSTM\u548cCNN+DeiT Transformer\uff0c\u901a\u8fc7\u4ece\u65b0\u9c9c\u5230\u5b8c\u5168\u8150\u8d25\u72b6\u6001\u91c7\u96c6\u852c\u83dc\u56fe\u50cf\u6784\u5efa\u6570\u636e\u96c6\uff0c\u540c\u65f6\u5904\u7406\u4e09\u4e2a\u4efb\u52a1\u3002", "result": "CNN+DeiT Transformer\u5728\u852c\u83dc\u5206\u7c7b\u548c\u8150\u8d25\u68c0\u6d4b\u7684F1\u5206\u6570\u5206\u522b\u4e3a0.98\u548c0.61\uff0c\u8150\u8d25\u9884\u6d4b\u7684MSE\u548cSMAPE\u5206\u522b\u4e3a3.58\u548c41.66%\uff0c\u4f18\u4e8eCNN\u3001VGG16\u3001ResNet50\u3001\u80f6\u56ca\u7f51\u7edc\u548cDeiT Transformer\u7b49\u6a21\u578b\u3002", "conclusion": "\u878d\u5408\u67b6\u6784\u5728\u852c\u83dc\u8150\u8d25\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u566a\u58f0\u56fe\u50cf\u9a8c\u8bc1\u4e86\u6a21\u578b\u53ef\u9760\u6027\uff0c\u5e76\u96c6\u6210LIME\u53ef\u89c6\u5316\u6a21\u578b\u51b3\u7b56\u3002"}}
{"id": "2601.13677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13677", "abs": "https://arxiv.org/abs/2601.13677", "authors": ["Carsten T. L\u00fcth", "Jeremias Traub", "Kim-Celine Kahl", "Till J. Bungert", "Lukas Klein", "Lars Kr\u00e4mer", "Paul F. J\u00e4ger", "Klaus Maier-Hein", "Fabian Isensee"], "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging", "comment": "Accepted at TMLR", "summary": "Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.", "AI": {"tldr": "\u63d0\u51faClaSP PE\u65b9\u6cd5\u89e3\u51b33D\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u4e3b\u52a8\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7c7b\u522b\u5206\u5c42\u67e5\u8be2\u548c\u566a\u58f0\u8c03\u5ea6\u7b56\u7565\uff0c\u572824\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u968f\u673a\u57fa\u7ebf\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u6570\u636e\u96c6\u3002", "motivation": "3D\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u57283D\u6570\u636e\u4e0a\u7a33\u5b9a\u8d85\u8d8a\u6539\u8fdb\u7684\u968f\u673a\u91c7\u6837\u57fa\u7ebf\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faClass-stratified Scheduled Power Predictive Entropy (ClaSP PE)\u65b9\u6cd5\uff1a1) \u7c7b\u522b\u5206\u5c42\u67e5\u8be2\u786e\u4fdd\u5bf9\u4ee3\u8868\u6027\u4e0d\u8db3\u7ed3\u6784\u7684\u8986\u76d6\uff1b2) \u5bf9\u6570\u5c3a\u5ea6\u529f\u7387\u566a\u58f0\u914d\u5408\u8870\u51cf\u8c03\u5ea6\uff0c\u5728\u65e9\u671fAL\u9636\u6bb5\u5f3a\u5236\u67e5\u8be2\u591a\u6837\u6027\uff0c\u540e\u671f\u9f13\u52b1\u5229\u7528\u3002", "result": "\u5728nnActive\u57fa\u51c6\u6d4b\u8bd5\u768424\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\uff0cClaSP PE\u662f\u552f\u4e00\u80fd\u7a33\u5b9a\u8d85\u8d8a\u6539\u8fdb\u968f\u673a\u57fa\u7ebf\u7684\u65b9\u6cd5\uff0c\u5728\u5206\u5272\u8d28\u91cf\u548c\u6807\u6ce8\u6548\u7387\u65b9\u9762\u5747\u6709\u7edf\u8ba1\u663e\u8457\u63d0\u5347\u3002\u5728\u56db\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u65e0\u9700\u624b\u52a8\u8c03\u6574\u4e5f\u80fd\u7a33\u5065\u6cdb\u5316\u3002", "conclusion": "ClaSP PE\u8bc1\u660e\u4e86\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u57283D\u5206\u5272\u4efb\u52a1\u4e2d\u7a33\u5b9a\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\uff0c\u5728\u63a5\u8fd1\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u540c\u65f6\u63d0\u5347\u6027\u80fd\u548c\u6807\u6ce8\u6548\u7387\u3002\u5f00\u6e90\u5b9e\u73b0\u548c\u90e8\u7f72\u6307\u5357\u4f7f\u5176\u6613\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2601.13683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13683", "abs": "https://arxiv.org/abs/2601.13683", "authors": ["Boyuan Cao", "Xingbo Yao", "Chenhui Wang", "Jiaxin Ye", "Yujie Wei", "Hongming Shan"], "title": "Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation", "comment": null, "summary": "Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.", "AI": {"tldr": "DyDiLA\u662f\u4e00\u79cd\u65b0\u578b\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u6295\u5f71\u3001\u52a8\u6001\u6d4b\u91cf\u6838\u548c\u4ee4\u724c\u5dee\u5206\u7b97\u5b50\u89e3\u51b3\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u6210\u4e3a\u4e3b\u8981\u6269\u5c55\u74f6\u9888\u3002\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u4f1a\u5bfc\u81f4\u751f\u6210\u6027\u80fd\u4e0b\u964d\uff0c\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5dee\u5206\u7ebf\u6027\u6ce8\u610f\u529b\uff08DyDiLA\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1\uff09\u52a8\u6001\u6295\u5f71\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u77e5\u8bc6\u5b66\u4e60\u5b9e\u73b0\u4ee4\u724c\u8868\u793a\u89e3\u8026\uff1b2\uff09\u52a8\u6001\u6d4b\u91cf\u6838\uff0c\u901a\u8fc7\u4e3a\u4ee4\u724c\u5904\u7406\u52a8\u6001\u5206\u914d\u6838\u51fd\u6570\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\uff1b3\uff09\u4ee4\u724c\u5dee\u5206\u7b97\u5b50\uff0c\u901a\u8fc7\u8ba1\u7b97\u4ee4\u724c\u4e0e\u5176\u52a8\u6001\u6d4b\u91cf\u6838\u4ea7\u751f\u7684\u4fe1\u606f\u5197\u4f59\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u67e5\u8be2-\u952e\u68c0\u7d22\u3002\u57fa\u4e8eDyDiLA\u6784\u5efa\u4e86\u6539\u8fdb\u7684\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668DyDi-LiT\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDyDi-LiT\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u5b9e\u9645\u6f5c\u529b\u3002", "conclusion": "DyDiLA\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u6001\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u6269\u6563\u53d8\u6362\u5668\u7684\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13798", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13798", "abs": "https://arxiv.org/abs/2601.13798", "authors": ["Kai Wittenmayer", "Sukrut Rao", "Amin Parchami-Araghi", "Bernt Schiele", "Jonas Fischer"], "title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders", "comment": "32 pages, 24 figures, 3 tables", "summary": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.", "AI": {"tldr": "Insight\u662f\u4e00\u4e2a\u8bed\u8a00\u5bf9\u9f50\u7684\u6982\u5ff5\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668\u81ea\u52a8\u63d0\u53d6\u591a\u5c42\u6b21\u3001\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\uff0c\u63d0\u4f9b\u7a7a\u95f4\u5b9a\u4f4d\u7684\u89e3\u91ca\uff0c\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1\u4e0d\u900f\u660e\u7684\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u5bf9\u9f50\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u867d\u7136\u5728\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u89e3\u91ca\u51b3\u7b56\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u5c1d\u8bd5\u5c06\u8868\u793a\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u4e14\u4ec5\u9650\u4e8e\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u5177\u6709\u5f3a\u8bed\u4e49\u8868\u793a\u7684\u57fa\u7840\u6a21\u578b\uff0c\u81ea\u52a8\u63d0\u53d6\u4e0d\u540c\u7c92\u5ea6\u7684\u6982\u5ff5\u3002\u901a\u8fc7\u5206\u6790\u6982\u5ff5\u95f4\u7684\u5c40\u90e8\u5171\u73b0\u4f9d\u8d56\u5173\u7cfb\u6765\u5b9a\u4e49\u6982\u5ff5\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u6539\u8fdb\u6982\u5ff5\u547d\u540d\u548c\u83b7\u5f97\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u4e0a\uff0cInsight\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e\u4e0d\u900f\u660e\u7684\u57fa\u7840\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u9ad8\u8d28\u91cf\u7684\u6982\u5ff5\u89e3\u91ca\u3002", "conclusion": "Insight\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u8bed\u8a00\u5bf9\u9f50\u7684\u6982\u5ff5\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u63d0\u4f9b\u7a7a\u95f4\u5b9a\u4f4d\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.13705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13705", "abs": "https://arxiv.org/abs/2601.13705", "authors": ["Maria Lymperaiou", "Vasileios Karampinis", "Giorgos Filandrianos", "Angelos Vlachos", "Chrysoula Zerva", "Athanasios Voulodimos"], "title": "Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles", "comment": null, "summary": "Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u89c6\u89c9\u8c1c\u9898\u4f5c\u4e3a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8bca\u65ad\u5de5\u5177\u7684\u8c03\u67e5\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u89c9\u8c1c\u9898\u957f\u671f\u4ee5\u6765\u4f5c\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u7684\u7d27\u51d1\u63a2\u6d4b\u5de5\u5177\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5148\u9a8c\u77e5\u8bc6\u4f9d\u8d56\u6765\u6d4b\u8bd5\u62bd\u8c61\u3001\u89c4\u5219\u53d1\u73b0\u548c\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\u3002\u6700\u8fd1\uff0c\u89c6\u89c9\u8c1c\u9898\u5df2\u6210\u4e3a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5927\u8bca\u65ad\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u53ef\u9a8c\u8bc1\u7684\u66ff\u4ee3\u65b9\u6848\u6765\u66ff\u4ee3\u5f00\u653e\u5f0f\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u7684\u62bd\u8c61\u6846\u67b6\u6765\u7406\u89e3\u89c6\u89c9\u8c1c\u9898\u63a8\u7406\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6309\u76ee\u6807\u63a8\u7406\u673a\u5236\uff08\u5f52\u7eb3\u3001\u7c7b\u6bd4\u3001\u7b97\u6cd5\u3001\u6f14\u7ece\u548c\u51e0\u4f55/\u7a7a\u95f4\uff09\u8fdb\u884c\u7ec4\u7ec7\uff0c\u4ece\u800c\u5c06\u8c1c\u9898\u8bbe\u8ba1\u4e0e\u89e3\u51b3\u6240\u9700\u7684\u8ba4\u77e5\u64cd\u4f5c\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u7efc\u5408\u8fd9\u4e9b\u7c7b\u522b\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u8bc6\u522b\u51fa\u5f53\u524d\u6a21\u578b\u7684\u4e00\u81f4\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8106\u5f31\u7684\u6cdb\u5316\u80fd\u529b\u3001\u611f\u77e5\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u7d27\u5bc6\u7ea0\u7f20\uff0c\u4ee5\u53ca\u6d41\u7545\u89e3\u91ca\u4e0e\u5fe0\u5b9e\u6267\u884c\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8c1c\u9898\u89c6\u4e3a\u8bca\u65ad\u5de5\u5177\u800c\u975e\u4efb\u52a1\u683c\u5f0f\uff0c\u8fd9\u7bc7\u8c03\u67e5\u8be6\u7ec6\u9610\u8ff0\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u73b0\u72b6\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a8\u7406\u611f\u77e5\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u6307\u660e\u4e86\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2601.13706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13706", "abs": "https://arxiv.org/abs/2601.13706", "authors": ["Xinhao Liu", "Yu Wang", "Xiansheng Guo", "Gordon Owusu Boateng", "Yu Cao", "Haonan Si", "Xingchen Guo", "Nirwan Ansari"], "title": "ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins", "comment": "35 pages, 10 figures. Submitted to ISPRS Journal of Photogrammetry and Remote Sensing. Under review", "summary": "High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/", "AI": {"tldr": "ParkingTwin\uff1a\u4e00\u79cd\u514d\u8bad\u7ec3\u3001\u8f7b\u91cf\u7ea7\u7684\u5728\u7ebf\u6d41\u5f0f3D\u91cd\u5efa\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u505c\u8f66\u573a\u6570\u5b57\u5b6a\u751f\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5bfc\u5411\u91cd\u5efa\u4e2d\u7684\u4e09\u96be\u95ee\u9898", "motivation": "\u505c\u8f66\u573a\u6570\u5b57\u5b6a\u751f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4ee3\u5ba2\u6cca\u8f66\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e09\u96be\u95ee\u9898\uff1a\u7a00\u758f\u524d\u5411\u89c6\u89d2\u5bfc\u81f4\u5f31\u89c6\u5dee\u548c\u51e0\u4f55\u4e0d\u9002\u5b9a\uff1b\u52a8\u6001\u906e\u6321\u548c\u6781\u7aef\u5149\u7167\u963b\u788d\u7a33\u5b9a\u7eb9\u7406\u878d\u5408\uff1b\u795e\u7ecf\u6e32\u67d3\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u4f18\u5316\uff0c\u8fdd\u53cd\u8fb9\u7f18\u7aef\u6d41\u5f0f\u7ea6\u675f", "method": "1) OSM\u5148\u9a8c\u9a71\u52a8\u7684\u51e0\u4f55\u6784\u5efa\uff1a\u5229\u7528OpenStreetMap\u8bed\u4e49\u62d3\u6251\u76f4\u63a5\u751f\u6210\u5ea6\u91cf\u4e00\u81f4\u7684TSDF\uff1b2) \u51e0\u4f55\u611f\u77e5\u7684\u52a8\u6001\u8fc7\u6ee4\uff1a\u91c7\u7528\u56db\u6a21\u6001\u7ea6\u675f\u573a\uff08\u6cd5\u7ebf/\u9ad8\u5ea6/\u6df1\u5ea6\u4e00\u81f4\u6027\uff09\u5b9e\u65f6\u5254\u9664\u79fb\u52a8\u8f66\u8f86\u548c\u77ac\u6001\u906e\u6321\uff1b3) CIELAB\u4e2d\u7684\u5149\u7167\u9c81\u68d2\u878d\u5408\uff1a\u901a\u8fc7\u81ea\u9002\u5e94L\u901a\u9053\u52a0\u6743\u548c\u6df1\u5ea6\u68af\u5ea6\u6291\u5236\u89e3\u8026\u4eae\u5ea6\u548c\u8272\u5ea6", "result": "\u5728\u5165\u95e8\u7ea7GTX 1660\u4e0a\u8fbe\u523030+ FPS\uff1b\u572868,000 m\u00b2\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SSIM 0.87\uff08\u63d0\u534716.0%\uff09\uff1b\u76f8\u6bd4\u6700\u5148\u8fdb\u76843D\u9ad8\u65af\u6cfc\u6e85\uff08\u9700\u8981\u9ad8\u7aefRTX 4090D\uff09\u5b9e\u73b0\u7ea615\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0cGPU\u5185\u5b58\u51cf\u5c1183.3%", "conclusion": "ParkingTwin\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7ebf\u751f\u6210\u4e0eUnity/Unreal\u6570\u5b57\u5b6a\u751f\u7ba1\u9053\u517c\u5bb9\u7684\u663e\u5f0f\u4e09\u89d2\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u505c\u8f66\u573a\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2601.13895", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13895", "abs": "https://arxiv.org/abs/2601.13895", "authors": ["Xu Zhang", "Danyang Li", "Yingjie Xia", "Xiaohang Dong", "Hualong Yu", "Jianye Wang", "Qicheng Li"], "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3", "comment": null, "summary": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.", "AI": {"tldr": "\u63d0\u51faOmniOVCD\u6846\u67b6\uff0c\u5229\u7528SAM 3\u7684\u89e3\u8026\u8f93\u51fa\u5934\uff0c\u901a\u8fc7SFID\u7b56\u7565\u878d\u5408\u8bed\u4e49\u3001\u5b9e\u4f8b\u548c\u5b58\u5728\u6027\u8f93\u51fa\u6784\u5efa\u571f\u5730\u8986\u76d6\u63a9\u7801\uff0c\u518d\u5206\u89e3\u4e3a\u5b9e\u4f8b\u63a9\u7801\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u514d\u8bad\u7ec3\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u591a\u4f7f\u7528CLIP\u8bc6\u522b\u7c7b\u522b\uff0c\u5e76\u9700\u8981\u989d\u5916\u6a21\u578b\u5982DINO\u63d0\u53d6\u7279\u5f81\uff0c\u4e0d\u540c\u6a21\u578b\u7ec4\u5408\u5e38\u5bfc\u81f4\u7279\u5f81\u5339\u914d\u95ee\u9898\u548c\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u3002SAM 3\u5c06\u5206\u5272\u548c\u8bc6\u522b\u80fd\u529b\u96c6\u6210\u5728\u4e00\u4e2a\u53ef\u63d0\u793a\u6a21\u578b\u4e2d\uff0c\u4e3aOVCD\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u63d0\u51faOmniOVCD\u6846\u67b6\uff0c\u5229\u7528SAM 3\u7684\u89e3\u8026\u8f93\u51fa\u5934\uff0c\u8bbe\u8ba1SFID\u7b56\u7565\uff1a\u9996\u5148\u878d\u5408SAM 3\u7684\u8bed\u4e49\u3001\u5b9e\u4f8b\u548c\u5b58\u5728\u6027\u8f93\u51fa\u6765\u6784\u5efa\u571f\u5730\u8986\u76d6\u63a9\u7801\uff0c\u7136\u540e\u5c06\u5176\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u5b9e\u4f8b\u63a9\u7801\u8fdb\u884c\u53d8\u5316\u6bd4\u8f83\uff0c\u4fdd\u6301\u7c7b\u522b\u8bc6\u522b\u9ad8\u7cbe\u5ea6\u548c\u8de8\u56fe\u50cf\u5b9e\u4f8b\u7ea7\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\uff08LEVIR-CD\u3001WHU-CD\u3001S2Looking\u548cSECOND\uff09\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0cIoU\u5206\u6570\u5206\u522b\u4e3a67.2\u300166.5\u300124.5\u548c27.1\uff08\u7c7b\u522b\u5e73\u5747\uff09\uff0c\u8d85\u8d8a\u6240\u6709\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "OmniOVCD\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528SAM 3\u7684\u89e3\u8026\u8f93\u51fa\u5934\u548cSFID\u7b56\u7565\uff0c\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684\u53d8\u5316\u63a9\u7801\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13942", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13942", "abs": "https://arxiv.org/abs/2601.13942", "authors": ["Hongbo Bai", "Yujin Zhou", "Yile Wu", "Chi-Min Chan", "Pengcheng Wen", "Kunhao Pan", "Sirui Han", "Yike Guo"], "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning", "comment": null, "summary": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.", "AI": {"tldr": "GoG\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u89c6\u89c9\u89c4\u5212\u89e3\u51b3LMM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f15\u5165\u9009\u62e9\u6027\u51dd\u89c6\u673a\u5236\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u957f\u5c3e\u5b9e\u4f53\u6216\u52a8\u6001\u4fe1\u606f\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u65f6\uff0c\u7531\u4e8e\u9759\u6001\u53c2\u6570\u77e5\u8bc6\u7684\u9650\u5236\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684\u641c\u7d22\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u5197\u4f59\u548c\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u8fed\u4ee3\u53cd\u601d\u3002", "method": "\u63d0\u51faGlance-or-Gaze\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u6027\u51dd\u89c6\u673a\u5236\uff08\u52a8\u6001\u9009\u62e9\u5168\u5c40\u4e0a\u4e0b\u6587\u6216\u9ad8\u4ef7\u503c\u533a\u57df\uff09\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fdb\u884c\u53cd\u601d\u6027GoG\u884c\u4e3a\u5bf9\u9f50\uff0c\u4ee5\u53ca\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6765\u589e\u5f3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u7684\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u9009\u62e9\u6027\u51dd\u89c6\u673a\u5236\u548c\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u5bf9\u6709\u6548\u89c6\u89c9\u641c\u7d22\u90fd\u662f\u5fc5\u9700\u7684\u3002", "conclusion": "GoG\u6846\u67b6\u901a\u8fc7\u4ece\u88ab\u52a8\u611f\u77e5\u8f6c\u5411\u4e3b\u52a8\u89c6\u89c9\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86LMM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u89c6\u89c9\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13715", "abs": "https://arxiv.org/abs/2601.13715", "authors": ["Yiwei Lu", "Hao Huang", "Tao Yan"], "title": "MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network", "comment": "This paper has been accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI-26). It contians 9 pages, 11 figures", "summary": "Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.", "AI": {"tldr": "MVGD-Net\uff1a\u5229\u7528\u8fd0\u52a8\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u89c6\u9891\u4e2d\u73bb\u7483\u8868\u9762\u7684\u65b0\u7f51\u7edc\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u591a\u6a21\u6001\u878d\u5408\u548c\u5386\u53f2\u5f15\u5bfc\u6ce8\u610f\u529b\u7b49\u6a21\u5757\uff0c\u5728\u81ea\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73bb\u7483\u8868\u9762\u5728\u65e5\u5e38\u751f\u6d3b\u548c\u4e13\u4e1a\u73af\u5883\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5bf9\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u7b49\u89c6\u89c9\u7cfb\u7edf\u6784\u6210\u6f5c\u5728\u5a01\u80c1\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u73bb\u7483\u8868\u9762\u68c0\u6d4b\uff08VGSD\uff09\uff0c\u4f46\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMVGD-Net\u7f51\u7edc\uff0c\u57fa\u4e8e\u73bb\u7483\u8868\u9762\u53cd\u5c04/\u900f\u5c04\u5c42\u7269\u4f53\u8fd0\u52a8\u8f83\u6162\u7684\u89c2\u5bdf\uff0c\u5229\u7528\u8fd0\u52a8\u4e0d\u4e00\u81f4\u6027\u7ebf\u7d22\u3002\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u8de8\u5c3a\u5ea6\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff08CMFM\uff09\u6574\u5408\u7a7a\u95f4\u7279\u5f81\u548c\u5149\u6d41\u56fe\uff0c\u5386\u53f2\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff08HGAM\uff09\u548c\u65f6\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff08TCAM\uff09\u589e\u5f3a\u65f6\u5e8f\u7279\u5f81\uff0c\u4ee5\u53ca\u65f6\u7a7a\u89e3\u7801\u5668\uff08TSD\uff09\u878d\u5408\u7279\u5f81\u751f\u6210\u73bb\u7483\u533a\u57df\u63a9\u7801\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b312\u4e2a\u591a\u6837\u5316\u73bb\u7483\u573a\u666f\u3001\u517119,268\u5e27\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660eMVGD-Net\u5728\u89c6\u9891\u73bb\u7483\u8868\u9762\u68c0\u6d4b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u76f8\u5173\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MVGD-Net\u901a\u8fc7\u6709\u6548\u5229\u7528\u8fd0\u52a8\u4e0d\u4e00\u81f4\u6027\u7ebf\u7d22\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u8de8\u5c3a\u5ea6\u591a\u6a21\u6001\u878d\u5408\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u89c6\u9891\u73bb\u7483\u8868\u9762\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u89c6\u89c9\u7cfb\u7edf\u9762\u4e34\u7684\u73bb\u7483\u8868\u9762\u5a01\u80c1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.14039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14039", "abs": "https://arxiv.org/abs/2601.14039", "authors": ["Wesam Moustafa", "Hossam Elsafty", "Helen Schneider", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation", "comment": null, "summary": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u901a\u7528\u5f03\u6743\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5ffd\u7565\u566a\u58f0\u6837\u672c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728CaDIS\u548cDSAD\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u4e25\u91cd\uff0c\u624b\u52a8\u6807\u6ce8\u56f0\u96be\u5bfc\u81f4\u566a\u58f0\u6807\u7b7e\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u7814\u7a76\u4e0d\u8db3\u3002\u5f03\u6743\u673a\u5236\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5728\u5206\u5272\u9886\u57df\u6f5c\u529b\u672a\u7ecf\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6a21\u5757\u5316\u5f03\u6743\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6307\u5bfc\u5f03\u6743\u884c\u4e3a\u7684\u77e5\u60c5\u6b63\u5219\u5316\u9879\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5e42\u5f8b\u7684\u81ea\u9002\u5e94\u8c03\u6574\u5f03\u6743\u60e9\u7f5a\u7b97\u6cd5\u3002\u6846\u67b6\u4e0e\u4e09\u79cd\u4e0d\u540c\u635f\u5931\u51fd\u6570\u96c6\u6210\uff0c\u521b\u5efa\u4e86GAC\u3001SAC\u548cADS\u4e09\u79cd\u566a\u58f0\u9c81\u68d2\u53d8\u4f53\u3002", "result": "\u5728CaDIS\u548cDSAD\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u975e\u5f03\u6743\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u4f7f\u6a21\u578b\u80fd\u591f\u9009\u62e9\u6027\u5ffd\u7565\u635f\u574f\u6837\u672c\u662f\u6784\u5efa\u66f4\u53ef\u9760\u5206\u5272\u6a21\u578b\u7684\u5f3a\u5927\u4e14\u53ef\u63a8\u5e7f\u7b56\u7565\uff0c\u5f03\u6743\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5206\u5272\u4efb\u52a1\u7684\u566a\u58f0\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13724", "abs": "https://arxiv.org/abs/2601.13724", "authors": ["Sam Cantrill", "David Ahmedt-Aristizabal", "Lars Petersson", "Hanna Suominen", "Mohammad Ali Armin"], "title": "Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement", "comment": null, "summary": "Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .", "AI": {"tldr": "\u63d0\u51faFacial Spatiotemporal Graph (STGraph)\u8868\u793a\u6cd5\u548cMeshPhys\u6a21\u578b\uff0c\u901a\u8fc73D\u9762\u90e8\u7f51\u683c\u5e8f\u5217\u8fdb\u884c\u8868\u9762\u5bf9\u9f50\u7684\u65f6\u7a7a\u5904\u7406\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0(rPPG)\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709rPPG\u65b9\u6cd5\u672a\u80fd\u5c06\u611f\u53d7\u91ce\u4e0e3D\u9762\u90e8\u8868\u9762\u5bf9\u9f50\uff0c\u800c\u9762\u90e8\u8868\u9762\u662frPPG\u4fe1\u53f7\u7684\u7a7a\u95f4\u652f\u6491\u533a\u57df\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faFacial STGraph\u8868\u793a\u6cd5\uff0c\u4f7f\u75283D\u9762\u90e8\u7f51\u683c\u5e8f\u5217\u7f16\u7801\u9762\u90e8\u989c\u8272\u548c\u7ed3\u6784\u4fe1\u606f\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u65f6\u7a7a\u56fe\u5377\u79ef\u7f51\u7edcMeshPhys\uff0c\u5728STGraph\u4e0a\u8fdb\u884c\u8868\u9762\u5bf9\u9f50\u7684\u65f6\u7a7a\u5904\u7406\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMeshPhys\u5728\u6570\u636e\u96c6\u5185\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u5c06\u611f\u53d7\u91ce\u7ea6\u675f\u5728\u9762\u90e8\u8868\u9762\u4f5c\u4e3a\u5f3a\u7ed3\u6784\u5148\u9a8c\uff0c\u8868\u9762\u5bf9\u9f50\u76843D\u611f\u77e5\u8282\u70b9\u7279\u5f81\u5bf9\u9c81\u68d2\u7f16\u7801\u9762\u90e8\u8868\u9762\u989c\u8272\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "STGraph\u548cMeshPhys\u6784\u6210\u4e86\u9762\u90e8rPPG\u7684\u65b0\u5efa\u6a21\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\uff0c\u4e3a\u8868\u9762\u5bf9\u9f50\u7684\u65f6\u7a7a\u5904\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2601.13751", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13751", "abs": "https://arxiv.org/abs/2601.13751", "authors": ["Daniel Kyselica", "Jon\u00e1\u0161 Herec", "Oliver Kutis", "Rado Pito\u0148\u00e1k"], "title": "HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection", "comment": "19 pages, 9 figures, submitted to conference", "summary": "Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHiT\u673a\u5236\uff0c\u7528\u4e8e\u536b\u661f\u4e0a\u7684\u6d2a\u6c34\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5386\u53f2\u6ce8\u5165\u51cf\u5c1199%\u6570\u636e\u5b58\u50a8\uff0c\u5728Jetson Orin Nano\u4e0a\u8fbe\u523043 FPS\uff0c\u652f\u6301\u5b9e\u65f6\u707e\u5bb3\u76d1\u6d4b\u3002", "motivation": "\u81ea\u7136\u707e\u5bb3\u76d1\u6d4b\u9700\u8981\u5904\u7406\u591a\u65f6\u76f8\u536b\u661f\u6570\u636e\uff0c\u4f46\u53d7\u9650\u4e8e\u5c0f\u578b\u536b\u661f\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u3002\u6d2a\u6c34\u68c0\u6d4b\u4f5c\u4e3a\u707e\u5bb3\u7ba1\u7406\u7684\u5173\u952e\u5e94\u7528\uff0c\u9700\u8981\u80fd\u5728\u661f\u4e0a\u5b9e\u65f6\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u5bf9\u5730\u9762\u5904\u7406\u8bbe\u65bd\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faHiT\uff08History Injection for Transformer\uff09\u673a\u5236\uff0c\u5728Transformer\u6a21\u578b\u4e2d\u7ef4\u62a4\u5386\u53f2\u89c2\u6d4b\u4e0a\u4e0b\u6587\uff0c\u5c06\u539f\u59cb\u56fe\u50cf\u6570\u636e\u5b58\u50a8\u51cf\u5c1199%\u4ee5\u4e0a\u3002\u57fa\u4e8ePrithvi-tiny\u57fa\u7840\u6a21\u578b\u6784\u5efaHiT-Prithvi\u6a21\u578b\uff0c\u5728STTORM-CD\u6d2a\u6c34\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "HiT\u673a\u5236\u5728\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u5b58\u50a8\u9700\u6c42\u3002HiT-Prithvi\u6a21\u578b\u5728Jetson Orin Nano\u4e0a\u8fbe\u523043 FPS\uff0c\u6ee1\u8db3\u5b9e\u65f6\u5904\u7406\u8981\u6c42\u3002\u4e0e\u53cc\u65f6\u76f8\u57fa\u7ebf\u76f8\u6bd4\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "HiT\u673a\u5236\u4e3a\u536b\u661f\u8fde\u7eed\u76d1\u6d4b\u81ea\u7136\u707e\u5bb3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u65f6\u707e\u5bb3\u8bc4\u4f30\uff0c\u51cf\u5c11\u5bf9\u5730\u9762\u5904\u7406\u8bbe\u65bd\u7684\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u5c0f\u578b\u536b\u661f\u7684\u661f\u4e0a\u5904\u7406\u9700\u6c42\u3002"}}
{"id": "2601.13797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13797", "abs": "https://arxiv.org/abs/2601.13797", "authors": ["Gabriele Serussi", "David Vainshtein", "Jonathan Kouchly", "Dotan Di Castro", "Chaim Baskin"], "title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval", "comment": null, "summary": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.", "AI": {"tldr": "PREGEN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5148\u8fdb\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8981\u4e48\u4f7f\u7528\u8fc7\u65f6\u67b6\u6784\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u5fae\u8c03\u548c\u7f13\u6162\u7684\u6807\u9898\u751f\u6210\u3002", "method": "PREGEN\u5c06\u67e5\u8be2\u89c6\u9891\u548c\u4fee\u6539\u6587\u672c\u8f93\u5165\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\uff0c\u63d0\u53d6\u6bcf\u5c42\u6700\u540e\u4e00\u4e2atoken\u7684\u9690\u85cf\u72b6\u6001\uff0c\u7136\u540e\u8bad\u7ec3\u7b80\u5355\u7f16\u7801\u5668\u5bf9\u8fd9\u4e9b\u6c60\u5316\u8868\u793a\u8fdb\u884c\u7f16\u7801\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u4e14\u7d27\u51d1\u7684\u68c0\u7d22\u5d4c\u5165\u3002", "result": "PREGEN\u663e\u8457\u63a8\u8fdb\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u6807\u51c6CoVR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u6709\u5148\u524d\u65b9\u6cd5\uff0cRecall@1\u5206\u522b\u63d0\u5347+27.23\u548c+69.59\uff0c\u5728\u4e0d\u540cVLM\u9aa8\u5e72\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5e76\u5bf9\u66f4\u590d\u6742\u7684\u6587\u672c\u4fee\u6539\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PREGEN\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3VLM\u7684\u8bed\u4e49\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4e14\u5f3a\u5927\u7684\u7ec4\u5408\u89c6\u9891\u68c0\u7d22\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u8bed\u4e49\u80fd\u529b\u3002"}}
{"id": "2601.14055", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14055", "abs": "https://arxiv.org/abs/2601.14055", "authors": ["Andrea Protani", "Marc Molina Van Den Bosch", "Lorenzo Giusti", "Heloisa Barbosa Da Silva", "Paolo Cacace", "Albert Sund Aillet", "Miguel Angel Gonzalez Ballester", "Friedhelm Hummel", "Luigi Serio"], "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI", "comment": "10 pages, 3 figures,", "summary": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.", "AI": {"tldr": "SVGFormer\uff1a\u4e00\u79cd\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u7684\u65e0\u89e3\u7801\u5668\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8bed\u4e49\u8d85\u4f53\u7d20\u56fe\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u5b66\u4e60\u548c\u53cc\u91cd\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf3D\u533b\u5b66\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u91c7\u7528\u53c2\u6570\u5bc6\u96c6\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u5927\u91cf\u53c2\u6570\u7528\u4e8e\u7a7a\u95f4\u91cd\u5efa\u800c\u975e\u7279\u5f81\u5b66\u4e60\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faSVGFormer\uff1a1\uff09\u5185\u5bb9\u611f\u77e5\u5206\u7ec4\u9636\u6bb5\u5c063D\u4f53\u7d20\u7f51\u683c\u5206\u5272\u6210\u8bed\u4e49\u8d85\u4f53\u7d20\u56fe\uff1b2\uff09\u5206\u5c42\u7f16\u7801\u5668\u7ed3\u5408\u8865\u4e01\u7ea7Transformer\u548c\u8d85\u4f53\u7d20\u7ea7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u8054\u5408\u5efa\u6a21\u7ec6\u7c92\u5ea6\u533a\u57df\u5185\u7279\u5f81\u548c\u533a\u57df\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728BraTS\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4e24\u4e2a\u4e13\u7528\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff1a\u8282\u70b9\u5206\u7c7b\u6a21\u578bF1\u5206\u65700.875\uff0c\u80bf\u7624\u6bd4\u4f8b\u56de\u5f52\u6a21\u578bMAE 0.028\uff0c\u8bc1\u660e\u4e86\u7f16\u7801\u5668\u5b66\u4e60\u5224\u522b\u6027\u548c\u5c40\u90e8\u5316\u7279\u5f81\u7684\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u4ec5\u7f16\u7801\u5668\u8303\u5f0f\u4e3a3D\u533b\u5b66\u56fe\u50cf\u8868\u793a\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u56fa\u6709\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u5168\u90e8\u53ef\u5b66\u4e60\u5bb9\u91cf\u96c6\u4e2d\u4e8e\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u63d0\u4f9b\u4ece\u8865\u4e01\u5230\u533a\u57df\u7ea7\u7684\u53cc\u91cd\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.14056", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14056", "abs": "https://arxiv.org/abs/2601.14056", "authors": ["Andrea Rigo", "Luca Stornaiuolo", "Weijie Wang", "Mauro Martino", "Bruno Lepri", "Nicu Sebe"], "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion", "comment": null, "summary": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.", "AI": {"tldr": "POCI-Diff\uff1a\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc73D\u5e03\u5c40\u63a7\u5236\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\u5b9e\u73b0\u4e00\u81f4\u3001\u4ea4\u4e92\u5f0f\u7684\u591a\u5bf9\u8c61\u573a\u666f\u751f\u6210\u4e0e\u7f16\u8f91", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u75282D\u7ebf\u7d22\u6216\u8fed\u4ee3\u590d\u5236-\u626d\u66f2-\u7c98\u8d34\u7b56\u7565\u6765\u6539\u5584\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4f46\u5e38\u5e38\u626d\u66f2\u5bf9\u8c61\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u4e14\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u4fdd\u6301\u4e00\u81f4\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5f3a\u5236\u6267\u884c3D\u51e0\u4f55\u7ea6\u675f\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPOCI-Diff\u6846\u67b6\uff1a1\uff09\u901a\u8fc7Blended Latent Diffusion\u5c06\u5355\u4e2a\u6587\u672c\u63cf\u8ff0\u7ed1\u5b9a\u5230\u7279\u5b9a3D\u8fb9\u754c\u6846\uff0c\u5b9e\u73b0\u663e\u5f0f\u7684\u6bcf\u5bf9\u8c61\u8bed\u4e49\u63a7\u5236\uff1b2\uff09\u4f7f\u7528\u65e0\u626d\u66f2\u7684\u751f\u6210\u7f16\u8f91\u7ba1\u9053\uff0c\u901a\u8fc7\u91cd\u65b0\u751f\u6210\u800c\u975e\u50cf\u7d20\u53d8\u5f62\u652f\u6301\u5bf9\u8c61\u63d2\u5165\u3001\u79fb\u9664\u548c\u53d8\u6362\uff1b3\uff09\u5229\u7528IP-Adapter\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u8c03\u8282\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u4ea4\u4e92\u5f0f3D\u7f16\u8f91\u4e2d\u4fdd\u6301\u5bf9\u8c61\u5916\u89c2\u4e00\u81f4\u6027\u548c\u5168\u5c40\u573a\u666f\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPOCI-Diff\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4e0e\u6307\u5b9a\u76843D\u5e03\u5c40\u548c\u7f16\u8f91\u4fdd\u6301\u4e00\u81f4\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5e03\u5c40\u9075\u5faa\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u626d\u66f2\u5f15\u8d77\u7684\u51e0\u4f55\u4f2a\u5f71\u3002", "conclusion": "POCI-Diff\u901a\u8fc7\u7edf\u4e00\u7684\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u4e863D\u51e0\u4f55\u7ea6\u675f\u548c\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u7ed1\u5b9a\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u5177\u6709\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u6027\u76843D\u5e03\u5c40\u63a7\u5236\u4e0e\u7f16\u8f91\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.13816", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13816", "abs": "https://arxiv.org/abs/2601.13816", "authors": ["Ra\u00fcl P\u00e9rez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "title": "Discriminant Learning-based Colorspace for Blade Segmentation", "comment": "Accepted to ICASSP 2026", "summary": "Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.", "AI": {"tldr": "\u63d0\u51faCSDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u975e\u7ebf\u6027\u5224\u522b\u5206\u6790\u4f18\u5316\u8272\u5f69\u8868\u793a\uff0c\u63d0\u5347\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8272\u5f69\u8868\u793a\u8fd9\u4e00\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u6b21\u4f18\u7684\u8272\u5f69\u8868\u793a\u4f1a\u963b\u788d\u51c6\u786e\u7684\u5206\u5272\u6548\u679c", "method": "\u63d0\u51faColorspace Discriminant Analysis (CSDA)\u7b97\u6cd5\uff0c\u5c06\u7ebf\u6027\u5224\u522b\u5206\u6790\u6269\u5c55\u5230\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6700\u5927\u5316\u591a\u7ef4\u6709\u7b26\u53f7\u7c7b\u95f4\u5206\u79bb\u5ea6\u5e76\u6700\u5c0f\u5316\u7c7b\u5185\u53d8\u5f02\u6027\u6765\u5b9a\u5236\u8272\u5f69\u8868\u793a\uff0c\u5f15\u5165\u4e09\u79cd\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316", "result": "\u5728\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e26\u6765\u4e86\u663e\u8457\u7684\u7cbe\u5ea6\u63d0\u5347", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5b9a\u5236\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff08\u7279\u522b\u662f\u8272\u5f69\u8868\u793a\u4f18\u5316\uff09\u81f3\u5173\u91cd\u8981"}}
{"id": "2601.13837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13837", "abs": "https://arxiv.org/abs/2601.13837", "authors": ["Xinya Ji", "Sebastian Weiss", "Manuel Kansy", "Jacek Naruniec", "Xun Cao", "Barbara Solenthaler", "Derek Bradley"], "title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation", "comment": null, "summary": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \\OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u7684\u524d\u9988\u5f0f\u5934\u90e8\u5316\u8eab\u751f\u6210\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u8f93\u5165\u56fe\u50cf\u5373\u53ef\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u7684\u5934\u90e8\u5316\u8eab", "motivation": "\u73b0\u67093D\u9ad8\u65af\u5934\u90e8\u5316\u8eab\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u6355\u6349\u6216\u5355\u76ee\u89c6\u9891\u4f18\u5316\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u65b9\u6cd5", "method": "1) \u4ece\u8f93\u5165\u56fe\u50cf\u76f4\u63a5\u5b66\u4e60\u9010\u50cf\u7d20\u9ad8\u65af\u8868\u793a\uff1b2) \u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u878d\u5408DINOv3\u548cStable Diffusion VAE\u7279\u5f81\uff1b3) \u6269\u5c55\u9ad8\u65af\u8868\u793a\u5e76\u5f15\u5165\u8f7b\u91cfMLP\u52a8\u6001\u7f51\u7edc\u9884\u6d4b\u53d8\u5f62\uff1b4) \u5229\u7528\u9884\u8bad\u7ec3\u5927\u91cd\u5efa\u6a21\u578b\u7684\u70b9\u4e91\u56fe\u8fdb\u884c\u51e0\u4f55\u76d1\u7763", "result": "\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u63a8\u7406\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u52a8\u6001\u5316\u8eab\u52a8\u753b", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u56fe\u50cf\u5373\u53ef\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u5b9e\u65f6\u52a8\u753b\u76843D\u9ad8\u65af\u5934\u90e8\u5316\u8eab\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6613\u7528\u6027\u95ee\u9898"}}
{"id": "2601.14069", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14069", "abs": "https://arxiv.org/abs/2601.14069", "authors": ["Nattapong Kurpukdee", "Adrian G. Bors"], "title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management", "comment": null, "summary": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u89c6\u9891\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u6e10\u8fdb\u5f0f\u805a\u7c7b\uff0c\u5728\u4e0d\u4f7f\u7528\u6807\u7b7e\u548c\u4efb\u52a1\u8fb9\u754c\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u89c6\u9891\u4fe1\u606f\u800c\u4e0d\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6807\u7b7e\u548c\u4efb\u52a1\u8fb9\u754c\u4fe1\u606f\uff0c\u8fd9\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4e0d\u73b0\u5b9e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e0\u76d1\u7763\u7684\u89c6\u9891\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u4f7f\u7528\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u83b7\u53d6\u6bcf\u4e2a\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u89c6\u9891\u7279\u5f81\uff0c\u7136\u540e\u6e10\u8fdb\u5f0f\u6784\u5efa\u6df1\u5ea6\u805a\u7c7b\u3002\u5728\u8fde\u7eed\u4efb\u52a1\u5b66\u4e60\u4e2d\uff0c\u5c06\u524d\u4e00\u4efb\u52a1\u66f4\u65b0\u540e\u7684\u6a21\u578b\u4f5c\u4e3a\u521d\u59cb\u72b6\u6001\uff0c\u4ee5\u5c06\u77e5\u8bc6\u8fc1\u79fb\u5230\u5f53\u524d\u5b66\u4e60\u4efb\u52a1\u3002", "result": "\u5728UCF101\u3001HMDB51\u548cSomething-to-Something V2\u4e09\u4e2a\u6807\u51c6\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6df1\u5165\u8bc4\u4f30\uff0c\u5ffd\u7565\u76d1\u7763\u8bbe\u7f6e\u4e2d\u7684\u6807\u7b7e\u3002\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65e0\u76d1\u7763\u89c6\u9891\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6ca1\u6709\u6807\u7b7e\u548c\u4efb\u52a1\u8fb9\u754c\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b66\u4e60\u89c6\u9891\u4fe1\u606f\u800c\u4e0d\u9057\u5fd8\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.13839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13839", "abs": "https://arxiv.org/abs/2601.13839", "authors": ["Aisha Al-Mohannadi", "Ayisha Firoz", "Yin Yang", "Muhammad Imran", "Ferda Ofli"], "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes", "comment": null, "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.", "AI": {"tldr": "DisasterVQA\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,395\u5f20\u771f\u5b9e\u707e\u5bb3\u56fe\u50cf\u548c4,405\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u707e\u5bb3\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u5728\u707e\u5bb3\u54cd\u5e94\u4e2d\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7684\u6001\u52bf\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u7684\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5728\u707e\u5bb3\u54cd\u5e94\u8fd9\u79cd\u590d\u6742\u3001\u5b89\u5168\u5173\u952e\u7684\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u707e\u5bb3\u573a\u666f\u7684\u57fa\u51c6\u6765\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u521b\u5efaDisasterVQA\u6570\u636e\u96c6\uff0c\u5305\u542b1,395\u5f20\u771f\u5b9e\u707e\u5bb3\u56fe\u50cf\uff08\u6d2a\u6c34\u3001\u91ce\u706b\u3001\u5730\u9707\u7b49\uff09\u548c4,405\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u95ee\u7b54\u5bf9\u3002\u95ee\u9898\u57fa\u4e8eFEMA ESF\u548cOCHA MIRA\u7b49\u4eba\u9053\u4e3b\u4e49\u6846\u67b6\u8bbe\u8ba1\uff0c\u6db5\u76d6\u4e8c\u5143\u9009\u62e9\u3001\u591a\u9879\u9009\u62e9\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u6d89\u53ca\u6001\u52bf\u611f\u77e5\u548c\u64cd\u4f5c\u51b3\u7b56\u4efb\u52a1\u3002", "result": "\u5bf97\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e8c\u5143\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u5b9a\u91cf\u63a8\u7406\u3001\u7269\u4f53\u8ba1\u6570\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u707e\u5bb3\u573a\u666f\u4e2d\u3002\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u3001\u707e\u5bb3\u7c7b\u522b\u3001\u533a\u57df\u548c\u4eba\u9053\u4e3b\u4e49\u4efb\u52a1\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "DisasterVQA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u7528\u6027\u7684\u57fa\u51c6\uff0c\u53ef\u4ee5\u6307\u5bfc\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u64cd\u4f5c\u610f\u4e49\u7684\u707e\u5bb3\u54cd\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2601.13852", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13852", "abs": "https://arxiv.org/abs/2601.13852", "authors": ["Ra\u00fcl P\u00e9rez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "title": "Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation", "comment": "Accepted to ICASSP 2026", "summary": "Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u5224\u522b\u5206\u6790(DDA)\u548c\u6982\u7387DDA(PDDA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u7f51\u7edc\u76f4\u63a5\u4f18\u5316Fisher\u51c6\u5219\uff0c\u89e3\u51b3\u7ebf\u6027\u5224\u522b\u5206\u6790\u5bf9\u975e\u7ebf\u6027\u53ef\u5206\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u98ce\u53f6\u7247\u5206\u5272\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7ebf\u6027\u5224\u522b\u5206\u6790\u80fd\u63d0\u9ad8\u7c7b\u522b\u53ef\u5206\u6027\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u53ef\u5206\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u6df1\u5ea6\u7f51\u7edc\u76f4\u63a5\u4f18\u5316Fisher\u51c6\u5219\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u6df1\u5ea6\u5224\u522b\u5206\u6790(DDA)\uff0c\u901a\u8fc7\u5f15\u5165\u5e26\u7b26\u53f7\u7684\u7c7b\u95f4\u65b9\u5dee\u3001\u4f7f\u7528sigmoid\u51fd\u6570\u7ea6\u675f\u8f93\u51fa\u3001\u5c06\u4e58\u6cd5\u5173\u7cfb\u8f6c\u6362\u4e3a\u52a0\u6cd5\u5173\u7cfb\u6765\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u6982\u7387DDA(PDDA)\uff0c\u5728DDA\u635f\u5931\u51fd\u6570\u57fa\u7840\u4e0a\u589e\u52a0\u6982\u7387\u635f\u5931\uff0c\u6700\u5c0f\u5316\u8f93\u51fa\u5206\u5e03\u4e2d\u7684\u7c7b\u522b\u91cd\u53e0\u3002", "result": "PDDA\u80fd\u6709\u6548\u51cf\u5c11\u7c7b\u5185\u65b9\u5dee\uff0c\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u3002\u5728\u98ce\u53f6\u7247\u5206\u5272\u4efb\u52a1\u4e2d\uff0cPDDA\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u4e00\u81f4\u6027\u6539\u8fdb\uff0c\u8fd9\u5bf9\u4e8e\u98ce\u80fd\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u662fDDA\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0cPDDA\u901a\u8fc7\u76f4\u63a5\u4f18\u5316Fisher\u51c6\u5219\u5e76\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u53ef\u5206\u6570\u636e\u7684\u5224\u522b\u5206\u6790\u95ee\u9898\uff0c\u5728\u98ce\u53f6\u7247\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.14086", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14086", "abs": "https://arxiv.org/abs/2601.14086", "authors": ["Nattapong Kurpukdee", "Adrian G. Bors"], "title": "Two-Stream temporal transformer for video action classification", "comment": null, "summary": "Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53cc\u6d41Transformer\u89c6\u9891\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u5185\u5bb9\u5e27\u548c\u5149\u6d41\u4fe1\u606f\uff0c\u5728\u4e09\u4e2a\u77e5\u540d\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u79c0\u5206\u7c7b\u7ed3\u679c\u3002", "motivation": "\u8fd0\u52a8\u8868\u793a\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u5e94\u7528\u4e8e\u89c6\u9891\u5206\u7c7b\u4e2d\u7ed3\u5408\u5185\u5bb9\u4e0e\u8fd0\u52a8\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u53cc\u6d41Transformer\u89c6\u9891\u5206\u7c7b\u5668\uff1a\u4e00\u4e2a\u6d41\u5904\u7406\u5185\u5bb9\u5e27\uff08\u7a7a\u95f4\u4fe1\u606f\uff09\uff0c\u53e6\u4e00\u4e2a\u6d41\u5904\u7406\u5149\u6d41\uff08\u8fd0\u52a8\u4fe1\u606f\uff09\u3002\u6a21\u578b\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u8054\u5408\u7684\u5149\u6d41\u548c\u65f6\u5e8f\u5e27\u57df\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728Transformer\u7f16\u7801\u5668\u4e2d\u8868\u793a\u5b83\u4eec\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u540d\u7684\u4eba\u7c7b\u6d3b\u52a8\u89c6\u9891\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u79c0\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u5185\u5bb9\u4e0e\u8fd0\u52a8\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cc\u6d41Transformer\u67b6\u6784\u80fd\u6709\u6548\u7ed3\u5408\u89c6\u9891\u7684\u5185\u5bb9\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u8054\u5408\u57df\u4e2d\u5efa\u6a21\u65f6\u7a7a\u5173\u7cfb\uff0c\u4e3a\u89c6\u9891\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13871", "abs": "https://arxiv.org/abs/2601.13871", "authors": ["Michail Spanakis", "Iason Oikonomidis", "Antonis Argyros"], "title": "OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting", "comment": null, "summary": "Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.", "AI": {"tldr": "OCCAM\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\u95ee\u9898\uff0c\u57fa\u4e8eSAM2\u548c\u81ea\u5b9a\u4e49FINCH\u7b97\u6cd5\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u3001\u4f9d\u8d56\u989d\u5916\u4fe1\u606f\uff08\u5982\u89c6\u89c9\u793a\u4f8b\u6216\u6587\u672c\u63d0\u793a\uff09\uff0c\u4e14\u5927\u591a\u5047\u8bbe\u6bcf\u5f20\u56fe\u50cf\u53ea\u6709\u4e00\u4e2a\u76ee\u6807\u7c7b\u522b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Segment Anything Model 2 (SAM2)\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u7684\u57fa\u4e8e\u9608\u503c\u7684First Integer Neighbor Clustering Hierarchy (FINCH)\u7b97\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u7c7b\u76ee\u6807\u8ba1\u6570\u3002", "result": "\u5728FSC-147\u548cCARPK\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u5408\u6210\u591a\u7c7b\u6570\u636e\u96c6\u548c\u66f4\u5408\u9002\u7684F1\u8bc4\u5206\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "OCCAM\u662f\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u65e0\u9700\u989d\u5916\u4fe1\u606f\u7684\u7c7b\u65e0\u5173\u76ee\u6807\u8ba1\u6570\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u7c7b\u8ba1\u6570\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13886", "abs": "https://arxiv.org/abs/2601.13886", "authors": ["Shangzhe Di", "Zhonghua Zhai", "Weidi Xie"], "title": "Revisiting Multi-Task Visual Representation Learning", "comment": "Code: https://github.com/Becomebright/MTV", "summary": "Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity \"expert\" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves \"best-of-both-worlds\" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.", "AI": {"tldr": "MTV\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u89c6\u89c9\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u5bc6\u96c6\u7a7a\u95f4\u76ee\u6807\uff0c\u7ed3\u5408\u4e86CLIP\u7684\u8bed\u4e49\u5bf9\u9f50\u548cMAE/DINO\u7684\u5c40\u90e8\u7ed3\u6784\u4f18\u52bf\uff0c\u5229\u7528\u4e13\u5bb6\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u7a7a\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u7406\u89e3\u7684\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u5b58\u5728\u5206\u88c2\uff1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u64c5\u957f\u5168\u5c40\u8bed\u4e49\u5bf9\u9f50\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u800c\u81ea\u76d1\u7763\u65b9\u6cd5\uff08\u5982MAE\u3001DINO\uff09\u80fd\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u4f46\u7f3a\u4e4f\u9ad8\u7ea7\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e24\u79cd\u8303\u5f0f\u672c\u8d28\u4e0a\u662f\u4e92\u8865\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u4efb\u52a1\u6846\u67b6\u96c6\u6210\u3002", "method": "\u63d0\u51faMTV\u591a\u4efb\u52a1\u89c6\u89c9\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u76ee\u6807\uff1a1\uff09\u89c6\u89c9\u8bed\u8a00\u5bf9\u6bd4\u5b66\u4e60\uff08\u8bed\u4e49\u5bf9\u9f50\uff09\uff0c2\uff09\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u5c40\u90e8\u7ed3\u6784\uff09\uff0c3\uff09\u5bc6\u96c6\u7a7a\u95f4\u76d1\u7763\uff08\u7a7a\u95f4\u7cbe\u5ea6\uff09\u3002\u4e3a\u907f\u514d\u4eba\u5de5\u6807\u6ce8\uff0c\u4f7f\u7528Depth Anything V2\u548cOWLv2\u7b49\u4e13\u5bb6\u6a21\u578b\u751f\u6210\u5927\u89c4\u6a21\u5bc6\u96c6\u4f2a\u6807\u7b7e\u3002", "result": "MTV\u5b9e\u73b0\u4e86\"\u4e24\u5168\u5176\u7f8e\"\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u3002\u7cfb\u7edf\u5206\u6790\u663e\u793a\uff1a1\uff09\u6bcf\u4e2a\u76ee\u6807\u90fd\u6709\u8fb9\u9645\u589e\u76ca\uff0c2\uff09\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u534f\u540c\u800c\u975e\u5e72\u6270\uff0c3\uff09\u5728\u4e0d\u540c\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u90fd\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u7ed3\u5408\u9ad8\u8d28\u91cf\u4f2a\u76d1\u7763\u662f\u6784\u5efa\u66f4\u901a\u7528\u89c6\u89c9\u7f16\u7801\u5668\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002MTV\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u4e0d\u540c\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u4e3a\u89c6\u89c9\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.14154", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14154", "abs": "https://arxiv.org/abs/2601.14154", "authors": ["Shubham Pandey", "Bhavin Jawade", "Srirangaraj Setlur", "Venu Govindaraju", "Kenneth Seastedt"], "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery", "comment": "Accepted to P2P-CV @ WACV 2026", "summary": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.", "AI": {"tldr": "MIRACLE\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u672f\u524d\u4e34\u5e8a\u548c\u653e\u5c04\u5b66\u6570\u636e\u6765\u9884\u6d4b\u80ba\u764c\u624b\u672f\u672f\u540e\u5e76\u53d1\u75c7\u98ce\u9669\uff0c\u91c7\u7528\u8d85\u7403\u9762\u5d4c\u5165\u7a7a\u95f4\u878d\u5408\u5f02\u8d28\u8f93\u5165\uff0c\u5e76\u5305\u542b\u53ef\u89e3\u91ca\u7684\u5e72\u9884\u6a21\u5757\u3002", "motivation": "\u672f\u540e\u5e76\u53d1\u75c7\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u9884\u540e\u5e76\u589e\u52a0\u533b\u7597\u6210\u672c\uff0c\u9700\u8981\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u6539\u5584\u80ba\u764c\u624b\u672f\u60a3\u8005\u7684\u98ce\u9669\u7ba1\u7406\u3002", "method": "\u63d0\u51faMIRACLE\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u91c7\u7528\u8d85\u7403\u9762\u5d4c\u5165\u7a7a\u95f4\u878d\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u8bb0\u5f55\u548c\u9ad8\u7ef4\u653e\u5c04\u5b66\u56fe\u50cf\uff0c\u5305\u542b\u5e72\u9884\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u548c\u4e34\u5e8a\u5efa\u8bae\u3002", "result": "\u5728\u5305\u542b3,094\u540d\u80ba\u764c\u624b\u672f\u60a3\u8005\u7684POC-L\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMIRACLE\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u3002", "conclusion": "MIRACLE\u4e3a\u4e2a\u6027\u5316\u3001\u53ef\u89e3\u91ca\u7684\u672f\u540e\u98ce\u9669\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u63d0\u4f9b\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.13899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13899", "abs": "https://arxiv.org/abs/2601.13899", "authors": ["Masoumeh Javanbakhat", "Piotr Komorowski", "Dilyara Bareeva", "Wei-Chang Lai", "Wojciech Samek", "Christoph Lippert"], "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging", "comment": null, "summary": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u4e3a\u6df1\u5ea6\u4e24\u6837\u672c\u6d4b\u8bd5\u63d0\u4f9b\u6837\u672c\u7ea7\u548c\u7279\u5f81\u7ea7\u89e3\u91ca\uff0c\u63ed\u793a\u54ea\u4e9b\u6837\u672c\u548c\u7279\u5f81\u9a71\u52a8\u663e\u8457\u7684\u7ec4\u95f4\u5dee\u5f02", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u4e24\u6837\u672c\u6d4b\u8bd5\u867d\u7136\u68c0\u6d4b\u80fd\u529b\u5f3a\uff0c\u4f46\u9ed1\u76d2\u6027\u8d28\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u5728\u751f\u7269\u533b\u5b66\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff1b\u73b0\u6709\u53ef\u89e3\u91ca\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u7c7b\u522b\u6807\u7b7e\uff0c\u4e0d\u9002\u7528\u4e8e\u65e0\u6807\u7b7e\u7684\u7edf\u8ba1\u6d4b\u8bd5\u573a\u666f", "method": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u589e\u5f3a\u6df1\u5ea6\u4e24\u6837\u672c\u6d4b\u8bd5\u7684\u6837\u672c\u7ea7\u548c\u7279\u5f81\u7ea7\u89e3\u91ca\u80fd\u529b\uff0c\u8bc6\u522b\u9a71\u52a8\u7edf\u8ba1\u663e\u8457\u5dee\u5f02\u7684\u4e2a\u4f53\u6837\u672c\u548c\u8f93\u5165\u7279\u5f81", "result": "\u65b9\u6cd5\u80fd\u7a81\u51fa\u663e\u793a\u54ea\u4e9b\u56fe\u50cf\u533a\u57df\u548c\u54ea\u4e9b\u4e2a\u4f53\u6837\u672c\u5bf9\u68c0\u6d4b\u5230\u7684\u7ec4\u95f4\u5dee\u5f02\u8d21\u732e\u6700\u5927\uff0c\u63d0\u4f9b\u7a7a\u95f4\u548c\u5b9e\u4f8b\u5c42\u9762\u7684\u6d1e\u5bdf\uff1b\u5e94\u7528\u4e8e\u751f\u7269\u533b\u5b66\u6210\u50cf\u6570\u636e\u65f6\uff0c\u80fd\u8bc6\u522b\u6709\u5f71\u54cd\u529b\u7684\u6837\u672c\u5e76\u7a81\u51fa\u4e0e\u75be\u75c5\u76f8\u5173\u53d8\u5f02\u76f8\u5173\u7684\u89e3\u5256\u5b66\u610f\u4e49\u533a\u57df", "conclusion": "\u8be5\u5de5\u4f5c\u6865\u63a5\u4e86\u7edf\u8ba1\u63a8\u65ad\u548c\u53ef\u89e3\u91caAI\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u6210\u50cf\u4e2d\u53ef\u89e3\u91ca\u3001\u65e0\u6807\u7b7e\u7684\u7fa4\u4f53\u5206\u6790"}}
{"id": "2601.13913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13913", "abs": "https://arxiv.org/abs/2601.13913", "authors": ["Pavlo Melnyk", "Cuong Le", "Urs Waldmann", "Per-Erik Forss\u00e9n", "Bastian Wandt"], "title": "On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation", "comment": null, "summary": "Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5b66\u4e602D\u65cb\u8f6c\u7b49\u53d8\u6027\uff0c\u800c\u975e\u8bbe\u8ba1\u7b49\u53d8\u6027\u6a21\u578b\uff0c\u6765\u6539\u8fdb\u5355\u76ee3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5728\u5e73\u9762\u65cb\u8f6c\u59ff\u6001\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u5148\u68c0\u6d4b2D\u5173\u8282\u70b9\uff0c\u518d\u8fdb\u884c2D\u52303D\u7684\u63d0\u5347\u3002\u4f46\u73b0\u6709\u63d0\u5347\u6a21\u578b\u5728\u5904\u7406\u65cb\u8f6c\u8f93\u5165\u65f6\u5bb9\u6613\u5931\u8d25\u3002\u4f5c\u8005\u8ba4\u4e3a\u5b66\u4e60\u5355\u4e2a\u4eba\u4f53\u59ff\u6001\u53ca\u5176\u5e73\u9762\u5185\u65cb\u8f6c\u6bd4\u76f4\u63a5\u5b66\u4e60\u70b9\u5bf9\u70b9\u6620\u5c04\u66f4\u5bb9\u6613\u4e14\u66f4\u7b26\u5408\u51e0\u4f55\u539f\u7406\u3002", "method": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u8ba9\u6a21\u578b\u5b66\u4e602D\u65cb\u8f6c\u7b49\u53d8\u6027\uff0c\u800c\u4e0d\u662f\u8bbe\u8ba1\u5177\u6709\u7b49\u53d8\u6027\u7ea6\u675f\u7684\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u65cb\u8f6c\u589e\u5f3a\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5e73\u9762\u5185\u65cb\u8f6c\u7684\u59ff\u6001\uff0c\u800c\u4e0d\u9700\u8981\u5728\u6a21\u578b\u53c2\u6570\u7a7a\u95f4\u4e2d\u663e\u5f0f\u7ea6\u675f\u7b49\u53d8\u6027\u3002", "result": "\u5728\u5e38\u89c1\u7684HPE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5b66\u4e60\u76842D\u65cb\u8f6c\u7b49\u53d8\u6027\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5e73\u9762\u65cb\u8f6c\u59ff\u6001\u4e0a\u7684\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u7b49\u53d8\u6027\u8bbe\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u589e\u5f3a\u5b66\u4e60\u65cb\u8f6c\u7b49\u53d8\u6027\u6bd4\u8bbe\u8ba1\u590d\u6742\u7684\u7b49\u53d8\u6027\u6a21\u578b\u66f4\u6709\u6548\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u66f4\u597d\uff0c\u800c\u4e14\u5b66\u4e60\u8fc7\u7a0b\u66f4\u76f4\u63a5\u7b80\u5355\u3002"}}
{"id": "2601.13935", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13935", "abs": "https://arxiv.org/abs/2601.13935", "authors": ["Anoushkrit Goel", "Simroop Singh", "Ankita Joshi", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Aditya Nigam", "Arnav Bhavsar"], "title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026", "summary": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.", "AI": {"tldr": "TrackletGPT\uff1a\u4e00\u79cd\u7528\u4e8e\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u7684\u7c7b\u8bed\u8a00GPT\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u7247\u6bb5\uff08tracklets\uff09\u91cd\u65b0\u5f15\u5165\u5e8f\u5217\u4fe1\u606f\uff0c\u5728\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u5bf9\u4e8e\u7814\u7a76\u5927\u8111\u7ed3\u6784\u8fde\u63a5\u6027\u3001\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u548c\u795e\u7ecf\u5916\u79d1\u624b\u672f\u81f3\u5173\u91cd\u8981\u3002\u8be5\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7ea4\u7ef4\u675f\u5728\u4e0d\u540c\u4e2a\u4f53\u548c\u6761\u4ef6\u4e0b\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u5728\u8de8\u534a\u7403\u548c\u4e2a\u4f53\u95f4\u5177\u6709\u76f8\u4f3c\u7684\u4e09\u7ef4\u7ed3\u6784\u3002", "method": "\u63d0\u51faTrackletGPT\uff0c\u4e00\u79cd\u7c7b\u8bed\u8a00\u7684GPT\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u8f68\u8ff9\u7247\u6bb5\uff08tracklets\uff09\u5728token\u4e2d\u91cd\u65b0\u5f15\u5165\u5e8f\u5217\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u65e0\u7f1d\u6cdb\u5316\u5230\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5e76\u7f16\u7801\u7ec6\u7c92\u5ea6\u7684\u5b50\u6d41\u7ebf\u7247\u6bb5\uff0c\u4ece\u800c\u6269\u5c55\u548c\u4f18\u5316\u4e86GPT\u6a21\u578b\u5728\u7ea4\u7ef4\u675f\u5206\u5272\u4e2d\u7684\u5e94\u7528\u3002", "result": "TrackletGPT\u5728TractoInferno\u548cHCP\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747DICE\u3001Overlap\u548cOverreach\u5206\u6570\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "TrackletGPT\u901a\u8fc7\u5f15\u5165\u8f68\u8ff9\u7247\u6bb5\u548c\u5e8f\u5217\u4fe1\u606f\uff0c\u4e3a\u767d\u8d28\u7ea4\u7ef4\u675f\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684GPT\u6846\u67b6\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.13951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13951", "abs": "https://arxiv.org/abs/2601.13951", "authors": ["Shengyi Wu", "Yan Hong", "Shengyao Chen", "Zheng Wang", "Xianbing Sun", "Jiahui Zhan", "Jun Lan", "Jianfu Zhang"], "title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content", "comment": null, "summary": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.", "AI": {"tldr": "VTONGuard\uff1a\u4e00\u4e2a\u5305\u542b\u8d85\u8fc777.5\u4e07\u5f20\u771f\u5b9e\u548c\u5408\u6210\u8bd5\u7a7f\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u865a\u62df\u8bd5\u7a7f\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u591a\u4efb\u52a1\u6846\u67b6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u865a\u62df\u8bd5\u7a7f(VTON)\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u7684\u8bd5\u7a7f\u5185\u5bb9\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u771f\u5b9e\u6027\u548c\u8d1f\u8d23\u4efb\u4f7f\u7528\u7684\u8feb\u5207\u62c5\u5fe7\u3002\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4fc3\u8fdbVTON\u6280\u672f\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "1) \u521b\u5efaVTONGuard\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc777.5\u4e07\u5f20\u771f\u5b9e\u548c\u5408\u6210\u8bd5\u7a7f\u56fe\u50cf\uff0c\u6db5\u76d6\u59ff\u6001\u3001\u80cc\u666f\u3001\u670d\u88c5\u98ce\u683c\u7b49\u591a\u6837\u771f\u5b9e\u573a\u666f\uff1b2) \u5728\u7edf\u4e00\u8bad\u7ec3\u548c\u6d4b\u8bd5\u534f\u8bae\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u68c0\u6d4b\u8303\u5f0f\uff1b3) \u8bbe\u8ba1\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u96c6\u6210\u8f85\u52a9\u5206\u5272\u4efb\u52a1\u4ee5\u589e\u5f3a\u8fb9\u754c\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u3002", "result": "1) \u63ed\u793a\u4e86\u5404\u79cd\u68c0\u6d4b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff1b2) \u7a81\u51fa\u4e86\u8de8\u8303\u5f0f\u6cdb\u5316\u7684\u6301\u7eed\u6311\u6218\uff1b3) \u63d0\u51fa\u7684\u591a\u4efb\u52a1\u6846\u67b6\u5728VTONGuard\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "VTONGuard\u57fa\u51c6\u6570\u636e\u96c6\u80fd\u591f\u5b9e\u73b0\u516c\u5e73\u6bd4\u8f83\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u6a21\u578b\u5f00\u53d1\uff0c\u63a8\u52a8VTON\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002"}}
{"id": "2601.14255", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14255", "abs": "https://arxiv.org/abs/2601.14255", "authors": ["Sangbeom Lim", "Seoung Wug Oh", "Jiahui Huang", "Heeji Yoon", "Seungryong Kim", "Joon-Young Lee"], "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior", "comment": "Project page: https://cvlab-kaist.github.io/VideoMaMa/", "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.", "AI": {"tldr": "VideoMaMa\u6a21\u578b\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u7c97\u7cd9\u5206\u5272\u63a9\u7801\u8f6c\u6362\u4e3a\u7cbe\u786ealpha\u906e\u7f69\uff0c\u4ec5\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u89c6\u9891\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4f2a\u6807\u6ce8\u89c6\u9891\u906e\u7f69\u6570\u636e\u96c6MA-V\u3002", "motivation": "\u89c6\u9891\u906e\u7f69\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u6cdb\u5316\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u73b0\u6709\u5206\u5272\u7ebf\u7d22\u5e76\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVideoMaMa\u6a21\u578b\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5c06\u7c97\u7cd9\u5206\u5272\u63a9\u7801\u8f6c\u6362\u4e3a\u7cbe\u786ealpha\u906e\u7f69\u3002\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u771f\u5b9e\u89c6\u9891\u3002\u57fa\u4e8e\u6b64\u6784\u5efa\u5927\u89c4\u6a21\u4f2a\u6807\u6ce8\u6d41\u7a0b\uff0c\u521b\u5efaMA-V\u6570\u636e\u96c6\uff08\u5305\u542b5\u4e07+\u771f\u5b9e\u89c6\u9891\u7684\u9ad8\u8d28\u91cf\u906e\u7f69\u6807\u6ce8\uff09\u3002", "result": "VideoMaMa\u5728\u771f\u5b9e\u89c6\u9891\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u5728MA-V\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684SAM2-Matte\u6a21\u578b\uff0c\u5728\u91ce\u5916\u89c6\u9891\u7684\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u73b0\u6709\u906e\u7f69\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u76f8\u540c\u6a21\u578b\u3002", "conclusion": "\u5927\u89c4\u6a21\u4f2a\u6807\u6ce8\u89c6\u9891\u906e\u7f69\u6570\u636e\u5bf9\u89c6\u9891\u906e\u7f69\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u751f\u6210\u5148\u9a8c\u548c\u53ef\u8bbf\u95ee\u7684\u5206\u5272\u7ebf\u7d22\u80fd\u591f\u63a8\u52a8\u89c6\u9891\u906e\u7f69\u7814\u7a76\u7684\u53ef\u6269\u5c55\u8fdb\u5c55\u3002"}}
{"id": "2601.13954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13954", "abs": "https://arxiv.org/abs/2601.13954", "authors": ["Adrien Meyer", "Didier Mutter", "Nicolas Padoy"], "title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging", "comment": null, "summary": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.", "AI": {"tldr": "DExTeR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u70b9\u5230\u6846\u56de\u5f52\u5668\uff0c\u4e13\u95e8\u4e3a\u533b\u5b66\u56fe\u50cf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7c7b\u5f15\u5bfc\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u3001CLICK-MoE\u4e13\u5bb6\u7cfb\u7edf\u548c\u591a\u70b9\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u89e3\u5256\u6807\u5fd7\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u4ecb\u5165\u6307\u5bfc\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u7684\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u5f31\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u4f7f\u7528\u70b9\u6807\u6ce8\u53ef\u51cf\u5c11\u6807\u6ce8\u65f6\u95f4\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u89e3\u5256\u7ed3\u6784\u91cd\u53e0\u3001\u5c3a\u5bf8\u53d8\u5316\u548c\u96be\u4ee5\u6349\u6478\u7684\u7ed3\u6784\u7ed9\u51c6\u786e\u7684\u8fb9\u754c\u6846\u63a8\u65ad\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u57fa\u4e8ePoint-DETR\u6784\u5efaDExTeR\uff0c\u5c06\u5355\u70b9\u6807\u6ce8\u7f16\u7801\u4e3a\u5bf9\u8c61\u67e5\u8be2\uff1b\u63d0\u51fa\u7c7b\u5f15\u5bfc\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u70b9\u5750\u6807\u548c\u7c7b\u522b\u6807\u7b7e\u5f15\u5bfc\u6ce8\u610f\u529b\u91c7\u6837\uff1b\u5f15\u5165CLICK-MoE\uff08\u7c7b\u522b\u3001\u5b9e\u4f8b\u548c\u901a\u7528\u77e5\u8bc6\u6df7\u5408\u4e13\u5bb6\uff09\u7cfb\u7edf\uff0c\u89e3\u8026\u7c7b\u522b\u548c\u5b9e\u4f8b\u8868\u793a\u4ee5\u51cf\u5c11\u6df7\u6dc6\uff1b\u5b9e\u65bd\u591a\u70b9\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u5bf9\u6807\u6ce8\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "DExTeR\u5728\u4e09\u4e2a\u4e0d\u540c\u533b\u5b66\u9886\u57df\u6570\u636e\u96c6\uff08\u5185\u7aa5\u955c\u3001\u80f8\u90e8X\u5149\u548c\u5185\u7aa5\u955c\u8d85\u58f0\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u6f5c\u529b\u3002", "conclusion": "DExTeR\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u6311\u6218\u8bbe\u8ba1\u7684Transformer\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u534a\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u96be\u9898\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13974", "abs": "https://arxiv.org/abs/2601.13974", "authors": ["Shih-Yao Lin"], "title": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames", "comment": "This paper corresponds to the camera-ready version of a WACV 2026 Workshop paper", "summary": "Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.\n  We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.\n  Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.\n  We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.", "AI": {"tldr": "\u63d0\u51faSTEC\uff08\u65f6\u7a7a\u71b5\u8986\u76d6\uff09\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5e27\u91c7\u6837\u7684\u8d28\u91cf\uff0c\u8861\u91cf\u91c7\u6837\u5e27\u662f\u5426\u5145\u5206\u6355\u6349\u89c6\u9891\u5185\u5bb9\u7684\u4fe1\u606f\u91cf\u548c\u4ee3\u8868\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\u6216\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u65e0\u6cd5\u8bc4\u4f30\u91c7\u6837\u5e27\u662f\u5426\u5145\u5206\u6355\u6349\u89c6\u9891\u7684\u4fe1\u606f\u5185\u5bb9\u548c\u4ee3\u8868\u6027\u5185\u5bb9\u3002\u9700\u8981\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5206\u6790\u6709\u9650\u9884\u7b97\u4e0b\u7684\u5e27\u91c7\u6837\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u65f6\u7a7a\u5e27\u71b5\uff08STFE\uff09\u6d4b\u91cf\u6bcf\u5e27\u7684\u7a7a\u95f4\u4fe1\u606f\uff08\u57fa\u4e8e\u71b5\u7684\u7ed3\u6784\u590d\u6742\u5ea6\uff09\uff0c\u7136\u540e\u8bc4\u4f30\u91c7\u6837\u5e27\u7684\u65f6\u5e8f\u8986\u76d6\u548c\u5197\u4f59\u5ea6\u3002\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4fe1\u606f\u5f3a\u5ea6\u3001\u65f6\u5e8f\u5206\u6563\u6027\u548c\u975e\u5197\u4f59\u6027\u3002", "result": "\u5728MSR-VTT\u6d4b\u8bd5\u96c6\u4e0a\uff0cSTEC\u80fd\u6e05\u6670\u533a\u5206\u968f\u673a\u3001\u5747\u5300\u548c\u5185\u5bb9\u611f\u77e5\u7b49\u5e38\u89c1\u91c7\u6837\u7b56\u7565\u3002\u80fd\u63ed\u793a\u5355\u4e2a\u89c6\u9891\u7684\u9c81\u68d2\u6027\u6a21\u5f0f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5e73\u5747\u6027\u80fd\u3002", "conclusion": "STEC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u539f\u5219\u6027\u7684\u91c7\u6837\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff0c\u4f5c\u4e3a\u901a\u7528\u8bc4\u4f30\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u91c7\u6837\u884c\u4e3a\uff0c\u4f46\u4e0d\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002"}}
{"id": "2601.13975", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13975", "abs": "https://arxiv.org/abs/2601.13975", "authors": ["Marco Piccolo", "Qiwei Han", "Astrid van Toor", "Joachim Vanneste"], "title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains", "comment": "9 pages, 4 figures 8 tables", "summary": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u5317\u6781\u548c\u5927\u897f\u6d0b\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u5165\u4fb5\u7269\u79cd\u76d1\u6d4b\u5efa\u7acb\u4e86\u57fa\u7840\u68c0\u6d4b\u5c42\uff0c\u901a\u8fc7\u7edf\u4e00\u4fe1\u606f\u7ba1\u9053\u6807\u51c6\u5316\u5f02\u6784\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u573a\u666f\u7ed3\u6784\u56e0\u7d20\uff08\u800c\u975e\u89c6\u89c9\u9000\u5316\uff09\u662f\u8de8\u57df\u6027\u80fd\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u8fd0\u884c\u53ef\u884c\u6027\u3002", "motivation": "\u6d77\u6d0b\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u9700\u8981\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\uff0c\u4ee5\u652f\u6301\u4fdd\u62a4\u548c\u5165\u4fb5\u7269\u79cd\u7ba1\u7406\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6848\u5728\u8fc1\u79fb\u5230\u65b0\u5730\u70b9\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u660e\u663e\u7684\u90e8\u7f72\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u4fe1\u606f\u7ba1\u9053\u6807\u51c6\u5316\u5f02\u6784\u6570\u636e\u96c6\u4e3a\u53ef\u6bd4\u4fe1\u606f\u6d41\uff0c\u5728\u53d7\u63a7\u8de8\u57df\u534f\u8bae\u4e0b\u8bc4\u4f30\u56fa\u5b9a\u90e8\u7f72\u76f8\u5173\u68c0\u6d4b\u5668\uff0c\u5206\u6790\u7ed3\u6784\u56e0\u7d20\uff08\u573a\u666f\u7ec4\u6210\u3001\u7269\u4f53\u5bc6\u5ea6\u3001\u4e0a\u4e0b\u6587\u5197\u4f59\uff09\u4e0e\u89c6\u89c9\u9000\u5316\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7ed3\u6784\u56e0\u7d20\u6bd4\u89c6\u89c9\u9000\u5316\uff08\u5982\u6d51\u6d4a\u5ea6\uff09\u66f4\u80fd\u89e3\u91ca\u8de8\u57df\u6027\u80fd\u635f\u5931\uff0c\u7a00\u758f\u573a\u666f\u5f15\u53d1\u7279\u6709\u7684\"\u4e0a\u4e0b\u6587\u5d29\u6e83\"\u5931\u6548\u6a21\u5f0f\u3002\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u63a8\u7406\u53ef\u884c\u6027\uff0c\u8fd0\u884c\u4f18\u5316\u53ef\u5b9e\u73b0\u8fdc\u7a0b\u76d1\u6d4b\u7684\u5b9e\u9645\u91c7\u6837\u7387\u3002", "conclusion": "\u7814\u7a76\u91cd\u70b9\u5e94\u4ece\u56fe\u50cf\u589e\u5f3a\u8f6c\u5411\u7ed3\u6784\u611f\u77e5\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u4e00\u81f4\u7684\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6c11\u4e3b\u5316\u5de5\u5177\uff0c\u652f\u6301\u957f\u671f\u5165\u4fb5\u7269\u79cd\u76d1\u6d4b\u8ba1\u5212\u3002"}}
{"id": "2601.13976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.13976", "abs": "https://arxiv.org/abs/2601.13976", "authors": ["Jing Zuo", "Lingzhou Mu", "Fan Jiang", "Chengcheng Ma", "Mu Xu", "Yonggang Qi"], "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation", "comment": null, "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.", "AI": {"tldr": "FantasyVLN\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u60f3\u8c61\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u63a8\u7406\u611f\u77e5\u7684\u5b9e\u65f6\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u907f\u514d\u4e86\u663e\u5f0fCoT\u63a8\u7406\u7684token\u81a8\u80c0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684CoT\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u7eaf\u6587\u672cCoT\u7f3a\u4e4f\u7a7a\u95f4\u57fa\u7840\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u7a00\u758f\u6807\u6ce8\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u800c\u591a\u6a21\u6001CoT\u4f1a\u56e0\u751f\u6210\u60f3\u8c61\u7684\u89c6\u89c9\u89c2\u5bdf\u5bfc\u81f4\u4e25\u91cd\u7684token\u81a8\u80c0\uff0c\u4f7f\u5f97\u5b9e\u65f6\u5bfc\u822a\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51faFantasyVLN\u7edf\u4e00\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u81ea\u56de\u5f52\u5668\uff08VAR\uff09\u5c06\u60f3\u8c61\u7684\u89c6\u89c9token\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u5728CoT\u63a8\u7406\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u4ece\u6587\u672c\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001CoT\u6a21\u5f0f\u4e2d\u8054\u5408\u5b66\u4e60\uff1b3\uff09\u91c7\u7528\u7edf\u4e00\u7684\u591aCoT\u7b56\u7565\uff1b4\uff09\u63a8\u7406\u65f6\u76f4\u63a5\u8fdb\u884c\u6307\u4ee4\u5230\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u611f\u77e5\u7684\u8868\u5f81\u3002", "result": "\u5728LH-VLN\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a8\u7406\u611f\u77e5\u7684\u5b9e\u65f6\u5bfc\u822a\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u76f8\u6bd4\u663e\u5f0fCoT\u65b9\u6cd5\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FantasyVLN\u6846\u67b6\u4fdd\u7559\u4e86CoT\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u907f\u514d\u4e86\u663e\u5f0ftoken\u5f00\u9500\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u5177\u6709\u63a8\u7406\u80fd\u529b\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13986", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13986", "abs": "https://arxiv.org/abs/2601.13986", "authors": ["Zhang Wen", "Jiangwei Xie", "Dongdong Chen"], "title": "Equivariant Learning for Unsupervised Image Dehazing", "comment": "Technical report", "summary": "Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEID\u7684\u65e0\u76d1\u7763\u56fe\u50cf\u53bb\u96fe\u6846\u67b6\uff0c\u5229\u7528\u56fe\u50cf\u4fe1\u53f7\u7684\u5bf9\u79f0\u6027\u4ece\u539f\u59cb\u6709\u96fe\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u56fe\u50cf\uff0c\u65e0\u9700\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5148\u9a8c\u6216\u5927\u91cf\u65e0\u96fe\u5730\u9762\u771f\u503c\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u53bb\u96fe\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5148\u9a8c\u6216\u5927\u91cf\u65e0\u96fe\u5730\u9762\u771f\u503c\uff0c\u8fd9\u4e9b\u5728\u79d1\u5b66\u6210\u50cf\u4e2d\u83b7\u53d6\u6210\u672c\u9ad8\u6216\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8fd9\u4e9b\u6602\u8d35\u8d44\u6e90\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEID\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5236\u96fe\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u7cfb\u7edf\u7b49\u53d8\u6027\uff0c\u5229\u7528\u56fe\u50cf\u4fe1\u53f7\u7684\u5bf9\u79f0\u6027\u4ece\u539f\u59cb\u6709\u96fe\u56fe\u50cf\u4e2d\u6062\u590d\u6e05\u6670\u6a21\u5f0f\uff1b\u540c\u65f6\u63d0\u51fa\u5bf9\u6297\u5b66\u4e60\u7b56\u7565\u6765\u5efa\u6a21\u672a\u77e5\u7684\u96fe\u7269\u7406\u7279\u6027\u4ee5\u4fc3\u8fdbEID\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u79d1\u5b66\u56fe\u50cf\u53bb\u96fe\u57fa\u51c6\uff08\u7ec6\u80de\u663e\u5fae\u955c\u548c\u533b\u5b66\u5185\u7aa5\u955c\uff09\u4ee5\u53ca\u81ea\u7136\u56fe\u50cf\u53bb\u96fe\u5b9e\u9a8c\u4e2d\uff0cEID\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7b49\u53d8\u5b66\u4e60\u4e0e\u96fe\u7269\u7406\u5efa\u6a21\u76f8\u7ed3\u5408\uff0cEID\u6709\u671b\u5728\u79d1\u5b66\u6210\u50cf\u4e2d\u5b9e\u73b0\u66f4\u901a\u7528\u548c\u6709\u6548\u7684\u96fe\u53bb\u9664\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2601.14030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14030", "abs": "https://arxiv.org/abs/2601.14030", "authors": ["Samuel W. Remedios", "Zhangxing Bian", "Shuwen Wei", "Aaron Carass", "Jerry L. Prince", "Blake E. Dewey"], "title": "Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution", "comment": null, "summary": "Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\\times/8\\times/16\\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u6269\u6563\u6a21\u578b\u4ece\u5355\u56fe\u50cf\u9006\u95ee\u9898\u6269\u5c55\u5230\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387MRI\uff0c\u901a\u8fc7\u53ef\u5206\u79bb\u68af\u5ea6\u5206\u89e3\u5b9e\u73b0\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u65e0\u9700\u4fee\u6539\u6269\u6563\u6a21\u578b\u6216\u589e\u52a0\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u5355\u56fe\u50cf\u9006\u95ee\u9898\uff0c\u4f46MRI\u7b49\u6a21\u6001\u901a\u5e38\u9700\u8981\u91c7\u96c6\u591a\u4e2a\u4e92\u8865\u7684\u4f4e\u5206\u8fa8\u7387\u6d4b\u91cf\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u5c06DPS\u4f3c\u7136\u6821\u6b63\u63a8\u5e7f\u5230\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u5229\u7528\u5176\u53ef\u5206\u79bb\u68af\u5ea6\u5206\u89e3\u7279\u6027\uff0c\u5728\u591a\u4e2a\u72ec\u7acb\u6d4b\u91cf\u95f4\u5206\u89e3\u68af\u5ea6\uff0c\u65e0\u9700\u6784\u5efa\u8054\u5408\u7b97\u5b50\u6216\u4fee\u6539\u6269\u6563\u6a21\u578b\u3002\u63d0\u51fa\u4e86MISR\u7248\u672c\u7684DPS\u3001DMAP\u3001DPPS\u548cPnP/ADMM\u65b9\u6cd5\u3002", "result": "\u57284\u00d7/8\u00d7/16\u00d7\u5404\u5411\u5f02\u6027\u9000\u5316\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u5404\u5411\u5f02\u6027MRI\u4f53\u79ef\u7684\u6700\u5148\u8fdb\u8d85\u5206\u8fa8\u7387\uff0c\u80fd\u591f\u4ece\u5e38\u89c42D\u591a\u5207\u7247\u91c7\u96c6\u91cd\u5efa\u8fd1\u5404\u5411\u540c\u6027\u89e3\u5256\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6269\u6563\u6a21\u578b\u6269\u5c55\u5230\u591a\u56fe\u50cfMRI\u8d85\u5206\u8fa8\u7387\uff0c\u901a\u8fc7\u53ef\u5206\u79bb\u68af\u5ea6\u5206\u89e3\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u56fe\u50cf\u91cd\u5efa\uff0c\u4e3a\u4e34\u5e8a\u5e38\u89c42D\u91c7\u96c6\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2601.14037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14037", "abs": "https://arxiv.org/abs/2601.14037", "authors": ["Kumar Ashutosh", "XuDong Wang", "Xi Yin", "Kristen Grauman", "Adam Polyak", "Ishan Misra", "Rohit Girdhar"], "title": "Human detectors are surprisingly powerful reward models", "comment": "Technical report", "summary": "Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.", "AI": {"tldr": "\u63d0\u51faHuDA\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u4f53\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u65f6\u5e8f\u63d0\u793a\u5bf9\u9f50\u5206\u6570\u6765\u91cf\u5316\u5e76\u6539\u8fdb\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u4e13\u95e8\u5fae\u8c03\u7684\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u5e8f\u8fde\u8d2f\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5408\u6210\u590d\u6742\u975e\u521a\u6027\u8fd0\u52a8\uff08\u5982\u4f53\u80b2\u3001\u821e\u8e48\u7b49\u4eba\u4f53\u52a8\u4f5c\uff09\u65f6\u4ecd\u5b58\u5728\u80a2\u4f53\u7f3a\u5931/\u591a\u4f59\u3001\u59ff\u6001\u626d\u66f2\u3001\u7269\u7406\u4e0d\u5408\u7406\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faHuDA\u5956\u52b1\u6a21\u578b\uff0c\u96c6\u6210\u4eba\u4f53\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff08\u8bc4\u4f30\u5916\u89c2\u8d28\u91cf\uff09\u548c\u65f6\u5e8f\u63d0\u793a\u5bf9\u9f50\u5206\u6570\uff08\u6355\u6349\u8fd0\u52a8\u771f\u5b9e\u6027\uff09\uff0c\u4f7f\u7528\u73b0\u6210\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff1b\u91c7\u7528Group Reward Policy Optimization (GRPO)\u5bf9\u89c6\u9891\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u4f18\u5316\u3002", "result": "HuDA\u5728\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u4e0a\u663e\u8457\u63d0\u5347\u89c6\u9891\u8d28\u91cf\uff0c\u8d85\u8d8aWan 2.1\u7b49SOTA\u6a21\u578b\uff08\u80dc\u738773%\uff09\uff1b\u4e0d\u4ec5\u6539\u5584\u4eba\u4f53\u751f\u6210\uff0c\u8fd8\u80fd\u63d0\u5347\u52a8\u7269\u89c6\u9891\u548c\u4eba\u7269-\u7269\u4f53\u4ea4\u4e92\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "HuDA\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u591f\u663e\u8457\u6539\u8fdb\u751f\u6210\u89c6\u9891\u4e2d\u7684\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\uff0c\u4e14\u5177\u6709\u6269\u5c55\u5230\u5176\u4ed6\u975e\u521a\u6027\u8fd0\u52a8\u751f\u6210\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.14038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14038", "abs": "https://arxiv.org/abs/2601.14038", "authors": ["Alexandre Justo Miro", "Ludvig af Klinteberg", "Bogdan Timus", "Aron Asefaw", "Ajinkya Khoche", "Thomas Gustafsson", "Sina Sharif Mansouri", "Masoud Daneshtalab"], "title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving", "comment": "Accepted to The IEEE/CVF Winter Conference on Applications of Computer Vision 2026", "summary": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u6821\u6b63\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u56e0\u4f20\u611f\u5668\u626b\u63cf\u6a21\u5f0f\u5bfc\u81f4\u76843D\u8fb9\u754c\u6846\u6807\u6ce8\u9519\u8bef\uff0c\u63d0\u9ad8\u6807\u6ce8\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e2d\u76843D\u8fb9\u754c\u6846\u6807\u6ce8\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\uff0c\u56e0\u4e3a\u7269\u4f53\u5728\u4e0d\u540c\u65f6\u95f4\u6233\u88ab\u89c2\u6d4b\u5230\u4e0d\u540c\u4f4d\u7f6e\uff0c\u73b0\u6709\u6807\u6ce8\u65b9\u6cd5\u672a\u6b63\u786e\u5904\u7406\u8fd9\u4e00\u73b0\u8c61\uff0c\u5bfc\u81f4\u6807\u6ce8\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u53ef\u884c\u7684\u8f68\u8ff9\u4f30\u8ba1\uff0c\u4f7f\u6807\u6ce8\u4e0e\u4f20\u611f\u5668\u6570\u636e\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e00\u81f4\uff0c\u9996\u6b21\u5b9a\u4e49\u4e86\u8be5\u95ee\u9898\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728Argoverse 2\u3001MAN TruckScenes\u548c\u4e13\u6709\u6570\u636e\u96c6\u4e0a\uff0c\u6807\u6ce8\u8d28\u91cf\u63d0\u5347\u8d85\u8fc717%\uff0c\u53d1\u73b0\u539f\u59cb\u6807\u6ce8\u504f\u79fb\u6700\u5927\u8fbe2.5\u7c73\uff0c\u52a8\u6001\u7269\u4f53\u53d7\u5f71\u54cd\u6700\u4e25\u91cd\uff0c\u6807\u6ce8\u8bef\u5dee\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f71\u54cd\u8d85\u8fc7SOTA\u65b9\u6cd5\u7684\u6539\u8fdb\u5e45\u5ea6\u3002", "conclusion": "\u51c6\u786e\u7684\u6807\u6ce8\u5bf9\u4e8e\u6b63\u786e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6821\u6b63\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.14042", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14042", "abs": "https://arxiv.org/abs/2601.14042", "authors": ["Jiaze Li", "Haoran Xu", "Wanyi Wu", "Changwei Wang", "Shuaiguang Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Youyang Qu", "Longxiang Gao", "Xudong Yang", "Lumin Xing"], "title": "Federated Balanced Learning", "comment": null, "summary": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u5e73\u8861\u5b66\u4e60(FBL)\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u6837\u672c\u5e73\u8861\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4f7f\u7528\u77e5\u8bc6\u586b\u5145\u548c\u77e5\u8bc6\u91c7\u6837\u5b9e\u73b0\u6837\u672c\u5e73\u8861\uff0c\u5e76\u8bbe\u8ba1\u4e86\u77e5\u8bc6\u5bf9\u9f50\u548c\u77e5\u8bc6\u4e22\u5f03\u7b56\u7565\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8bbe\u7f6e\u4e2d\uff0c\u5168\u5c40\u6a21\u578b\u4f1a\u51fa\u73b0\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u635f\u5931\u51fd\u6570\u6216\u68af\u5ea6\u6765\u7ea0\u6b63\u5df2\u7ecf\u504f\u79bb\u7684\u5168\u5c40\u6a21\u578b\uff0c\u4f46\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u6837\u672c\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u5e73\u8861\u5b66\u4e60(FBL)\uff0c\u5728\u5ba2\u6237\u7aef\u4fa7\u901a\u8fc7\u77e5\u8bc6\u586b\u5145\u548c\u77e5\u8bc6\u91c7\u6837\u5b9e\u73b0\u6837\u672c\u5e73\u8861\uff0c\u4f7f\u7528\u8fb9\u7f18\u4fa7\u751f\u6210\u6a21\u578b\u5728\u56fa\u5b9a\u6570\u636e\u6837\u672c\u6570\u91cf\u9650\u5236\u4e0b\u5e73\u8861\u6570\u636e\u3002\u8bbe\u8ba1\u4e86\u77e5\u8bc6\u5bf9\u9f50\u7b56\u7565\u6765\u5f25\u5408\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u53ca\u77e5\u8bc6\u4e22\u5f03\u7b56\u7565\u8fdb\u884c\u6b63\u5219\u5316\u3002\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u771f\u5b9e\u590d\u6742\u573a\u666f\uff0c\u5141\u8bb8\u4e0d\u540c\u5ba2\u6237\u7aef\u91c7\u7528\u4e0d\u540c\u65b9\u6cd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u5ba2\u6237\u7aef\u4fa7\u7684\u89d2\u8272\uff0c\u4ece\u6e90\u5934\u4e0a\u901a\u8fc7\u6837\u672c\u5e73\u8861\u9884\u9632\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u7684FBL\u65b9\u6cd5\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.14044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14044", "abs": "https://arxiv.org/abs/2601.14044", "authors": ["Kaiyu Wu", "Pucheng Han", "Hualong Zhang", "Naigeng Wu", "Keze Wang"], "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology", "comment": null, "summary": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWeatherQA\u6c14\u8c61\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u548cLoCo-RFT\u65b9\u6cd5\uff0c\u89e3\u51b3VLMs\u5728\u6c14\u8c61\u9886\u57df\u7684\u9886\u57df\u9e3f\u6c9f\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u5177\u6709\u903b\u8f91\u5fe0\u5b9e\u6027\u7684\u6c14\u8c61\u63a8\u7406VLM\u6a21\u578bWeather-R1\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u8c61\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u9886\u57df\u9e3f\u6c9f\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\u9e3f\u6c9f\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u4e3b\u6d41\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u81ea\u76f8\u77db\u76fe\u63a8\u7406\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u7684\u6c14\u8c61\u9886\u57df\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002", "method": "\u6784\u5efaWeatherQA\u6c14\u8c61\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\uff1b\u63d0\u51fa\u903b\u8f91\u4e00\u81f4\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u903b\u8f91\u4e00\u81f4\u6027\u5956\u52b1\u6765\u89e3\u51b3\u81ea\u76f8\u77db\u76fe\u63a8\u7406\u95ee\u9898\uff1b\u5f00\u53d1Weather-R1\u6a21\u578b\u3002", "result": "Weather-R1\u5728WeatherQA\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u53479.8\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\uff0c\u751a\u81f3\u8d85\u8d8a\u539f\u59cbQwen2.5-VL-32B\u6a21\u578b\u3002", "conclusion": "LoCo-RFT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76f8\u77db\u76fe\u63a8\u7406\u95ee\u9898\uff0cWeather-R1\u5728\u6c14\u8c61\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2601.14052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14052", "abs": "https://arxiv.org/abs/2601.14052", "authors": ["Haoran Xu", "Yanlin Liu", "Zizhao Tong", "Jiaze Li", "Kexue Fu", "Yuyang Zhang", "Longxiang Gao", "Shuaiguang Li", "Xingyu Li", "Yanran Xu", "Changwei Wang"], "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model", "comment": null, "summary": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.", "AI": {"tldr": "\u63d0\u51faMM-OOD\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672cOOD\u68c0\u6d4b\uff0c\u9488\u5bf9\u8fd1OOD\u548c\u8fdcOOD\u4efb\u52a1\u5206\u522b\u8bbe\u8ba1\u4e0d\u540c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cOOD\u68c0\u6d4b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5ffd\u89c6\u4e86\u56fe\u50cf\u7a7a\u95f4\u68c0\u6d4b\u7684\u56fa\u6709\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u6765\u540c\u65f6\u5904\u7406\u8fd1OOD\u548c\u8fdcOOD\u4efb\u52a1\u3002", "method": "\u63d0\u51faMM-OOD\u7ba1\u9053\uff1a1) \u8fd1OOD\u4efb\u52a1\uff1a\u76f4\u63a5\u5c06ID\u56fe\u50cf\u548c\u6587\u672c\u63d0\u793a\u8f93\u5165MLLMs\u8bc6\u522b\u5f02\u5e38\uff1b2) \u8fdcOOD\u4efb\u52a1\uff1a\u91c7\u7528\u8349\u56fe-\u751f\u6210-\u9610\u8ff0\u6846\u67b6\uff0c\u5148\u7528\u6587\u672c\u63d0\u793a\u8349\u56fe\u5f02\u5e38\uff0c\u751f\u6210\u89c6\u89c9OOD\u6837\u672c\uff0c\u518d\u7528\u591a\u6a21\u6001\u63d0\u793a\u8fdb\u884c\u9610\u8ff0\u3002", "result": "\u5728Food-101\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u5728ImageNet-1K\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MM-OOD\u65b9\u6cd5\u901a\u8fc7\u5229\u7528MLLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u548c\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u96f6\u6837\u672cOOD\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8fd1OOD\u548c\u8fdcOOD\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.14060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14060", "abs": "https://arxiv.org/abs/2601.14060", "authors": ["Yongcong Ye", "Kai Zhang", "Yanghai Zhang", "Enhong Chen", "Longfei Li", "Jun Zhou"], "title": "Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration", "comment": null, "summary": "Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.", "AI": {"tldr": "\u63d0\u51faCVSI\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e92\u8865\u7684\u89c6\u89c9-\u8bed\u4e49\u96c6\u6210\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709ZS-CIR\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u53d8\u5316\u548c\u6709\u6548\u6574\u5408\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u8f6c\u6362\u6216LLM\u751f\u6210\u63cf\u8ff0\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e92\u8865\u4fe1\u606f", "method": "CVSI\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u89c6\u89c9\u4fe1\u606f\u63d0\u53d6\uff08\u5168\u5c40\u7279\u5f81+\u4f2a\u4ee4\u724c+\u53ef\u80fd\u6dfb\u52a0\u7684\u5bf9\u8c61\uff09\uff1b2) \u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\uff08\u591a\u63cf\u8ff0\u751f\u6210+LLM\u4fee\u6539+\u53ef\u80fd\u6dfb\u52a0\u7684\u5bf9\u8c61\uff09\uff1b3) \u4e92\u8865\u4fe1\u606f\u68c0\u7d22\uff08\u6574\u5408\u67e5\u8be2\u548c\u6570\u636e\u5e93\u56fe\u50cf\u4fe1\u606f\uff09", "result": "\u5728CIRR\u3001CIRCO\u548cFashionIQ\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCVSI\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "CVSI\u901a\u8fc7\u4e92\u8865\u7684\u89c6\u89c9-\u8bed\u4e49\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86ZS-CIR\u4e2d\u7684\u7ec6\u7c92\u5ea6\u53d8\u5316\u6355\u6349\u95ee\u9898\uff0c\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u90fd\u80fd\u9ad8\u6548\u5904\u7406\u68c0\u7d22\u67e5\u8be2"}}
{"id": "2601.14066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14066", "abs": "https://arxiv.org/abs/2601.14066", "authors": ["Hendrik M\u00f6ller", "Hanna Schoen", "Robert Graf", "Matan Atad", "Nathan Molinier", "Anjany Sekuboyina", "Bettina K. Budai", "Fabian Bamberg", "Steffen Ringhof", "Christopher Schlett", "Tobias Pischon", "Thoralf Niendorf", "Josua A. Decker", "Marc-Andr\u00e9 Weber", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences", "comment": null, "summary": "The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing \"Vertebra Identification with Anomaly Handling\" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 M\u00f6ller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.", "AI": {"tldr": "VERIDAH\u662f\u4e00\u79cd\u65b0\u7684\u690e\u9aa8\u6807\u8bb0\u7b97\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u548c\u5904\u7406\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff0c\u5728T2w\u548cCT\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff08\u598211\u621613\u4e2a\u80f8\u690e\u30014\u62166\u4e2a\u8170\u690e\uff09\u5177\u6709\u4e34\u5e8a\u610f\u4e49\uff0c\u4f46\u4e34\u5e8a\u62a5\u544a\u4e2d\u5f88\u5c11\u63cf\u8ff0\uff0c\u4e14\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u6807\u8bb0\u8fd9\u4e9b\u5f02\u5e38\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faVERIDAH\u7b97\u6cd5\uff0c\u57fa\u4e8e\u591a\u4e2a\u5206\u7c7b\u5934\u7ed3\u5408\u52a0\u6743\u690e\u9aa8\u5e8f\u5217\u9884\u6d4b\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u89c6\u91ce\u56fe\u50cf\u4e2d\u7684\u690e\u9aa8\u6807\u8bb0\u548c\u8ba1\u6570\u5f02\u5e38\u3002", "result": "\u5728T2w TSE\u77e2\u72b6\u4f4d\u56fe\u50cf\u4e0a\u6b63\u786e\u6807\u8bb0\u6240\u6709\u690e\u9aa8\u7684\u6bd4\u4f8b\u4ece94.24%\u63d0\u5347\u81f398.30%\uff0cCT\u56fe\u50cf\u4e0a\u4ece77.26%\u63d0\u5347\u81f399.18%\u3002\u80f8\u690e\u8ba1\u6570\u5f02\u5e38\u8bc6\u522b\u51c6\u786e\u7387\u5728T2w\u548cCT\u4e0a\u5206\u522b\u4e3a87.80%\u548c96.30%\uff0c\u8170\u690e\u5f02\u5e38\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u4e3a94.48%\u548c97.22%\u3002", "conclusion": "VERIDAH\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u81ea\u52a8\u8bc6\u522b\u810a\u67f1\u8ba1\u6570\u5f02\u5e38\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.14079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14079", "abs": "https://arxiv.org/abs/2601.14079", "authors": ["Paul Walker", "James A. D. Gardner", "Andreea Ardelean", "William A. P. Smith", "Bernhard Egger"], "title": "VENI: Variational Encoder for Natural Illumination", "comment": "Project Repo - https://github.com/paul-pw/veni Project page - https://paul-pw.github.io/veni", "summary": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65cb\u8f6c\u7b49\u53d8\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5728\u7403\u9762\u4e0a\u5efa\u6a21\u81ea\u7136\u5149\u7167\uff0c\u65e0\u97002D\u6295\u5f71\uff0c\u901a\u8fc7VN-ViT\u7f16\u7801\u5668\u548c\u65cb\u8f6c\u7b49\u53d8\u6761\u4ef6\u795e\u7ecf\u573a\u89e3\u7801\u5668\u5b9e\u73b0SO(2)\u7b49\u53d8\u6027\uff0c\u63d0\u4f9b\u66f4\u5e73\u6ed1\u7684\u6f5c\u5728\u7a7a\u95f4\u63d2\u503c\u3002", "motivation": "\u9006\u6e32\u67d3\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u4e86\u5149\u7167\u73af\u5883\u7684\u7403\u9762\u548c\u65cb\u8f6c\u7b49\u53d8\u7279\u6027\uff0c\u8981\u4e48\u6ca1\u6709\u63d0\u4f9b\u826f\u597d\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u73af\u5883\u56feSO(2)\u7b49\u53d8\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Vector Neuron Vision Transformer (VN-ViT)\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u65cb\u8f6c\u7b49\u53d8\u6761\u4ef6\u795e\u7ecf\u573a\u4f5c\u4e3a\u89e3\u7801\u5668\u3002\u5728\u7f16\u7801\u5668\u4e2d\u901a\u8fc7\u65b0\u9896\u7684SO(2)\u7b49\u53d8\u5168\u8fde\u63a5\u5c42\u5c06\u7b49\u53d8\u6027\u4eceSO(3)\u964d\u81f3SO(2)\uff0c\u8fd9\u662fVector Neurons\u7684\u6269\u5c55\u3002", "result": "\u63d0\u51fa\u7684SO(2)\u7b49\u53d8\u5168\u8fde\u63a5\u5c42\u5728SO(2)\u7b49\u53d8\u6a21\u578b\u4e2d\u4f18\u4e8e\u6807\u51c6Vector Neurons\u3002\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u6f5c\u5728\u7a7a\u95f4\u63d2\u503c\uff0c\u63d0\u4f9b\u4e86\u66f4\u826f\u597d\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5efa\u6a21\u4e86\u7403\u9762\u4e0a\u7684\u81ea\u7136\u5149\u7167\uff0c\u4fdd\u6301\u4e86SO(2)\u7b49\u53d8\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65cb\u8f6c\u7b49\u53d8\u6027\u548c\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u9006\u6e32\u67d3\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5149\u7167\u5148\u9a8c\u3002"}}
{"id": "2601.14101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14101", "abs": "https://arxiv.org/abs/2601.14101", "authors": ["Emily Kim", "Allen Wu", "Jessica Hodgins"], "title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition", "comment": null, "summary": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.\n  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.\n  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u8bfe\u7a0b\u5b66\u4e60\u5728\u8de8\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u9e1f\u77b0\u6570\u636e\u548c\u771f\u5b9e\u5730\u9762\u6570\u636e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5730\u9762\u89c6\u89d2\u6570\u636e\u8bad\u7ec3\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u9e1f\u77b0\u89c6\u89d2\u7b49\u4e0d\u540c\u9886\u57df\u3002\u9700\u8981\u63a2\u7d22\u4e0d\u4f9d\u8d56\u771f\u5b9e\u9e1f\u77b0\u6570\u636e\u5c31\u80fd\u63d0\u5347\u8de8\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a1\uff09\u4e24\u9636\u6bb5\u76f4\u63a5\u5fae\u8c03\u6cd5\uff1a\u5148\u5728\u5408\u6210\u9e1f\u77b0\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u518d\u5728\u771f\u5b9e\u5730\u9762\u6570\u636e\u4e0a\u5fae\u8c03\uff1b2\uff09\u591a\u9636\u6bb5\u6e10\u8fdb\u6cd5\uff1a\u5206\u591a\u4e2a\u9636\u6bb5\u9010\u6b65\u6269\u5c55\u6570\u636e\u96c6\u540e\u518d\u5fae\u8c03\u3002\u4f7f\u7528SlowFast\uff08CNN\uff09\u548cMViTv2\uff08Transformer\uff09\u67b6\u6784\u5728REMAG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u7ed3\u5408\u4e24\u79cd\u57df\u5916\u6570\u636e\u6e90\u660e\u663e\u4f18\u4e8e\u5355\u57df\u8bad\u7ec3\u3002\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u5728\u4fdd\u6301top-1\u51c6\u786e\u7387\uff08\u76f8\u5dee3%\u4ee5\u5185\uff09\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff1a\u4e24\u9636\u6bb5\u6cd5\u4f7fSlowFast\u51cf\u5c1137%\u8fed\u4ee3\u3001MViTv2\u51cf\u5c1130%\uff1b\u591a\u9636\u6bb5\u6cd5\u8fdb\u4e00\u6b65\u51cf\u5c119%\uff08SlowFast\uff09\u548c30%\uff08MViTv2\uff09\u8fed\u4ee3\u3002", "conclusion": "\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u80fd\u5728\u8de8\u89c6\u89d2\u52a8\u4f5c\u8bc6\u522b\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u4e0d\u4f9d\u8d56\u771f\u5b9e\u76ee\u6807\u57df\u6570\u636e\u7684\u8de8\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14103", "abs": "https://arxiv.org/abs/2601.14103", "authors": ["Xiaolu Liu", "Yicong Li", "Qiyuan He", "Jiayin Zhu", "Wei Ji", "Angela Yao", "Jianke Zhu"], "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing", "comment": "22 pages, 12 figures", "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.", "AI": {"tldr": "Interp3D\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u751f\u6210\u5148\u9a8c\u7684\u7eb9\u74063D\u53d8\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5bf9\u9f50\u5b9e\u73b0\u51e0\u4f55\u4fdd\u771f\u548c\u7eb9\u7406\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u5904\u7406\u51e0\u4f55\u5f62\u72b6\u5ffd\u7565\u7eb9\u7406\uff0c\u8981\u4e48\u5c062D\u63d2\u503c\u6269\u5c55\u52303D\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u3001\u7ed3\u6784\u9519\u4f4d\u548c\u7eb9\u7406\u6a21\u7cca\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u7eb9\u7406\u5bf9\u9f50\u548c\u9c81\u68d2\u6027", "method": "\u63d0\u51faInterp3D\u6846\u67b6\uff0c\u91c7\u7528\u6e10\u8fdb\u5bf9\u9f50\u539f\u5219\uff1a1\uff09\u6761\u4ef6\u7a7a\u95f4\u8bed\u4e49\u5bf9\u9f50\u63d2\u503c\uff1b2\uff09SLAT\u5f15\u5bfc\u7684\u7ed3\u6784\u63d2\u503c\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff1b3\uff09\u7ec6\u7c92\u5ea6\u7eb9\u7406\u878d\u5408\u4f20\u9012\u5916\u89c2\u7ec6\u8282", "result": "\u6784\u5efaInterp3DData\u6570\u636e\u96c6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u7c7b\u7814\u7a76\u5747\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u3001\u8fc7\u6e21\u5e73\u6ed1\u6027\u548c\u5408\u7406\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5", "conclusion": "Interp3D\u901a\u8fc7\u8054\u5408\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u5e73\u6ed1\u53ef\u4fe1\u7684\u7eb9\u74063D\u53d8\u5f62\uff0c\u57283D\u751f\u6210\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2601.14111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14111", "abs": "https://arxiv.org/abs/2601.14111", "authors": ["Jiaying Wu", "Can Gao", "Jinglu Hu", "Hui Li", "Xiaofeng Cao", "Jingcai Guo"], "title": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning", "comment": null, "summary": "Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D", "AI": {"tldr": "PMCE\uff1a\u4e00\u79cd\u6982\u7387\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8bed\u4e49\u548c\u5b57\u5e55\u5f15\u5bfc\u589e\u5f3a\u6765\u6539\u8fdb\u539f\u578b\u4f30\u8ba1\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u4ece\u5c11\u91cf\u6570\u636e\u4f30\u8ba1\u7684\u539f\u578b\u901a\u5e38\u5b58\u5728\u504f\u5dee\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u73b0\u6709\u7684\u8bed\u4e49\u65b9\u6cd5\u4e3b\u8981\u5e94\u7528\u4e8e\u652f\u6301\u96c6\u4fa7\uff0c\u800c\u67e5\u8be2\u8868\u793a\u4fdd\u6301\u4e0d\u53d8\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u6784\u5efa\u975e\u53c2\u6570\u77e5\u8bc6\u5e93\u5b58\u50a8\u57fa\u7c7b\u7684\u89c6\u89c9\u7edf\u8ba1\u548cCLIP\u7f16\u7801\u7684\u7c7b\u522b\u540d\u79f0\u5d4c\u5165\uff1b\u5728\u5143\u6d4b\u8bd5\u65f6\uff0c\u57fa\u4e8e\u7c7b\u522b\u540d\u79f0\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u6700\u76f8\u5173\u7684\u57fa\u7c7b\uff0c\u5c06\u7edf\u8ba1\u4fe1\u606f\u805a\u5408\u4e3a\u7c7b\u522b\u7279\u5b9a\u5148\u9a8c\uff0c\u901a\u8fc7MAP\u66f4\u65b0\u4e0e\u652f\u6301\u96c6\u539f\u578b\u878d\u5408\uff1b\u540c\u65f6\u4f7f\u7528\u51bb\u7ed3\u7684BLIP\u5b57\u5e55\u751f\u6210\u5668\u63d0\u4f9b\u65e0\u6807\u7b7e\u7684\u5b9e\u4f8b\u7ea7\u56fe\u50cf\u63cf\u8ff0\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u589e\u5f3a\u5668\u5728\u57fa\u7c7b\u4e0a\u8bad\u7ec3\uff0c\u5728\u5f52\u7eb3\u534f\u8bae\u4e0b\u4f18\u5316\u652f\u6301\u539f\u578b\u548c\u67e5\u8be2\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7a33\u5b9a\u566a\u58f0\u5b57\u5e55\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPMCE\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MiniImageNet\u76841-shot\u8bbe\u7f6e\u4e2d\uff0c\u6bd4\u6700\u5f3a\u7684\u8bed\u4e49\u7ade\u4e89\u5bf9\u624b\u5b9e\u73b0\u4e867.71%\u7684\u7edd\u5bf9\u589e\u76ca\u63d0\u5347\u3002", "conclusion": "PMCE\u901a\u8fc7\u7ed3\u5408\u591a\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u548c\u5b57\u5e55\u5f15\u5bfc\u589e\u5f3a\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u539f\u578b\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c11\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.14130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14130", "abs": "https://arxiv.org/abs/2601.14130", "authors": ["Till Aczel", "David F. Jenny", "Simon B\u00fchrer", "Andreas Plesner", "Antonio Di Maio", "Roger Wattenhofer"], "title": "GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression", "comment": null, "summary": "Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.", "AI": {"tldr": "GIC-DLC\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u7070\u5ea6\u56fe\u50cf\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u8bad\u7ec3\u67e5\u627e\u8868\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u4e0e\u5e03\u5c14\u8fd0\u7b97\u7684\u6548\u7387\u76f8\u7ed3\u5408\uff0c\u5728\u538b\u7f29\u6548\u7387\u4e0a\u8d85\u8d8a\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u795e\u7ecf\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u867d\u7136\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff08\u5982PNG\u6216JPEG-XL\uff09\u6709\u66f4\u9ad8\u7684\u538b\u7f29\u6bd4\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5728\u667a\u80fd\u624b\u673a\u3001\u76f8\u673a\u3001\u65e0\u4eba\u673a\u7b49\u80fd\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faGIC-DLC\uff08\u7070\u5ea6\u56fe\u50cf\u538b\u7f29\u4e0e\u53ef\u5fae\u903b\u8f91\u7535\u8def\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u8bad\u7ec3\u67e5\u627e\u8868\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u4e0e\u5e03\u5c14\u8fd0\u7b97\u7684\u6548\u7387\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u7070\u5ea6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGIC-DLC\u5728\u538b\u7f29\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0c\u540c\u65f6\u5141\u8bb8\u5927\u5e45\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "conclusion": "\u5b66\u4e60\u578b\u538b\u7f29\u53ef\u4ee5\u662f\u786c\u4ef6\u53cb\u597d\u7684\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u4f4e\u529f\u8017\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2601.14161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14161", "abs": "https://arxiv.org/abs/2601.14161", "authors": ["Yitong Dong", "Qi Zhang", "Minchao Jiang", "Zhiqiang Wu", "Qingnan Fan", "Ying Feng", "Huaqi Zhang", "Hujun Bao", "Guofeng Zhang"], "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion", "comment": null, "summary": "We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u56fe\u50cf\u8fdb\u884c\u9ad8\u4fdd\u771f\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u901a\u8fc7\u53cc\u57df\u7ec6\u8282\u611f\u77e5\u6a21\u5757\u548c\u7279\u5f81\u5f15\u5bfc\u6269\u6563\u7f51\u7edc\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c3D\u4e00\u81f4\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eViT\u76843D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u53d7\u8ba1\u7b97\u6210\u672c\u9650\u5236\u53ea\u80fd\u5904\u7406\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u4e14\u73b0\u6709\u751f\u6210\u589e\u5f3a\u65b9\u6cd5\u5f80\u5f80\u662f3D\u65e0\u5173\u7684\uff0c\u5bfc\u81f4\u8de8\u89c6\u89d2\u7ed3\u6784\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u533a\u57df\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u57df\u7ec6\u8282\u611f\u77e5\u6a21\u5757\uff0c\u4f7f\u7cfb\u7edf\u80fd\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u800c\u4e0d\u53d7ViT\u4e3b\u5e72\u9650\u5236\uff1b\u5f00\u53d1\u4e86\u7279\u5f81\u5f15\u5bfc\u6269\u6563\u7f51\u7edc\uff0c\u5728\u6062\u590d\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u9ad8\u9891\u7ec6\u8282\uff1b\u5f15\u5165\u4e86\u7edf\u4e00\u8bad\u7ec3\u7b56\u7565\uff0c\u8054\u5408\u4f18\u5316ViT\u51e0\u4f55\u4e3b\u5e72\u548c\u6269\u6563\u7ec6\u5316\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5353\u8d8a\u7684\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c3D\u4e00\u81f4\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u751f\u6210\u7ec6\u5316\uff0c\u5b9e\u73b0\u4e86\u4ece\u7a00\u758f\u56fe\u50cf\u7684\u9ad8\u4fdd\u771f\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u5728\u4fdd\u63013D\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7ec6\u8282\u8d28\u91cf\u3002"}}
{"id": "2601.14165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14165", "abs": "https://arxiv.org/abs/2601.14165", "authors": ["Zhenghong Li", "Wensheng Cheng", "Congwu Du", "Yingtian Pan", "Zhaozheng Yin", "Haibin Ling"], "title": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction", "comment": "17 pages, 11 figures", "summary": "Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.", "AI": {"tldr": "\u63d0\u51faASBA\u7f51\u7edc\uff0c\u901a\u8fc7A\u7ebfROI\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548cB\u7ebf\u76f8\u4f4d\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u9ad8\u5ea6\u7a00\u758f\u91c7\u6837\u7684\u539f\u59cbA\u626b\u63cf\u91cd\u5efaODT\u56fe\u50cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5149\u5b66\u591a\u666e\u52d2\u5c42\u6790\u6210\u50cf\uff08ODT\uff09\u4f9d\u8d56\u5bc6\u96c6\u91c7\u6837\uff0c\u5bfc\u81f4\u626b\u63cf\u65f6\u95f4\u957f\u3001\u5b58\u50a8\u9700\u6c42\u5927\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u5feb\u901f\u8840\u6d41\u52a8\u6001\u3002\u73b0\u6709\u7a00\u758f\u91c7\u6837\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u91c7\u6837\u7387\u4fdd\u5b88\u4e14\u5bf9\u8840\u6d41\u548c\u80cc\u666f\u4fe1\u53f7\u91c7\u7528\u7edf\u4e00\u5efa\u6a21\u3002", "method": "\u63d0\u51faASBA\u7f51\u7edc\uff1a1\uff09A\u7ebfROI\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u63d0\u53d6A\u7ebf\u7a00\u758f\u5206\u5e03\u7684\u8840\u6d41\u7279\u5f81\uff1b2\uff09B\u7ebf\u76f8\u4f4d\u6ce8\u610f\u529b\u673a\u5236\u57fa\u4e8e\u76f8\u4f4d\u5dee\u6355\u83b7B\u7ebf\u957f\u7a0b\u8840\u6d41\u4fe1\u53f7\uff1b3\uff09\u8840\u6d41\u611f\u77e5\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5148\u51c6\u786e\u91cd\u5efa\u8840\u6d41\u4fe1\u53f7\u3002", "result": "\u5728\u771f\u5b9e\u52a8\u7269\u6570\u636e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "ASBA\u7f51\u7edc\u80fd\u591f\u4ece\u9ad8\u5ea6\u7a00\u758f\u91c7\u6837\u7684\u539f\u59cbA\u626b\u63cf\u6709\u6548\u91cd\u5efaODT\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u5f53\u524dODT\u6210\u50cf\u7684\u91c7\u6837\u5bc6\u5ea6\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2601.14180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14180", "abs": "https://arxiv.org/abs/2601.14180", "authors": ["Yichao Liu", "Yueyang Teng", "Junwen Guo"], "title": "Progressive self-supervised blind-spot denoising method for LDCT denoising", "comment": null, "summary": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528\u4f4e\u5242\u91cfCT\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u76f2\u70b9\u53bb\u566a\u673a\u5236\u548c\u9ad8\u65af\u566a\u58f0\u6b63\u5219\u5316\uff0c\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u83b7\u53d6\u914d\u5bf9\u6b63\u5e38\u5242\u91cfCT\u6570\u636e\u56f0\u96be\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u4f4e\u5242\u91cfCT\u53bb\u566a\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u76f2\u70b9\u53bb\u566a\u673a\u5236\uff0c\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u9010\u6b65\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u53bb\u566a\u5b66\u4e60\uff1b\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0\u4f5c\u4e3a\u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\uff1b\u4ec5\u4f7f\u7528\u4f4e\u5242\u91cfCT\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Mayo\u4f4e\u5242\u91cfCT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e\u591a\u4e2a\u4ee3\u8868\u6027\u76d1\u7763\u53bb\u566a\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5242\u91cfCT\u53bb\u566a\u4e2d\u914d\u5bf9\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u76f2\u70b9\u53bb\u566a\u548c\u566a\u58f0\u6b63\u5219\u5316\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u53bb\u566a\u6027\u80fd\u3002"}}
{"id": "2601.14188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14188", "abs": "https://arxiv.org/abs/2601.14188", "authors": ["Liang Shi", "Wei Li", "Kevin M Beussman", "Lin Chen", "Yun Fu"], "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models", "comment": null, "summary": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.", "AI": {"tldr": "\u63d0\u51faIIR-VLM\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4e13\u5bb6\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u589e\u5f3aVLM\u7684\u5b9e\u4f8b\u8bc6\u522b\u80fd\u529b\uff0c\u5b9e\u73b0\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fdc\u4f4e\u4e8e\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86VLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bc6\u522b\u719f\u6089\u4eba\u7269\u548c\u7269\u4f53\u7684\u573a\u666f\u4e2d", "method": "IIR-VLM\u65b9\u6cd5\uff1a1\uff09\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u4e13\u5bb6\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u63d0\u4f9b\u4e13\u95e8\u7684\u7279\u5f81\u8868\u793a\uff1b2\uff09\u4f7fVLM\u80fd\u591f\u901a\u8fc7\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b0\u5b9e\u4f8b\uff1b3\uff09\u5229\u7528\u5b66\u5230\u7684\u77e5\u8bc6\u8fdb\u884c\u5b9e\u4f8b\u611f\u77e5\u7684\u89c6\u89c9\u7406\u89e3", "result": "\u5728\u73b0\u6709\u5b9e\u4f8b\u4e2a\u6027\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5e76\u5728\u5305\u542b\u4eba\u7269\u3001\u4eba\u8138\u3001\u5ba0\u7269\u548c\u901a\u7528\u7269\u4f53\u7684\u65b0\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u6027\u80fd", "conclusion": "IIR-VLM\u901a\u8fc7\u96c6\u6210\u4e13\u5bb6\u6a21\u578b\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u7684\u5b9e\u4f8b\u7ea7\u8bc6\u522b\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4e3aVLM\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5b9e\u4f8b\u611f\u77e5\u7406\u89e3\u80fd\u529b"}}
{"id": "2601.14246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14246", "abs": "https://arxiv.org/abs/2601.14246", "authors": ["Zeyuan Chen", "Kai Zhang", "Zhuowen Tu", "Yuanjun Xiong"], "title": "Soft Tail-dropping for Adaptive Visual Tokenization", "comment": null, "summary": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.", "AI": {"tldr": "STAT\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u89c6\u89c9\u5206\u8bcd\u5668\uff0c\u53ef\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8f93\u51fatoken\u6570\u91cf\uff0c\u4f7f\u56e0\u679c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd", "motivation": "\u4f20\u7edf\u89c6\u89c9\u5206\u8bcd\u5668\u901a\u5e38\u8f93\u51fa\u56fa\u5b9a\u957f\u5ea6\u7684token\u5e8f\u5217\uff0c\u8fd9\u9650\u5236\u4e86\u56e0\u679c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574token\u6570\u91cf", "method": "\u63d0\u51faSoft Tail-dropping Adaptive Tokenizer (STAT)\uff0c\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u79bb\u6563\u4ee3\u7801\u5e8f\u5217\u53ca\u6bcf\u4e2atoken\u7684\u4fdd\u7559\u6982\u7387\uff0c\u901a\u8fc7\u5355\u8c03\u9012\u51cf\u6b63\u5219\u5316\u548c\u4e0e\u56fe\u50cf\u590d\u6742\u5ea6\u5bf9\u9f50\u6765\u8bad\u7ec3", "result": "\u5728ImageNet-1k\u4e0a\uff0c\u914d\u5907STAT\u7684\u56e0\u679c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c6\u89c9\u751f\u6210\u8d28\u91cf\u4e0a\u8fbe\u5230\u6216\u4f18\u4e8e\u5176\u4ed6\u6982\u7387\u6a21\u578b\u5bb6\u65cf\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u7f29\u653e\u884c\u4e3a", "conclusion": "STAT\u901a\u8fc7\u81ea\u9002\u5e94token\u5316\u89e3\u51b3\u4e86\u56e0\u679c\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u957f\u5ea6\u56fa\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6a21\u578b\u7f29\u653e\u80fd\u529b"}}
{"id": "2601.14250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14250", "abs": "https://arxiv.org/abs/2601.14250", "authors": ["Pengze Zhang", "Yanze Wu", "Mengtian Li", "Xu Bai", "Songtao Zhao", "Fulong Ye", "Chong Mou", "Xinghui Li", "Zhuowei Chen", "Qian He", "Mingyuan Gao"], "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "comment": "Github Page: https://pangzecheung.github.io/OmniTransfer/", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "AI": {"tldr": "OmniTransfer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u7a7a\u89c6\u9891\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u4fe1\u606f\u548c\u65f6\u5e8f\u7ebf\u7d22\u5b9e\u73b0\u5916\u89c2\u4e00\u81f4\u6027\u548c\u7cbe\u7ec6\u65f6\u5e8f\u63a7\u5236\uff0c\u5728\u5404\u79cd\u89c6\u9891\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u6216\u4efb\u52a1\u7279\u5b9a\u7684\u65f6\u5e8f\u5148\u9a8c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u56fa\u6709\u7684\u4e30\u5bcc\u65f6\u7a7a\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faOmniTransfer\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u4efb\u52a1\u611f\u77e5\u4f4d\u7f6e\u504f\u7f6e\u3001\u53c2\u8003\u89e3\u8026\u56e0\u679c\u5b66\u4e60\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u7edf\u4e00\u5904\u7406\u5404\u79cd\u89c6\u9891\u8fc1\u79fb\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\uff0cOmniTransfer\u5728\u5916\u89c2\uff08ID\u548c\u98ce\u683c\uff09\u548c\u65f6\u5e8f\u8fc1\u79fb\uff08\u76f8\u673a\u8fd0\u52a8\u548c\u89c6\u9891\u6548\u679c\uff09\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u8fd0\u52a8\u8fc1\u79fb\u65b9\u9762\u4e0e\u4f7f\u7528\u59ff\u6001\u5f15\u5bfc\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "OmniTransfer\u4e3a\u7075\u6d3b\u3001\u9ad8\u4fdd\u771f\u7684\u89c6\u9891\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u5145\u5206\u5229\u7528\u89c6\u9891\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8fc1\u79fb\u6548\u679c\u3002"}}
{"id": "2601.14251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14251", "abs": "https://arxiv.org/abs/2601.14251", "authors": ["Said Taghadouini", "Adrien Cavaill\u00e8s", "Baptiste Aubertin"], "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR", "comment": null, "summary": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.", "AI": {"tldr": "LightOnOCR-2-1B\u662f\u4e00\u4e2a10\u4ebf\u53c2\u6570\u7684\u7aef\u5230\u7aef\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u5c06\u6587\u6863\u56fe\u50cf\u8f6c\u6362\u4e3a\u5e72\u51c0\u3001\u81ea\u7136\u6392\u5e8f\u7684\u6587\u672c\uff0c\u65e0\u9700\u4f20\u7edfOCR\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u6bd4\u73b0\u6709\u6700\u4f73\u6a21\u578b\u5c0f9\u500d\u4e14\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edfOCR\u6d41\u7a0b\u8106\u5f31\u4e14\u590d\u6742\uff0c\u9700\u8981\u591a\u6b65\u9aa4\u5904\u7406\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u7406\u6587\u6863\u56fe\u50cf\uff08\u5982PDF\uff09\uff0c\u8f93\u51fa\u5e72\u51c0\u3001\u6709\u5e8f\u7684\u6587\u672c\uff0c\u540c\u65f6\u652f\u6301\u591a\u8bed\u8a00\u548c\u79d1\u5b66\u6587\u6863\u5904\u7406\u3002", "method": "1. \u4f7f\u7528\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u84b8\u998f\u6df7\u5408\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8986\u76d6\u626b\u63cf\u6587\u6863\u3001\u6cd5\u8bed\u6587\u6863\u548c\u79d1\u5b66PDF\uff1b2. \u6269\u5c55\u8f93\u51fa\u683c\u5f0f\u4ee5\u9884\u6d4b\u5d4c\u5165\u56fe\u50cf\u7684\u5f52\u4e00\u5316\u8fb9\u754c\u6846\uff1b3. \u901a\u8fc7resume\u7b56\u7565\u5728\u9884\u8bad\u7ec3\u4e2d\u5f15\u5165\u5b9a\u4f4d\u80fd\u529b\uff1b4. \u4f7f\u7528\u57fa\u4e8eIoU\u5956\u52b1\u7684RLVR\u8fdb\u884c\u7ec6\u5316\uff1b5. \u91c7\u7528\u68c0\u67e5\u70b9\u5e73\u5747\u548c\u4efb\u52a1\u7b97\u672f\u5408\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728OlmOCR-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u5c0f9\u500d\u4e14\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002\u6a21\u578b\u5728Apache 2.0\u8bb8\u53ef\u4e0b\u53d1\u5e03\uff0c\u540c\u65f6\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548cLightOnOCR-bbox-bench\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "LightOnOCR-2-1B\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u6587\u6863\u7406\u89e3\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\uff0c\u4e3a\u6587\u6863\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14253", "abs": "https://arxiv.org/abs/2601.14253", "authors": ["Hongyuan Chen", "Xingyu Chen", "Youjia Zhang", "Zexiang Xu", "Anpei Chen"], "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis", "comment": "Project page: https://motion3-to-4.github.io/. Code: https://github.com/Inception3D/Motion324", "summary": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.", "AI": {"tldr": "Motion 3-to-4\uff1a\u4ece\u5355\u76ee\u89c6\u9891\u548c\u53ef\u90093D\u53c2\u8003\u7f51\u683c\u5408\u6210\u9ad8\u8d28\u91cf4D\u52a8\u6001\u7269\u4f53\u7684\u524d\u9988\u6846\u67b6", "motivation": "4D\u5408\u6210\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u5355\u76ee\u89c6\u89d2\u4e0b\u51e0\u4f55\u4e0e\u8fd0\u52a8\u6062\u590d\u7684\u56fa\u6709\u6a21\u7cca\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u76844D\u52a8\u6001\u7269\u4f53", "method": "\u5c064D\u5408\u6210\u5206\u89e3\u4e3a\u9759\u60013D\u5f62\u72b6\u751f\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\uff0c\u4f7f\u7528\u89c4\u8303\u53c2\u8003\u7f51\u683c\u5b66\u4e60\u7d27\u51d1\u8fd0\u52a8\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u9010\u5e27\u9876\u70b9\u8f68\u8ff9\u9884\u6d4b\u6062\u590d\u5b8c\u6574\u65f6\u5e8f\u4e00\u81f4\u7684\u51e0\u4f55\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u9010\u5e27transformer\u5904\u7406\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMotion 3-to-4\u5728\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "Motion 3-to-4\u901a\u8fc7\u5206\u89e34D\u5408\u6210\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u6709\u9650\u548c\u5355\u76ee\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u80fd\u591f\u4ece\u5355\u76ee\u89c6\u9891\u5408\u6210\u9ad8\u8d28\u91cf\u76844D\u52a8\u6001\u7269\u4f53"}}
{"id": "2601.14256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14256", "abs": "https://arxiv.org/abs/2601.14256", "authors": ["Matthew Gwilliam", "Xiao Wang", "Xuefeng Hu", "Zhenheng Yang"], "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding", "comment": "18 pages, 16 tables, 4 figures", "summary": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u56fe\u50cf\u8bc6\u522b\u548c\u751f\u6210\u4efb\u52a1\u7684\u65b0\u578b\u8868\u793a\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u5b66\u4e60\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u538b\u7f29\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u540c\u65f6\u9002\u7528\u4e8e\u8bc6\u522b\u548c\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u8868\u793a\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u5206\u522b\u9488\u5bf9\u8bc6\u522b\u4efb\u52a1\uff08\u5982\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\uff09\u6216\u751f\u6210\u4efb\u52a1\u8bbe\u8ba1\u3002\u8bc6\u522b\u6a21\u578b\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u83b7\u5f97\u6709\u7528\u5d4c\u5165\uff0c\u751f\u6210\u6a21\u578b\u901a\u8fc7\u91cd\u5efa\u635f\u5931\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\u3002\u4f5c\u8005\u5e0c\u671b\u7edf\u4e00\u8fd9\u4e24\u4e2a\u65b9\u5411\uff0c\u5f00\u53d1\u9996\u4e2a\u540c\u65f6\u9002\u7528\u4e8e\u8bc6\u522b\u548c\u751f\u6210\u7684\u8868\u793a\u5b66\u4e60\u6a21\u578b\u3002", "method": "1. \u5c06\u6a21\u578b\u8bad\u7ec3\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u8d85\u7f51\u7edc\uff0c\u5b66\u4e60\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u6a21\u578b\u6743\u91cd\u4ee5\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u91cd\u5efa\uff1b2. \u5c06INR\u8d85\u7f51\u7edc\u4e0e\u77e5\u8bc6\u84b8\u998f\u96c6\u6210\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff1b3. \u5b66\u4e60\u524d\u6240\u672a\u6709\u7684\u538b\u7f29\u5d4c\u5165\u7a7a\u95f4\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "result": "1. \u6a21\u578b\u5728\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7ed3\u679c\u7ade\u4e89\uff1b2. \u901a\u8fc7\u9ad8\u8d28\u91cf\u5fae\u5c0f\u5d4c\u5165\u5b9e\u73b0\u751f\u6210\u80fd\u529b\uff1b3. \u5b66\u4e60\u5230\u7684\u538b\u7f29\u5d4c\u5165\u7a7a\u95f4\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b4. \u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u9996\u4e2a\u540c\u65f6\u9002\u7528\u4e8e\u8bc6\u522b\u548c\u751f\u6210\u7684\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7INR\u8d85\u7f51\u7edc\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u538b\u7f29\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
