<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 213]
- [cs.CL](#cs.CL) [Total: 140]
- [cs.AI](#cs.AI) [Total: 64]
- [cs.GR](#cs.GR) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement](https://arxiv.org/abs/2509.16221)
*Martin Preiß*

Main category: cs.CV

TL;DR: 该研究探讨了集成学习与OCR技术结合在数字化历史病历手写记录中的应用，发现集成学习能提高OCR准确率，且训练数据集大小对此无影响。


<details>
  <summary>Details</summary>
Motivation: 为Lippert教授研究组的学士项目数字化历史病历手写记录，医疗领域对准确率要求极高，需要探索能提高现有方法准确率的技术。

Method: 采用集成学习方法，结合多个机器学习模型与OCR技术，研究其在病历数字化中的应用效果。

Result: 研究发现集成学习能够提高OCR的准确率，确定了实现这一效果的具体方法，并发现训练数据集的大小对结果没有影响。

Conclusion: 集成学习与OCR技术的结合为病历数字化创造了附加价值，证明了该方法在提高准确率方面的有效性，且不受训练数据规模的限制。

Abstract: For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.

</details>


### [2] [Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute](https://arxiv.org/abs/2509.16343)
*Chung-En,Yu,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: VRA是一个无需训练的智能视觉推理框架，通过Think-Critique-Act循环提升视觉语言模型和纯视觉系统的鲁棒性，在挑战性视觉推理基准上实现高达40%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 为高风险领域（如遥感和医疗诊断）开发可信赖的智能视觉系统，需要在不进行昂贵重新训练的情况下实现广泛的鲁棒性。

Method: 提出Visual Reasoning Agent (VRA)框架，将现成的视觉语言模型和纯视觉系统封装在Think-Critique-Act循环中，无需训练即可使用。

Result: 虽然VRA在测试时带来显著额外计算开销，但在挑战性视觉推理基准上实现了高达40%的绝对准确率提升。

Conclusion: 未来工作将优化查询路由和早停机制，以减少推理开销，同时在视觉任务中保持可靠性。

Abstract: Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.

</details>


### [3] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

TL;DR: ForestGen3D是一个生成式建模框架，使用条件去噪扩散概率模型从稀疏的航空激光雷达数据生成高保真度的3D森林结构，能够重建被遮挡的冠层下细节。


<details>
  <summary>Details</summary>
Motivation: 生态系统中3D结构对生态过程至关重要，但广泛测量成本高昂且不可行。需要一种方法能够从可获得的航空激光雷达数据准确表征3D植被结构。

Method: 基于条件去噪扩散概率模型，使用配准的航空/地面激光雷达数据训练，引入几何包含先验确保生态合理性，生成地面激光雷达级别的3D点云。

Result: 在混合针叶林生态系统中，该方法在树木、样地和景观尺度上产生高保真重建，在几何相似性和生物物理指标方面与地面激光雷达参考数据高度匹配。

Conclusion: ForestGen3D成为在仅有航空激光雷达环境下进行生态建模、野火模拟和结构燃料表征的可扩展工具，包含属性可在缺乏地面真值时代替生成质量评估。

Abstract: The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.

</details>


### [4] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

TL;DR: 本文提出了一个新的NP难问题——可调整锚定区域打包问题（RARP），用于合成图像数据生成，并提供了一个基于贪心策略的通用启发式算法来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 传统的图像数据生成方法存在优化问题，需要一种能够有效处理任意数量、任意形状区域在图像画布上合理布局的新方法。

Method: 提出了一种基于贪心策略的启发式算法，通过迭代方式仔细打包区域对，同时满足优化约束条件。

Result: 算法通过生成大规模合成异常检测数据集得到验证，视觉检查和解决方案正确性证明了算法的有效性。

Conclusion: 随着生成建模在深度学习中的兴起，新引入的RARP问题将在图像科学社区中获得重视，为解决合成数据生成中的布局优化问题提供了新思路。

Abstract: The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.

</details>


### [5] [Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor](https://arxiv.org/abs/2509.16382)
*Saurabh Saini,Kapil Ahuja,Marc C. Steinbach,Thomas Wick*

Main category: cs.CV

TL;DR: 本研究开发了一种新的甲状腺癌CAD系统，重点在于特征提取。基于三个假设：DCT是最佳纹理特征描述符、局部DCT更适合甲状腺超声图像、需要结合ILBP描述符以提高噪声鲁棒性。提出的BPD-LDCT描述符结合非线性SVM，在两个公开数据集上取得了接近100%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 甲状腺超声图像分类具有挑战性，因为甲状腺被复杂的解剖结构包围，导致组织密度变化和超声波散射。现有方法难以准确捕捉纹理特征，需要开发更有效的特征提取方法来提高甲状腺癌分类的准确性。

Method: 提出BPD-LDCT（二进制模式驱动的局部离散余弦变换）描述符，结合局部DCT和改进的局部二进制模式（ILBP）。使用非线性SVM进行最终分类，在两个公开数据集（TDID和AUITD）上进行两阶段评估：第一阶段区分良恶性，第二阶段对恶性病例进行TI-RADS分级。

Result: 第一阶段分类在TDID数据集上达到接近100%的准确率，在AUITD数据集上达到97%。第二阶段分类在TDID数据集上接近100%，在AUITD数据集上达到99%。

Conclusion: BPD-LDCT描述符能有效捕捉甲状腺超声图像的纹理特征，结合非线性SVM的分类系统在两个公开数据集上表现出卓越性能，证明了该方法在甲状腺癌CAD系统中的有效性。

Abstract: In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.

</details>


### [6] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

TL;DR: StereoAdapter是一个参数高效的自监督框架，用于水下立体深度估计，通过LoRA适配的单目基础编码器和循环立体细化模块，在模拟和真实水下环境中相比现有方法提升了6.11%和5.12%的性能。


<details>
  <summary>Details</summary>
Motivation: 解决水下立体深度估计中的两个关键挑战：参数高效地适配大型视觉基础编码器到水下领域，以及紧密融合全局一致但尺度模糊的单目先验与局部度量但光度脆弱的立体对应关系。

Method: 提出StereoAdapter框架，包括LoRA适配的单目基础编码器和循环立体细化模块，引入动态LoRA适配进行高效秩选择，并在合成数据集UW-StereoDepth-40K上进行预训练以增强鲁棒性。

Result: 在模拟和真实世界基准测试中，TartanAir上提升6.11%，SQUID上提升5.12%，BlueROV2机器人实际部署进一步证明了方法的鲁棒性。

Conclusion: StereoAdapter框架有效解决了水下立体深度估计的挑战，在多个基准测试中显著优于现有方法，并展示了实际应用的鲁棒性。

Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [7] [AHA - Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](https://arxiv.org/abs/2509.16421)
*Aiden Chang,Celso De Melo,Stephanie M. Lukin*

Main category: cs.CV

TL;DR: Aha是一个自回归高光检测框架，能够在实时视频流中预测每帧与自然语言任务的相关性，无需访问未来帧，在标准基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法通常假设可以访问完整视频，不适合在线或流式场景。智能代理在高风险环境中需要实时理解连续视频流以支持逐步推理和实时决策。

Method: 使用多模态视觉语言模型和轻量级解耦头，在大型人工标注数据集上训练。引入动态SinkCache机制实现无限长度流的恒定内存使用。

Result: 在TVSum和Mr. Hisum基准测试中分别比现有方法提升5.9%和8.3%的mAP，甚至超过离线全上下文方法。

Conclusion: Aha展示了作为实时推理模块在机器人应用中的潜力，支持下游规划和长视野理解。

Abstract: Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr. Hisum in mAP (mean Average
Precision). We explore Aha's potential for real-world robotics applications
given a task-oriented natural language input and a continuous, robot-centric
video. Both experiments demonstrate Aha's potential effectiveness as a
real-time reasoning module for downstream planning and long-horizon
understanding.

</details>


### [8] [3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction](https://arxiv.org/abs/2509.16423)
*Maria Taktasheva,Lily Goli,Alessandro Fiorini,Zhen,Li,Daniel Rebain,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 提出了一种混合2D/3D高斯表示方法，通过联合优化约束平面（2D）高斯和自由形式（3D）高斯来解决平坦无纹理表面重建问题，在室内场景重建中实现了最先进的深度估计和网格提取效果。


<details>
  <summary>Details</summary>
Motivation: 当前辐射场和新视角合成方法在处理平坦、无纹理表面时会产生不均匀和半透明的重建结果，而表面重建方法虽然解决了这个问题但牺牲了视觉质量。

Method: 提出端到端的混合2D/3D表示方法，动态检测和优化平面区域，联合优化约束平面高斯（用于平坦表面）和自由形式高斯（用于其他场景）。

Result: 在ScanNet++和ScanNetv2数据集上实现了最先进的深度估计效果，在网格提取方面表现出色，且不会过拟合特定相机模型。

Conclusion: 该方法能有效生成高质量的室内场景重建，在视觉保真度和几何精度方面都有显著提升。

Abstract: Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.

</details>


### [9] [Preconditioned Deformation Grids](https://arxiv.org/abs/2509.18097)
*Julian Kaltheuner,Alexander Oebel,Hannah Droege,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 提出了一种名为Preconditioned Deformation Grids的新技术，用于从点云序列直接估计连贯的变形场，无需显式对应关系，在长序列重建中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要多个正则化项或大量训练数据，导致重建精度妥协、过度平滑或对未见物体和运动的泛化能力差。

Method: 使用多分辨率体素网格捕捉不同空间尺度的整体运动，结合基于网格的Sobolev预处理和梯度优化，应用Chamfer损失和弱等距损失确保变形精度和时间一致性。

Result: 广泛评估表明该方法在长序列重建中达到最先进技术的优越结果。

Conclusion: 该方法通过创新的预处理变形网格技术，成功解决了点云序列动态表面重建中的关键挑战，实现了高精度的变形估计。

Abstract: Dynamic surface reconstruction of objects from point cloud sequences is a
challenging field in computer graphics. Existing approaches either require
multiple regularization terms or extensive training data which, however, lead
to compromises in reconstruction accuracy as well as over-smoothing or poor
generalization to unseen objects and motions. To address these lim- itations,
we introduce Preconditioned Deformation Grids, a novel technique for estimating
coherent deformation fields directly from unstructured point cloud sequences
without requiring or forming explicit correspondences. Key to our approach is
the use of multi-resolution voxel grids that capture the overall motion at
varying spatial scales, enabling a more flexible deformation representation. In
conjunction with incorporating grid-based Sobolev preconditioning into
gradient-based optimization, we show that applying a Chamfer loss between the
input point clouds as well as to an evolving template mesh is sufficient to
obtain accurate deformations. To ensure temporal consistency along the object
surface, we include a weak isometry loss on mesh edges which complements the
main objective without constraining deformation fidelity. Extensive evaluations
demonstrate that our method achieves superior results, particularly for long
sequences, compared to state-of-the-art techniques.

</details>


### [10] [TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks](https://arxiv.org/abs/2509.16429)
*Itzik Waizman,Yakov Gusakov,Itay Benou,Tammy Riklin Raviv*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer和CNN的新型白质纤维束追踪方法，通过整合轨迹上下文和扩散MRI测量来改进神经通路映射的精度和完整性


<details>
  <summary>Details</summary>
Motivation: 白质纤维束追踪面临交叉、合并和扇形配置等挑战，传统方法难以处理噪声和模糊测量，需要更精确的建模方法

Method: 结合Transformer建模白质流线的序列特性，使用CNN提取局部微结构特征，整合轨迹上下文和当前扩散MRI测量来预测纤维方向

Result: 在Tractometer工具包评估中达到与最先进方法竞争的性能，在TractoInferno数据集上展示出对真实数据的强泛化能力

Conclusion: 该方法通过结合Transformer和CNN的优势，显著提升了白质纤维束追踪的精度和完整性，为神经通路映射提供了更可靠的解决方案

Abstract: White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.

</details>


### [11] [Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation](https://arxiv.org/abs/2509.16436)
*Zhejia Zhang,Junjie Wang,Le Zhang*

Main category: cs.CV

TL;DR: 提出基于mmFormer架构的多模态MRI分类模型，通过自适应模块处理任意缺失模态组合，在肝脏纤维化分期任务中取得良好性能


<details>
  <summary>Details</summary>
Motivation: 解决临床MRI中因设备差异或患者配合问题导致的模态缺失问题，提升模型在真实场景下的鲁棒性

Method: 保留mmFormer的混合模态特定编码器和模态相关编码器，集成缺失模态补偿模块（零填充、模态可用性掩码、可学习统计参数的Delta函数），并采用交叉验证集成策略

Result: 在CARE 2025挑战赛测试集上，肝硬化检测准确率66.67%、AUC 71.73%，显著纤维化检测准确率74.17%、AUC 68.48%

Conclusion: 所提方法能有效处理MRI模态缺失问题，在肝脏疾病分类任务中表现出色，具有临床应用价值

Abstract: In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.

</details>


### [12] [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
*Mohamed Eltahir,Osamah Sarraj,Abdulrahman Alfrihidi,Taha Alshatiri,Mohammed Khurd,Mohammed Bremoo,Tanveer Hussain*

Main category: cs.CV

TL;DR: AutoArabic是一个三阶段框架，利用LLMs将非阿拉伯语视频文本基准翻译成现代标准阿拉伯语，显著减少人工修订需求，并开发了阿拉伯语视频检索基准DiDeMo-AR。


<details>
  <summary>Details</summary>
Motivation: 当前视频文本检索领域主要依赖英语基准，阿拉伯语缺乏本地化评估指标，需要填补这一空白。

Method: 采用三阶段框架：1）LLM翻译非阿拉伯语基准；2）错误检测模块自动标记翻译错误；3）生成阿拉伯语变体DiDeMo-AR。

Result: 成功创建包含40,144条流畅阿拉伯语描述的DiDeMo-AR基准，错误检测准确率达97%，阿拉伯语与英语版本性能差距约3个百分点。

Conclusion: AutoArabic框架有效支持阿拉伯语本地化，LLM原始输出即可使用，性能随人工修订增加而提升，框架可复用于其他语言。

Abstract: Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.

</details>


### [13] [KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models](https://arxiv.org/abs/2509.16452)
*Son Hai Nguyen,Diwei Wang,Jinhyeok Jang,Hyewon Seo*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型的知识增强提示学习方法，用于室内日常动作识别，在ETRI-Activity3D数据集上达到超过95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂真实环境中安全可靠运行的自主机器人需要准确的视觉动作识别能力。

Method: 采用提示学习框架，将动作的类别级文本描述作为可学习提示嵌入到预训练的视觉语言模型中，设计了多种文本描述结构和编码策略。

Result: 在ETRI-Activity3D数据集上仅使用RGB视频输入就实现了超过95%的准确率，优于现有最先进方法。

Conclusion: 知识增强提示方法能够以最小监督实现鲁棒的动作识别，证明了该方法的有效性。

Abstract: Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.

</details>


### [14] [Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models](https://arxiv.org/abs/2509.16472)
*Parth Agarwal,Sangaa Chatterjee,Md Faisal Kabir,Suman Saha*

Main category: cs.CV

TL;DR: 提出双分支CNN-LSTM框架，结合1D关节特征和3D轮廓特征，通过SHAP和Grad-CAM提供可解释性，在步态分析中达到98.6%的准确率


<details>
  <summary>Details</summary>
Motivation: 当前步态诊断模型缺乏可解释性且依赖单一数据集，需要开发跨数据集的可解释步态分析系统

Method: 使用双分支CNN-LSTM框架：1D分支处理GAVD数据集的关节特征，3D分支处理OU-MVLP数据集的轮廓特征，结合SHAP和Grad-CAM进行时空可解释性分析

Result: 在保留测试集上达到98.6%的准确率，具有强大的召回率和F1分数

Conclusion: 该方法在临床和生物识别领域推进了可解释步态分析的发展

Abstract: Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.

</details>


### [15] [Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion](https://arxiv.org/abs/2509.16474)
*Gabrielle Chavez,Laureano Moro-Velazquez,Ankur Butala,Najim Dehak,Thomas Thebaud*

Main category: cs.CV

TL;DR: 提出一个结合时间序列和图像的手写分析框架，用于检测帕金森病和阿尔茨海默病等神经系统疾病，在多个数据集上实现了最先进的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理手写任务时难以在时间序列和图像特征之间实现泛化，特别是在跨数据集场景下表现不佳。

Method: 基于ResNet50预训练模型构建联合分类器，同时利用手写的时间序列数据和图像数据。

Result: 在二进制分类实验中达到最先进性能，特别是在Draw Clock和Spiral任务上表现显著提升，PD检测F1分数高达98。

Conclusion: 该模型能够有效泛化不同形式的手写信号，增强神经系统疾病中运动缺陷的检测能力。

Abstract: Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.

</details>


### [16] [Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs](https://arxiv.org/abs/2509.16476)
*Qinyu Chen,Jiawen Qi*

Main category: cs.CV

TL;DR: GazeVLM是一个无需训练的高效视觉语言模型框架，利用人类注视点作为监督信号来减少视觉令牌冗余，在保持答案质量的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型存在视觉令牌冗余问题，导致推理效率低下，阻碍在边缘设备上的实时应用。现有方法需要架构修改或中间激活访问，增加了计算和内存开销，且存在提示与感兴趣区域不对齐的问题

Method: 通过提取注视驱动的感兴趣区域，并结合低分辨率全局视图，模拟人类视网膜中央凹-外周感知机制，在保留任务相关细节的同时减少冗余视觉令牌

Result: 在VOILA-COCO基准测试中，GazeVLM将视觉令牌减少高达93.1%，总令牌减少59.6%，FLOPs减少50%，同时保持比全分辨率基线更好的答案质量

Conclusion: 将模型计算与人类注视对齐，为消费设备上的高效VLM推理提供了一种简单即插即用的解决方案

Abstract: Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.

</details>


### [17] [Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture](https://arxiv.org/abs/2509.16479)
*Christopher Silver,Thangarajah Akilan*

Main category: cs.CV

TL;DR: 提出了一种基于双向卷积长短时记忆网络（BiConvLSTM）的热成像跌倒检测方法，通过多种注意力机制增强模型性能，在TSF数据集上达到99.7%的ROC-AUC，并在新的TF-66基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒是一个重大公共卫生问题，现有解决方案在可靠性、用户依从性和实用性方面存在挑战。利益相关者更倾向于无需用户交互、非穿戴式、被动式、保护隐私且实时的跌倒检测系统。

Method: 使用BiConvLSTM模型，结合空间、时间、特征、自注意力和通用注意力机制，通过系统实验探索了数百种模型变体，包括注意力机制、循环模块和运动流的集成。

Result: BiConvLSTM在TSF数据集上达到99.7%的ROC-AUC，在TF-66基准测试中也表现出鲁棒性能，证明了模型的泛化能力和实用性。

Conclusion: 该方法为热成像跌倒检测设立了新标准，为实现可部署的高性能解决方案铺平了道路。

Abstract: Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.

</details>


### [18] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

TL;DR: 本文提出了一种统一的3D语义场景完成、扩展和生成框架Octree Latent Semantic Diffusion，通过双八叉图潜在表示实现跨室内外场景的语义场景合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将3D场景完成、扩展和生成问题解耦处理，且多为领域特定模型，无法实现跨域兼容。本文旨在开发一个统一的框架来解决这些问题。

Method: 采用双八叉图潜在表示，将合成过程分为两个阶段：结构扩散预测粗粒度八叉树，潜在语义扩散生成语义嵌入并通过图VAE解码为体素级语义标签。利用推理时潜在修复和外绘技术实现场景完成和扩展。

Result: 实验表明该方法能够从单次LiDAR扫描中实现高质量的结构完成、连贯的语义生成，并对分布外LiDAR数据具有零样本泛化能力。

Conclusion: 基于双八叉图潜在空间的生成式完成方法为真实世界机器人感知任务提供了一种实用且可扩展的替代方案，优于基于回归的流程。

Abstract: The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.

</details>


### [19] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出RLGF方法，通过几何反馈强化学习优化视频扩散模型，解决合成数据在自动驾驶感知任务中的几何失真问题，显著提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视频生成模型虽然视觉逼真，但存在细微的几何失真，限制了其在自动驾驶下游感知任务中的实用性。研究发现合成数据与真实数据在3D目标检测性能上存在显著差距。

Method: 引入RLGF（带几何反馈的强化学习）方法，通过专门的潜在空间自动驾驶感知模型提供奖励来优化视频扩散模型。核心组件包括高效的潜在空间窗口优化技术和分层几何奖励系统。

Result: 在nuScenes数据集上应用RLGF后，几何误差显著降低（VP误差减少21%，深度误差减少57%），3D目标检测mAP提升12.7%，缩小了与真实数据性能的差距。

Conclusion: RLGF提供了一种即插即用的解决方案，能够生成几何准确可靠的合成视频，促进自动驾驶系统的发展。

Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.

</details>


### [20] [CommonForms: A Large, Diverse Dataset for Form Field Detection](https://arxiv.org/abs/2509.16506)
*Joe Barrow*

Main category: cs.CV

TL;DR: 本文介绍了CommonForms数据集和FFDNet模型，用于表单字段检测任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模的表单字段检测数据集和开源模型，商业解决方案功能有限且不开放。

Method: 通过过滤Common Crawl中的可填充PDF文档构建数据集，将表单字段检测建模为目标检测问题，开发了FFDNet-Small和FFDNet-Large模型。

Result: 构建了包含55k文档、450k页面的多样化数据集，FFDNet模型在测试集上达到高精度，训练成本低于500美元，优于商业PDF阅读器。

Conclusion: 这是首个大规模表单字段检测数据集和开源模型，支持复选框检测等商业解决方案不具备的功能。

Abstract: This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms

</details>


### [21] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

TL;DR: 提出OS-DiffVSR，一种一步扩散模型，用于真实世界视频超分辨率，在保持高质量的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频超分辨率方法在视频质量和推理效率之间存在权衡，需要多步采样导致效率低下

Method: 设计了相邻帧对抗训练范式和多帧融合机制，通过一步扩散实现高质量视频重建并保持帧间时序一致性

Result: 在多个VSR基准测试中，OS-DiffVSR甚至优于需要数十步采样的现有扩散方法

Conclusion: OS-DiffVSR成功解决了扩散模型在视频超分辨率中的效率瓶颈，实现了质量与效率的平衡

Abstract: Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.

</details>


### [22] [SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2509.16509)
*Haijin Zeng,Xuan Lu,Yurong Zhang,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: SlowFast-SCI是一个双速深度学习框架，将慢速预训练与快速自适应相结合，用于光谱压缩成像，显著减少计算量并提升对分布外数据的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度展开方法仅依赖缓慢的预训练过程，缺乏快速适应新光学配置的能力，导致在分布外相机上表现不佳且计算成本高。

Method: 采用双阶段设计：慢速学习阶段预训练骨干网络并通过成像指导蒸馏到紧凑模型；快速学习阶段嵌入轻量自适应模块，通过双域损失进行自监督测试时训练。

Result: 参数和FLOPs减少70%以上，分布外数据PSNR提升达5.79dB，适应速度快4倍，保持跨域适应性。

Conclusion: 该框架首次实现测试时自适应的深度展开，为自适应性、可部署成像系统开辟了新途径，具有模块化特性可扩展到其他计算成像模态。

Abstract: Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.

</details>


### [23] [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
*Burak Satar,Zhixin Ma,Patrick A. Irawan,Wilfried A. Mulyawan,Jing Jiang,Ee-Peng Lim,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 本文提出了Seeing Culture Benchmark (SCB)，这是一个专注于文化推理的多模态视觉语言模型基准测试，通过两阶段任务评估模型的文化理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文化数据集在提供文化推理方面存在不足，且对许多文化代表性不足，特别是东南亚文化经常被忽视。

Method: SCB采用两阶段方法：1）通过多选视觉问答选择正确的视觉选项；2）分割相关文化文物作为推理证据。视觉选项按来源国家系统组织为三种类型。

Result: SCB包含1,065张图像，涵盖7个东南亚国家的138种文化文物，包含3,178个问题。评估显示现有VLMs在跨模态文化推理方面存在复杂性，视觉推理与空间定位存在差距。

Conclusion: SCB基准测试对于识别多模态模型在文化推理方面的不足至关重要，为未来文化推理领域的发展提供指导。

Abstract: Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture

</details>


### [24] [FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers](https://arxiv.org/abs/2509.16518)
*Sankeerth Durvasula,Kavya Sreedhar,Zain Moustafa,Suraj Kothawade,Ashish Gondimalla,Suvinay Subramanian,Narges Shahidi,Nandita Vijaykumar*

Main category: cs.CV

TL;DR: FG-Attn是一种用于长上下文扩散变换器的细粒度稀疏注意力机制，通过异步聚集加载操作实现比块稀疏注意力更精细的计算跳过，在视频扩散模型中实现了1.41-1.65倍的加速。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器生成真实视频需要大量计算，注意力层是主要瓶颈。现有块稀疏注意力方法跳过整个M×M块，无法充分利用注意力图中的稀疏性。

Method: 提出FG-Attn稀疏注意力机制，在M×1切片粒度跳过计算；开发异步聚集加载操作，仅加载与查询相关的稀疏键值向量到共享内存。

Result: 在单H100 GPU上，5秒480p视频平均加速1.55倍（最高1.65倍），5秒720p视频平均加速1.41倍（最高1.49倍）。

Conclusion: FG-Attn通过细粒度稀疏注意力有效减少了扩散变换器的计算开销，显著提升了视频生成效率。

Abstract: Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.

</details>


### [25] [PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality](https://arxiv.org/abs/2509.16519)
*Yang Han*

Main category: cs.CV

TL;DR: PM25Vision (PM25V)是迄今为止最大最全面的空气质量数据集，通过街景图像估算PM2.5浓度，包含11,114张图像与PM2.5读数匹配，空间精度达5公里。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量数据集规模有限且空间精度不足，需要更大规模、更高精度的数据集来支持基于图像的PM2.5浓度估算研究。

Method: 构建数据收集、同步和清理流程，收集了11年期间3,261个空气质量监测站的图像和PM2.5读数，使用CNN和Transformer架构建立基线模型。

Result: 创建了包含11,114张图像的大规模数据集，空间精度达到5公里，显著超过现有基准数据集规模，并提供了基线模型性能。

Conclusion: PM25V数据集公开可用，为基于图像的空气质量监测研究提供了重要资源，推动了该领域的发展。

Abstract: We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.

</details>


### [26] [Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity](https://arxiv.org/abs/2509.16527)
*Guangze Zheng,Shijie Lin,Haobo Zuo,Si Si,Ming-Shan Wang,Changhong Fu,Jia Pan*

Main category: cs.CV

TL;DR: 本文提出了一种基于格子玻尔兹曼模型（LBM）的视觉跟踪方法，通过分解视觉表示为动态像素格子，并通过碰撞-流过程解决像素运动状态，实现实时在线视觉跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有视觉跟踪方法在处理真实世界像素动态性方面存在局限性，需要一种能够在线实时适应真实世界视觉跟踪任务的高效方法。

Method: LBM通过多层预测-更新网络获取目标像素的高维分布，预测阶段在目标像素的空间邻域内制定格子碰撞，在时间视觉上下文中发展格子流；更新阶段使用在线视觉表示修正像素分布。

Result: 在TAP-Vid和RoboTap等真实世界点跟踪基准测试中验证了LBM的效率，在TAO、BFT和OVT-B等大规模开放世界目标跟踪基准测试中进一步证明了其实用性。

Conclusion: LBM展示了在在线实时方式下的实际适用性，能够高效适应真实世界视觉跟踪任务，相比现有方法具有更好的实用性。

Abstract: This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.

</details>


### [27] [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
*Shubhashis Roy Dipta,Tz-Ying Wu,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种无需参考真值的视频字幕质量评估框架VC-Inspector，通过事实基础确保字幕质量评估的准确性，利用大语言模型生成伪字幕训练多模态评估器，在多个数据集上表现出与人类判断的优越一致性。


<details>
  <summary>Details</summary>
Motivation: 获取人工标注的视频字幕成本高昂且不切实际，现有基于参考真值的评估方法在多样化视频领域面临挑战，需要一种无需真值字幕的评估框架。

Method: 提出VC-Inspector评估器，利用大语言模型生成不同质量的伪字幕，基于监督数据训练多模态模型Qwen2.5-VL作为评估器，实现参考无关的事实基础评估。

Result: 在VATEX-Eval数据集上表现出与人类判断的优越一致性，性能优于现有方法，并能推广到图像字幕数据集Flickr8K-Expert和Flickr8K-CF。

Conclusion: VC-Inspector为视频字幕的事实准确性评估提供了可扩展和可推广的解决方案，为多样化视频领域提供了更有效和客观的评估方法。

Abstract: Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.

</details>


### [28] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

TL;DR: RFfusion是一种基于Rectified Flow的高效一步扩散模型，用于图像融合任务，通过优化采样路径和引入任务特定的VAE架构，在保持高质量融合结果的同时显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在图像融合任务中计算复杂、推理时间长的问题，提高方法的实用性

Method: 将Rectified Flow引入图像融合任务以拉直扩散模型中的采样路径，实现无需额外训练的一步采样；提出针对图像融合的任务特定VAE架构，将融合操作嵌入潜在空间；采用两阶段训练策略解决传统VAE目标与图像融合需求之间的差异

Result: 大量实验表明，该方法在推理速度和融合质量方面均优于其他最先进方法

Conclusion: RFfusion通过高效的扩散模型架构和优化的训练策略，成功实现了高质量图像融合与快速推理的平衡

Abstract: Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.

</details>


### [29] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

TL;DR: 提出ST-GS框架，通过空间聚合和时间融合策略增强基于高斯的方法在3D占据预测中的时空建模能力


<details>
  <summary>Details</summary>
Motivation: 现有3D语义高斯方法在自动驾驶场景理解中存在多视角空间交互不足和多帧时间一致性有限的问题

Method: 采用双模式注意力机制的引导信息空间聚合策略，以及几何感知的时间融合方案来利用历史上下文

Result: 在nuScenes占据预测基准上达到最先进性能，时间一致性显著优于现有高斯方法

Conclusion: ST-GS框架有效解决了高斯方法在时空建模方面的局限性，为视觉中心自动驾驶提供了更好的场景理解方案

Abstract: 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.

</details>


### [30] [Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose](https://arxiv.org/abs/2509.16557)
*Muhammad Hamza,Danish Hamid,Muhammad Tahir Akram*

Main category: cs.CV

TL;DR: I2S是一个多阶段框架，通过3D手部姿态分析进行人机交互识别，实现无干扰的用户身份认证，在AR辅助系统中达到97.52%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 在飞机驾驶舱、航空航天维护和手术等高风险环境中，需要可靠的无干扰用户身份认证技术来支持AR个性化辅助系统。

Method: 提出I2S框架，使用3D手部姿态特征，通过顺序特征增强：先识别物体类别，再进行人机交互识别，最后进行用户身份认证。特征包括空间、频率、运动学、方向和新型IHSE描述符。

Result: 在ARCTIC和H2O数据集上评估，最佳配置达到97.52%的平均F1分数，模型大小小于4MB，推理时间0.1秒。

Conclusion: I2S在保持轻量级的同时实现了最先进的性能，非常适合安全关键的AR系统实时设备认证。

Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.

</details>


### [31] [Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization](https://arxiv.org/abs/2509.16560)
*Ji Soo Lee,Byungoh Ko,Jaewon Cho,Howoong Lee,Jaewoon Byun,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: CaRe-DPO是一个检索框架，通过检索相关性分数直接优化字幕生成，解决多模态大语言模型生成的字幕过于通用且难以区分视觉相似视频的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型生成的辅助字幕往往过于通用，无法区分视觉相似的视频，限制了细粒度检索的效果。传统字幕评估指标（如BLEU）也不适合需要区分候选者的检索任务。

Method: 提出CaRe-DPO框架，核心是双组直接偏好优化（DG-DPO），通过建模不同视频和字幕对组间的偏好来监督字幕生成。还设计了基于MLLM的检索模型，使用角色嵌入来区分不同功能的文本输入。

Result: 通过大量实验证明，CaRe-DPO能有效利用辅助知识生成细粒度字幕，显著提升检索性能。

Conclusion: CaRe-DPO框架通过直接优化检索相关性的字幕生成方法，成功解决了现有字幕生成在细粒度检索中的局限性，为文本-视频检索任务提供了有效解决方案。

Abstract: In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.

</details>


### [32] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出了一种无需训练的即插即用黑盒反事实生成框架，利用预训练图像编辑扩散模型生成人类级别的反事实解释，揭示了人类推理与神经网络行为之间的解释差距。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒反事实生成框架忽视语义内容且过度依赖训练，需要一种无需训练即可生成高质量反事实解释的方法。

Method: 基于理论保证的最优编辑步骤，使用预训练图像编辑扩散模型进行逐步编辑，无需访问分类器内部结构。

Result: 实验表明该框架能生成人类级别的反事实解释，并通过CNN、ViT和LVLM分类器验证了人类与神经网络行为之间的解释差距。

Conclusion: 该框架提供了一种可解释的反事实生成过程，有效揭示了深度学习模型决策与人类认知之间的差异。

Abstract: Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.

</details>


### [33] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 提出DeepSSIM，一种用于量化生成模型中记忆化的自监督度量方法，在医学影像生成中有效检测训练数据泄露


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在医学影像中可能记忆敏感训练数据，存在患者信息泄露风险，需要可扩展的检测方法

Method: DeepSSIM通过将图像投影到学习嵌入空间，使嵌入间的余弦相似度与图像空间的SSIM分数匹配，结合结构保持增强来捕获解剖特征

Result: 在脑部MRI合成数据实验中，DeepSSIM相比现有最佳方法平均提升F1分数52.03%

Conclusion: DeepSSIM是检测生成模型记忆化的有效工具，在医学影像领域具有重要应用价值

Abstract: Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [34] [SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving](https://arxiv.org/abs/2509.16588)
*Haiming Zhang,Yiyao Zhu,Wending Zhou,Xu Yan,Yingjie Cai,Bingbing Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: SQS是一种专为稀疏感知模型设计的查询驱动预训练方法，通过3D高斯表示和自监督溅射技术提升自动驾驶场景中的3D感知任务性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏感知模型虽然计算效率高，但在细粒度特征学习方面存在不足。SQS旨在通过预训练增强稀疏查询的表示能力，提升下游任务的性能。

Method: SQS引入可插拔模块，在预训练阶段从稀疏查询预测3D高斯表示，通过多视角图像和深度图的重构进行自监督学习。微调时通过查询交互机制将预训练查询与任务特定查询结合。

Result: 在自动驾驶基准测试中，SQS在占用预测和3D目标检测任务上显著优于现有方法，占用预测提升1.3 mIoU，3D检测提升1.0 NDS。

Conclusion: SQS为稀疏感知模型提供了一种有效的预训练框架，通过查询驱动的溅射预训练显著提升了3D感知任务的性能，证明了该方法在自动驾驶领域的实用价值。

Abstract: Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).

</details>


### [35] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

TL;DR: 本文提出了FakeChain基准测试，用于研究多步骤混合深度伪造的检测挑战，发现检测器主要依赖最后一步的伪造痕迹而非累积痕迹，导致泛化能力受限。


<details>
  <summary>Details</summary>
Motivation: 现有检测模型主要针对单步伪造，而现实中深度伪造往往采用多步骤混合方法，这种复合伪造方式对检测模型构成新的技术挑战。

Method: 构建包含1步、2步和3步伪造的大规模基准测试FakeChain，使用五种最先进的生成器合成伪造图像，分析检测性能和频谱特性。

Result: 检测性能高度依赖最终伪造类型，当与训练分布不同时F1分数最多下降58.83%，表明检测器依赖最后阶段痕迹而非累积痕迹。

Conclusion: 检测模型需要显式考虑伪造历史和序列，FakeChain基准反映了现实世界中合成复杂性和多样性的增长需求。

Abstract: Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [36] [Describe-to-Score: Text-Guided Efficient Image Complexity Assessment](https://arxiv.org/abs/2509.16609)
*Shipeng Liu,Zhonglin Zhang,Dengfeng Chen,Liang Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉-文本融合的图像复杂度评估方法D2S框架，通过生成图像描述并融合多模态信息来提高评估准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像复杂度评估方法主要依赖视觉特征，忽略了高层语义信息，限制了准确性和泛化能力。

Method: 提出D2S框架，使用预训练视觉语言模型生成图像描述，通过特征对齐和熵分布对齐机制融合视觉和文本模态信息。训练时使用多模态信息，推理时仅需视觉分支。

Result: 在IC9600数据集上优于现有方法，在无参考图像质量评估基准上保持竞争力。

Conclusion: 多模态融合在复杂度相关任务中具有有效性和高效性，D2S框架实现了准确且高效的单模态推理。

Abstract: Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S

</details>


### [37] [Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model](https://arxiv.org/abs/2509.16617)
*David Kreismann*

Main category: cs.CV

TL;DR: 该研究通过微调地理空间基础模型，预测未来气候情景下的城市地表温度，并探索其对土地覆盖变化的响应，实现了高精度的温度预测。


<details>
  <summary>Details</summary>
Motivation: 随着城市化和气候变化加剧，城市热岛效应日益严重，需要详细的气温数据来制定缓解计划。传统机器学习模型和有限数据基础设施的预测方法在服务不足地区往往不准确。

Method: 本研究微调了一个基于全球非结构化数据训练的地理空间基础模型，用于预测未来气候情景下的城市地表温度，并通过模拟植被策略探索模型对土地覆盖变化的响应。

Result: 微调后的模型实现了像素级降尺度误差低于1.74°C，与地面真实数据模式一致，并展现出高达3.62°C的外推能力。

Conclusion: 地理空间基础模型在传统方法受限的情况下提供了有效的替代方案，具有较强的泛化能力和最小化微调需求，能够为城市热岛效应缓解策略提供可靠支持。

Abstract: As urbanization and climate change progress, urban heat island effects are
becoming more frequent and severe. To formulate effective mitigation plans,
cities require detailed air temperature data. However, predictive analytics
methods based on conventional machine learning models and limited data
infrastructure often provide inaccurate predictions, especially in underserved
areas. In this context, geospatial foundation models trained on unstructured
global data demonstrate strong generalization and require minimal fine-tuning,
offering an alternative for predictions where traditional approaches are
limited. This study fine-tunes a geospatial foundation model to predict urban
land surface temperatures under future climate scenarios and explores its
response to land cover changes using simulated vegetation strategies. The
fine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C and
aligned with ground truth patterns, demonstrating an extrapolation capacity up
to 3.62 {\deg}C.

</details>


### [38] [Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery](https://arxiv.org/abs/2509.16618)
*Pengfei Hao,Hongqiu Wang,Shuaibo Li,Zhaohu Xing,Guang Yang,Kaishun Wu,Lei Zhu*

Main category: cs.CV

TL;DR: Surgical-MambaLLM是首个将Mamba2与LLM结合用于手术视觉问答定位任务的方法，通过跨模态双向Mamba2集成模块和手术器械感知扫描模式，有效提升对手术场景的空间理解和多模态依赖关系建模。


<details>
  <summary>Details</summary>
Motivation: 当前方法在手术视觉问答定位任务中难以建立文本与视觉细节之间的复杂依赖关系，并且难以感知手术场景的空间信息。

Method: 提出Surgical-MambaLLM方法，包含跨模态双向Mamba2集成模块进行多模态融合，以及针对手术场景几何特征设计的手术器械感知扫描模式。

Result: 在EndoVis17-VQLA和EndoVis18-VQLA数据集上的实验表明，该方法超越了现有最先进方法，显著提升了手术视觉问答定位任务的性能。

Conclusion: Surgical-MambaLLM通过结合Mamba2和LLM，有效解决了手术场景中多模态依赖关系和空间信息感知的挑战，为手术视觉问答定位任务提供了新的解决方案。

Abstract: In recent years, Visual Question Localized-Answering in robotic surgery
(Surgical-VQLA) has gained significant attention for its potential to assist
medical students and junior doctors in understanding surgical scenes. Recently,
the rapid development of Large Language Models (LLMs) has provided more
promising solutions for this task. However, current methods struggle to
establish complex dependencies between text and visual details, and have
difficulty perceiving the spatial information of surgical scenes. To address
these challenges, we propose a novel method, Surgical-MambaLLM, which is the
first to combine Mamba2 with LLM in the surgical domain, that leverages
Mamba2's ability to effectively capture cross-modal dependencies and perceive
spatial information in surgical scenes, thereby enhancing the LLMs'
understanding of surgical images. Specifically, we propose the Cross-modal
Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective
multimodal fusion, with its cross-modal integration capabilities. Additionally,
tailored to the geometric characteristics of surgical scenes, we design the
Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the
surgical images, enhancing the model's spatial understanding of the surgical
scene. Extensive experiments demonstrate that our Surgical-MambaLLM model
outperforms the state-of-the-art methods on the EndoVis17-VQLA and
EndoVis18-VQLA datasets, significantly improving the performance of the
Surgical-VQLA task.

</details>


### [39] [CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition](https://arxiv.org/abs/2509.16623)
*Junjie Zhou,Haijun Xiong,Junhao Lu,Ziyu Lin,Bin Feng*

Main category: cs.CV

TL;DR: 提出CGTGait框架，结合图卷积和Transformer进行步态情感识别，通过双向跨流融合模块整合姿态和运动特征，在降低82.2%计算复杂度的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注空间和局部时序信息，无法捕捉长距离时序表示，需要更有效的时空特征提取方法。

Method: 使用多个CGT块，每个块用图卷积提取帧级空间拓扑，用Transformer建模全局时序依赖，并引入双向跨流融合模块整合姿态和运动特征。

Result: 在Emotion-Gait和ELMD数据集上达到SOTA或竞争性性能，测试时计算复杂度降低82.2%（仅需0.34G FLOPs）。

Conclusion: CGTGait框架能有效提取判别性时空特征，在保持高性能的同时显著降低计算成本，为步态情感识别提供了高效解决方案。

Abstract: Skeleton-based gait emotion recognition has received significant attention
due to its wide-ranging applications. However, existing methods primarily focus
on extracting spatial and local temporal motion information, failing to capture
long-range temporal representations. In this paper, we propose
\textbf{CGTGait}, a novel framework that collaboratively integrates graph
convolution and transformers to extract discriminative spatiotemporal features
for gait emotion recognition. Specifically, CGTGait consists of multiple CGT
blocks, where each block employs graph convolution to capture frame-level
spatial topology and the transformer to model global temporal dependencies.
Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to
effectively aggregate posture and motion spatiotemporal features, facilitating
the exchange of complementary information between the two streams. We evaluate
our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating
that our CGTGait achieves state-of-the-art or at least competitive performance
while reducing computational complexity by approximately \textbf{82.2\%} (only
requiring 0.34G FLOPs) during testing. Code is available at
\small{https://github.com/githubzjj1/CGTGait.}

</details>


### [40] [Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning](https://arxiv.org/abs/2509.16628)
*Janak Kapuriya,Anwar Shaikh,Arnav Goel,Medha Hira,Apoorv Singh,Jay Saraf,Sanjana,Vaibhav Nauriyal,Avinash Anand,Zhengkui Wang,Rajiv Ratn Shah*

Main category: cs.CV

TL;DR: 本文提出VCASFT学习范式，利用图像描述作为零样本提示来提升小型视觉语言模型在科学视觉问答任务上的性能，并在ScienceQA和自建的HiSciVQA数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决小型视觉语言模型在科学视觉问答任务上性能不足的问题，特别是针对低资源语言场景，需要开发更有效的训练方法和评估体系。

Method: VCASFT范式：使用图像描述作为零样本提示，结合问答对进行指令调优；开发了HiSciVQA印地语数据集；提出了基于LLM的新型评估方案。

Result: VCASFT在ScienceQA上表现出良好的适应性和有效性，在多种语言、学科和领域的问题上都取得了显著性能提升。

Conclusion: VCASFT为小型VLMs在科学VQA任务上提供了有效的训练方法，HiSciVQA数据集填补了低资源语言多模态问答数据集的空白，新型评估方案能更深入分析模型效果。

Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning
(VCASFT), a novel learning paradigm designed to enhance the performance of
smaller Vision Language Models(VLMs) on scientific visual question
answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts
alongside question-answer pairs and instruction-tunes models to yield
significant performance improvements. To comprehensively evaluate VCASFT, we
benchmark it on ScienceQA, which consists of questions across diverse
languages, subjects, and fields, demonstrating its adaptability and
effectiveness in a variety of educational contexts. Additionally, to further
demonstrate the effectiveness of this technique on lowresource languages, we
developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated
Hindi multimodal Q&A pairs. This dataset addresses the critical need for
low-resource language Q&A datasets and serves as a foundation for testing
VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to
evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness
surpassing traditional n-gram matching accuracy metrics. We are committed to
advancing the field by open-sourcing all code files and the HiSciVQA dataset
for the research community.

</details>


### [41] [Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation](https://arxiv.org/abs/2509.16630)
*Yue Ma,Zexuan Yan,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Zhifeng Li,Wei Liu,Linfeng Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: Follow-Your-Emoji-Faster是一个基于扩散模型的高效肖像动画框架，通过面部关键点驱动，解决了身份保持、表情准确传递和长期时间一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在肖像动画任务中面临身份保持困难、表情传递不准确、长期时间一致性差以及生成效率低等核心问题。

Method: 采用增强的Stable Diffusion框架，包含表情感知关键点作为显式运动信号和细粒度面部损失函数；提出渐进生成策略和泰勒插值缓存实现2.6倍无损加速。

Result: 在EmojiBench++基准测试中表现出优越的动画质量和可控性，支持真实人脸、卡通、雕塑和动物等多种肖像类型。

Conclusion: 该方法在保持高质量生成的同时实现了高效动画，使技术更加用户友好和易于访问，相关代码和数据集将开源。

Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework
for freestyle portrait animation driven by facial landmarks. The main
challenges in this task are preserving the identity of the reference portrait,
accurately transferring target expressions, and maintaining long-term temporal
consistency while ensuring generation efficiency. To address identity
preservation and accurate expression retargeting, we enhance Stable Diffusion
with two key components: a expression-aware landmarks as explicit motion
signals, which improve motion alignment, support exaggerated expressions, and
reduce identity leakage; and a fine-grained facial loss that leverages both
expression and facial masks to better capture subtle expressions and faithfully
preserve the reference appearance. With these components, our model supports
controllable and expressive animation across diverse portrait types, including
real faces, cartoons, sculptures, and animals. However, diffusion-based
frameworks typically struggle to efficiently generate long-term stable
animation results, which remains a core challenge in this task. To address
this, we propose a progressive generation strategy for stable long-term
animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless
acceleration. These two strategies ensure that our method produces high-quality
results efficiently, making it user-friendly and accessible. Finally, we
introduce EmojiBench++, a more comprehensive benchmark comprising diverse
portraits, driving videos, and landmark sequences. Extensive evaluations on
EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior
performance in both animation quality and controllability. The code, training
dataset and benchmark will be found in https://follow-your-emoji.github.io/.

</details>


### [42] [DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration](https://arxiv.org/abs/2509.16632)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: DA-Font是一个少样本字体生成框架，通过双注意力混合模块解决现有方法存在的笔画错误、伪影和模糊问题，在多种字体风格和字符上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 少样本字体生成旨在用少量字形参考创建新字体，可显著降低人工字体设计成本。但现有方法由于字体风格的多样性和复杂性，生成结果常存在可见缺陷。

Method: 提出DA-Font框架，集成双注意力混合模块（DAHM），包含组件注意力块和关系注意力块。组件注意力块利用内容图像的组件信息指导风格迁移，关系注意力块通过内容特征与原始和风格化组件表示的交互来细化空间关系。还设计了角点一致性损失和弹性网格特征损失来改善几何对齐。

Result: 大量实验表明，DA-Font在多种字体风格和字符上优于最先进方法，有效增强了结构完整性和局部保真度。

Conclusion: DA-Font通过双注意力机制和新的损失函数，成功解决了少样本字体生成中的结构完整性和局部保真度问题，为字体设计提供了高效解决方案。

Abstract: Few-shot font generation aims to create new fonts with a limited number of
glyph references. It can be used to significantly reduce the labor cost of
manual font design. However, due to the variety and complexity of font styles,
the results generated by existing methods often suffer from visible defects,
such as stroke errors, artifacts and blurriness. To address these issues, we
propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid
Module (DAHM). Specifically, we introduce two synergistic attention blocks: the
component attention block that leverages component information from content
images to guide the style transfer process, and the relation attention block
that further refines spatial relationships through interacting the content
feature with both original and stylized component-wise representations. These
two blocks collaborate to preserve accurate character shapes and stylistic
textures. Moreover, we also design a corner consistency loss and an elastic
mesh feature loss to better improve geometric alignment. Extensive experiments
show that our DA-Font outperforms the state-of-the-art methods across diverse
font styles and characters, demonstrating its effectiveness in enhancing
structural integrity and local fidelity. The source code can be found at
\href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.

</details>


### [43] [When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs](https://arxiv.org/abs/2509.16633)
*Abhirama Subramanyam Penamakuri,Navlika Singh,Piyush Arora,Anand Mishra*

Main category: cs.CV

TL;DR: 提出了Model Parity Aligner (MPA)框架，通过无标签图像和从大型视觉语言模型的有效知识转移来系统提升小型视觉语言模型的性能，弥补与大模型之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(L-VLMs)计算成本高，不适合资源受限场景；小型视觉语言模型(S-VLMs)效率高但性能存在显著差距。需要一种方法在保持效率的同时提升小模型性能。

Method: MPA采用基于对等性的策略方法，精确识别S-VLMs和L-VLMs之间的知识差异，并仅针对这些差异进行优化训练，而不是依赖标注训练数据的传统知识蒸馏方法。

Result: 在TextVQA、ST-VQA、ChartQA和OKVQA四个多样化VQA基准测试中，MPA一致提升了S-VLMs的性能，缩小了性能差距同时保持了计算效率。

Conclusion: MPA框架有效解决了S-VLMs性能不足的问题，为资源受限环境下的视觉语言任务提供了实用解决方案。

Abstract: Large Vision-Language Models (L-VLMs) have demonstrated remarkable
performance in various vision and language tasks, including visual question
answering (VQA). However, their high computational cost makes them impractical
for resource-constrained settings and inference-heavy applications. In
contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer
from a significant performance gap compared to their larger counterparts. In
this work, we introduce the Model Parity Aligner (MPA), a novel framework
designed to systematically improve S-VLMs by leveraging unlabeled images and
effective knowledge transfer from L-VLMs. Instead of traditional knowledge
distillation methods that rely on labeled training data, MPA employs a
strategic parity-based approach that precisely identifies the knowledge
disparities between S-VLMs and L-VLMs, and optimizes training by targeting only
these disparities. We conduct extensive experiments on four diverse VQA
benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires
specialized reasoning capabilities such as text recognition, chart
interpretation, and commonsense and factual understanding. Our results
demonstrate that MPA consistently enhances the performance of S-VLMs on all
benchmarks, reducing the performance gap while maintaining computational
efficiency. We make our code publicly available.

</details>


### [44] [Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification](https://arxiv.org/abs/2509.16635)
*Xulin Li,Yan Lu,Bin Liu,Jiaze Li,Qinhong Yang,Tao Gong,Qi Chu,Mang Ye,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了Anytime Person Re-identification (AT-ReID)新任务，旨在解决现有ReID方法在时间跨度上的局限性，通过构建AT-USTC大规模数据集和Uni-AT统一模型，实现多场景下的有效人员检索。


<details>
  <summary>Details</summary>
Motivation: 现有ReID任务和数据集受限于特定时间场景，无法满足实际应用中需要全天候、长时期的人员检索需求。

Method: 收集AT-USTC数据集（403k图像，21个月跨度），提出Uni-AT模型，包含多场景ReID框架、属性专家混合模块和分层动态加权策略。

Result: 实验表明该模型在所有场景下都取得了满意结果，并展现出优秀的泛化能力。

Conclusion: AT-ReID任务具有重要研究价值，提出的数据集和模型为全天候人员重识别提供了有效解决方案。

Abstract: In real applications, person re-identification (ReID) is expected to retrieve
the target person at any time, including both daytime and nighttime, ranging
from short-term to long-term. However, existing ReID tasks and datasets can not
meet this requirement, as they are constrained by available time and only
provide training and evaluation for specific scenarios. Therefore, we
investigate a new task called Anytime Person Re-identification (AT-ReID), which
aims to achieve effective retrieval in multiple scenarios based on variations
in time. To address the AT-ReID problem, we collect the first large-scale
dataset, AT-USTC, which contains 403k images of individuals wearing multiple
clothes captured by RGB and IR cameras. Our data collection spans 21 months,
and 270 volunteers were photographed on average 29.1 times across different
dates or scenes, 4-15 times more than current datasets, providing conditions
for follow-up investigations in AT-ReID. Further, to tackle the new challenge
of multi-scenario retrieval, we propose a unified model named Uni-AT, which
comprises a multi-scenario ReID (MS-ReID) framework for scenario-specific
features learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate
inter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)
strategy to ensure balanced training across all scenarios. Extensive
experiments show that our model leads to satisfactory results and exhibits
excellent generalization to all scenarios.

</details>


### [45] [Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination](https://arxiv.org/abs/2509.16639)
*Shangzhuo Xie,Qianqian Yang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的Grouping-Feature Coordination Module (GF-Core)，通过协调分组层和特征提取层来提升点云分析性能，同时设计了专门的自监督预训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注网络结构设计，但传统点云架构的潜力未被充分利用。研究发现通过模块集成而非结构修改可以获得显著性能提升。

Method: 提出GF-Core模块同时调节分组层和特征提取层，实现更精细的特征聚合；引入专门针对点云输入的自监督预训练策略。

Result: 在ModelNet40数据集上达到94.0%准确率，与先进框架性能相当但结构更简单；在ScanObjectNN三个变体上分别提升2.96%、6.34%和6.32%。

Conclusion: 通过战略性模块集成而非结构修改，可以有效提升点云分析性能，证明了传统架构的未开发潜力。

Abstract: Point cloud analysis has evolved with diverse network architectures, while
existing works predominantly focus on introducing novel structural designs.
However, conventional point-based architectures - processing raw points through
sequential sampling, grouping, and feature extraction layers - demonstrate
underutilized potential. We notice that substantial performance gains can be
unlocked through strategic module integration rather than structural
modifications. In this paper, we propose the Grouping-Feature Coordination
Module (GF-Core), a lightweight separable component that simultaneously
regulates both grouping layer and feature extraction layer to enable more
nuanced feature aggregation. Besides, we introduce a self-supervised
pretraining strategy specifically tailored for point-based inputs to enhance
model robustness in complex point cloud analysis scenarios. On ModelNet40
dataset, our method elevates baseline networks to 94.0% accuracy, matching
advanced frameworks' performance while preserving architectural simplicity. On
three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,
6.34%, and 6.32% respectively.

</details>


### [46] [ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents](https://arxiv.org/abs/2509.16645)
*Yichen Wang,Hangtao Zhang,Hewen Pan,Ziqi Zhou,Xianlong Wang,Peijin Guo,Lulu Xue,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: ADVEDM是一个针对视觉语言模型的细粒度对抗攻击框架，通过修改关键对象的感知来影响智能体决策，同时保持其他区域的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击要么需要完全了解受害者VLM（不切实际），要么因破坏过多语义信息导致与任务上下文不匹配，产生无效输出。需要一种更有效的攻击方法来影响物理世界中的智能体行为。

Method: 提出ADVEDM框架，包含两个变体：ADVEDM-R（移除特定对象语义）和ADVEDM-A（添加新对象语义），通过选择性修改关键对象感知来保持任务上下文一致性。

Result: 在通用场景和EDM任务中的实验结果表明，该方法实现了细粒度控制并具有优异的攻击性能。

Conclusion: ADVEDM能够有效影响VLM输出有效但错误的决策，对物理世界中的智能体安全构成实质性威胁。

Abstract: Vision-Language Models (VLMs), with their strong reasoning and planning
capabilities, are widely used in embodied decision-making (EDM) tasks in
embodied agents, such as autonomous driving and robotic manipulation. Recent
research has increasingly explored adversarial attacks on VLMs to reveal their
vulnerabilities. However, these attacks either rely on overly strong
assumptions, requiring full knowledge of the victim VLM, which is impractical
for attacking VLM-based agents, or exhibit limited effectiveness. The latter
stems from disrupting most semantic information in the image, which leads to a
misalignment between the perception and the task context defined by system
prompts. This inconsistency interrupts the VLM's reasoning process, resulting
in invalid outputs that fail to affect interactions in the physical world. To
this end, we propose a fine-grained adversarial attack framework, ADVEDM, which
modifies the VLM's perception of only a few key objects while preserving the
semantics of the remaining regions. This attack effectively reduces conflicts
with the task context, making VLMs output valid but incorrect decisions and
affecting the actions of agents, thus posing a more substantial safety threat
in the physical world. We design two variants of based on this framework,
ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific
object from the image and add the semantics of a new object into the image. The
experimental results in both general scenarios and EDM tasks demonstrate
fine-grained control and excellent attack performance.

</details>


### [47] [Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?](https://arxiv.org/abs/2509.16654)
*Xin Chen,Jia He,Maozheng Li,Dongliang Xu,Tianyu Wang,Yixiao Chen,Zhixin Lin,Yue Yao*

Main category: cs.CV

TL;DR: 本文系统评估了视觉语言模型在道路拓扑理解方面的能力，发现现有模型在空间推理方面存在显著瓶颈，尤其是开源模型表现较差，而模型能力与模型规模、推理标记长度和示例数量呈正相关。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在多模态推理方面取得了显著进展，但在自动驾驶领域的应用仍然有限，特别是在理解道路拓扑这一关键安全导航要求方面，现有研究关注不足且性能不理想。

Method: 将多视角图像投影到统一的平面坐标系中，融合成鸟瞰图车道，并基于这些鸟瞰图车道设计了四个拓扑相关的诊断性视觉问答任务，以捕捉空间拓扑推理的关键组成部分。

Result: 前沿闭源模型在某些任务中达到相对较高的准确率，但在一些人类能够回答的时间性问题上表现不佳（如GPT-4o在二分类问题中仅达到67.8%）。开源模型即使达到300亿参数规模也表现显著困难。

Conclusion: 空间推理仍然是当前视觉语言模型的基本瓶颈，模型能力与模型规模、推理标记长度和提供的示例数量呈正相关，这为未来研究指明了方向。

Abstract: Vision-Language Models (VLMs) have recently shown remarkable progress in
multimodal reasoning, yet their applications in autonomous driving remain
limited. In particular, the ability to understand road topology, a key
requirement for safe navigation, has received relatively little attention.
While some recent works have begun to explore VLMs in driving contexts, their
performance on topology reasoning is far from satisfactory. In this work, we
systematically evaluate VLMs' capabilities in road topology understanding.
Specifically, multi-view images are projected into unified ground-plane
coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these
BEV lanes, we formulate four topology-related diagnostic VQA tasks, which
together capture essential components of spatial topology reasoning. Through
extensive evaluation, we find that while frontier closed-source models (e.g.,
GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some
temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in
vector, a two-class classification problem). Furthermore, we find open-source
VLMs, even at 30B scale, struggle significantly. These results indicate that
spatial reasoning remains a fundamental bottleneck for current VLMs. We also
find that the model's capability is positively correlated with model size,
length of reasoning tokens and shots provided as examples, showing direction
for future research.

</details>


### [48] [MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness](https://arxiv.org/abs/2509.16673)
*Sinuo Wang,Yutong Xie,Yuyuan Liu,Qi Wu*

Main category: cs.CV

TL;DR: MedCutMix是一种新颖的多模态疾病中心数据增强方法，通过在医学报告中执行诊断句子CutMix，并建立诊断句子与医学图像之间的跨注意力机制来指导图像模态的注意力流形混合，以解决医学视觉语言预训练中数据多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言预训练（VLP）虽然能减少手动标注需求并增强下游任务的语义理解，但其依赖图像-文本数据集存在隐私问题和标注成本高的挑战。现有数据增强方法难以捕捉医学数据中微妙复杂的变化，多样性有限。

Method: 提出MedCutMix方法：在医学报告内执行诊断句子CutMix，建立诊断句子与医学图像之间的跨注意力机制，指导图像模态的注意力流形混合。

Result: 在四个下游放射学诊断数据集上超越了先前的方法，显示出在放射学VLP中提升性能和泛化能力的有效性。

Conclusion: MedCutMix通过疾病中心的多模态数据增强策略，成功解决了医学VLP中的数据多样性挑战，显著提升了放射学诊断任务的性能。

Abstract: Vision-Language Pre-training (VLP) is drawing increasing interest for its
ability to minimize manual annotation requirements while enhancing semantic
understanding in downstream tasks. However, its reliance on image-text datasets
poses challenges due to privacy concerns and the high cost of obtaining paired
annotations. Data augmentation emerges as a viable strategy to address this
issue, yet existing methods often fall short of capturing the subtle and
complex variations in medical data due to limited diversity. To this end, we
propose MedCutMix, a novel multi-modal disease-centric data augmentation
method. MedCutMix performs diagnostic sentence CutMix within medical reports
and establishes the cross-attention between the diagnostic sentence and medical
image to guide attentive manifold mix within the imaging modality. Our approach
surpasses previous methods across four downstream radiology diagnosis datasets,
highlighting its effectiveness in enhancing performance and generalizability in
radiology VLP.

</details>


### [49] [FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World](https://arxiv.org/abs/2509.16674)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li*

Main category: cs.CV

TL;DR: FitPro是一个面向开放世界的交互式零样本文本行人检索框架，通过特征对比解码、增量语义挖掘和查询感知分层检索三个创新组件，解决了模型泛化能力和语义理解不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在受限环境下取得进展，但在开放世界交互检索中面临模型泛化能力有限和语义理解不足的挑战。

Method: FitPro包含三个核心组件：特征对比解码（FCD）用于生成高质量结构化行人描述，增量语义挖掘（ISM）构建多视角整体行人表示，查询感知分层检索（QHR）根据查询类型动态优化检索流程。

Result: 在五个公共数据集和两种评估协议上的大量实验表明，FitPro显著克服了现有方法在交互检索中的泛化限制和语义建模约束。

Conclusion: FitPro为实际部署铺平了道路，代码和数据将在GitHub上发布。

Abstract: Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target
pedestrians in visual scenes according to natural language descriptions.
Although existing methods have achieved progress under constrained settings,
interactive retrieval in the open-world scenario still suffers from limited
model generalization and insufficient semantic understanding. To address these
challenges, we propose FitPro, an open-world interactive zero-shot TPR
framework with enhanced semantic comprehension and cross-scene adaptability.
FitPro has three innovative components: Feature Contrastive Decoding (FCD),
Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval
(QHR). The FCD integrates prompt-guided contrastive decoding to generate
high-quality structured pedestrian descriptions from denoised images,
effectively alleviating semantic drift in zero-shot scenarios. The ISM
constructs holistic pedestrian representations from multi-view observations to
achieve global semantic modeling in multi-turn interactions,thereby improving
robustness against viewpoint shifts and fine-grained variations in
descriptions. The QHR dynamically optimizes the retrieval pipeline according to
query types, enabling efficient adaptation to multi-modal and multi-view
inputs. Extensive experiments on five public datasets and two evaluation
protocols demonstrate that FitPro significantly overcomes the generalization
limitations and semantic modeling constraints of existing methods in
interactive retrieval, paving the way for practical deployment. The code and
data will be released at https://github.com/
lilo4096/FitPro-Interactive-Person-Retrieval.

</details>


### [50] [Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence](https://arxiv.org/abs/2509.16677)
*Wenxin Li,Kunyu Peng,Di Wen,Ruiping Liu,Mengfei Duan,Kai Luo,Kailun Yang*

Main category: cs.CV

TL;DR: 本文首次研究了基于动作的视频对象分割在标签噪声下的问题，提出了两种标签噪声类型（文本提示噪声和掩码标注噪声），建立了首个基准数据集ActiSeg-NL，并分析了不同学习策略的鲁棒性特征。


<details>
  <summary>Details</summary>
Motivation: 基于动作的视频对象分割依赖于大规模标注和提示，但这些标注成本高、不一致且容易受到多模态噪声（如不精确的掩码和指代模糊）的影响。目前这一挑战尚未被探索。

Method: 1）引入两种标签噪声类型；2）建立ActiSeg-NL基准数据集并适配六种标签噪声学习策略；3）提出并行掩码头机制（PMHM）来处理掩码标注噪声；4）进行定性评估和比较分析。

Result: 研究发现不同学习策略表现出不同的鲁棒性特征，受前景-背景权衡的支配。定性评估揭示了边界泄漏、定位错误等特征性失败模式。PMHM机制有效处理掩码标注噪声。

Conclusion: 本文为基于动作的视频对象分割在标签噪声下的研究奠定了基础，建立的基准数据集和源代码将为后续研究提供支持，揭示了噪声类型与失败模式之间的联系以及不同学习策略的鲁棒性特征。

Abstract: Embodied intelligence relies on accurately segmenting objects actively
involved in interactions. Action-based video object segmentation addresses this
by linking segmentation with action semantics, but it depends on large-scale
annotations and prompts that are costly, inconsistent, and prone to multimodal
noise such as imprecise masks and referential ambiguity. To date, this
challenge remains unexplored. In this work, we take the first step by studying
action-based video object segmentation under label noise, focusing on two
sources: textual prompt noise (category flips and within-category noun
substitutions) and mask annotation noise (perturbed object boundaries to mimic
imprecise supervision). Our contributions are threefold. First, we introduce
two types of label noises for the action-based video object segmentation task.
Second, we build up the first action-based video object segmentation under a
label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies
to this setting, and establish protocols for evaluating them under textual,
boundary, and mixed noise. Third, we provide a comprehensive analysis linking
noise types to failure modes and robustness gains, and we introduce a Parallel
Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative
evaluations further reveal characteristic failure modes, including boundary
leakage and mislocalization under boundary perturbations, as well as occasional
identity substitutions under textual flips. Our comparative analysis reveals
that different learning strategies exhibit distinct robustness profiles,
governed by a foreground-background trade-off where some achieve balanced
performance while others prioritize foreground accuracy at the cost of
background precision. The established benchmark and source code will be made
publicly available at https://github.com/mylwx/ActiSeg-NL.

</details>


### [51] [IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation](https://arxiv.org/abs/2509.16678)
*Suorong Yang,Hongchao Yang,Suhan Guo,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: IPF-RDA是一个新颖的信息保留框架，旨在增强数据增强方法的鲁棒性，通过识别对数据增强操作最敏感的关键点并自适应保留重要信息，从而提高深度模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 数据增强虽然能提升模型性能，但会引入分布偏移和噪声，限制深度网络的潜力并降低性能。需要一种方法来增强数据增强的鲁棒性。

Method: 提出IPF-RDA框架，包含：(1)类判别信息估计算法，识别对数据增强操作最脆弱的点及其重要性分数；(2)信息保留方案，自适应保留增强样本中的关键信息并确保数据多样性。将数据增强方法按操作类型分为三类并整合到框架中。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet、CUHK03、Market1501、Oxford Flower和MNIST等多个数据集上的广泛实验表明，IPF-RDA能持续改进多种常用最先进数据增强方法的性能。

Conclusion: IPF-RDA虽然简单，但能有效增强数据增强方法的鲁棒性，充分发挥其潜力，在各种深度模型和数据集上都表现出良好的性能和可扩展性。

Abstract: Data augmentation is widely utilized as an effective technique to enhance the
generalization performance of deep models. However, data augmentation may
inevitably introduce distribution shifts and noises, which significantly
constrain the potential and deteriorate the performance of deep networks. To
this end, we propose a novel information-preserving framework, namely IPF-RDA,
to enhance the robustness of data augmentations in this paper. IPF-RDA combines
the proposal of (i) a new class-discriminative information estimation algorithm
that identifies the points most vulnerable to data augmentation operations and
corresponding importance scores; And (ii) a new information-preserving scheme
that preserves the critical information in the augmented samples and ensures
the diversity of augmented data adaptively. We divide data augmentation methods
into three categories according to the operation types and integrate these
approaches into our framework accordingly. After being integrated into our
framework, the robustness of data augmentation methods can be enhanced and
their full potential can be unleashed. Extensive experiments demonstrate that
although being simple, IPF-RDA consistently improves the performance of
numerous commonly used state-of-the-art data augmentation methods with popular
deep models on a variety of datasets, including CIFAR-10, CIFAR-100,
Tiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its
performance and scalability are stressed. The implementation is available at
https://github.com/Jackbrocp/IPF-RDA.

</details>


### [52] [ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering](https://arxiv.org/abs/2509.16680)
*Xingjian Diao,Weiyi Wu,Keyi Kong,Peijun Qing,Xinwen Xu,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: ProtoVQA是一个基于原型的可解释视觉问答框架，通过学习问题感知原型作为推理锚点，将答案与判别性图像区域连接起来，提供细粒度的解释。


<details>
  <summary>Details</summary>
Motivation: 随着VQA在医疗影像和自动驾驶等安全关键领域的应用增加，模型不仅需要提供准确答案，还需要提供人类易于理解和验证的解释。原型建模在纯视觉推理任务中显示出可解释性潜力，但在VQA领域尚未充分探索。

Method: ProtoVQA框架包含三个关键组件：(i)学习问题感知原型作为推理锚点；(ii)应用空间约束匹配确保选择的证据具有连贯性和语义相关性；(iii)通过共享原型骨干网络同时支持答案生成和定位任务。

Result: 在Visual7W数据集上的实验表明，ProtoVQA在保持竞争力的准确性的同时，能够产生忠实、细粒度的解释。提出的VLAS指标能够有效衡量模型关注区域与真实证据的对齐程度。

Conclusion: ProtoVQA推进了透明和可信VQA系统的发展，为安全关键领域的应用提供了更好的可解释性支持。

Abstract: Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.

</details>


### [53] [Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels](https://arxiv.org/abs/2509.16684)
*Qi Zhang,Bin Li,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了一种主动视图选择方法（AVS），用于多视角人群计数和定位任务，该方法能够在跨场景设置下以有限的标注需求获得更好的场景级结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注在输入视图上准确预测人群，而忽视了选择'最佳'相机视角以全面感知场景中所有人群的问题。现有视图选择方法需要大量标注视图和图像，且缺乏跨场景能力，限制了应用场景。

Method: 首先提出独立视图选择方法（IVS），考虑视图和场景几何信息进行视图选择；然后提出主动视图选择方法（AVS），联合优化视图选择、标注和下游任务，在视图选择过程中同时考虑视图/场景几何信息和下游任务模型的预测结果。

Result: 在多视角计数和定位任务上的实验表明，所提出的主动视图选择方法（AVS）具有跨场景能力和有限标注需求优势，优于现有方法且具有更广泛的应用场景。

Conclusion: AVS方法通过主动视图选择策略，有效解决了多视角人群分析中的视图选择问题，在保持高性能的同时显著降低了标注需求并增强了跨场景适用性。

Abstract: Multi-view crowd counting and localization fuse the input multi-views for
estimating the crowd number or locations on the ground. Existing methods mainly
focus on accurately predicting on the crowd shown in the input views, which
neglects the problem of choosing the `best' camera views to perceive all crowds
well in the scene. Besides, existing view selection methods require massive
labeled views and images, and lack the ability for cross-scene settings,
reducing their application scenarios. Thus, in this paper, we study the view
selection issue for better scene-level multi-view crowd counting and
localization results with cross-scene ability and limited label demand, instead
of input-view-level results. We first propose an independent view selection
method (IVS) that considers view and scene geometries in the view selection
strategy and conducts the view selection, labeling, and downstream tasks
independently. Based on IVS, we also put forward an active view selection
method (AVS) that jointly optimizes the view selection, labeling, and
downstream tasks. In AVS, we actively select the labeled views and consider
both the view/scene geometries and the predictions of the downstream task
models in the view selection process. Experiments on multi-view counting and
localization tasks demonstrate the cross-scene and the limited label demand
advantages of the proposed active view selection method (AVS), outperforming
existing methods and with wider application scenarios.

</details>


### [54] [Towards a Transparent and Interpretable AI Model for Medical Image Classifications](https://arxiv.org/abs/2509.16685)
*Binbin Wen,Yihang Wu,Tareef Daqqaq,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文研究了可解释人工智能（XAI）在医学领域的应用，通过医疗数据集模拟展示XAI如何提高AI决策的透明度，并讨论了该领域面临的挑战。


<details>
  <summary>Details</summary>
Motivation: AI在医学中的应用虽然前景广阔，但复杂模型的不可解释性限制了其临床实用性，因此需要研究XAI方法来提高AI决策的透明度和可解释性。

Method: 使用多种医疗数据集进行模拟实验，阐明XAI模型的内部工作机制，展示XAI如何有效解释AI预测结果。

Result: 数据集驱动的模拟表明XAI能够有效解释AI预测，改善医疗专业人员的决策过程。

Conclusion: 需要持续开发和探索XAI方法，特别是从多样化医疗数据集的角度，以促进其在医疗领域的采用和有效性。

Abstract: The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.

</details>


### [55] [Spectral Compressive Imaging via Chromaticity-Intensity Decomposition](https://arxiv.org/abs/2509.16690)
*Xiaodong Wang,Zijun He,Ping Wang,Lishun Wang,Yanan Hu,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种色度-强度分解框架CIDNet，用于解决编码孔径快照光谱成像(CASSI)中的HSI重建问题，通过将HSI分解为空间平滑的强度图和光谱变化的色度立方体，实现了对光照不变反射率的恢复。


<details>
  <summary>Details</summary>
Motivation: CASSI捕获的测量数据纠缠了空间和光谱信息，导致HSI重建成为一个严重不适定的逆问题。此外，捕获的辐射度依赖于场景光照，难以恢复对光照条件不变的固有光谱反射率。

Method: 提出色度-强度分解框架，将HSI分解为强度图和色度立方体；开发CIDNet网络，集成混合空间-光谱Transformer重建精细色度，以及退化感知的空间自适应噪声估计模块。

Result: 在合成和真实世界CASSI数据集上的广泛实验表明，该方法在光谱和色度保真度方面均实现了优越性能。

Conclusion: CIDNet通过色度-强度分解有效解决了CASSI中的HSI重建挑战，能够恢复光照不变的反射率特征，代码和模型将公开可用。

Abstract: In coded aperture snapshot spectral imaging (CASSI), the captured measurement
entangles spatial and spectral information, posing a severely ill-posed inverse
problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured
radiance inherently depends on scene illumination, making it difficult to
recover the intrinsic spectral reflectance that remains invariant to lighting
conditions. To address these challenges, we propose a chromaticity-intensity
decomposition framework, which disentangles an HSI into a spatially smooth
intensity map and a spectrally variant chromaticity cube. The chromaticity
encodes lighting-invariant reflectance, enriched with high-frequency spatial
details and local spectral sparsity. Building on this decomposition, we develop
CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a
dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral
Transformer tailored to reconstruct fine-grained and sparse spectral
chromaticity and a degradation-aware, spatially-adaptive noise estimation
module that captures anisotropic noise across iterative stages. Extensive
experiments on both synthetic and real-world CASSI datasets demonstrate that
our method achieves superior performance in both spectral and chromaticity
fidelity. Code and models will be publicly available.

</details>


### [56] [InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention](https://arxiv.org/abs/2509.16691)
*Qiang Xiang,Shuang Sun,Binglei Li,Dejia Song,Huaxia Li,Nemo Chen,Xu Tang,Yao Hu,Junping Zhang*

Main category: cs.CV

TL;DR: 提出InstanceAssemble架构，通过实例组装注意力机制实现布局到图像生成，支持边界框位置控制和多模态内容控制，兼容现有DiT模型，并创建Denselayout基准和Layout Grounding Score评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法性能仍不理想，需要更精确的位置控制和内容控制能力。

Method: 基于实例组装注意力机制，通过轻量级LoRA模块适配现有DiT文本到图像模型，支持边界框和多模态内容控制。

Result: 在复杂布局条件下达到最先进性能，与多样化风格LoRA模块兼容性好。

Conclusion: InstanceAssemble方法在布局控制精度和生成质量方面表现优异，为布局到图像生成提供了有效解决方案。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules.

</details>


### [57] [Animalbooth: multimodal feature enhancement for animal subject personalization](https://arxiv.org/abs/2509.16702)
*Chen Liu,Haitao Wu,Kafeng Wang,Xiaowang Zhang*

Main category: cs.CV

TL;DR: AnimalBooth是一个用于个性化动物图像生成的框架，通过Animal Net和自适应注意力模块增强身份保持，并利用频率控制的特征集成模块实现从全局结构到细节纹理的渐进生成。


<details>
  <summary>Details</summary>
Motivation: 个性化动物图像生成面临外观线索丰富和形态变异大的挑战，现有方法存在跨域特征对齐错误导致身份漂移的问题。

Method: 提出AnimalBooth框架，包含Animal Net、自适应注意力模块和频率控制的特征集成模块（使用离散余弦变换在潜在空间进行滤波），并构建了AnimalBench高分辨率数据集。

Result: 在多个基准测试中，AnimalBooth持续优于强基线方法，在身份保真度和感知质量方面都有显著提升。

Conclusion: AnimalBooth通过有效的跨域对齐和渐进式生成策略，成功解决了动物图像个性化生成中的身份保持问题。

Abstract: Personalized animal image generation is challenging due to rich appearance
cues and large morphological variability. Existing approaches often exhibit
feature misalignment across domains, which leads to identity drift. We present
AnimalBooth, a framework that strengthens identity preservation with an Animal
Net and an adaptive attention module, mitigating cross domain alignment errors.
We further introduce a frequency controlled feature integration module that
applies Discrete Cosine Transform filtering in the latent space to guide the
diffusion process, enabling a coarse to fine progression from global structure
to detailed texture. To advance research in this area, we curate AnimalBench, a
high resolution dataset for animal personalization. Extensive experiments show
that AnimalBooth consistently outperforms strong baselines on multiple
benchmarks and improves both identity fidelity and perceptual quality.

</details>


### [58] [When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2509.16704)
*Pan Liu,Jinshi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为置信度可分离学习（CSL）的新方法，用于解决半监督语义分割中伪标签选择的问题。该方法通过凸优化建立样本特定的决策边界，并引入随机掩码机制来学习低可靠性区域的上下文关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定置信度阈值选择伪标签，但无法应对网络过度自信倾向，导致正确和错误预测在高置信度区域重叠，难以分离并放大模型认知偏差。同时，直接丢弃低置信度预测会破坏空间语义连续性，造成关键上下文丢失。

Method: CSL将伪标签选择建模为置信度分布特征空间中的凸优化问题，建立样本特定的决策边界来区分可靠和不可靠预测。此外，引入可靠像素的随机掩码，指导网络从低可靠性区域学习上下文关系。

Result: 在Pascal、Cityscapes和COCO基准测试上的大量实验结果表明，CSL相比最先进方法表现更优。

Conclusion: CSL有效解决了半监督语义分割中伪标签选择的局限性，通过优化决策边界和上下文学习机制，显著提升了分割性能。

Abstract: While significant advances exist in pseudo-label generation for
semi-supervised semantic segmentation, pseudo-label selection remains
understudied. Existing methods typically use fixed confidence thresholds to
retain high-confidence predictions as pseudo-labels. However, these methods
cannot cope with network overconfidence tendency, where correct and incorrect
predictions overlap significantly in high-confidence regions, making separation
challenging and amplifying model cognitive bias. Meanwhile, the direct
discarding of low-confidence predictions disrupts spatial-semantic continuity,
causing critical context loss. We propose Confidence Separable Learning (CSL)
to address these limitations. CSL formulates pseudo-label selection as a convex
optimization problem within the confidence distribution feature space,
establishing sample-specific decision boundaries to distinguish reliable from
unreliable predictions. Additionally, CSL introduces random masking of reliable
pixels to guide the network in learning contextual relationships from
low-reliability regions, thereby mitigating the adverse effects of discarding
uncertain predictions. Extensive experimental results on the Pascal,
Cityscapes, and COCO benchmarks show that CSL performs favorably against
state-of-the-art methods. Code and model weights are available at
https://github.com/PanLiuCSU/CSL.

</details>


### [59] [Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding](https://arxiv.org/abs/2509.16721)
*Haoyuan Li,Rui Liu,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Text-Scene是一个自动将3D场景解析为文本描述以进行场景理解的框架，通过结合几何分析和多模态大语言模型，生成准确、详细且人类可解释的描述，并提出了InPlan3D基准来评估3D任务规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在3D场景理解中的挑战，包括3D环境涉及更丰富的概念（如空间关系、功能、物理、布局等）以及缺乏大规模3D视觉语言数据集的问题。

Method: 给定3D场景，模型识别物体属性和空间关系，然后生成整个场景的连贯摘要，通过几何分析和多模态大语言模型相结合的方式，无需人工干预即可弥合3D观察与语言之间的差距。

Result: 在基准测试上的实验结果表明，文本解析能够忠实表示3D场景并有益于下游任务。提出的InPlan3D基准包含636个室内场景中的3174个长期规划任务。

Conclusion: 该方法强调清晰性和可访问性，旨在通过语言使3D场景内容可理解。代码和数据集将发布。

Abstract: Enabling agents to understand and interact with complex 3D scenes is a
fundamental challenge for embodied artificial intelligence systems. While
Multimodal Large Language Models (MLLMs) have achieved significant progress in
2D image understanding, extending such capabilities to 3D scenes remains
difficult: 1) 3D environment involves richer concepts such as spatial
relationships, affordances, physics, layout, and so on, 2) the absence of
large-scale 3D vision-language datasets has posed a significant obstacle. In
this paper, we introduce Text-Scene, a framework that automatically parses 3D
scenes into textual descriptions for scene understanding. Given a 3D scene, our
model identifies object attributes and spatial relationships, and then
generates a coherent summary of the whole scene, bridging the gap between 3D
observation and language without requiring human-in-the-loop intervention. By
leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions
that are accurate, detailed, and human-interpretable, capturing object-level
details and global-level context. Experimental results on benchmarks
demonstrate that our textual parses can faithfully represent 3D scenes and
benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we
present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of
3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity
and accessibility in our approach, aiming to make 3D scene content
understandable through language. Code and datasets will be released.

</details>


### [60] [Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment](https://arxiv.org/abs/2509.16727)
*Xin Lei Lin,Soroush Mehraban,Abhishek Moturu,Babak Taati*

Main category: cs.CV

TL;DR: 3DPain是一个大规模合成数据集，ViTPain是基于视觉Transformer的跨模态蒸馏框架，共同为自动化疼痛评估提供了可控、多样且临床可靠的基础。


<details>
  <summary>Details</summary>
Motivation: 自动化疼痛评估对于非交流患者（如痴呆症患者）至关重要，但现有数据集存在严重的人口统计和标签不平衡问题，且当前生成模型无法精确控制面部动作单元、面部结构或临床验证的疼痛水平。

Method: 采用三阶段框架生成多样化的3D网格，使用扩散模型进行纹理处理，并应用AU驱动的面部绑定来合成多视角面部图像，包含配对的中性和疼痛图像、AU配置、PSPI评分以及首个数据集级别的疼痛区域热图注释。

Result: 数据集包含82,500个样本，涵盖25,000个疼痛表情热图和2,500个合成身份，在年龄、性别和种族方面保持平衡。ViTPain框架通过热图训练的教师模型指导RGB图像训练的学生模型，提高了准确性、可解释性和临床可靠性。

Conclusion: 3DPain和ViTPain共同建立了一个可控、多样化且临床基础扎实的框架，为通用自动化疼痛评估提供了可靠基础。

Abstract: Automated pain assessment from facial expressions is crucial for
non-communicative patients, such as those with dementia. Progress has been
limited by two challenges: (i) existing datasets exhibit severe demographic and
label imbalance due to ethical constraints, and (ii) current generative models
cannot precisely control facial action units (AUs), facial structure, or
clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for
automated pain assessment, featuring unprecedented annotation richness and
demographic diversity. Our three-stage framework generates diverse 3D meshes,
textures them with diffusion models, and applies AU-driven face rigging to
synthesize multi-view faces with paired neutral and pain images, AU
configurations, PSPI scores, and the first dataset-level annotations of
pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain
expression heatmaps and 2,500 synthetic identities balanced by age, gender, and
ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal
distillation framework in which a heatmap-trained teacher guides a student
trained on RGB images, enhancing accuracy, interpretability, and clinical
reliability. Together, 3DPain and ViTPain establish a controllable, diverse,
and clinically grounded foundation for generalizable automated pain assessment.

</details>


### [61] [Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2509.16738)
*Kai Jiang,Zhengyan Shi,Dell Zhang,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于信息理论的混合噪声方法（Min），用于解决类增量学习中预训练模型参数漂移的问题，通过在中间特征中嵌入有益噪声来保留模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 类增量学习中，预训练模型在微调时会出现参数漂移，这会损害模型的泛化能力。研究发现噪声并不总是有害的，适当的噪声可以抑制低相关性特征，为未来任务留出空间。

Method: 提出Mixture of Noise（Min）方法：1）从新任务的高维特征中学习任务特定噪声；2）动态调整权重以优化不同任务噪声的混合；3）将有益噪声嵌入中间特征以掩盖无效模式的响应。

Result: 在六个基准数据集上的广泛实验表明，Min在大多数增量设置中实现了最先进的性能，特别是在50步增量设置中表现尤为突出。

Conclusion: 研究表明有益噪声在持续学习中具有显著潜力，Min方法有效缓解了骨干网络因适应新任务而导致的泛化能力下降问题。

Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories
while retaining the knowledge of old ones. Pre-trained models (PTMs) show
promising capabilities in CIL. However, existing approaches that apply
lightweight fine-tuning to backbones still induce parameter drift, thereby
compromising the generalization capability of pre-trained models. Parameter
drift can be conceptualized as a form of noise that obscures critical patterns
learned for previous tasks. However, recent researches have shown that noise is
not always harmful. For example, the large number of visual patterns learned
from pre-training can be easily abused by a single task, and introducing
appropriate noise can suppress some low-correlation features, thus leaving a
margin for future tasks. To this end, we propose learning beneficial noise for
CIL guided by information theory and propose Mixture of Noise (Min), aiming to
mitigate the degradation of backbone generalization from adapting new tasks.
Specifically, task-specific noise is learned from high-dimension features of
new tasks. Then, a set of weights is adjusted dynamically for optimal mixture
of different task noise. Finally, Min embeds the beneficial noise into the
intermediate features to mask the response of inefficient patterns. Extensive
experiments on six benchmark datasets demonstrate that Min achieves
state-of-the-art performance in most incremental settings, with particularly
outstanding results in 50-steps incremental settings. This shows the
significant potential for beneficial noise in continual learning. Code is
available at https://github.com/ASCIIJK/MiN-NeurIPS2025.

</details>


### [62] [CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding](https://arxiv.org/abs/2509.16745)
*Ritabrata Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: CAMBench-QR是一个结构感知的基准测试，利用QR码的几何结构来评估视觉解释方法是否能够准确定位关键子结构并避免背景干扰。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉解释方法往往看似合理但缺乏结构忠实性，需要一种能够测试方法是否真正关注必要结构的评估基准。

Method: 通过合成QR码/非QR码数据，使用精确掩码和受控失真，开发结构感知指标（查找器/定时线质量比、背景泄漏、覆盖AUC、结构距离等）来评估代表性CAM方法。

Result: 在零样本和最后块微调两种实用场景下，对LayerCAM、EigenGrad-CAM、XGrad-CAM等方法进行了基准测试。

Conclusion: CAMBench-QR提供了一个简单、可复现的结构感知评估标准，可作为视觉解释是否真正结构感知的试金石。

Abstract: Visual explanations are often plausible but not structurally faithful. We
introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical
geometry of QR codes (finder patterns, timing lines, module grid) to test
whether CAM methods place saliency on requisite substructures while avoiding
background. CAMBench-QR synthesizes QR/non-QR data with exact masks and
controlled distortions, and reports structure-aware metrics (Finder/Timing Mass
Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside
causal occlusion, insertion/deletion faithfulness, robustness, and latency. We
benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)
under two practical regimes of zero-shot and last-block fine-tuning. The
benchmark, metrics, and training recipes provide a simple, reproducible
yardstick for structure-aware evaluation of visual explanations. Hence we
propose that CAMBENCH-QR can be used as a litmus test of whether visual
explanations are truly structure-aware.

</details>


### [63] [HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis](https://arxiv.org/abs/2509.16748)
*Heyuan Li,Kenkun Liu,Lingteng Qiu,Qi Zuo,Keru Zheng,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种新的混合平面（hy-plane）表示方法，结合了平面和球形平面的优点，解决了传统tri-plane表示中的特征纠缠、特征映射不均匀和特征穿透问题，在头部图像合成中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统tri-plane表示在3D感知GAN中存在特征纠缠导致镜像伪影的问题，而球形tri-plane虽然解决了特征纠缠但存在特征映射不均匀和特征利用率低的问题。两种方法都存在特征穿透问题，限制了其性能潜力。

Method: 提出混合平面表示，结合平面和球形平面的优势；引入近等面积扭曲策略替代传统的theta-phi扭曲，最大化方形特征图的有效利用；生成器合成单通道统一特征图而非多通道分离特征图，消除特征穿透。

Result: HyPlaneHead方法在完整头部图像合成任务中实现了最先进的性能，显著改善了图像细节生成质量。

Conclusion: hy-plane表示通过系统性的技术改进，成功解决了tri-plane表示中的关键问题，为3D感知GAN提供了更有效的表示方法。

Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for
head image synthesis and other 3D object/scene modeling tasks due to their
efficiency. However, querying features via Cartesian coordinate projection
often leads to feature entanglement, which results in mirroring artifacts. A
recent work, SphereHead, attempted to address this issue by introducing
spherical tri-planes based on a spherical coordinate system. While it
successfully mitigates feature entanglement, SphereHead suffers from uneven
mapping between the square feature maps and the spherical planes, leading to
inefficient feature map utilization during rendering and difficulties in
generating fine image details. Moreover, both tri-plane and spherical tri-plane
representations share a subtle yet persistent issue: feature penetration across
convolutional channels can cause interference between planes, particularly when
one plane dominates the others. These challenges collectively prevent
tri-plane-based methods from reaching their full potential. In this paper, we
systematically analyze these problems for the first time and propose innovative
solutions to address them. Specifically, we introduce a novel hybrid-plane
(hy-plane for short) representation that combines the strengths of both planar
and spherical planes while avoiding their respective drawbacks. We further
enhance the spherical plane by replacing the conventional theta-phi warping
with a novel near-equal-area warping strategy, which maximizes the effective
utilization of the square feature map. In addition, our generator synthesizes a
single-channel unified feature map instead of multiple feature maps in separate
channels, thereby effectively eliminating feature penetration. With a series of
technical improvements, our hy-plane representation enables our method,
HyPlaneHead, to achieve state-of-the-art performance in full-head image
synthesis.

</details>


### [64] [DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images](https://arxiv.org/abs/2509.16767)
*Ozgur Kara,Harris Nisar,James M. Rehg*

Main category: cs.CV

TL;DR: DiffEye是一个基于扩散模型的训练框架，用于生成连续且多样化的眼动轨迹，解决了现有方法无法捕捉人类视觉注意力多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有眼动预测模型通常基于离散的注视点序列，丢弃了原始轨迹的丰富信息，且无法捕捉不同观察者之间的变异性，通常只能生成单一固定长度的扫描路径。

Method: 提出DiffEye扩散模型，引入对应位置嵌入(CPE)组件，将空间注视信息与视觉输入的基于补丁的语义特征对齐，利用原始眼动轨迹而非扫描路径进行训练。

Result: DiffEye能够生成高质量、真实的眼动模式，在扫描路径生成方面达到最先进性能，并首次实现了连续眼动轨迹的生成。

Conclusion: DiffEye是首个在自然图像上使用扩散模型并充分利用原始眼动数据丰富性的方法，生成的轨迹能更准确地反映人类视觉注意力的分布。

Abstract: Numerous models have been developed for scanpath and saliency prediction,
which are typically trained on scanpaths, which model eye movement as a
sequence of discrete fixation points connected by saccades, while the rich
information contained in the raw trajectories is often discarded. Moreover,
most existing approaches fail to capture the variability observed among human
subjects viewing the same image. They generally predict a single scanpath of
fixed, pre-defined length, which conflicts with the inherent diversity and
stochastic nature of real-world visual attention. To address these challenges,
we propose DiffEye, a diffusion-based training framework designed to model
continuous and diverse eye movement trajectories during free viewing of natural
images. Our method builds on a diffusion model conditioned on visual stimuli
and introduces a novel component, namely Corresponding Positional Embedding
(CPE), which aligns spatial gaze information with the patch-based semantic
features of the visual input. By leveraging raw eye-tracking trajectories
rather than relying on scanpaths, DiffEye captures the inherent variability in
human gaze behavior and generates high-quality, realistic eye movement
patterns, despite being trained on a comparatively small dataset. The generated
trajectories can also be converted into scanpaths and saliency maps, resulting
in outputs that more accurately reflect the distribution of human visual
attention. DiffEye is the first method to tackle this task on natural images
using a diffusion model while fully leveraging the richness of raw eye-tracking
data. Our extensive evaluation shows that DiffEye not only achieves
state-of-the-art performance in scanpath generation but also enables, for the
first time, the generation of continuous eye movement trajectories. Project
webpage: https://diff-eye.github.io/

</details>


### [65] [MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation](https://arxiv.org/abs/2509.16768)
*Omid Bonakdar,Nasser Mozayani*

Main category: cs.CV

TL;DR: MMPart是一个从单张图像生成部件感知3D模型的创新框架，通过视觉语言模型生成提示，控制对象分离和遮挡部分重建，最终生成可编辑的3D组件模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法大多生成封闭网格，缺乏结构信息，限制了编辑、动画和语义理解能力。现有部件感知方法存在用户无法控制对象分离方式和遮挡部分重建的问题。

Method: 1. 使用VLM根据输入图像和用户描述生成提示集；2. 基于初始图像和提示生成每个对象的独立图像；3. 多视图生成阶段生成不同视角的一致图像；4. 重建模型将多视图图像转换为3D模型。

Result: MMPart框架能够生成具有语义意义的部件分解3D模型，用户可以通过描述控制对象分离方式和遮挡区域的重建。

Conclusion: 该方法解决了现有部件感知3D生成方法的局限性，提供了更好的用户控制和更准确的结构信息，为3D模型的编辑和应用提供了更好的基础。

Abstract: Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,
metaverse, and robotics. However, most methods represent the target object as a
closed mesh devoid of any structural information, limiting editing, animation,
and semantic understanding. Part-aware 3D generation addresses this problem by
decomposing objects into meaningful components, but existing pipelines face
challenges: in existing methods, the user has no control over which objects are
separated and how model imagine the occluded parts in isolation phase. In this
paper, we introduce MMPart, an innovative framework for generating part-aware
3D models from a single image. We first use a VLM to generate a set of prompts
based on the input image and user descriptions. In the next step, a generative
model generates isolated images of each object based on the initial image and
the previous step's prompts as supervisor (which control the pose and guide
model how imagine previously occluded areas). Each of those images then enters
the multi-view generation stage, where a number of consistent images from
different views are generated. Finally, a reconstruction model converts each of
these multi-view images into a 3D model.

</details>


### [66] [Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm](https://arxiv.org/abs/2509.16771)
*Xiaohan Chen,Hongrui Gu,Cunshi Wang,Haiyang Mu,Jie Zheng,Junju Du,Jing Ren,Zhou Fan,Jing Li*

Main category: cs.CV

TL;DR: 提出结合U-Net深度学习网络和LSD算法的卫星轨迹检测模型，用于识别天文图像中的卫星干扰条纹


<details>
  <summary>Details</summary>
Motivation: 随着人造卫星数量快速增加，卫星反射阳光产生的条纹状伪影严重干扰天文成像观测，需要准确识别卫星轨迹位置

Method: 使用U-Net进行图像分割，结合Line Segment Detector算法，在375张模拟卫星轨迹图像上训练模型

Result: 对于信噪比大于3的轨迹检测率超过99%，在Mini-SiTian阵列真实观测数据上召回率79.57%，精确率74.56%

Conclusion: 该模型能有效检测卫星轨迹，为天文观测数据质量控制提供可靠工具

Abstract: With the rapid increase in the number of artificial satellites, astronomical
imaging is experiencing growing interference. When these satellites reflect
sunlight, they produce streak-like artifacts in photometry images. Such
satellite trails can introduce false sources and cause significant photometric
errors. As a result, accurately identifying the positions of satellite trails
in observational data has become essential. In this work, we propose a
satellite trail detection model that combines the U-Net deep neural network for
image segmentation with the Line Segment Detector (LSD) algorithm. The model is
trained on 375 simulated images of satellite trails, generated using data from
the Mini-SiTian Array. Experimental results show that for trails with a
signal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.
Additionally, when applied to real observational data from the Mini-SiTian
Array, the model achieves a recall of 79.57 and a precision of 74.56.

</details>


### [67] [Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models](https://arxiv.org/abs/2509.16805)
*Md. Atabuzzaman,Ali Asgarov,Chris Thomas*

Main category: cs.CV

TL;DR: 本文研究了大型视觉语言模型在多选题问答中的选择偏差问题，提出了基于推理时logit级别的去偏方法，无需重新训练即可有效缓解偏差并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然LVLMs在视觉问答任务中表现出色，但多选题问答中的选择偏差问题（如模型偏好特定选项标记或位置）尚未得到充分研究。

Method: 通过细粒度MCQA基准测试分析选择偏差，并提出推理时logit级去偏方法：从通用和上下文提示中估计集成偏差向量，并应用置信度自适应校正。

Result: 实验表明LVLMs存在一致的选择偏差，且随任务难度增加而加剧；所提出的去偏方法显著减少偏差，在挑战性场景中提高准确率。

Conclusion: 本研究揭示了LVLMs在MCQA中的局限性，提供了一种实用的方法来提高其在细粒度视觉推理中的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved strong performance on
vision-language tasks, particularly Visual Question Answering (VQA). While
prior work has explored unimodal biases in VQA, the problem of selection bias
in Multiple-Choice Question Answering (MCQA), where models may favor specific
option tokens (e.g., "A") or positions, remains underexplored. In this paper,
we investigate both the presence and nature of selection bias in LVLMs through
fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,
defined by the semantic similarity of the options. We further propose an
inference-time logit-level debiasing method that estimates an ensemble bias
vector from general and contextual prompts and applies confidence-adaptive
corrections to the model's output. Our method mitigates bias without retraining
and is compatible with frozen LVLMs. Extensive experiments across several
state-of-the-art models reveal consistent selection biases that intensify with
task difficulty, and show that our mitigation approach significantly reduces
bias while improving accuracy in challenging settings. This work offers new
insights into the limitations of LVLMs in MCQA and presents a practical
approach to improve their robustness in fine-grained visual reasoning. Datasets
and code are available at:
https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs

</details>


### [68] [MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging](https://arxiv.org/abs/2509.16806)
*Kacper Marzol,Ignacy Kolton,Weronika Smolak-Dyżewska,Joanna Kaleta,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: MedGS是一个基于高斯溅射的半监督神经隐式表面重建框架，用于多模态3D医学影像数据，能够实现鲁棒的帧间插值和高保真表面重建。


<details>
  <summary>Details</summary>
Motivation: 传统方法在医学影像表面重建和帧间插值中面临图像噪声和帧间信息不完整的限制，需要更有效的解决方案。

Method: 使用高斯溅射(GS)插值机制，将医学影像数据表示为3D空间中连续的2D帧，采用基于高斯分布的建模方法。

Result: MedGS比传统神经隐式方法训练更高效，具有更好的噪声鲁棒性、灵活编辑能力和更少的伪影，能够精确建模复杂解剖结构。

Conclusion: MedGS框架非常适合医学影像的可扩展和实际应用。

Abstract: Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.

</details>


### [69] [Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models](https://arxiv.org/abs/2509.16822)
*Townim Faisal Chowdhury,Vu Minh Hieu Phan,Kewen Liao,Nanyu Dong,Minh-Son To,Anton Hengel,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: 提出了Mirror-CFE方法，直接在分类器特征空间中生成反事实解释，将决策边界视为镜子来反映特征表示，无需额外图像编码器和生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法依赖额外图像编码器和生成模型创建可信图像，但忽略了分类器自身特征空间和决策边界，无法解释分类器学习的内在特征空间和决策边界。

Method: Mirror-CFE在分类器特征空间中操作，将决策边界作为镜子反射特征表示，学习从特征空间到图像空间的映射函数，同时保持距离关系，实现源图像与其反事实之间的平滑过渡。

Result: 在四个图像数据集上的实验表明，Mirror-CFE在保持输入相似性的同时，在有效性方面优于最先进的解释方法。

Conclusion: Mirror-CFE通过生成逐步过渡提供分类器决策过程的可解释可视化，揭示特征如何随分类置信度变化而演化。

Abstract: Counterfactual explanations (CFE) for deep image classifiers aim to reveal
how minimal input changes lead to different model decisions, providing critical
insights for model interpretation and improvement. However, existing CFE
methods often rely on additional image encoders and generative models to create
plausible images, neglecting the classifier's own feature space and decision
boundaries. As such, they do not explain the intrinsic feature space and
decision boundaries learned by the classifier. To address this limitation, we
propose Mirror-CFE, a novel method that generates faithful counterfactual
explanations by operating directly in the classifier's feature space, treating
decision boundaries as mirrors that ``reflect'' feature representations in the
mirror. Mirror-CFE learns a mapping function from feature space to image space
while preserving distance relationships, enabling smooth transitions between
source images and their counterfactuals. Through extensive experiments on four
image datasets, we demonstrate that Mirror-CFE achieves superior performance in
validity while maintaining input resemblance compared to state-of-the-art
explanation methods. Finally, mirror-CFE provides interpretable visualization
of the classifier's decision process by generating step-wise transitions that
reveal how features evolve as classification confidence changes.

</details>


### [70] [L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models](https://arxiv.org/abs/2509.16832)
*Ziyang Xu,Benedikt Schwab,Yihui Yang,Thomas H. Kolbe,Christoph Holst*

Main category: cs.CV

TL;DR: L2M-Reg是一种基于平面的精细配准方法，专门解决LiDAR点云与语义3D城市模型在LoD2级别下的配准问题，通过考虑模型不确定性来提高配准精度。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云与语义3D城市模型的精确配准是城市数字孪生的基础，但在单个建筑级别的LoD2模型配准中，由于模型不确定性导致现有方法精度不足。

Method: L2M-Reg包含三个关键步骤：建立可靠的平面对应关系、构建伪平面约束的Gauss-Helmert模型、自适应估计垂直平移。

Result: 在三个真实世界数据集上的实验表明，L2M-Reg比现有的ICP基和平面基方法更准确且计算效率更高。

Conclusion: L2M-Reg为存在模型不确定性的LiDAR到模型配准问题提供了一种新颖的建筑级别解决方案。

Abstract: Accurate registration between LiDAR (Light Detection and Ranging) point
clouds and semantic 3D city models is a fundamental topic in urban digital
twinning and a prerequisite for downstream tasks, such as digital construction,
change detection and model refinement. However, achieving accurate
LiDAR-to-Model registration at individual building level remains challenging,
particularly due to the generalization uncertainty in semantic 3D city models
at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing
L2M-Reg, a plane-based fine registration method that explicitly accounts for
model uncertainty. L2M-Reg consists of three key steps: establishing reliable
plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,
and adaptively estimating vertical translation. Experiments on three real-world
datasets demonstrate that L2M-Reg is both more accurate and computationally
efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg
provides a novel building-level solution regarding LiDAR-to-Model registration
when model uncertainty is present.

</details>


### [71] [ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression](https://arxiv.org/abs/2509.16853)
*Jinhao Wang,Cihan Ruan,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种通用、数据集无关的方法来识别和预训练VAE图像压缩模型中重要通道的组织结构，通过参数统计而非暴力评估来估计通道重要性，从而提升编码和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有学习图像压缩研究中发现只有少量潜在通道对重建至关重要，多数通道信息有限。利用这种不平衡性可提升编码和计算效率，但现有方法依赖昂贵的数据集特定消融测试且孤立分析通道，忽略了通道间的相互依赖关系。

Method: 利用内在参数统计（权重方差、偏置大小和成对相关性）来估计通道重要性，发现了一种称为不变显著通道空间（ISCS）的一致组织结构，其中显著核心通道捕获主导结构，显著辅助通道提供补充细节。基于ISCS提出了确定性通道排序和分组策略，支持切片并行解码。

Result: 在多种LIC架构上的实验表明，该方法有效降低了比特率和计算量，同时保持了重建质量。

Conclusion: 该方法为现有学习压缩框架提供了一种实用且模块化的增强方案，通过通道重要性分析和组织策略实现了编码效率和计算效率的提升。

Abstract: Prior studies in learned image compression (LIC) consistently show that only
a small subset of latent channels is critical for reconstruction, while many
others carry limited information. Exploiting this imbalance could improve both
coding and computational efficiency, yet existing approaches often rely on
costly, dataset-specific ablation tests and typically analyze channels in
isolation, ignoring their interdependencies.
  We propose a generalizable, dataset-agnostic method to identify and organize
important channels in pretrained VAE-based LIC models. Instead of brute-force
empirical evaluations, our approach leverages intrinsic parameter
statistics-weight variances, bias magnitudes, and pairwise correlations-to
estimate channel importance. This analysis reveals a consistent organizational
structure, termed the Invariant Salient Channel Space (ISCS), where
Salient-Core channels capture dominant structures and Salient-Auxiliary
channels provide complementary details. Building on ISCS, we introduce a
deterministic channel ordering and grouping strategy that enables
slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.
  Experiments across multiple LIC architectures demonstrate that our method
effectively reduces bitrate and computation while maintaining reconstruction
quality, providing a practical and modular enhancement to existing learned
compression frameworks.

</details>


### [72] [ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM](https://arxiv.org/abs/2509.16863)
*Amanuel T. Dufera,Yuan-Li Cai*

Main category: cs.CV

TL;DR: ConfidentSplat是一个基于3D高斯泼溅的RGB-only SLAM系统，通过置信度加权融合机制结合多视图几何和单目深度先验，实现鲁棒的高保真重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-only 3DGS SLAM方法因不可靠深度估计导致的几何不准确问题。

Method: 采用置信度加权融合机制，动态整合多视图几何深度线索和单目深度先验；使用可变形3DGS地图进行在线优化；结合DROID-SLAM风格的前端和后端优化。

Result: 在标准基准测试和移动数据集上显著提升了重建精度（L1深度误差）和新视角合成质量（PSNR、SSIM、LPIPS），特别是在挑战性条件下表现优异。

Conclusion: ConfidentSplat证明了基于置信度的传感器融合方法在推进密集视觉SLAM技术方面的有效性。

Abstract: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.

</details>


### [73] [$\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation](https://arxiv.org/abs/2509.16873)
*Yuanzhi Li,Lebin Zhou,Nam Ling,Zhenghao Chen,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 本文介绍了M³VIR数据集，这是一个专为游戏内容设计的大规模多模态多视图数据集，旨在解决现有数据集在捕捉游戏内容独特特征方面的不足，并为超分辨率、新视角合成和可控视频生成等任务提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集通常局限于特定领域或依赖人工降质，无法准确捕捉游戏内容的独特特征，且缺乏可控视频生成的基准。

Method: 使用Unreal Engine 5渲染多样化的高保真游戏内容，提供真实的LR-HR配对和多视图帧，涵盖80个场景的8个类别，包括M³VIR_MR用于超分辨率和新视角合成任务，以及M³VIR_MS用于可控视频生成研究。

Result: 构建了M³VIR数据集，并基准测试了多种先进的超分辨率和新视角合成方法，为可控视频生成领域提供了首个基准。

Conclusion: 通过发布M³VIR数据集，旨在促进AI驱动的恢复、压缩和可控内容生成研究，推动下一代云游戏和娱乐的发展。

Abstract: The gaming and entertainment industry is rapidly evolving, driven by
immersive experiences and the integration of generative AI (GAI) technologies.
Training such models effectively requires large-scale datasets that capture the
diversity and context of gaming environments. However, existing datasets are
often limited to specific domains or rely on artificial degradations, which do
not accurately capture the unique characteristics of gaming content. Moreover,
benchmarks for controllable video generation remain absent.
  To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale,
multi-modal, multi-view dataset specifically designed to overcome the
shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$
provides diverse, high-fidelity gaming content rendered with Unreal Engine 5,
offering authentic ground-truth LR-HR paired and multi-view frames across 80
scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution
(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and
$\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set
enabling research on controlled video generation. Additionally, we benchmark
several state-of-the-art SR and NVS methods to establish performance baselines.
While no existing approaches directly handle controlled video generation,
$\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing
the dataset, we aim to facilitate research in AI-powered restoration,
compression, and controllable content generation for next-generation cloud
gaming and entertainment.

</details>


### [74] [SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation](https://arxiv.org/abs/2509.16886)
*Yingzhen Hu,Yiheng Zhong,Ruobing Li,Yingxue Su,Jiabao An,Feilong Tang,Jionglong Su,Imran Razzak*

Main category: cs.CV

TL;DR: SAM-DCE是一种针对医学图像的改进分割模型，通过平衡局部判别性和全局语义，缓解了原始SAM模型在医学图像分割中的局限性。


<details>
  <summary>Details</summary>
Motivation: 原始SAM模型在自然图像上表现出色，但在医学图像分割中面临领域偏移、解剖结构变异和依赖用户提示等问题。现有的无提示适配方法仍存在鲁棒性不足和适应性有限的问题。

Method: 提出SAM-DCE模型，通过平衡局部判别性和全局语义，缓解令牌均匀性问题，增强类间可分性，并使用细粒度一致的表征来丰富掩码解码。

Result: 在多个医学图像基准测试上的广泛实验验证了该方法的有效性。

Conclusion: SAM-DCE成功解决了医学图像分割中的关键挑战，为医学图像分析提供了更鲁棒和自适应的分割解决方案。

Abstract: The Segment Anything Model (SAM) demonstrates impressive zero-shot
segmentation ability on natural images but encounters difficulties in medical
imaging due to domain shifts, anatomical variability, and its reliance on
user-provided prompts. Recent prompt-free adaptations alleviate the need for
expert intervention, yet still suffer from limited robustness and adaptability,
often overlooking the issues of semantic over-smoothing and token uniformity.
We propose SAM-DCE, which balances local discrimination and global semantics
while mitigating token uniformity, enhancing inter-class separability, and
enriching mask decoding with fine-grained, consistent representations.
Extensive experiments on diverse medical benchmarks validate its effectiveness.

</details>


### [75] [Rethinking Evaluation of Infrared Small Target Detection](https://arxiv.org/abs/2509.16888)
*Youwei Pang,Xiaoqi Zhao,Lihe Zhang,Huchuan Lu,Georges El Fakhri,Xiaofeng Liu,Shijian Lu*

Main category: cs.CV

TL;DR: 该论文针对红外小目标检测领域的评估协议局限性，提出了混合级度量、系统误差分析和跨数据集评估方法，旨在建立更全面的分层分析框架。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测的评估协议存在三个主要问题：1）使用碎片化的像素级和目标级特定度量，无法全面评估模型能力；2）过度关注整体性能分数而忽视关键误差分析；3）主要采用数据集特定的训练-测试范式，阻碍了对模型鲁棒性和泛化能力的理解。

Method: 1）引入结合像素级和目标级性能的混合级度量；2）提出系统误差分析方法；3）强调跨数据集评估的重要性。

Result: 开发了一个开源工具包来促进标准化基准测试。

Conclusion: 这些方法旨在提供更全面和合理的分层分析框架，最终促进开发更有效和鲁棒的红外小目标检测模型。

Abstract: As an essential vision task, infrared small target detection (IRSTD) has seen
significant advancements through deep learning. However, critical limitations
in current evaluation protocols impede further progress. First, existing
methods rely on fragmented pixel- and target-level specific metrics, which
fails to provide a comprehensive view of model capabilities. Second, an
excessive emphasis on overall performance scores obscures crucial error
analysis, which is vital for identifying failure modes and improving real-world
system performance. Third, the field predominantly adopts dataset-specific
training-testing paradigms, hindering the understanding of model robustness and
generalization across diverse infrared scenarios. This paper addresses these
issues by introducing a hybrid-level metric incorporating pixel- and
target-level performance, proposing a systematic error analysis method, and
emphasizing the importance of cross-dataset evaluation. These aim to offer a
more thorough and rational hierarchical analysis framework, ultimately
fostering the development of more effective and robust IRSTD models. An
open-source toolkit has be released to facilitate standardized benchmarking.

</details>


### [76] [Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning](https://arxiv.org/abs/2509.16892)
*Jiahe Qian,Yaoyu Fang,Ziqiao Weng,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: CoMTIP是一个用于空间转录组学的对比掩码文本-图像预训练框架，通过联合学习图像、基因名称和表达值，同时捕获细粒度视觉上下文，超越了现有方法并实现了零样本基因表达预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组学跨模态预训练方法仅单独使用基因名称或表达值，这剥夺了基因分支的基本语义并破坏了基因与其定量大小之间的关联。此外，这些方法仅关注图像-文本对齐，忽略了学习鲁棒图像特征所需的内在视觉线索。

Method: CoMTIP框架包含视觉分支和文本分支：视觉分支使用掩码特征建模重建遮挡的图像块以学习上下文感知的图像嵌入；文本分支采用可扩展的基因-文本编码器并行处理所有基因句子，通过专用嵌入丰富每个基因及其数值，并使用配对感知对抗训练保持正确的基因-值关联。图像和文本表示在共享的InfoNCE优化空间中对齐。

Result: 在公共空间转录组学数据集上的实验表明，CoMTIP不仅在多样化下游任务上超越了先前方法，还实现了零样本基因表达预测能力，这是现有方法所不具备的。

Conclusion: CoMTIP是第一个联合学习图像、基因名称和表达值的对比掩码文本-图像预训练框架，能够捕获细粒度视觉上下文，为空间转录组学提供了更强大的跨模态表示学习方法。

Abstract: Spatial transcriptomics aims to connect high-resolution histology images with
spatially resolved gene expression. To achieve better performance on downstream
tasks such as gene expression prediction, large-scale pre-training is required
to obtain generalisable representations that can bridge histology and
transcriptomics across tissues, protocols, and laboratories. Existing
cross-modal pre-training approaches for spatial transcriptomics rely on either
gene names or expression values in isolation, which strips the gene branch of
essential semantics and breaks the association between each gene and its
quantitative magnitude. In addition, by restricting supervision to image-text
alignment, these methods ignore intrinsic visual cues that are critical for
learning robust image features. We present CoMTIP, the first Contrastive Masked
Text-Image Pretraining framework that jointly learns from images, gene names,
and expression values while capturing fine-grained visual context for spatial
transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct
occluded patches and learn context-aware image embeddings. The text branch
applies a scalable Gene-Text Encoder that processes all gene sentences in
parallel, enriches each gene and its numerical value with dedicated embeddings,
and employs Pair-aware Adversarial Training (PAAT) to preserve correct
gene-value associations. Image and text representations are aligned in a shared
InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets
show that CoMTIP not only surpasses previous methods on diverse downstream
tasks but also achieves zero-shot gene expression prediction, a capability that
existing approaches do not provide.

</details>


### [77] [PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion](https://arxiv.org/abs/2509.16897)
*Xuewan He,Jielei Wang,Zihan Cheng,Yuchen Su,Shiyue Huang,Guoming Lu*

Main category: cs.CV

TL;DR: PRISM是一种基于精度-召回率的数据自由知识蒸馏方法，通过能量引导分布对齐和多样化提示工程解决大规模图像合成中的模式崩溃问题，提升知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据自由知识蒸馏方法在小规模图像上表现良好，但在合成大规模图像时会出现模式崩溃，导致知识迁移受限。现有方法使用现成扩散模型生成数据集面临精度-召回挑战：1）确保合成数据与真实分布对齐；2）确保覆盖真实分布流形。

Method: 提出PRISM方法，包含两个核心组件：1）能量引导分布对齐，避免生成分布外样本；2）多样化提示工程，增强对真实分布流形的覆盖。

Result: 在大规模图像数据集上的广泛实验表明PRISM的优越性，使用PRISM训练的模型展现出强大的领域泛化能力。

Conclusion: PRISM通过解决精度-召回挑战，有效提升了数据自由知识蒸馏在大规模图像上的性能，并具有良好的领域泛化能力。

Abstract: Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to
a student without access to the real in-distribution (ID) data. While existing
methods perform well on small-scale images, they suffer from mode collapse when
synthesizing large-scale images, resulting in limited knowledge transfer.
Recently, leveraging advanced generative models to synthesize photorealistic
images has emerged as a promising alternative. Nevertheless, directly using
off-the-shelf diffusion to generate datasets faces the precision-recall
challenges: 1) ensuring synthetic data aligns with the real distribution, and
2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a
precision-recall informed synthesis method. Specifically, we introduce
Energy-guided Distribution Alignment to avoid the generation of
out-of-distribution samples, and design the Diversified Prompt Engineering to
enhance coverage of the real ID manifold. Extensive experiments on various
large-scale image datasets demonstrate the superiority of PRISM. Moreover, we
demonstrate that models trained with PRISM exhibit strong domain
generalization.

</details>


### [78] [ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis](https://arxiv.org/abs/2509.16900)
*Chengsheng Zhang,Linhao Qu,Xiaoyu Liu,Zhijian Song*

Main category: cs.CV

TL;DR: 提出了一种多专家Mamba系统（ME-Mamba），用于整合病理图像和基因组数据的多模态生存分析，通过病理专家、基因组专家和协同专家的协作实现高效融合和准确预测。


<details>
  <summary>Details</summary>
Motivation: 病理图像通常只有幻灯片级别的标签，这阻碍了从千兆像素全切片图像中学习判别性表示。整合病理图像和基因组数据的多模态生存分析成为一种有前景的方法。

Method: 1）引入病理专家和基因组专家分别处理单模态数据，采用Mamba架构结合传统扫描和基于注意力的扫描机制；2）设计协同专家负责模态融合，通过最优传输学习token级局部对应关系，并通过最大均值差异实现全局跨模态融合。

Result: 在癌症基因组图谱（TCGA）的五个数据集上的广泛实验证明了该方法的先进性能。

Conclusion: 该方法通过多专家协作实现了稳定准确的生存分析，且计算复杂度相对较低。

Abstract: Survival analysis using whole-slide images (WSIs) is crucial in cancer
research. Despite significant successes, pathology images typically only
provide slide-level labels, which hinders the learning of discriminative
representations from gigapixel WSIs. With the rapid advancement of
high-throughput sequencing technologies, multimodal survival analysis
integrating pathology images and genomics data has emerged as a promising
approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures
discriminative pathological and genomic features while enabling efficient
integration of both modalities. This approach achieves complementary
information fusion without losing critical information from individual
modalities, thereby facilitating accurate cancer survival analysis.
Specifically, we first introduce a Pathology Expert and a Genomics Expert to
process unimodal data separately. Both experts are designed with Mamba
architectures that incorporate conventional scanning and attention-based
scanning mechanisms, allowing them to extract discriminative features from long
instance sequences containing substantial redundant or irrelevant information.
Second, we design a Synergistic Expert responsible for modality fusion. It
explicitly learns token-level local correspondences between the two modalities
via Optimal Transport, and implicitly enhances distribution consistency through
a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused
feature representations are then passed to a mamba backbone for further
integration. Through the collaboration of the Pathology Expert, Genomics
Expert, and Synergistic Expert, our method achieves stable and accurate
survival analysis with relatively low computational complexity. Extensive
experimental results on five datasets in The Cancer Genome Atlas (TCGA)
demonstrate our state-of-the-art performance.

</details>


### [79] [SLAM-Former: Putting SLAM into One Transformer](https://arxiv.org/abs/2509.16909)
*Yijun Yuan,Zhuoguang Chen,Kenan Li,Weibang Wang,Hang Zhao*

Main category: cs.CV

TL;DR: SLAM-Former是一个基于Transformer的完整SLAM系统，包含前端和后端模块，通过交替执行实现实时增量建图跟踪和全局优化，在密集SLAM任务中达到或超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统通常需要复杂的模块化设计，作者希望开发一个统一的神经网络架构来集成完整的SLAM功能，简化系统设计并提升性能。

Method: 使用Transformer架构构建完整的SLAM系统，前端处理序列单目图像进行实时增量建图和跟踪，后端进行全局优化以确保几何一致性，前后端交替执行相互促进。

Result: 实验结果表明，SLAM-Former在密集SLAM任务中实现了优越或极具竞争力的性能，超越了现有最先进方法。

Conclusion: SLAM-Former证明了基于Transformer的统一架构可以有效集成完整SLAM功能，为SLAM系统设计提供了新的神经网络解决方案。

Abstract: We present SLAM-Former, a novel neural approach that integrates full SLAM
capabilities into a single transformer. Similar to traditional SLAM systems,
SLAM-Former comprises both a frontend and a backend that operate in tandem. The
frontend processes sequential monocular images in real-time for incremental
mapping and tracking, while the backend performs global refinement to ensure a
geometrically consistent result. This alternating execution allows the frontend
and backend to mutually promote one another, enhancing overall system
performance. Comprehensive experimental results demonstrate that SLAM-Former
achieves superior or highly competitive performance compared to
state-of-the-art dense SLAM methods.

</details>


### [80] [Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](https://arxiv.org/abs/2509.16935)
*Lavish Ramchandani,Gunjan Deotale,Dev Kumar Das*

Main category: cs.CV

TL;DR: 本文研究了使用大型视觉基础模型（Virchow、Virchow2和UNI）结合LoRA参数高效微调方法，用于非典型有丝分裂图像分类任务，在MIDOG 2025挑战中取得88.37%的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂（AMFs）是肿瘤侵袭性和不良预后的重要标志，但由于形态特征细微、类别不平衡和病理学家间观察差异，其检测具有挑战性。MIDOG 2025挑战为此提供了系统性评估平台。

Method: 采用Virchow、Virchow2和UNI等大型视觉基础模型，结合LoRA（低秩适应）进行参数高效微调。实验包括不同LoRA秩的测试，以及随机和基于组的数据分割策略，分析模型在不同条件下的鲁棒性。

Result: 最佳方法（Virchow模型+LoRA秩8+三折交叉验证集成）在初步测试集上达到88.37%的平衡准确率，在挑战排行榜中并列第9名。

Conclusion: 研究证明了基础模型结合高效适应策略在非典型有丝分裂分类中的潜力，同时指出了在特异性和领域泛化方面仍需改进。

Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated
with tumor aggressiveness and poor prognosis. Their detection remains a
significant challenge due to subtle morphological cues, class imbalance, and
inter-observer variability among pathologists. The MIDOG 2025 challenge
introduced a dedicated track for atypical mitosis classification, enabling
systematic evaluation of deep learning methods. In this study, we investigated
the use of large vision foundation models, including Virchow, Virchow2, and
UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We
conducted extensive experiments with different LoRA ranks, as well as random
and group-based data splits, to analyze robustness under varied conditions. Our
best approach, Virchow with LoRA rank 8 and ensemble of three-fold
cross-validation, achieved a balanced accuracy of 88.37% on the preliminary
test set, ranking joint 9th in the challenge leaderboard. These results
highlight the promise of foundation models with efficient adaptation strategies
for the classification of atypical mitosis, while underscoring the need for
improvements in specificity and domain generalization.

</details>


### [81] [Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.16942)
*Bin Wang,Fei Deng,Zeyu Chen,Zhicheng Yu,Yiguang Liu*

Main category: cs.CV

TL;DR: ProSFDA是一个原型引导的无源域自适应框架，用于遥感图像语义分割，通过原型加权伪标签和原型对比策略解决伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 无源域自适应在遥感图像语义分割中面临伪标签噪声问题，这些噪声阻碍了有效缓解域偏移。

Method: 采用原型加权伪标签进行可靠的自训练，并引入原型对比策略促进同类特征的聚合，从而学习有区分度的目标域表示。

Result: 大量实验表明，该方法显著优于现有方法。

Conclusion: ProSFDA框架有效解决了SFDA中的伪标签噪声问题，提升了域自适应性能。

Abstract: Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic
segmentation of Remote Sensing Images (RSIs) using only a well-trained source
model and unlabeled target domain data. However, the lack of ground-truth
labels in the target domain often leads to the generation of noisy
pseudo-labels. Such noise impedes the effective mitigation of domain shift
(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA
framework. It employs prototype-weighted pseudo-labels to facilitate reliable
self-training (ST) under pseudo-labels noise. We, in addition, introduce a
prototype-contrast strategy that encourages the aggregation of features
belonging to the same class, enabling the model to learn discriminative target
domain representations without relying on ground-truth supervision. Extensive
experiments show that our approach substantially outperforms existing methods.

</details>


### [82] [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://arxiv.org/abs/2509.16944)
*Yuheng Shi,Xiaohuan Pei,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 提出了SD-RPN方法，通过自蒸馏区域提议网络解决MLLMs高分辨率图像处理的计算效率问题，无需标注数据即可实现精确的区域定位


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要高分辨率视觉信息进行细粒度感知，但处理全分辨率图像计算成本过高。现有RoI方法存在训练依赖大规模标注数据或计算效率低的问题

Method: 使用自蒸馏方法将MLLM中间层的噪声注意力图转化为高质量伪RoI标签，训练轻量级区域提议网络，实现单次前向传播的RoI预测

Result: 在LLaVA-1.5架构上验证，仅用少量数据训练就在TextVQA、DocVQA和V-Star等基准上实现超过10%的准确率提升

Conclusion: SD-RPN为增强MLLMs细粒度感知提供了实用且可扩展的解决方案，无需昂贵监督或全模型微调

Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual
information to perform fine-grained perception, yet processing entire
high-resolution images is computationally prohibitive. While recent methods
leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they
typically present a difficult trade-off: training-based approaches depend on
large-scale annotated datasets, while training-free methods that utilize the
model's internal attention are computationally inefficient and less accurate,
requiring either multi-pass prefill stages or reliance on the slow
auto-regressive decoding process. In this paper, we propose an efficient,
annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves
this trade-off. The SD-RPN is built around a pipeline that transforms the noisy
attention maps from the MLLM's middle layers into high-quality pseudo-RoI
labels by explicitly denoising the signal and resolving ambiguity. We use these
labels to train a lightweight Region Proposal Network (RPN) that learns a more
precise localization. This RPN is also highly efficient, predicting the RoI in
a single forward pass using features from the MLLM's middle layers, decoupling
RoI identification from the auto-regressive generation and avoiding costly
multi-pass operations.To validate our approach, we integrate the framework into
the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)
question-answer pairs, our method demonstrates exceptional data efficiency and
generalization, achieving over a 10% absolute accuracy improvement on unseen
benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a
practical and scalable solution for enhancing the fine-grained perception of
MLLMs without requiring costly supervision or full model fine-tuning. Code is
available at https://github.com/YuHengsss/SD-RPN.

</details>


### [83] [Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation](https://arxiv.org/abs/2509.16949)
*Ruicong Liu,Takehiko Ohkawa,Tze Ho Elden Tse,Mingfang Zhang,Angela Yao,Yoichi Sato*

Main category: cs.CV

TL;DR: RPEP是首个基于事件的3D手部姿态估计预训练方法，利用标记的RGB图像和未配对的未标记事件数据，通过分解手部运动为逐步动作来生成更真实的事件数据。


<details>
  <summary>Details</summary>
Motivation: 事件数据具有高时间分辨率和低延迟的优势，但在手部姿态估计中的应用受到标记训练数据稀缺的限制。

Method: RPEP通过构建伪事件-RGB对，将手部运动分解为较小的逐步动作，并引入运动反转约束来正则化事件生成。

Result: 在真实事件数据上，RPEP显著优于现有最先进方法，在EvRealHands上实现了高达24%的改进，并且在微调时仅需少量标记样本即可获得强性能。

Conclusion: RPEP是一种适用于实际部署的高效预训练方法，能够有效利用事件数据进行3D手部姿态估计。

Abstract: This paper presents RPEP, the first pre-training method for event-based 3D
hand pose estimation using labeled RGB images and unpaired, unlabeled event
data. Event data offer significant benefits such as high temporal resolution
and low latency, but their application to hand pose estimation is still limited
by the scarcity of labeled training data. To address this, we repurpose real
RGB datasets to train event-based estimators. This is done by constructing
pseudo-event-RGB pairs, where event data is generated and aligned with the
ground-truth poses of RGB images. Unfortunately, existing pseudo-event
generation techniques assume stationary objects, thus struggling to handle
non-stationary, dynamically moving hands. To overcome this, RPEP introduces a
novel generation strategy that decomposes hand movements into smaller,
step-by-step motions. This decomposition allows our method to capture temporal
changes in articulation, constructing more realistic event data for a moving
hand. Additionally, RPEP imposes a motion reversal constraint, regularizing
event generation using reversed motion. Extensive experiments show that our
pre-trained model significantly outperforms state-of-the-art methods on real
event data, achieving up to 24% improvement on EvRealHands. Moreover, it
delivers strong performance with minimal labeled samples for fine-tuning,
making it well-suited for practical deployment.

</details>


### [84] [VidCLearn: A Continual Learning Approach for Text-to-Video Generation](https://arxiv.org/abs/2509.16956)
*Luca Zanchetta,Lorenzo Papa,Luca Maiano,Irene Amerini*

Main category: cs.CV

TL;DR: VidCLearn是一个用于基于扩散的文本到视频生成的持续学习框架，通过师生架构和生成重放来解决现有模型难以融入新数据的问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型依赖静态知识，难以在不重新训练的情况下融入新数据，限制了模型的适应性和扩展性。

Method: 采用师生架构，学生模型通过新文本-视频对增量更新，教师模型通过生成重放保留已学知识；引入时间一致性损失增强运动平滑性，视频检索模块在推理时提供结构指导。

Result: 实验结果表明VidCLearn在视觉质量、语义对齐和时间一致性方面优于基线方法。

Conclusion: VidCLearn提供了一个计算效率高且性能良好的持续学习解决方案，能够有效解决文本到视频生成中的知识更新问题。

Abstract: Text-to-video generation is an emerging field in generative AI, enabling the
creation of realistic, semantically accurate videos from text prompts. While
current models achieve impressive visual quality and alignment with input text,
they typically rely on static knowledge, making it difficult to incorporate new
data without retraining from scratch. To address this limitation, we propose
VidCLearn, a continual learning framework for diffusion-based text-to-video
generation. VidCLearn features a student-teacher architecture where the student
model is incrementally updated with new text-video pairs, and the teacher model
helps preserve previously learned knowledge through generative replay.
Additionally, we introduce a novel temporal consistency loss to enhance motion
smoothness and a video retrieval module to provide structural guidance at
inference. Our architecture is also designed to be more computationally
efficient than existing models while retaining satisfactory generation
performance. Experimental results show VidCLearn's superiority over baseline
methods in terms of visual quality, semantic alignment, and temporal coherence.

</details>


### [85] [MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image](https://arxiv.org/abs/2509.16957)
*Leiyu Wang,Biao Jin,Feng Huang,Liqiong Chen,Zhengyong Wang,Xiaohai He,Honggang Chen*

Main category: cs.CV

TL;DR: MO R-CNN是一个轻量级多光谱定向检测框架，通过异构特征提取网络、单模态监督和基于条件的多模态标签融合来解决多光谱图像中模态内和模态间差异带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像的定向目标检测面临模态内和模态间差异的挑战，现有方法虽然通过复杂网络架构提高了检测精度，但高计算复杂度和内存消耗限制了其性能。受大核卷积在遥感领域成功的启发，作者希望开发一个轻量级解决方案。

Method: 提出MO R-CNN框架，包含三个核心组件：1）异构特征提取网络（HFEN）利用模态间差异自适应对齐、融合和增强多模态特征；2）单模态监督（SMS）约束多尺度特征并让模型从多模态学习；3）基于条件的多模态标签融合（CMLF）根据特定规则融合多模态标签，提供更鲁棒和一致的监督信号。

Result: 在DroneVehicle、VEDAI和OGSOD数据集上的实验证明了该方法的优越性。

Conclusion: MO R-CNN是一个有效的轻量级多光谱定向检测框架，通过创新的特征提取、监督机制和标签融合策略，在保持高性能的同时显著降低了计算复杂度。

Abstract: Oriented object detection for multi-spectral imagery faces significant
challenges due to differences both within and between modalities. Although
existing methods have improved detection accuracy through complex network
architectures, their high computational complexity and memory consumption
severely restrict their performance. Motivated by the success of large kernel
convolutions in remote sensing, we propose MO R-CNN, a lightweight framework
for multi-spectral oriented detection featuring heterogeneous feature
extraction network (HFEN), single modality supervision (SMS), and
condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal
differences to adaptively align, merge, and enhance multi-modal features. SMS
constrains multi-scale features and enables the model to learn from multiple
modalities. CMLF fuses multimodal labels based on specific rules, providing the
model with a more robust and consistent supervisory signal. Experiments on the
DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The
source code is available at:https://github.com/Iwill-github/MORCNN.

</details>


### [86] [Penalizing Boundary Activation for Object Completeness in Diffusion Models](https://arxiv.org/abs/2509.16968)
*Haoyang Xu,Tianhao Zhao,Sibei Yang,Yutian Lin*

Main category: cs.CV

TL;DR: 本文分析了扩散模型中物体显示不完整的问题，发现RandomCrop数据增强方法是主要原因，并提出了一种无需重新训练的方法来改善物体完整性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现出色，但存在物体显示不完整的问题，这影响了模型在下游应用中的性能。

Method: 提出了一种训练免费的解决方案，在早期去噪步骤中对图像边界的激活值进行惩罚，该方法可轻松应用于预训练的Stable Diffusion模型。

Result: 大量实验证明该方法有效，显著提高了物体完整性和图像质量。

Conclusion: 通过分析RandomCrop对物体连续性的破坏，提出的边界激活惩罚方法能有效解决物体不完整问题，且计算开销极小。

Abstract: Diffusion models have emerged as a powerful technique for text-to-image (T2I)
generation, creating high-quality, diverse images across various domains.
However, a common limitation in these models is the incomplete display of
objects, where fragments or missing parts undermine the model's performance in
downstream applications. In this study, we conduct an in-depth analysis of the
incompleteness issue and reveal that the primary factor behind incomplete
object generation is the usage of RandomCrop during model training. This widely
used data augmentation method, though enhances model generalization ability,
disrupts object continuity during training. To address this, we propose a
training-free solution that penalizes activation values at image boundaries
during the early denoising steps. Our method is easily applicable to
pre-trained Stable Diffusion models with minimal modifications and negligible
computational overhead. Extensive experiments demonstrate the effectiveness of
our method, showing substantial improvements in object integrity and image
quality.

</details>


### [87] [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](https://arxiv.org/abs/2509.16970)
*Wei Liao,Chunyan Xu,Chenxu Wang,Zhen Cui*

Main category: cs.CV

TL;DR: 本文提出了一种LLM辅助的语义引导框架，用于稀疏标注的遥感目标检测，通过利用大语言模型的语义推理能力来生成高置信度的伪标签，解决了现有密集伪标签方法的选择模糊性和置信度估计不一致问题。


<details>
  <summary>Details</summary>
Motivation: 遥感目标检测中稀疏标注面临密集目标分布和类别不平衡的挑战，现有密集伪标签方法存在选择模糊性和置信度估计不一致的局限性。

Method: 提出LLM辅助语义引导框架，包括类感知密集伪标签分配机制和自适应硬负样本重加权模块，利用LLM生成的语义先验自适应分配伪标签并稳定监督学习分支。

Result: 在DOTA和HRSC2016数据集上的实验表明，该方法优于现有的基于单阶段检测器的框架，在稀疏标注下显著提升了检测性能。

Conclusion: LLM辅助的语义引导框架有效解决了稀疏标注遥感目标检测的挑战，通过语义先验和自适应机制提升了检测性能。

Abstract: Sparse annotation in remote sensing object detection poses significant
challenges due to dense object distributions and category imbalances. Although
existing Dense Pseudo-Label methods have demonstrated substantial potential in
pseudo-labeling tasks, they remain constrained by selection ambiguities and
inconsistencies in confidence estimation.In this paper, we introduce an
LLM-assisted semantic guidance framework tailored for sparsely annotated remote
sensing object detection, exploiting the advanced semantic reasoning
capabilities of large language models (LLMs) to distill high-confidence
pseudo-labels.By integrating LLM-generated semantic priors, we propose a
Class-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns
pseudo-labels for both unlabeled and sparsely labeled data, ensuring robust
supervision across varying data distributions. Additionally, we develop an
Adaptive Hard-Negative Reweighting Module to stabilize the supervised learning
branch by mitigating the influence of confounding background information.
Extensive experiments on DOTA and HRSC2016 demonstrate that the proposed method
outperforms existing single-stage detector-based frameworks, significantly
improving detection performance under sparse annotations.

</details>


### [88] [The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA](https://arxiv.org/abs/2509.16972)
*Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji*

Main category: cs.CV

TL;DR: SaSaSa2VA方法通过增加分割增强和选择性平均策略，解决了RVOS任务中稀疏帧采样和单一[SEG]令牌的瓶颈问题，在LSVOS挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 基于Sa2VA框架，发现稀疏帧采样和依赖单一[SEG]令牌限制了视频对象分割性能，需要更精细的外观和运动理解。

Method: 提出Segmentation Augmented and Selective Averaged Sa2VA (SaSaSa2VA)，结合分割增强和测试时集成策略，改进多模态大语言模型在RVOS任务中的表现。

Result: 在第7届LSVOS挑战赛(RVOS赛道)中，SaSaSa2VA获得J&F分数67.45，排名第一，比第二名高出2.80分。

Conclusion: 有效的分割增强和测试时集成策略显著提升了基于MLLM的RVOS模型性能。

Abstract: Referring video object segmentation (RVOS) requires segmenting and tracking
objects in videos conditioned on natural-language expressions, demanding
fine-grained understanding of both appearance and motion. Building on Sa2VA,
which couples a Multi-modal Large Language Model (MLLM) with the video
segmentation model SAM2, we identify two key bottlenecks that limit
segmentation performance: sparse frame sampling and reliance on a single [SEG]
token for an entire video. We propose Segmentation Augmented and Selective
Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge
(RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and
surpassing the runner-up by 2.80 points. This result and ablation studies
demonstrate that efficient segmentation augmentation and test-time ensembling
substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA
repository: https://github.com/magic-research/Sa2VA.

</details>


### [89] [Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime](https://arxiv.org/abs/2509.16977)
*Petros Georgoulas Wraight,Giorgos Sfikas,Ioannis Kordonis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 提出了一种基于最优传输的迭代视觉语义对齐框架，用于解决低资源手写文本识别问题，能够在标注数据稀缺的情况下利用词汇特征知识提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统HTR方法需要大量标注数据，但在历史档案等低资源领域难以应用。本文旨在开发一种能够在标注数据稀缺情况下有效工作的识别框架。

Method: 采用迭代自举方法，通过最优传输将未标注图像特征与语义词表示对齐，从少量标注样本开始，迭代匹配词图像与文本标签，为高置信度对齐生成伪标签，并在增长的数据集上重新训练识别器。

Result: 数值实验表明，该迭代视觉语义对齐方案在低资源HTR基准测试中显著提高了识别准确率。

Conclusion: 该框架为低资源HTR提供了一种有效解决方案，能够利用有限的标注数据和词汇特征知识实现高质量的文本识别。

Abstract: Handwritten Text Recognition (HTR) is a task of central importance in the
field of document image understanding. State-of-the-art methods for HTR require
the use of extensive annotated sets for training, making them impractical for
low-resource domains like historical archives or limited-size modern
collections. This paper introduces a novel framework that, unlike the standard
HTR model paradigm, can leverage mild prior knowledge of lexical
characteristics; this is ideal for scenarios where labeled data are scarce. We
propose an iterative bootstrapping approach that aligns visual features
extracted from unlabeled images with semantic word representations using
Optimal Transport (OT). Starting with a minimal set of labeled examples, the
framework iteratively matches word images to text labels, generates
pseudo-labels for high-confidence alignments, and retrains the recognizer on
the growing dataset. Numerical experiments demonstrate that our iterative
visual-semantic alignment scheme significantly improves recognition accuracy on
low-resource HTR benchmarks.

</details>


### [90] [VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation](https://arxiv.org/abs/2509.16986)
*Feng Han,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文提出了Visual Contrast Exploitation (VCE)框架，用于保护自回归图像生成模型免受不安全概念（如NSFW内容和版权侵权风格）的影响。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型（如GPT-4o和LlamaGen）能够生成逼真图像，但可能产生NSFW内容或侵犯版权，而现有的概念擦除方法主要针对扩散模型，不适用于自回归模型。

Method: VCE框架包括：(1)创新的对比图像对构建范式，精确解耦不安全概念与其相关语义；(2)基于DPO的训练方法，增强模型识别和利用图像对中的视觉对比特征，实现精确概念擦除。

Result: 在三个挑战性任务（艺术家风格擦除、显式内容擦除和对象移除）上的综合实验表明，该方法有效保护模型，在擦除不安全概念的同时保持无关安全概念的完整性，达到最先进水平。

Conclusion: VCE框架为解决自回归图像生成模型的安全问题提供了有效方案，代码和模型已开源。

Abstract: Recently, autoregressive image generation models have wowed audiences with
their remarkable capability in creating surprisingly realistic images. Models
such as GPT-4o and LlamaGen can not only produce images that faithfully mimic
renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also
potentially generate Not-Safe-For-Work (NSFW) content, raising significant
concerns regarding copyright infringement and ethical use. Despite these
concerns, methods to safeguard autoregressive text-to-image models remain
underexplored. Previous concept erasure methods, primarily designed for
diffusion models that operate in denoising latent space, are not directly
applicable to autoregressive models that generate images token by token. To
address this critical gap, we propose Visual Contrast Exploitation (VCE), a
novel framework comprising: (1) an innovative contrastive image pair
construction paradigm that precisely decouples unsafe concepts from their
associated content semantics, and (2) a sophisticated DPO-based training
approach that enhances the model's ability to identify and leverage visual
contrastive features from image pairs, enabling precise concept erasure. Our
comprehensive experiments across three challenging tasks-artist style erasure,
explicit content erasure, and object removal-demonstrate that our method
effectively secures the model, achieving state-of-the-art results while erasing
unsafe concepts and maintaining the integrity of unrelated safe concepts. The
code and models are available at https://github.com/Maplebb/VCE.

</details>


### [91] [A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection](https://arxiv.org/abs/2509.16988)
*Mingshuai Sheng,Bhatti Uzair Aslam,Junfeng Zhang,Siling Feng,Yonis Gulzar*

Main category: cs.CV

TL;DR: 本文提出了一种基于多尺度编码器-解码器架构的跨层次多特征融合网络（CHMFFN），用于高光谱变化检测，通过多尺度特征提取、双核通道-空间注意力模块和自适应特征融合，有效提升了变化检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱变化检测方法存在多尺度特征利用不足和差异特征融合效率低的问题，需要开发更有效的特征提取和融合机制。

Method: 采用多尺度编码器-解码器架构，前端使用多尺度特征提取子网络，包含残差连接和双核通道-空间注意力模块（DCCSA）来提取光谱-空间-时间特征。编码器通过残差块和不同感受野的卷积核捕获多尺度特征，解码器通过跳跃连接恢复空间分辨率并抑制噪声。还包含光谱-时间变化特征学习模块（STCFL）和自适应融合高级特征模块（AFAF）。

Result: 在四个公开高光谱数据集上的实验表明，CHMFFN优于现有最先进方法，验证了其有效性。

Conclusion: 提出的CHMFFN网络通过创新的多尺度特征提取和自适应融合机制，成功解决了高光谱变化检测中的关键挑战，为环境监测和灾害评估提供了有效的技术支撑。

Abstract: Hyperspectral change detection (HCD) aims to accurately identify land-cover
changes in hyperspectral images of the same area acquired at different times,
with key applications in environmental monitoring and disaster assessment. To
address limitations of existing methods, such as insufficient use of multiscale
features and low efficiency in differential feature fusion, this paper proposes
a cross-hierarchical multi-feature fusion network (CHMFFN) based on a
multiscale encoder-decoder architecture. The front-end adopts a multiscale
feature extraction subnetwork, built on an encoder-decoder backbone with
residual connections and a dual-core channel-spatial attention (DCCSA) module
to extract spectral-spatial-temporal features (SSTF). The encoder captures
multiscale features from shallow details to deep semantics via residual blocks
and convolutional kernels with varying receptive fields. The decoder restores
spatial resolution and suppresses noise information through skip connections
integrating encoder features. Additionally, a spectral-temporal change feature
learning (STCFL) module learns cross-temporal change features at different
levels, strengthening inter-temporal difference capture. An adaptive fusion of
advanced features (AFAF) module dynamically balances hierarchical differential
features via adaptive weights, enhancing representation of complex changes.
Experiments on four public hyperspectral datasets show CHMFFN outperforms
state-of-the-art methods, verifying its effectiveness.

</details>


### [92] [DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment](https://arxiv.org/abs/2509.17012)
*Zhichao Ma,Fan Huang,Lu Zhao,Fengjun Guo,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了一个主观文档图像质量评估数据集DIQA-5000，并开发了一个专门的无参考DIQA模型，该模型利用文档布局特征和多级特征融合来降低计算成本并提高评估准确性。


<details>
  <summary>Details</summary>
Motivation: 文档图像质量评估在OCR、文档修复和文档图像处理系统评估中具有重要作用。现有方法通常基于通用图像质量评估模型，未能充分利用文档特有的结构特征。

Method: 构建包含5,000张文档图像的DIQA-5000数据集，每张图像由15名受试者在三个维度评分。提出专门的无参考DIQA模型，利用文档布局特征降低分辨率计算成本，设计特征融合模块整合多级特征，并使用独立的质量头预测各维度的分数分布。

Result: 实验结果表明，该方法在DIQA-5000数据集和另一个专注于OCR准确性的文档图像数据集上，均优于当前最先进的通用IQA模型。

Conclusion: 所提出的专门DIQA模型通过利用文档特有特征和多维度评估方法，在文档图像质量评估任务中表现出优越性能，为文档处理应用提供了有效的质量评估工具。

Abstract: Document image quality assessment (DIQA) is an important component for
various applications, including optical character recognition (OCR), document
restoration, and the evaluation of document image processing systems. In this
paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset
comprises 5,000 document images, generated by applying multiple document
enhancement techniques to 500 real-world images with diverse distortions. Each
enhanced image was rated by 15 subjects across three rating dimensions: overall
quality, sharpness, and color fidelity. Furthermore, we propose a specialized
no-reference DIQA model that exploits document layout features to maintain
quality perception at reduced resolutions to lower computational cost.
Recognizing that image quality is influenced by both low-level and high-level
visual features, we designed a feature fusion module to extract and integrate
multi-level features from document images. To generate multi-dimensional
scores, our model employs independent quality heads for each dimension to
predict score distributions, allowing it to learn distinct aspects of document
image quality. Experimental results demonstrate that our method outperforms
current state-of-the-art general-purpose IQA models on both DIQA-5000 and an
additional document image dataset focused on OCR accuracy.

</details>


### [93] [When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration](https://arxiv.org/abs/2509.17024)
*Wenxuan Fang,Jili Fan,Chao Wang,Xiantao Hu,Jiangwei Weng,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: LCDiff是一个新的恶劣天气图像恢复框架，通过亮度-色度分解网络和亮度引导扩散模型，在YCbCr色彩空间中分别处理退化相关亮度和退化不变色度，无需显式退化提示即可实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定方法难以泛化到未见过的复杂退化类型，而现有的提示学习方法过度依赖视觉语言模型的退化估计能力，导致恢复结果不一致。

Method: 提出LCDiff框架：1）亮度-色度分解网络（LCDN）在YCbCr空间分别处理亮度和色度；2）亮度引导扩散模型（LGDM）利用退化相关亮度作为引导条件；3）引入动态时间步损失优化去噪网络。

Result: 在提出的DriveWeather数据集上进行了广泛实验，结果表明该方法超越了现有最先进方法，为恶劣天气图像恢复设立了新的基准。

Conclusion: LCDiff通过创新的亮色分离和亮度引导扩散机制，有效解决了恶劣天气图像恢复中的泛化和一致性难题，无需依赖退化提示即可实现高质量恢复。

Abstract: Adverse Weather Image Restoration (AWIR) is a highly challenging task due to
the unpredictable and dynamic nature of weather-related degradations.
Traditional task-specific methods often fail to generalize to unseen or complex
degradation types, while recent prompt-learning approaches depend heavily on
the degradation estimation capabilities of vision-language models, resulting in
inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel
framework comprising two key components: \textit{Lumina-Chroma Decomposition
Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN
processes degraded images in the YCbCr color space, separately handling
degradation-related luminance and degradation-invariant chrominance components.
This decomposition effectively mitigates weather-induced degradation while
preserving color fidelity. To further enhance restoration quality, LGDM
leverages degradation-related luminance information as a guiding condition,
eliminating the need for explicit degradation prompts. Additionally, LGDM
incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising
network, ensuring a balanced recovery of both low- and high-frequency features
in the image. Finally, we present DriveWeather, a comprehensive all-weather
driving dataset designed to enable robust evaluation. Extensive experiments
demonstrate that our approach surpasses state-of-the-art methods, setting a new
benchmark in AWIR. The dataset and code are available at:
https://github.com/fiwy0527/LCDiff.

</details>


### [94] [Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views](https://arxiv.org/abs/2509.17027)
*Zhenya Yang*

Main category: cs.CV

TL;DR: 提出基于高斯泼溅的框架，从内窥镜数据直接重建交互式手术场景，通过虚拟相机正则化和深度正则化解决视角受限导致的过拟合问题，并采用基于稀疏控制节点的材料点方法实现快速变形模拟。


<details>
  <summary>Details</summary>
Motivation: 传统手术模拟构建方法繁琐、耗时且难以扩展，导致细节差、模拟不真实。需要一种能高效重建高质量交互式手术场景的方法。

Method: 使用高斯泼溅表示法重建手术场景，引入虚拟相机正则化方法采样虚拟视角优化几何精度，结合深度正则化细化场景几何，采用稀疏控制节点的材料点方法实现实时物理变形模拟。

Result: 在代表性手术数据上的实验表明，该方法能高效从稀疏内窥镜视图重建和模拟手术场景，仅需几分钟完成重建，并能实时产生物理合理的变形。

Conclusion: 该方法成功解决了手术模拟中的重建质量和实时交互问题，为医疗训练提供了高效、真实的模拟解决方案。

Abstract: Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.

</details>


### [95] [From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning](https://arxiv.org/abs/2509.17040)
*Hang Du,Jiayang Zhang,Guoshun Nan,Wendi Deng,Zhenyan Chen,Chenyang Zhang,Wang Xiao,Shan Huang,Yuqi Pan,Tao Qi,Sicong Leng*

Main category: cs.CV

TL;DR: 该论文提出了MIR基准，用于评估多模态大语言模型在多图像交错推理任务上的能力，通过阶段式课程学习策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多图像基准测试忽视了交错文本上下文和图像与文本之间的特定关系，限制了模型对复杂场景的理解和跨模态关联的捕捉能力。

Method: 引入MIR基准要求模型在交错文本上下文中对多图像进行联合推理，并提出阶段式课程学习策略，采用从易到难的方法逐步提升模型能力。

Result: 大量实验表明，该方法显著提升了多模态大语言模型在MIR基准和其他现有基准上的推理性能。

Conclusion: MIR基准将推动多图像交错推理研究的进一步发展，促进多模态大语言模型处理复杂跨模态任务能力的提升。

Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language
Models (MLLMs) ability to jointly comprehend and reason across multiple images
and their associated textual contexts, introducing unique challenges beyond
single-image or non-interleaved multi-image tasks. While current multi-image
benchmarks overlook interleaved textual contexts and neglect distinct
relationships between individual images and their associated texts, enabling
models to reason over multi-image interleaved data may significantly enhance
their comprehension of complex scenes and better capture cross-modal
correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring
joint reasoning over multiple images accompanied by interleaved textual
contexts to accurately associate image regions with corresponding texts and
logically connect information across images. To enhance MLLMs ability to
comprehend multi-image interleaved data, we introduce reasoning steps for each
instance within the benchmark and propose a stage-wise curriculum learning
strategy. This strategy follows an "easy to hard" approach, progressively
guiding models from simple to complex scenarios, thereby enhancing their
ability to handle challenging tasks. Extensive experiments benchmarking
multiple MLLMs demonstrate that our method significantly enhances models
reasoning performance on MIR and other established benchmarks. We believe that
MIR will encourage further research into multi-image interleaved reasoning,
facilitating advancements in MLLMs capability to handle complex inter-modal
tasks.Our code and dataset are available at
https://github.com/Shelly-coder239/MIRBench.

</details>


### [96] [Towards Generalized Synapse Detection Across Invertebrate Species](https://arxiv.org/abs/2509.17041)
*Samia Mohinta,Daniel Franco-Barranco,Shi Yan Lee,Albert Cardona*

Main category: cs.CV

TL;DR: 本文提出了SimpSyn，一种轻量级的单阶段神经网络模型，用于电子显微镜图像中的突触检测，在多个无脊椎动物数据集上超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 由于稀疏标注、形态变异性和跨数据集域偏移等问题，大规模可靠检测突触仍然具有挑战性，需要开发更高效实用的解决方案。

Method: 采用单阶段残差U-Net架构，预测围绕突触前和突触后位点的双通道球形掩码，强调训练和推理速度以及标注效率而非架构复杂性。

Result: SimpSyn在所有数据集的突触位点检测F1分数上均优于Synful方法，在组合队列训练时达到竞争性性能，简单后处理策略即可获得强性能。

Conclusion: 轻量级模型与任务结构对齐时，可为大规模连接组学管道中的突触检测提供实用且可扩展的解决方案。

Abstract: Behavioural differences across organisms, whether healthy or pathological,
are closely tied to the structure of their neural circuits. Yet, the fine-scale
synaptic changes that give rise to these variations remain poorly understood,
in part due to persistent challenges in detecting synapses reliably and at
scale. Volume electron microscopy (EM) offers the resolution required to
capture synaptic architecture, but automated detection remains difficult due to
sparse annotations, morphological variability, and cross-dataset domain shifts.
To address this, we make three key contributions. First, we curate a diverse EM
benchmark spanning four datasets across two invertebrate species: adult and
larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,
we propose SimpSyn, a single-stage Residual U-Net trained to predict
dual-channel spherical masks around pre- and post-synaptic sites, designed to
prioritize training and inference speeds and annotation efficiency over
architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s
Synful [1], a state-of-the-art multi-task model that jointly infers synaptic
pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in
F1-score across all volumes for synaptic site detection. While generalization
across datasets remains limited, SimpSyn achieves competitive performance when
trained on the combined cohort. Finally, ablations reveal that simple
post-processing strategies - such as local peak detection and distance-based
filtering - yield strong performance without complex test-time heuristics.
Taken together, our results suggest that lightweight models, when aligned with
task structure, offer a practical and scalable solution for synapse detection
in large-scale connectomic pipelines.

</details>


### [97] [AgriDoctor: A Multimodal Intelligent Assistant for Agriculture](https://arxiv.org/abs/2509.17044)
*Mingqing Zhang,Zhuoning Xu,Peijie Wang,Rongji Li,Liang Wang,Qiang Liu,Jian Xu,Xuyao Zhang,Shu Wu,Liang Wang*

Main category: cs.CV

TL;DR: AgriDoctor是一个用于作物病害诊断的多模态框架，通过整合视觉和语言模型解决现有方法缺乏领域知识和交互能力的问题


<details>
  <summary>Details</summary>
Motivation: 现有作物病害诊断方法主要依赖单模态模型，无法融入农业领域知识且缺乏语言交互支持，而现有大语言模型在农业领域表现有限

Method: 提出AgriDoctor模块化框架，包含路由器、分类器、检测器、知识检索器和LLMs五个核心组件，并构建AgriMM基准数据集进行训练

Result: 实验表明AgriDoctor在精细农业任务上显著优于现有最先进的大视觉语言模型

Conclusion: AgriDoctor为智能和可持续农业应用建立了新的范式

Abstract: Accurate crop disease diagnosis is essential for sustainable agriculture and
global food security. Existing methods, which primarily rely on unimodal models
such as image-based classifiers and object detectors, are limited in their
ability to incorporate domain-specific agricultural knowledge and lack support
for interactive, language-based understanding. Recent advances in large
language models (LLMs) and large vision-language models (LVLMs) have opened new
avenues for multimodal reasoning. However, their performance in agricultural
contexts remains limited due to the absence of specialized datasets and
insufficient domain adaptation. In this work, we propose AgriDoctor, a modular
and extensible multimodal framework designed for intelligent crop disease
diagnosis and agricultural knowledge interaction. As a pioneering effort to
introduce agent-based multimodal reasoning into the agricultural domain,
AgriDoctor offers a novel paradigm for building interactive and domain-adaptive
crop health solutions. It integrates five core components: a router,
classifier, detector, knowledge retriever and LLMs. To facilitate effective
training and evaluation, we construct AgriMM, a comprehensive benchmark
comprising 400000 annotated disease images, 831 expert-curated knowledge
entries, and 300000 bilingual prompts for intent-driven tool selection.
Extensive experiments demonstrate that AgriDoctor, trained on AgriMM,
significantly outperforms state-of-the-art LVLMs on fine-grained agricultural
tasks, establishing a new paradigm for intelligent and sustainable farming
applications.

</details>


### [98] [Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization](https://arxiv.org/abs/2509.17049)
*Peng Wang,Yong Li,Lin Zhao,Xiu-Shen Wei*

Main category: cs.CV

TL;DR: 提出了一种基于可学习查询的属性感知哈希码学习方法，通过定制查询集捕获属性级信息，并引入辅助分支建模高阶属性交互，提升细粒度图像检索的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 细粒度哈希在视觉相似类别的高区分度场景中具有重要作用，但现有方法难以让每个哈希位对应特定视觉属性，且低比特哈希码优化面临复杂景观挑战。

Method: 使用可学习查询进行属性感知哈希码学习，部署定制查询集捕获属性级信息；引入辅助分支建模高阶属性交互，缓解低比特哈希码优化难题。

Result: 在基准数据集上的实验表明，该方法生成的属性感知哈希码在检索准确性和鲁棒性方面持续优于现有技术，特别在低比特哈希码场景下表现突出。

Conclusion: 该方法在细粒度图像哈希任务中具有显著潜力，能够生成可解释且相关的哈希位，有效提升检索性能。

Abstract: Fine-grained hashing has become a powerful solution for rapid and efficient
image retrieval, particularly in scenarios requiring high discrimination
between visually similar categories. To enable each hash bit to correspond to
specific visual attributes, we propoe a novel method that harnesses learnable
queries for attribute-aware hash codes learning. This method deploys a tailored
set of queries to capture and represent nuanced attribute-level information
within the hashing process, thereby enhancing both the interpretability and
relevance of each hash bit. Building on this query-based optimization
framework, we incorporate an auxiliary branch to help alleviate the challenges
of complex landscape optimization often encountered with low-bit hash codes.
This auxiliary branch models high-order attribute interactions, reinforcing the
robustness and specificity of the generated hash codes. Experimental results on
benchmark datasets demonstrate that our method generates attribute-aware hash
codes and consistently outperforms state-of-the-art techniques in retrieval
accuracy and robustness, especially for low-bit hash codes, underscoring its
potential in fine-grained image hashing tasks.

</details>


### [99] [Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition](https://arxiv.org/abs/2509.17050)
*Junhao Jia,Yunyou Liu,Yifei Sun,Huangwei Chen,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 提出了GeoProto框架，通过扩散空间和Nyström插值在深度特征的固有几何中锚定相似性，用于原型驱动的细粒度识别


<details>
  <summary>Details</summary>
Motivation: 深度视觉特征普遍存在非线性流形结构，欧几里得距离无法捕捉真实相似性，这在需要捕捉细微语义差异的细粒度识别中尤为严重

Method: 将每个类的潜在流形结构蒸馏到扩散空间，引入可微分的Nyström插值，使用紧凑的每类地标集并定期更新以保持与演化骨干网络的对齐

Result: 在CUB-200-2011和Stanford Cars数据集上的实验表明，GeoProto框架产生的原型聚焦于语义对齐的部分，显著优于欧几里得原型网络

Conclusion: 该框架通过利用深度特征的固有几何结构，有效解决了原型驱动细粒度识别中的相似性度量问题

Abstract: Nonlinear manifolds are widespread in deep visual features, where Euclidean
distances often fail to capture true similarity. This limitation becomes
particularly severe in prototype-based interpretable fine-grained recognition,
where subtle semantic distinctions are essential. To address this challenge, we
propose a novel paradigm for prototype-based recognition that anchors
similarity within the intrinsic geometry of deep features. Specifically, we
distill the latent manifold structure of each class into a diffusion space and
introduce a differentiable Nystr\"om interpolation, making the geometry
accessible to both unseen samples and learnable prototypes. To ensure
efficiency, we employ compact per-class landmark sets with periodic updates.
This design keeps the embedding aligned with the evolving backbone, enabling
fast and scalable inference. Extensive experiments on the CUB-200-2011 and
Stanford Cars datasets show that our GeoProto framework produces prototypes
focusing on semantically aligned parts, significantly outperforming Euclidean
prototype networks.

</details>


### [100] [CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner](https://arxiv.org/abs/2509.17065)
*Yao Du,Jiarong Guo,Xiaomeng Li*

Main category: cs.CV

TL;DR: CardiacCLIP是一个基于视频的框架，通过注意力机制帧聚合和多分辨率输入缩放来提升左心室射血分数（LVEF）预测精度，解决了现有方法依赖大规模标注数据和忽略时间动态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LVEF估计方法依赖大规模标注视频数据集，成本高且临床适应性差。现有的视觉语言模型如EchoCLIP无法捕捉关键的时间动态和局部心脏结构，影响诊断准确性。

Method: 提出CardiacCLIP框架，包含MFL（多帧学习）注意力机制选择性融合信息帧，以及EchoZoom多尺度特征提取策略优化心脏结构空间表示。

Result: 在EchoNet-Dynamic数据集1-shot设置下，MAE降低了2.07，显著提升了诊断准确性。

Conclusion: CardiacCLIP作为CLIP模型在少样本超声心动图视频分析中的新适应方法，有效提升了LVEF预测性能，代码已开源。

Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment,
with left ventricular ejection fraction (LVEF) serving as a key indicator of
heart function. Existing LVEF estimation methods depend on large-scale
annotated video datasets, which are costly and limit adaptability across
various clinical settings. Recent vision-language models for echocardiography,
such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial
temporal dynamics and localized cardiac structures essential for accurate
diagnosis. To address these challenges, we propose CardiacCLIP, a video-based
framework that enhances LVEF prediction through attention-based frame
aggregation and multi-resolution input scaling. Specifically, we introduce MFL
(Multi Frame Learning), a novel attention-based mechanism for selectively
fusing informative frames, and EchoZoom, a multi-scale feature extraction
strategy that refines spatial representations of cardiac structures. As a novel
adaptation of CLIP models for few-shot echocardiogram video analysis, our
approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on
the EchoNet-Dynamic dataset under 1-shot setting. The code is available at
https://github.com/xmed-lab/CardiacCLIP.

</details>


### [101] [Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models](https://arxiv.org/abs/2509.17074)
*Qian Zhang,Lin Zhang,Xing Fang,Mingxin Zhang,Zhiyuan Wei,Ran Song,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息理论的文本引导视觉可供性学习框架，通过最大化视觉特征与文本提示之间的互信息来实现特征对齐，在少量样本学习场景下取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉可供性学习方法虽然利用预训练的视觉语言基础模型，但忽视了视觉图像与语言描述之间的特征对齐重要性，导致识别结果不理想。

Method: 设计了两种信息约束：可供性互信息约束（最大化可供性区域特征与对应文本提示的互信息）和对象级信息约束（最大化对象视觉特征与类别文本特征的互信息）。

Result: 在AGD20K数据集上的实验结果表明，该方法超越了现有方法，在单样本可供性学习中达到了新的最优性能。

Conclusion: 通过信息理论约束实现文本-图像特征对齐是提升视觉可供性学习效果的有效途径，特别是在数据稀缺场景下。

Abstract: Visual affordance learning is crucial for robots to understand and interact
effectively with the physical world. Recent advances in this field attempt to
leverage pre-trained knowledge of vision-language foundation models to learn
affordance properties with limited training data, providing a novel paradigm
for visual affordance learning. However, these methods overlook the
significance of maintaining feature alignment between visual images and
language descriptions for identifying affordance areas with textual guidance,
and thus may lead to suboptimal results. In this paper, we present an
informative framework for text-guided affordance learning, which involves
information-based constraints to achieve text-image alignment at feature level.
Specifically, we design an affordance mutual information constraint that helps
learn appropriate textual prompts and task-oriented visual features
simultaneously by maximizing the mutual information between the features of the
affordance areas in the input images and the corresponding textual prompts. In
addition, we propose an object-level information constraint that maximizes the
mutual information between the visual features of a given object and the text
features of the category it belongs to. This enables the model to capture
high-quality representations for the object, providing more reliable semantic
priors for identifying affordance regions. Experimental results on the AGD20K
dataset show that the proposed method outperforms existing approaches and
achieves the new state-of-the-art in one-shot affordance learning.

</details>


### [102] [Enhanced Detection of Tiny Objects in Aerial Images](https://arxiv.org/abs/2509.17078)
*Kihyun Kim,Michalis Lazarou,Tania Stathaki*

Main category: cs.CV

TL;DR: 本文提出了三种增强策略（输入图像分辨率调整、数据增强和注意力机制）来提升YOLOv8在微小目标检测上的性能，并设计了MoonNet网络管道，通过集成SE Block和CBAM注意力模块显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 一阶段检测器如YOLOv8虽然训练速度快，但在检测小目标时性能较差，特别是在航空影像中由于低分辨率目标和杂乱背景的问题更加严重。

Method: 采用三种增强策略：输入图像分辨率调整、数据增强和注意力机制；设计了MoonNet管道，将SE Block和CBAM注意力模块集成到YOLOv8骨干网络中，并增加通道数。

Result: 图像尺寸放大和适当的数据增强可以带来性能提升；MoonNet骨干网络相比原始YOLOv8获得了改进的检测精度；与YOLC模型集成后在微小目标基准测试中达到了最先进的性能。

Conclusion: 提出的增强策略和MoonNet网络有效解决了YOLOv8在微小目标检测上的局限性，特别是在航空影像应用中表现出良好的适应性和潜力。

Abstract: While one-stage detectors like YOLOv8 offer fast training speed, they often
under-perform on detecting small objects as a trade-off. This becomes even more
critical when detecting tiny objects in aerial imagery due to low-resolution
targets and cluttered backgrounds. To address this, we introduce three
enhancement strategies -- input image resolution adjustment, data augmentation,
and attention mechanisms -- that can be easily implemented on YOLOv8. We
demonstrate that image size enlargement and the proper use of augmentation can
lead to enhancement. Additionally, we designed a Mixture of Orthogonal
Neural-modules Network (MoonNet) pipeline which consists of attention-augmented
CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE
Block) and the Convolutional Block Attention Module (CBAM), were integrated
into the backbone of YOLOv8 with an increased number of channels, and the
MoonNet backbone obtained improved detection accuracy compared to the original
YOLOv8. MoonNet further proved its adaptability and potential by achieving
state-of-the-art performance on a tiny-object benchmark when integrated with
the YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet

</details>


### [103] [A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion](https://arxiv.org/abs/2509.17079)
*Yuhong Feng,Hongtao Chen,Qi Zhang,Jie Chen,Zhaoxi He,Mingzhe Liu,Jianghai Liao*

Main category: cs.CV

TL;DR: 提出双调制框架（Dual Modulation Framework）用于RGB-热成像（RGB-T）人群计数，包含空间调制注意力（SMA）和自适应融合调制（AFM）两个模块，解决了Transformer方法中注意力扩散到背景区域和跨模态融合困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的RGB-T人群计数方法虽然擅长捕捉全局上下文，但缺乏空间归纳偏置导致注意力扩散到无关背景区域，影响人群定位精度，且不同模态间的有效融合仍是一个主要挑战。

Method: 双调制框架包含：1）空间调制注意力（SMA），使用可学习的空间衰减掩码惩罚远距离token之间的注意力，防止注意力扩散到背景；2）自适应融合调制（AFM），通过动态门控机制优先选择最可靠的模态进行自适应跨模态融合。

Result: 在RGB-T人群计数数据集上的大量实验表明，该方法相比先前工作具有优越性能。

Conclusion: 提出的双调制框架有效提升了RGB-T人群计数的精度，特别是在挑战性条件下对人群定位的改进显著，代码已开源。

Abstract: Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in
challenging conditions. While recent Transformer-based methods excel at
capturing global context, their inherent lack of spatial inductive bias causes
attention to spread to irrelevant background regions, compromising crowd
localization precision. Furthermore, effectively bridging the gap between these
distinct modalities remains a major hurdle. To tackle this, we propose the Dual
Modulation Framework, comprising two modules: Spatially Modulated Attention
(SMA), which improves crowd localization by using a learnable Spatial Decay
Mask to penalize attention between distant tokens and prevent focus from
spreading to the background; and Adaptive Fusion Modulation (AFM), which
implements a dynamic gating mechanism to prioritize the most reliable modality
for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting
datasets demonstrate the superior performance of our method compared to
previous works. Code available at
https://github.com/Cht2924/RGBT-Crowd-Counting.

</details>


### [104] [HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis](https://arxiv.org/abs/2509.17083)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: HyRF是一种结合显式高斯和神经场的混合辐射场表示方法，通过分解场景为紧凑的高斯集合和网格神经场，在保持实时性能的同时将模型大小减少20倍以上。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射(3DGS)虽然能够实现实时高质量新视角合成，但存在显著的内存开销问题。现有的神经场压缩方法难以捕捉高斯属性的高频空间变化，导致细节重建质量下降。

Method: HyRF将场景分解为：(1)存储关键高频参数的紧凑显式高斯集合；(2)预测剩余属性的网格神经场。采用解耦神经场架构分别建模几何和视图相关颜色，并提出混合渲染方案结合高斯溅射和神经场预测背景。

Result: 实验表明HyRF在保持实时性能的同时，实现了最先进的渲染质量，模型大小相比3DGS减少超过20倍。

Conclusion: HyRF通过显式和隐式表示的有机结合，有效解决了3DGS的内存开销问题，同时保持了高质量渲染能力，为实时3D场景表示提供了新的解决方案。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.

</details>


### [105] [MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors](https://arxiv.org/abs/2509.17084)
*Binhua Huang,Nan Wang,Arjun Parakash,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCLIP-Lite是一个高效的双流视频动作识别框架，结合冻结的CLIP图像编码器和轻量级运动向量网络，仅训练小型MLP头，在UCF101数据集上达到89.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别模型计算成本高且依赖大量视频预训练，而CLIP模型具有强大的零样本能力但缺乏时序信息，运动向量能高效提供动态信息。需要结合两者的优势。

Method: 提出双流后期融合框架：一个流使用冻结的CLIP图像编码器提取静态特征，另一个流使用轻量级监督网络处理原始运动向量。融合时两个骨干网络都冻结，仅训练小型MLP头。

Result: 在UCF101数据集上达到89.2%的Top-1准确率，显著优于零样本基线（65.0%）和仅使用运动向量的基线（66.5%）。

Conclusion: 该工作为视频理解提供了一个高效的新基准，有效桥接了大型静态模型和低成本动态运动线索之间的差距。

Abstract: Video action recognition is a fundamental task in computer vision, but
state-of-the-art models are often computationally expensive and rely on
extensive video pre-training. In parallel, large-scale vision-language models
like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot
capabilities on static images, while motion vectors (MV) provide highly
efficient temporal information directly from compressed video streams. To
synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple
yet powerful two-stream late fusion framework for efficient video recognition.
Our approach combines features from a frozen CLIP image encoder with features
from a lightweight, supervised network trained on raw MV. During fusion, both
backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is
trained, ensuring extreme efficiency. Through comprehensive experiments on the
UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,
significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)
baselines. Our work provides a new, highly efficient baseline for video
understanding that effectively bridges the gap between large static models and
dynamic, low-cost motion cues. Our code and models are available at
https://github.com/microa/MoCLIP-Lite.

</details>


### [106] [SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks](https://arxiv.org/abs/2509.17086)
*Jie Chen,Yuhong Feng,Tao Dai,Mingzhe Liu,Hongtao Chen,Zhaoxi He,Jiancong Bai*

Main category: cs.CV

TL;DR: SFN-YOLO是一种创新的家禽检测方法，通过尺度感知融合技术结合局部细节和全局上下文，在自由放养环境中实现高效实时检测。


<details>
  <summary>Details</summary>
Motivation: 在自由放养环境中，家禽检测面临多尺度目标、遮挡和复杂动态背景等挑战，需要开发更有效的检测方法。

Method: 提出SFN-YOLO方法，采用尺度感知融合技术，结合局部特征和全局上下文信息，并创建了专门针对自由放养条件的M-SCOPE数据集。

Result: 模型仅用7.2M参数就达到80.7%的mAP，比基准模型参数减少35.1%，同时保持强大的跨域泛化能力。

Conclusion: SFN-YOLO的高效实时检测能力支持自动化智能家禽养殖，代码和数据集已开源。

Abstract: Detecting and localizing poultry is essential for advancing smart poultry
farming. Despite the progress of detection-centric methods, challenges persist
in free-range settings due to multiscale targets, obstructions, and complex or
dynamic backgrounds. To tackle these challenges, we introduce an innovative
poultry detection approach named SFN-YOLO that utilizes scale-aware fusion.
This approach combines detailed local features with broader global context to
improve detection in intricate environments. Furthermore, we have developed a
new expansive dataset (M-SCOPE) tailored for varied free-range conditions.
Comprehensive experiments demonstrate our model achieves an mAP of 80.7% with
just 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining
strong generalization capability across different domains. The efficient and
real-time detection capabilities of SFN-YOLO support automated smart poultry
farming. The code and dataset can be accessed at
https://github.com/chenjessiee/SFN-YOLO.

</details>


### [107] [AlignedGen: Aligning Style Across Generated Images](https://arxiv.org/abs/2509.17088)
*Jiexuan Zhang,Yiheng Du,Qian Wang,Weiqi Li,Yu Gu,Jian Zhang*

Main category: cs.CV

TL;DR: AlignedGen是一个无需训练的框架，通过Shifted Position Embedding和Advanced Attention Sharing技术解决DiT模型中风格一致性问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在相同风格提示下难以保持图像间的风格一致性，现有方法局限于U-Net架构且与DiT不兼容

Method: 提出ShiftPE解决位置嵌入冲突，开发AAS技术充分释放注意力共享潜力，并设计特征提取算法支持外部风格参考

Result: 实验验证该方法有效增强生成图像间的风格一致性，同时保持精确的文图对齐

Conclusion: AlignedGen为DiT模型提供了有效的训练免费风格一致性解决方案，具有广泛适用性

Abstract: Despite their generative power, diffusion models struggle to maintain style
consistency across images conditioned on the same style prompt, hindering their
practical deployment in creative workflows. While several training-free methods
attempt to solve this, they are constrained to the U-Net architecture, which
not only leads to low-quality results and artifacts like object repetition but
also renders them incompatible with superior Diffusion Transformer (DiT). To
address these issues, we introduce AlignedGen, a novel training-free framework
that enhances style consistency across images generated by DiT models. Our work
first reveals a critical insight: naive attention sharing fails in DiT due to
conflicting positional signals from improper position embeddings. We introduce
Shifted Position Embedding (ShiftPE), an effective solution that resolves this
conflict by allocating a non-overlapping set of positional indices to each
image. Building on this foundation, we develop Advanced Attention Sharing
(AAS), a suite of three techniques meticulously designed to fully unleash the
potential of attention sharing within the DiT. Furthermore, to broaden the
applicability of our method, we present an efficient query, key, and value
feature extraction algorithm, enabling our method to seamlessly incorporate
external images as style references. Extensive experimental results validate
that our method effectively enhances style consistency across generated images
while maintaining precise text-to-image alignment.

</details>


### [108] [Uncertainty-Supervised Interpretable and Robust Evidential Segmentation](https://arxiv.org/abs/2509.17098)
*Yuzhu Li,An Sui,Fuping Wu,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种自监督的不确定性估计方法，通过设计基于图像梯度和噪声关系的监督损失，提高了医学图像分割中不确定性的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割中的不确定性估计方法缺乏有效监督，导致预测的可解释性和鲁棒性不足。

Method: 提出自监督方法，基于不确定性、边界图像梯度和噪声之间的关系设计三个原则，并构建两个不确定性监督损失函数。

Result: 实验结果表明，该方法在分割性能上与最先进方法相当，在分布外场景下表现更优，同时显著提高了不确定性估计的可解释性和鲁棒性。

Conclusion: 所提出的自监督不确定性估计方法能够有效提升医学图像分割的可靠性和解释性，为临床决策提供更可信的支持。

Abstract: Uncertainty estimation has been widely studied in medical image segmentation
as a tool to provide reliability, particularly in deep learning approaches.
However, previous methods generally lack effective supervision in uncertainty
estimation, leading to low interpretability and robustness of the predictions.
In this work, we propose a self-supervised approach to guide the learning of
uncertainty. Specifically, we introduce three principles about the
relationships between the uncertainty and the image gradients around boundaries
and noise. Based on these principles, two uncertainty supervision losses are
designed. These losses enhance the alignment between model predictions and
human interpretation. Accordingly, we introduce novel quantitative metrics for
evaluating the interpretability and robustness of uncertainty. Experimental
results demonstrate that compared to state-of-the-art approaches, the proposed
method can achieve competitive segmentation performance and superior results in
out-of-distribution (OOD) scenarios while significantly improving the
interpretability and robustness of uncertainty estimation. Code is available
via https://github.com/suiannaius/SURE.

</details>


### [109] [The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment](https://arxiv.org/abs/2509.17100)
*Deepak Alapatt,Jennifer Eckhoff,Zhiliang Lyu,Yutong Ban,Jean-Paul Mazellier,Sarah Choksi,Kunyi Yang,2024 CVS Challenge Consortium,Quanzheng Li,Filippo Filicori,Xiang Li,Pietro Mascagni,Daniel A. Hashimoto,Guy Rosman,Ozanan Meireles,Nicolas Padoy*

Main category: cs.CV

TL;DR: SAGES CVS Challenge是首个由外科学会组织的AI竞赛，旨在通过腹腔镜胆囊切除术中的安全关键视图来评估手术质量，促进AI在手术质量评估中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术民主化手术专业知识的获取，解决手术质量评估中存在的性能、不确定性和临床变异性等关键障碍。

Method: 开发了EndoGlacier框架管理大规模异质手术视频和多标注者工作流，组织全球54个机构的国际合作，收集1000个由20名专家标注的视频。

Result: 13个国际团队参与，相对评估性能提升17%，校准误差降低80%以上，鲁棒性相对提升17%。

Conclusion: 该挑战赛为开发稳健、可临床部署的手术质量评估AI提供了方法论指导，推动了该领域的研究进展。

Abstract: Advances in artificial intelligence (AI) for surgical quality assessment
promise to democratize access to expertise, with applications in training,
guidance, and accreditation. This study presents the SAGES Critical View of
Safety (CVS) Challenge, the first AI competition organized by a surgical
society, using the CVS in laparoscopic cholecystectomy, a universally
recommended yet inconsistently performed safety step, as an exemplar of
surgical quality assessment. A global collaboration across 54 institutions in
24 countries engaged hundreds of clinicians and engineers to curate 1,000
videos annotated by 20 surgical experts according to a consensus-validated
protocol. The challenge addressed key barriers to real-world deployment in
surgery, including achieving high performance, capturing uncertainty in
subjective assessment, and ensuring robustness to clinical variability. To
enable this scale of effort, we developed EndoGlacier, a framework for managing
large, heterogeneous surgical video and multi-annotator workflows. Thirteen
international teams participated, achieving up to a 17\% relative gain in
assessment performance, over 80\% reduction in calibration error, and a 17\%
relative improvement in robustness over the state-of-the-art. Analysis of
results highlighted methodological trends linked to model performance,
providing guidance for future research toward robust, clinically deployable AI
for surgical quality assessment.

</details>


### [110] [CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception](https://arxiv.org/abs/2509.17107)
*Lingzhao Kong,Jiacheng Lin,Siyu Li,Kai Luo,Zhiyong Li,Kailun Yang*

Main category: cs.CV

TL;DR: CoBEVMoE是一个新颖的协作感知框架，在BEV空间中使用动态专家混合架构，通过动态生成专家来建模特征相似性和异质性，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有中间融合方法主要关注对齐相似特征，忽略了智能体之间的感知多样性。由于视角和空间位置的差异，智能体往往获得异构观测，需要更好地处理这种异质性。

Method: 提出CoBEVMoE框架，在BEV空间中使用动态专家混合架构，每个专家基于特定智能体的输入特征动态生成，能够提取独特可靠的线索并关注共享语义。引入动态专家度量损失来增强专家间多样性。

Result: 在OPV2V和DAIR-V2X-C数据集上，CoBEVMoE实现了最先进性能：相机BEV分割IoU提升+1.5%，LiDAR 3D目标检测AP@50提升+3.0%。

Conclusion: 基于专家的异构特征建模在多智能体协作感知中是有效的，验证了所提方法的有效性。

Abstract: Collaborative perception aims to extend sensing coverage and improve
perception accuracy by sharing information among multiple agents. However, due
to differences in viewpoints and spatial positions, agents often acquire
heterogeneous observations. Existing intermediate fusion methods primarily
focus on aligning similar features, often overlooking the perceptual diversity
among agents. To address this limitation, we propose CoBEVMoE, a novel
collaborative perception framework that operates in the Bird's Eye View (BEV)
space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In
DMoE, each expert is dynamically generated based on the input features of a
specific agent, enabling it to extract distinctive and reliable cues while
attending to shared semantics. This design allows the fusion process to
explicitly model both feature similarity and heterogeneity across agents.
Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance
inter-expert diversity and improve the discriminability of the fused
representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets
demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,
it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the
AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the
effectiveness of expert-based heterogeneous feature modeling in multi-agent
collaborative perception. The source code will be made publicly available at
https://github.com/godk0509/CoBEVMoE.

</details>


### [111] [Stencil: Subject-Driven Generation with Context Guidance](https://arxiv.org/abs/2509.17120)
*Gordon Chen,Ziqi Huang,Cheston Tan,Ziwei Liu*

Main category: cs.CV

TL;DR: Stencil是一个新颖的文本到图像生成框架，通过联合使用两个扩散模型来解决主题一致性问题，在保持高质量的同时实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在跨生成和上下文中难以保持主题一致性，现有微调方法存在质量与效率的权衡问题：微调大模型质量高但计算昂贵，微调轻量模型效率高但图像保真度差。

Method: Stencil在推理过程中联合使用两个扩散模型：高效微调一个轻量模型来处理主题图像，同时使用一个大型冻结预训练模型提供上下文指导，注入丰富的先验知识以最小开销增强生成效果。

Result: Stencil能够在不到一分钟内生成高质量、新颖的主题呈现，实现了最先进的性能，为主题驱动生成设定了新的基准。

Conclusion: Stencil框架成功解决了主题一致性生成中的质量-效率权衡问题，通过双模型协同工作实现了高效且高质量的主题驱动图像生成。

Abstract: Recent text-to-image diffusion models can generate striking visuals from text
prompts, but they often fail to maintain subject consistency across generations
and contexts. One major limitation of current fine-tuning approaches is the
inherent trade-off between quality and efficiency. Fine-tuning large models
improves fidelity but is computationally expensive, while fine-tuning
lightweight models improves efficiency but compromises image fidelity.
Moreover, fine-tuning pre-trained models on a small set of images of the
subject can damage the existing priors, resulting in suboptimal results. To
this end, we present Stencil, a novel framework that jointly employs two
diffusion models during inference. Stencil efficiently fine-tunes a lightweight
model on images of the subject, while a large frozen pre-trained model provides
contextual guidance during inference, injecting rich priors to enhance
generation with minimal overhead. Stencil excels at generating high-fidelity,
novel renditions of the subject in less than a minute, delivering
state-of-the-art performance and setting a new benchmark in subject-driven
generation.

</details>


### [112] [SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM](https://arxiv.org/abs/2509.17136)
*Yuhao Tian,Zheming Yang*

Main category: cs.CV

TL;DR: SAEC是一个面向工业视觉检测的场景感知边缘-云协作框架，通过MLLM实现复杂缺陷检测，在保持高精度的同时显著降低计算成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决工业视觉检测中MLLM计算成本过高与轻量级边缘模型在复杂场景下性能不足的矛盾，实现精度与效率的平衡。

Method: 包含三个核心组件：高效MLLM微调用于复杂缺陷检测、轻量级多尺度场景复杂度估计、自适应边缘-云调度器，根据场景复杂度动态分配计算资源。

Result: 在MVTec AD和KSDD2数据集上分别达到85.11%和82.72%的准确率，相比Qwen提升22.1%和20.8%，相比LLaVA提升33.3%和31.6%，同时运行时降低22.4%，每正确决策能耗降低40%-74%。

Conclusion: SAEC框架成功解决了工业视觉检测中的精度-效率权衡问题，为资源受限环境下的高质量检测提供了可行方案。

Abstract: Industrial vision inspection requires high accuracy under stringent resource
constraints, yet existing approaches face a fundamental trade-off. Multimodal
LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive
computational costs, while lightweight edge models often fail on complex cases.
In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative
industrial vision inspection framework with MLLM. The framework is composed of
three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect
Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)
Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect
detection by tailoring multimodal reasoning to scene complexity and dynamically
balancing computation between edge and cloud resources. Experimental results on
MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%
accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It
also reduces runtime by up to 22.4% and cuts energy per correct decision by
40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.

</details>


### [113] [SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction](https://arxiv.org/abs/2509.17172)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出MD-Net双流架构，结合预训练扩散模型和视觉Mamba模型，在面部美观度预测任务上取得新SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型擅长局部特征提取但难以建模长距离依赖，ViT模型能建模全局关系但计算成本高，需要解决这一权衡问题

Method: 采用双流架构：第一流使用预训练潜在扩散模型的U-Net编码器提取细粒度美学特征；第二流使用视觉Mamba模型高效捕获全局面部结构；通过交叉注意力机制整合互补表示

Result: 在SCUT-FBP5500基准测试中达到Pearson相关系数0.9235，创下新SOTA

Conclusion: 融合生成模型和序列建模范式的混合架构在复杂视觉评估任务中具有显著潜力

Abstract: The automated prediction of facial beauty is a benchmark task in affective
computing that requires a sophisticated understanding of both local aesthetic
details (e.g., skin texture) and global facial harmony (e.g., symmetry,
proportions). Existing models, based on either Convolutional Neural Networks
(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases
that limit their performance; CNNs excel at local feature extraction but
struggle with long-range dependencies, while ViTs model global relationships at
a significant computational cost. This paper introduces the
\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture
that resolves this trade-off by delegating specialized roles to
state-of-the-art models. The first stream leverages a frozen U-Net encoder from
a pre-trained latent diffusion model, providing a powerful generative prior for
fine-grained aesthetic qualities. The second stream employs a Vision Mamba
(Vim), a modern state-space model, to efficiently capture global facial
structure with linear-time complexity. By synergistically integrating these
complementary representations through a cross-attention mechanism, MD-Net
creates a holistic and nuanced feature space for prediction. Evaluated on the
SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson
Correlation of \textbf{0.9235} and demonstrating the significant potential of
hybrid architectures that fuse generative and sequential modeling paradigms for
complex visual assessment tasks.

</details>


### [114] [Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge](https://arxiv.org/abs/2509.17187)
*Lalith Bharadwaj Baru,Kamalaker Dadi,Tapabrata Chakraborti,Raju S. Bapi*

Main category: cs.CV

TL;DR: 本文提出了Segmentation Schödinger Bridge (SSB)方法，首次将Schrödinger Bridge应用于模糊医学图像分割，通过建模图像-掩码联合动态来提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临病灶边界不清晰和掩码变异性大的挑战，需要一种能够处理模糊边界并保持结构完整性的方法。

Method: SSB方法建模图像和掩码的联合动态，使用新颖的损失函数来保持结构完整性、描绘模糊边界并维持多样性。同时提出了Diversity Divergence Index (D_{DDI})来量化标注者间的变异性。

Result: SSB在LIDC-IDRI、COCA和RACER（内部）数据集上实现了最先进的性能。

Conclusion: SSB是首个将Schrödinger Bridge应用于模糊医学图像分割的方法，能够有效处理边界不清晰的问题，并在多个数据集上表现出优越性能。

Abstract: Accurate segmentation of medical images is challenging due to unclear lesion
boundaries and mask variability. We introduce \emph{Segmentation Sch\"{o}dinger
Bridge (SSB)}, the first application of Sch\"{o}dinger Bridge for ambiguous
medical image segmentation, modelling joint image-mask dynamics to enhance
performance. SSB preserves structural integrity, delineates unclear boundaries
without additional guidance, and maintains diversity using a novel loss
function. We further propose the \emph{Diversity Divergence Index} ($D_{DDI}$)
to quantify inter-rater variability, capturing both diversity and consensus.
SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER
(in-house) datasets.

</details>


### [115] [Echo-Path: Pathology-Conditioned Echo Video Generation](https://arxiv.org/abs/2509.17190)
*Kabir Hamzah Muhammad,Marawan Elbatel,Yi Qin,Xiaomeng Li*

Main category: cs.CV

TL;DR: Echo-Path是一个生成式框架，能够根据特定心脏病理条件生成超声心动图视频，用于解决罕见心脏疾病数据稀缺问题，提升自动化诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，但某些病理的超声心动图数据稀缺，限制了自动化诊断模型的开发。需要生成逼真的病理特异性超声视频来增强训练数据。

Method: 在先进的超声视频生成器中引入病理条件机制，使模型能够学习和控制疾病特定的心脏结构和运动模式，专注于房间隔缺损和肺动脉高压两种病理。

Result: 合成视频达到低分布距离，具有高视觉保真度；生成的超声图像显示合理的病理标记；使用合成数据训练的分类器在真实数据上表现良好，将ASD和PAH的诊断准确率分别提高了7%和8%。

Conclusion: Echo-Path框架能够有效生成病理特异性超声心动图视频，解决了数据稀缺问题，显著提升了心脏疾病自动化诊断的性能。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
globally, and echocardiography is critical for diagnosis of both common and
congenital cardiac conditions. However, echocardiographic data for certain
pathologies are scarce, hindering the development of robust automated diagnosis
models. In this work, we propose Echo-Path, a novel generative framework to
produce echocardiogram videos conditioned on specific cardiac pathologies.
Echo-Path can synthesize realistic ultrasound video sequences that exhibit
targeted abnormalities, focusing here on atrial septal defect (ASD) and
pulmonary arterial hypertension (PAH). Our approach introduces a
pathology-conditioning mechanism into a state-of-the-art echo video generator,
allowing the model to learn and control disease-specific structural and motion
patterns in the heart. Quantitative evaluation demonstrates that the synthetic
videos achieve low distribution distances, indicating high visual fidelity.
Clinically, the generated echoes exhibit plausible pathology markers.
Furthermore, classifiers trained on our synthetic data generalize well to real
data and, when used to augment real training sets, it improves downstream
diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset
are available here https://github.com/Marshall-mk/EchoPathv1

</details>


### [116] [VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery](https://arxiv.org/abs/2509.17191)
*Jinchao Ge,Tengfei Cheng,Biao Wu,Zeyu Zhang,Shiya Huang,Judith Bishop,Gillian Shepherd,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: VaseVL是一个针对古希腊陶器分析的MLLM系统，通过SFT-then-RL方法将评估转化为监督，结合类型分类和组合性导向的奖励工程，在风格分类和历史归属任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在文化遗产分析中存在挑战：通用模型缺乏领域专业知识，而SFT容易过拟合表面模式，导致认证和历史归属的推理脆弱。需要为MLLM配备稳健的专家级推理能力。

Method: 构建问题类型分类法，探测SFT模型以定位类型特定的性能差距，使用类型条件、组合性导向的奖励针对这些差距进行优化。发布包含31,773张图像的VaseVQA基准数据集。

Result: 在风格分类和历史归属任务上取得最先进结果，相比仅使用SFT的基线模型，在组合鲁棒性方面有显著提升。

Conclusion: 验证了诊断引导、分类法条件的奖励工程的有效性，为未来研究提供了可重复使用的资源。

Abstract: Analyzing cultural-heritage artifacts remains challenging for MLLMs: general
models lack domain expertise, and SFT often overfits superficial patterns,
yielding brittle reasoning for authentication and historical attribution. This
raises the question of how to equip MLLMs with robust, expert-level reasoning
for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns
evaluation into supervision: we construct a taxonomy of question types, probe
the SFT model to localize type-specific performance gaps, and optimize with
type-conditioned, compositionality-oriented rewards targeting those gaps. We
also release VaseVQA, a comprehensive benchmark of 31,773 images designed to
probe deep understanding. Experiments show state-of-the-art results on style
classification and historical attribution with marked gains in compositional
robustness over SFT-only baselines, validating diagnosis-guided,
taxonomy-conditioned reward engineering and providing a reusable resource for
future research. Code and dataset will be available at
https://github.com/AIGeeksGroup/VaseVQA.

</details>


### [117] [Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation](https://arxiv.org/abs/2509.17206)
*Gunner Stone,Sushmita Sarker,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的3D点云生成框架，将逐点语义条件直接嵌入生成过程，实现几何和语义的联合合成。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法主要捕捉几何信息，语义信息通常通过外部分割或聚类后处理添加，而不是集成到生成过程本身。

Method: 采用扩散模型框架，每个点关联语义标签作为条件变量，指导扩散动态过程，实现几何和语义的联合生成。

Result: 该方法生成的点云具有结构一致性和分割感知能力，在生成过程中明确表示对象部件，实验验证了方法的有效性。

Conclusion: 语义条件变量对扩散动态和生成质量有显著影响，提出的方法能够生成针对特定部件和特征的详细准确3D点云。

Abstract: Generating realistic 3D point clouds is a fundamental problem in computer
vision with applications in remote sensing, robotics, and digital object
modeling. Existing generative approaches primarily capture geometry, and when
semantics are considered, they are typically imposed post hoc through external
segmentation or clustering rather than integrated into the generative process
itself. We propose a diffusion-based framework that embeds per-point semantic
conditioning directly within generation. Each point is associated with a
conditional variable corresponding to its semantic label, which guides the
diffusion dynamics and enables the joint synthesis of geometry and semantics.
This design produces point clouds that are both structurally coherent and
segmentation-aware, with object parts explicitly represented during synthesis.
Through a comparative analysis of guided and unguided diffusion processes, we
demonstrate the significant impact of conditional variables on diffusion
dynamics and generation quality. Extensive experiments validate the efficacy of
our approach, producing detailed and accurate 3D point clouds tailored to
specific parts and features.

</details>


### [118] [Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds](https://arxiv.org/abs/2509.17207)
*Gunner Stone,Youngsook Choi,Alireza Tavakkoli,Ankita Shukla*

Main category: cs.CV

TL;DR: Point-RTD是一种新颖的3D点云预训练策略，通过替换令牌去噪的破坏-重建框架提升模型鲁棒性，相比传统掩码重建方法显著提升性能和效率


<details>
  <summary>Details</summary>
Motivation: 传统基于掩码的重建方法在隐藏数据段后预测存在局限性，需要更有效的预训练策略来学习3D点云的结构先验知识

Method: 采用破坏-重建框架，通过破坏点云令牌并利用判别器-生成器架构进行去噪，而非传统的掩码重建方法

Result: 在ShapeNet数据集上重建误差比PointMAE降低93%以上，测试集Chamfer距离降低14倍以上，收敛更快且在多个基准数据集上分类准确率更高

Conclusion: Point-RTD明显优于基线Point-MAE框架，在性能和效率方面都有显著提升，证明了该预训练策略的有效性

Abstract: Pre-training strategies play a critical role in advancing the performance of
transformer-based models for 3D point cloud tasks. In this paper, we introduce
Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to
improve token robustness through a corruption-reconstruction framework. Unlike
traditional mask-based reconstruction tasks that hide data segments for later
prediction, Point-RTD corrupts point cloud tokens and leverages a
discriminator-generator architecture for denoising. This shift enables more
effective learning of structural priors and significantly enhances model
performance and efficiency. On the ShapeNet dataset, Point-RTD reduces
reconstruction error by over 93% compared to PointMAE, and achieves more than
14x lower Chamfer Distance on the test set. Our method also converges faster
and yields higher classification accuracy on ShapeNet, ModelNet10, and
ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework
in every case.

</details>


### [119] [MirrorSAM2: Segment Mirror in Videos with Depth Perception](https://arxiv.org/abs/2509.17220)
*Mingchen Xu,Yukun Lai,Ze Ji,Jing Wu*

Main category: cs.CV

TL;DR: MirrorSAM2是首个将Segment Anything Model 2（SAM2）适配到RGB-D视频镜面分割任务的框架，通过四个定制模块解决镜面检测中的关键挑战，在VMD和DVMD基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决镜面检测中的反射模糊和纹理混淆等关键挑战，将SAM2的能力扩展到无提示设置下的RGB-D视频镜面分割任务。

Method: 引入四个定制模块：深度扭曲模块用于RGB和深度对齐、深度引导多尺度点提示生成器用于自动提示生成、频率细节注意力融合模块增强结构边界、带有可学习镜面标记的镜面掩码解码器用于精细化分割。

Result: 在VMD和DVMD基准测试中达到最先进的性能，即使在小型镜面、弱边界和强反射等挑战性条件下也表现出色。

Conclusion: MirrorSAM2成功将SAM2扩展到自动视频镜面分割任务，通过充分利用RGB和深度信息的互补性，为镜面检测提供了有效的解决方案。

Abstract: This paper presents MirrorSAM2, the first framework that adapts Segment
Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.
MirrorSAM2 addresses key challenges in mirror detection, such as reflection
ambiguity and texture confusion, by introducing four tailored modules: a Depth
Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point
Prompt Generator for automatic prompt generation, a Frequency Detail Attention
Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with
a learnable mirror token for refined segmentation. By fully leveraging the
complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities
to the prompt-free setting. To our knowledge, this is the first work to enable
SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD
benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under
challenging conditions such as small mirrors, weak boundaries, and strong
reflections.

</details>


### [120] [DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction](https://arxiv.org/abs/2509.17232)
*Bo Liu,Runlong Li,Li Zhou,Yan Zhou*

Main category: cs.CV

TL;DR: DT-NeRF方法结合扩散模型和Transformers，显著提升了3D场景重建的细节恢复和多视角一致性，在稀疏视角下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在稀疏视角下细节恢复不足，多视角一致性较差，需要更有效的3D场景重建方法。

Method: 提出DT-NeRF方法，将扩散模型与Transformers结合，通过扩散模型优化细节恢复，Transformers保持多视角一致性。

Result: 在Matterport3D和ShapeNet数据集上，DT-NeRF在PSNR、SSIM、Chamfer Distance和Fidelity等指标上显著优于传统NeRF和其他先进方法。

Conclusion: DT-NeRF展示了模块间的协同效应，为3D场景重建提供了高效准确的解决方案，未来可优化模型并探索更先进的生成模型和网络架构。

Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.

</details>


### [121] [SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2509.17246)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplatV2是一个无需真实相机位姿的3D高斯泼溅框架，可从稀疏多视角图像高效重建3D场景，在训练和推理阶段都不需要真实位姿监督。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法通常依赖真实相机位姿作为监督信号，这限制了在无位姿标注的大规模数据集上的应用。本文旨在开发一个不依赖真实位姿的端到端框架。

Method: 使用共享特征提取主干网络，在规范空间中同时预测3D高斯基元和相机位姿。引入掩码注意力机制高效估计目标位姿，并通过重投影损失强制像素对齐的高斯基元。

Result: 在域内和域外新视角合成任务上达到最先进性能，即使在极端视角变化和有限图像重叠的情况下也表现优异，超越了依赖几何监督的相对位姿估计方法。

Conclusion: 该方法通过消除对真实位姿的依赖，为利用更大更多样化的数据集提供了可扩展性，展示了无位姿监督3D重建的潜力。

Abstract: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.

</details>


### [122] [Optimized Learned Image Compression for Facial Expression Recognition](https://arxiv.org/abs/2509.17262)
*Xiumei Li,Marc Windsheimer,Misha Sadeghi,Björn Eskofier,André Kaup*

Main category: cs.CV

TL;DR: 该研究提出了一种端到端模型，通过定制损失函数平衡压缩效率和面部表情识别精度，在保持图像细节的同时显著提升压缩效率和分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决面部表情识别任务中，有损压缩导致特征退化和识别准确率下降的问题，需要在压缩效率和识别性能之间找到平衡。

Method: 采用端到端模型设计，引入定制损失函数优化模型，通过调整损失项权重来平衡压缩和识别性能，包括单独微调压缩模型和联合优化两种策略。

Result: 单独微调压缩模型使分类准确率提升0.71%，压缩效率提升49.32%；联合优化实现准确率提升4.04%，压缩效率提升89.12%。优化后的模型在压缩和非压缩数据上均保持高精度。

Conclusion: 该方法有效解决了面部表情识别中的压缩-识别权衡问题，证明了联合优化策略在保持图像细节和提升性能方面的优越性，为视觉数据压缩与识别任务提供了有效解决方案。

Abstract: Efficient data compression is crucial for the storage and transmission of
visual data. However, in facial expression recognition (FER) tasks, lossy
compression often leads to feature degradation and reduced accuracy. To address
these challenges, this study proposes an end-to-end model designed to preserve
critical features and enhance both compression and recognition performance. A
custom loss function is introduced to optimize the model, tailored to balance
compression and recognition performance effectively. This study also examines
the influence of varying loss term weights on this balance. Experimental
results indicate that fine-tuning the compression model alone improves
classification accuracy by 0.71% and compression efficiency by 49.32%, while
joint optimization achieves significant gains of 4.04% in accuracy and 89.12%
in efficiency. Moreover, the findings demonstrate that the jointly optimized
classification model maintains high accuracy on both compressed and
uncompressed data, while the compression model reliably preserves image
details, even at high compression rates.

</details>


### [123] [Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity](https://arxiv.org/abs/2509.17282)
*Xiangmin Xu,Zhen Meng,Kan Chen,Jiaming Yang,Emma Li,Philip G. Zhao,David Flynn*

Main category: cs.CV

TL;DR: 本文提出了一种基于上下文bandit PPO框架的无线网络图像选择方法，通过结合信息年龄和语义信息来优化3D场景表示，在保持低延迟的同时提高表示保真度。


<details>
  <summary>Details</summary>
Motivation: 实时3D场景表示在数字制造、VR/AR/MR和元宇宙等应用中至关重要，但在无线网络环境下如何平衡实时性和保真度仍然是一个挑战。

Method: 提出了上下文bandit近端策略优化框架，结合信息年龄和语义信息来优化图像选择，设计了ω-阈值和ω-等待两种策略，并在标准数据集和基线3D场景表示模型上进行了评估。

Result: 实验结果表明，该方法在保持低延迟的同时提高了表示保真度，并为模型的决策过程提供了洞察。

Conclusion: 这项工作通过优化动态环境中实时性和保真度之间的权衡，推进了实时3D场景表示技术的发展。

Abstract: Real-time Three-dimensional (3D) scene representation is a foundational
element that supports a broad spectrum of cutting-edge applications, including
digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and
the emerging metaverse. Despite advancements in real-time communication and
computing, achieving a balance between timeliness and fidelity in 3D scene
representation remains a challenge. This work investigates a wireless network
where multiple homogeneous mobile robots, equipped with cameras, capture an
environment and transmit images to an edge server over channels for 3D
representation. We propose a contextual-bandit Proximal Policy Optimization
(PPO) framework incorporating both Age of Information (AoI) and semantic
information to optimize image selection for representation, balancing data
freshness and representation quality. Two policies -- the $\omega$-threshold
and $\omega$-wait policies -- together with two benchmark methods are
evaluated, timeliness embedding and weighted sum, on standard datasets and
baseline 3D scene representation models. Experimental results demonstrate
improved representation fidelity while maintaining low latency, offering
insight into the model's decision-making process. This work advances real-time
3D scene representation by optimizing the trade-off between timeliness and
fidelity in dynamic environments.

</details>


### [124] [Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models](https://arxiv.org/abs/2509.17283)
*Licheng Zhan,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新的建筑合规检查任务：自动化设施枚举，通过结合门检测和LLM推理来验证设施数量是否符合法规要求


<details>
  <summary>Details</summary>
Motivation: 建筑合规检查中的设施类型和空间分布枚举问题在文献中被忽视，手动执行耗时耗力，LLM的发展为自动化提供了新机会

Method: 提出了一种集成门检测与基于LLM推理的新方法，首次将LLM应用于此任务，并通过思维链管道增强性能

Result: 在真实世界和合成平面图数据上的实验证明了该方法的有效性和鲁棒性，能够很好地泛化到不同数据集和设施类型

Conclusion: 该方法为建筑合规检查中的自动化设施枚举提供了有效的解决方案，展示了LLM在该领域的应用潜力

Abstract: Building compliance checking (BCC) is a critical process for ensuring that
constructed facilities meet regulatory standards. A core component of BCC is
the accurate enumeration of facility types and their spatial distribution.
Despite its importance, this problem has been largely overlooked in the
literature, posing a significant challenge for BCC and leaving a critical gap
in existing workflows. Performing this task manually is time-consuming and
labor-intensive. Recent advances in large language models (LLMs) offer new
opportunities to enhance automation by combining visual recognition with
reasoning capabilities. In this paper, we introduce a new task for BCC:
automated facility enumeration, which involves validating the quantity of each
facility type against statutory requirements. To address it, we propose a novel
method that integrates door detection with LLM-based reasoning. We are the
first to apply LLMs to this task and further enhance their performance through
a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse
datasets and facility types. Experiments on both real-world and synthetic floor
plan data demonstrate the effectiveness and robustness of our method.

</details>


### [125] [DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking](https://arxiv.org/abs/2509.17323)
*Buyin Deng,Lingxin Huang,Kai Luo,Fei Teng,Kailun Yang*

Main category: cs.CV

TL;DR: DepTR-MOT是一个基于DETR的多目标跟踪方法，通过引入实例级深度信息来解决遮挡和近距离交互问题，在机器人环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标跟踪方法主要依赖2D线索（如边界框和运动建模），在遮挡和近距离交互场景下表现不佳。机器人环境中目标密集且遮挡频繁，深度信息有潜力解决这些问题，但现有数据集缺乏深度标注。

Method: 提出DepTR-MOT，采用两个关键创新：(1)基于基础模型的实例级软深度标签监督来优化深度预测；(2)密集深度图蒸馏以保持全局深度一致性。这些策略使模型能够在推理时输出实例级深度，无需基础模型且不增加计算成本。

Result: 在QuadTrack和DanceTrack数据集上的实验表明，该方法分别达到27.59和44.47的HOTA分数。特别是在机器人平台数据集QuadTrack上，该方法在处理遮挡和近距离挑战方面表现出优势。

Conclusion: 通过引入深度线索，DepTR-MOT增强了TBD范式的鲁棒性，有效解决了遮挡和近距离交互问题，特别适用于机器人环境中的多目标跟踪任务。

Abstract: Visual Multi-Object Tracking (MOT) is a crucial component of robotic
perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D
cues, such as bounding boxes and motion modeling, which struggle under
occlusions and close-proximity interactions. Trackers relying on these 2D cues
are particularly unreliable in robotic environments, where dense targets and
frequent occlusions are common. While depth information has the potential to
alleviate these issues, most existing MOT datasets lack depth annotations,
leading to its underexploited role in the domain. To unveil the potential of
depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based
detector enhanced with instance-level depth information. Specifically, we
propose two key innovations: (i) foundation model-based instance-level soft
depth label supervision, which refines depth prediction, and (ii) the
distillation of dense depth maps to maintain global depth consistency. These
strategies enable DepTR-MOT to output instance-level depth during inference,
without requiring foundation models and without additional computational cost.
By incorporating depth cues, our method enhances the robustness of the TBD
paradigm, effectively resolving occlusion and close-proximity challenges.
Experiments on both the QuadTrack and DanceTrack datasets demonstrate the
effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,
respectively. In particular, results on QuadTrack, a robotic platform MOT
dataset, highlight the advantages of our method in handling occlusion and
close-proximity challenges in robotic tracking. The source code will be made
publicly available at https://github.com/warriordby/DepTR-MOT.

</details>


### [126] [UIPro: Unleashing Superior Interaction Capability For GUI Agents](https://arxiv.org/abs/2509.17328)
*Hongxin Li,Jingran Su,Jingfan Chen,Zheng Ju,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: UIPro是一个新型通用GUI代理，通过多平台多任务GUI交互数据和统一动作空间训练，在多个GUI任务基准上表现出优越性能


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的GUI代理存在场景有限、数据规模不足和动作空间异构等问题，阻碍了通用GUI代理的发展

Method: 首先构建包含2060万GUI理解任务的综合数据集进行预训练，然后建立统一动作空间整合异构GUI代理任务数据集，通过持续微调提升动作预测能力

Result: 实验结果表明UIPro在多个平台的各种GUI任务基准上表现出优越性能

Conclusion: 该方法有效解决了通用GUI代理开发中的关键问题，证明了统一动作空间和综合数据集训练的有效性

Abstract: Building autonomous agents that perceive and operate graphical user
interfaces (GUIs) like humans has long been a vision in the field of artificial
intelligence. Central to these agents is the capability for GUI interaction,
which involves GUI understanding and planning capabilities. Existing methods
have tried developing GUI agents based on the multi-modal comprehension ability
of vision-language models (VLMs). However, the limited scenario, insufficient
size, and heterogeneous action spaces hinder the progress of building
generalist GUI agents. To resolve these issues, this paper proposes
\textbf{UIPro}, a novel generalist GUI agent trained with extensive
multi-platform and multi-task GUI interaction data, coupled with a unified
action space. We first curate a comprehensive dataset encompassing 20.6 million
GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding
capability, which is key to downstream GUI agent tasks. Subsequently, we
establish a unified action space to harmonize heterogeneous GUI agent task
datasets and produce a merged dataset to foster the action prediction ability
of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's
superior performance across multiple GUI task benchmarks on various platforms,
highlighting the effectiveness of our approach.

</details>


### [127] [SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction](https://arxiv.org/abs/2509.17329)
*Neham Jain,Andrew Jong,Sebastian Scherer,Ioannis Gkioulekas*

Main category: cs.CV

TL;DR: SmokeSeer是一种从视频中同时进行3D场景重建和烟雾去除的方法，利用热成像和RGB图像，通过3D高斯泼溅技术将场景分解为烟雾和非烟雾成分，能够处理各种烟雾密度和时变烟雾。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的烟雾会严重降低图像质量并阻碍可见性。现有的图像恢复方法要么依赖容易产生幻觉的数据驱动先验，要么仅限于静态低密度烟雾。

Method: 基于3D高斯泼溅技术，融合热成像和RGB图像信息，利用热成像中散射减少的特性透过烟雾观察场景，并将场景显式分解为烟雾和非烟雾成分。

Result: 在合成数据和真实世界多视角烟雾数据集上验证了方法的有效性，能够处理广泛的烟雾密度范围并适应时变烟雾。

Conclusion: SmokeSeer提供了一种有效的烟雾去除和3D场景重建解决方案，项目提供了开源代码和数据集。

Abstract: Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.

</details>


### [128] [Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model](https://arxiv.org/abs/2509.17365)
*Amanuel Tafese Dufera*

Main category: cs.CV

TL;DR: 本文提出使用Transformer模型进行图像描述生成，克服了传统CNN-LSTM方法的局限性，通过自注意力机制实现高效并行化训练和推理。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN和LSTM方法在图像描述生成中存在训练和推理速度慢的问题，且LSTM在处理长序列时难以保留早期信息。Transformer的自注意力机制能够更好地捕捉数据中的短程和长程依赖关系。

Method: 采用Transformer架构，结合EfficientNetB0 CNN进行特征提取，在Flickr30k数据集上进行数据预处理和模型训练，并融入注意力机制。

Result: 该方法展示了利用并行化实现高效训练和推理的能力，项目已在GitHub上开源。

Conclusion: Transformer模型在图像描述生成任务中具有显著优势，能够有效解决传统方法的局限性，为相关研究提供了实用的实现指南。

Abstract: Automatic image captioning, a multifaceted task bridging computer vision and
natural lan- guage processing, aims to generate descriptive textual content
from visual input. While Convolutional Neural Networks (CNNs) and Long
Short-Term Memory (LSTM) networks have achieved significant advancements, they
present limitations. The inherent sequential nature of RNNs leads to sluggish
training and inference times. LSTMs further struggle with retaining information
from earlier sequence elements when dealing with very long se- quences. This
project presents a comprehensive guide to constructing and comprehending
transformer models for image captioning. Transformers employ self-attention
mechanisms, capturing both short- and long-range dependencies within the data.
This facilitates efficient parallelization during both training and inference
phases. We leverage the well-established Transformer architecture, recognized
for its effectiveness in managing sequential data, and present a meticulous
methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,
construct a model architecture that integrates an EfficientNetB0 CNN for fea-
ture extraction, and train the model with attention mechanisms incorporated.
Our approach exemplifies the utilization of parallelization for efficient
training and inference. You can find the project on GitHub.

</details>


### [129] [Revisiting Vision Language Foundations for No-Reference Image Quality Assessment](https://arxiv.org/abs/2509.17374)
*Ankit Yadav,Ta Duc Huy,Lingqiao Liu*

Main category: cs.CV

TL;DR: 本文首次系统评估了六种预训练骨干网络（CLIP、SigLIP2、DINOv2、DINOv3、Perception、ResNet）在无参考图像质量评估任务中的表现，发现SigLIP2表现优异且激活函数选择对模型泛化能力至关重要，提出可学习激活选择机制实现新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言预训练在无参考图像质量评估中显示出潜力，但现代Vision Transformer基础的相对优势尚不清楚，需要系统评估不同预训练骨干网络在该任务中的表现。

Method: 使用相同的轻量级MLP头对六种预训练骨干网络进行微调，发现sigmoid激活函数优于ReLU和GELU，并引入可学习激活选择机制自适应确定每个通道的非线性。

Result: SigLIP2表现一致强劲，可学习激活选择机制在CLIVE、KADID10K和AGIQA3K数据集上实现了新的SRCC最优性能，广泛消融实验证实了该方法的有效性。

Conclusion: 研究建立了强大且资源高效的无参考图像质量评估基线，揭示了激活函数选择对模型性能的关键影响，为后续研究提供了重要参考。

Abstract: Large-scale vision language pre-training has recently shown promise for
no-reference image-quality assessment (NR-IQA), yet the relative merits of
modern Vision Transformer foundations remain poorly understood. In this work,
we present the first systematic evaluation of six prominent pretrained
backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task
of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an
identical lightweight MLP head. Our study uncovers two previously overlooked
factors: (1) SigLIP2 consistently achieves strong performance; and (2) the
choice of activation function plays a surprisingly crucial role, particularly
for enhancing the generalization ability of image quality assessment models.
Notably, we find that simple sigmoid activations outperform commonly used ReLU
and GELU on several benchmarks. Motivated by this finding, we introduce a
learnable activation selection mechanism that adaptively determines the
nonlinearity for each channel, eliminating the need for manual activation
design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and
AGIQA3K. Extensive ablations confirm the benefits across architectures and
regimes, establishing strong, resource-efficient NR-IQA baselines.

</details>


### [130] [Interpreting vision transformers via residual replacement model](https://arxiv.org/abs/2509.17401)
*Jinyeong Kim,Junhyeok Kim,Yumin Shim,Joohyeok Kim,Sunyoung Jung,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文通过稀疏自编码器分析6.6K个特征，并引入残差替换模型，系统性地研究了视觉Transformer（ViT）如何表示和处理世界。


<details>
  <summary>Details</summary>
Motivation: 理解ViT如何表示和处理世界是一个长期存在的问题，本文旨在通过系统性分析来揭示ViT的内部工作机制。

Method: 使用稀疏自编码器提取所有层的6.6K个特征，并引入残差替换模型，将ViT计算替换为残差流中的可解释特征。

Result: 分析揭示了特征从低层模式到高层语义的演化过程，以及ViT如何通过专门的特征类型编码曲线和空间位置。残差替换模型显著简化了原始计算，产生了忠实且简洁的可解释电路。

Conclusion: 该框架能够直观理解ViT机制，并在消除虚假相关性方面展示了实用性。

Abstract: How do vision transformers (ViTs) represent and process the world? This paper
addresses this long-standing question through the first systematic analysis of
6.6K features across all layers, extracted via sparse autoencoders, and by
introducing the residual replacement model, which replaces ViT computations
with interpretable features in the residual stream. Our analysis reveals not
only a feature evolution from low-level patterns to high-level semantics, but
also how ViTs encode curves and spatial positions through specialized feature
types. The residual replacement model scalably produces a faithful yet
parsimonious circuit for human-scale interpretability by significantly
simplifying the original computations. As a result, this framework enables
intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility
of our framework in debiasing spurious correlations.

</details>


### [131] [Diff-GNSS: Diffusion-based Pseudorange Error Estimation](https://arxiv.org/abs/2509.17397)
*Jiaqi Zhu,Shouyi Lu,Ziyao Li,Guirong Zhuo,Lu Xiong*

Main category: cs.CV

TL;DR: Diff-GNSS是一个基于条件扩散模型的粗到精GNSS伪距误差估计框架，通过Mamba模块进行粗估计，再用扩散模型进行细粒度建模，显著提高了城市环境中GNSS定位的精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城市定位中面临多径和非视距接收导致的测量误差问题，传统学习方法受限于复杂误差分布，需要更有效的误差建模方法。

Method: 提出两阶段框架：1）Mamba模块进行粗估计；2）条件扩散模型进行细粒度误差建模，使用GNSS测量质量特征作为条件控制生成过程，并加入逐卫星不确定性建模。

Result: 在公开和自收集数据集上的实验表明，Diff-GNSS在多个指标上均优于现有最先进方法。

Conclusion: 这是扩散模型在伪距误差估计中的首次应用，提出的扩散精炼模块具有即插即用特性，可显著提升现有网络的估计精度。

Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban
positioning. However, multipath and non-line-of-sight reception often introduce
large measurement errors that degrade accuracy. Learning-based methods for
predicting and compensating pseudorange errors have gained traction, but their
performance is limited by complex error distributions. To address this
challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement
(pseudorange) error estimation framework that leverages a conditional diffusion
model to capture such complex distributions. Firstly, a Mamba-based module
performs coarse estimation to provide an initial prediction with appropriate
scale and trend. Then, a conditional denoising diffusion layer refines the
estimate, enabling fine-grained modeling of pseudorange errors. To suppress
uncontrolled generative diversity and achieve controllable synthesis, three key
features related to GNSS measurement quality are used as conditions to
precisely guide the reverse denoising process. We further incorporate
per-satellite uncertainty modeling within the diffusion stage to assess the
reliability of the predicted errors. We have collected and publicly released a
real-world dataset covering various scenes. Experiments on public and
self-collected datasets show that DiffGNSS consistently outperforms
state-of-the-art baselines across multiple metrics. To the best of our
knowledge, this is the first application of diffusion models to pseudorange
error estimation. The proposed diffusion-based refinement module is
plug-and-play and can be readily integrated into existing networks to markedly
improve estimation accuracy.

</details>


### [132] [Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture](https://arxiv.org/abs/2509.17406)
*Jonathan Wuntu,Muhamad Dwisnanto Putro,Rendy Syahputra*

Main category: cs.CV

TL;DR: 本研究探索使用YOLOv10-nano深度学习模型在印度尼西亚水域进行实时海洋鱼类检测，结果显示该模型在保持低计算需求的同时实现了高检测精度，适合在数据有限环境中进行海洋鱼类监测和保护应用。


<details>
  <summary>Details</summary>
Motivation: 印度尼西亚的海洋生态系统是全球公认的珊瑚三角区的一部分，生物多样性极为丰富，需要高效的监测工具来支持保护工作。传统的鱼类检测方法耗时且需要专业知识，因此需要自动化解决方案。

Method: 本研究采用YOLOv10-nano深度学习模型，该模型具有CSPNet骨干网络、PAN特征融合和金字塔空间注意力块等改进架构，能够在复杂环境中实现高效准确的目标检测。模型在DeepFish和OpenImages V7-Fish数据集上进行评估。

Result: YOLOv10-nano实现了高检测精度，mAP50为0.966，mAP50:95为0.606，同时保持低计算需求（270万参数，8.4 GFLOPs）。在CPU上的平均推理速度为29.29 FPS，适合实时部署。OpenImages V7-Fish数据集虽然单独使用时准确率较低，但与DeepFish结合使用可增强模型鲁棒性。

Conclusion: 该研究证明了YOLOv10-nano在数据有限环境中进行高效、可扩展的海洋鱼类监测和保护应用的潜力。

Abstract: Indonesia's marine ecosystems, part of the globally recognized Coral
Triangle, are among the richest in biodiversity, requiring efficient monitoring
tools to support conservation. Traditional fish detection methods are
time-consuming and demand expert knowledge, prompting the need for automated
solutions. This study explores the implementation of YOLOv10-nano, a
state-of-the-art deep learning model, for real-time marine fish detection in
Indonesian waters, using test data from Bunaken National Marine Park. YOLOv10's
architecture, featuring improvements like the CSPNet backbone, PAN for feature
fusion, and Pyramid Spatial Attention Block, enables efficient and accurate
object detection even in complex environments. The model was evaluated on the
DeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano
achieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606
while maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It
also delivered an average inference speed of 29.29 FPS on the CPU, making it
suitable for real-time deployment. Although OpenImages V7-Fish alone provided
lower accuracy, it complemented DeepFish in enhancing model robustness.
Overall, this study demonstrates YOLOv10-nano's potential for efficient,
scalable marine fish monitoring and conservation applications in data-limited
environments.

</details>


### [133] [Training-Free Label Space Alignment for Universal Domain Adaptation](https://arxiv.org/abs/2509.17452)
*Dujin Lee,Sojung An,Jungmyung Wi,Kuniaki Saito,Donghyun Kim*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言基础模型的通用域自适应方法，通过标签空间对齐而非视觉空间对齐来解决域自适应问题，在DomainBed基准测试中取得了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 传统通用域自适应方法主要关注视觉空间对齐，但由于内容差异导致的视觉模糊性限制了其鲁棒性和泛化能力。本文旨在利用视觉语言模型的零样本能力，专注于标签空间对齐来提高自适应稳定性

Method: 首先使用生成式视觉语言模型识别目标域中的未知类别，然后提出无需训练的方法来过滤和精炼域间的噪声标签，构建一个整合共享知识和目标私有类别信息的通用分类器

Result: 在DomainBed基准测试中，H-score平均提升7.9%，H³-score平均提升6.1%。结合自训练后，H-score和H³-score进一步提升了1.6%

Conclusion: 该方法通过标签空间对齐而非视觉空间对齐，有效解决了通用域自适应中的挑战，显著提升了性能，证明了视觉语言模型在域自适应任务中的潜力

Abstract: Universal domain adaptation (UniDA) transfers knowledge from a labeled source
domain to an unlabeled target domain, where label spaces may differ and the
target domain may contain private classes. Previous UniDA methods primarily
focused on visual space alignment but often struggled with visual ambiguities
due to content differences, which limited their robustness and
generalizability. To overcome this, we introduce a novel approach that
leverages the strong \textit{zero-shot capabilities} of recent vision-language
foundation models (VLMs) like CLIP, concentrating solely on label space
alignment to enhance adaptation stability. CLIP can generate task-specific
classifiers based only on label names. However, adapting CLIP to UniDA is
challenging because the label space is not fully known in advance. In this
study, we first utilize generative vision-language models to identify unknown
categories in the target domain. Noise and semantic ambiguities in the
discovered labels -- such as those similar to source labels (e.g., synonyms,
hypernyms, hyponyms) -- complicate label alignment. To address this, we propose
a training-free label-space alignment method for UniDA (\ours). Our method
aligns label spaces instead of visual spaces by filtering and refining noisy
labels between the domains. We then construct a \textit{universal classifier}
that integrates both shared knowledge and target-private class information,
thereby improving generalizability under domain shifts. The results reveal that
the proposed method considerably outperforms existing UniDA techniques across
key DomainBed benchmarks, delivering an average improvement of
\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.
Furthermore, incorporating self-training further enhances performance and
achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and
H$^3$-scores.

</details>


### [134] [Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling](https://arxiv.org/abs/2509.17427)
*Hodaka Kawachi,Jose Reinaldo Cunha Santos A. V. Silva Neto,Yasushi Yagi,Hajime Nagahara,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出一种基于扩散先验的单次快照离焦深度重建方法，用于编码孔径成像，无需配对训练数据，通过优化框架结合可微分前向模型和扩散正则化实现高精度深度重建


<details>
  <summary>Details</summary>
Motivation: 传统DFD方法依赖手工先验，U-Net方法需要配对训练数据且与特定相机配置绑定。本文旨在开发一种无需配对数据、不依赖特定相机配置的鲁棒深度重建方法

Method: 使用扩散先验作为正则化，结合可微分前向模型进行优化，在去噪图像域中引导解，确保测量一致性

Result: 在综合仿真和原型相机实验中，该方法在不同噪声水平下均表现出色，优于U-Net基线和传统编码孔径DFD方法

Conclusion: 该方法实现了无需配对训练数据的稳定深度重建，具有更高的准确性和鲁棒性，适用于各种相机配置

Abstract: We propose a single-snapshot depth-from-defocus (DFD) reconstruction method
for coded-aperture imaging that replaces hand-crafted priors with a learned
diffusion prior used purely as regularization. Our optimization framework
enforces measurement consistency via a differentiable forward model while
guiding solutions with the diffusion prior in the denoised image domain,
yielding higher accuracy and stability than clas- sical optimization. Unlike
U-Net-style regressors, our approach requires no paired defocus-RGBD training
data and does not tie training to a specific camera configuration. Experiments
on comprehensive simulations and a prototype camera demonstrate consistently
strong RGBD reconstructions across noise levels, outperforming both U-Net
baselines and a classical coded- aperture DFD method.

</details>


### [135] [Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks](https://arxiv.org/abs/2509.17457)
*Paweł Jakub Borsukiewicz,Jordan Samhi,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 本文提出LEAM技术，通过分析人脸识别模型中的个体特异性激活模式，识别对识别贡献最大的面部区域，为个性化隐私保护系统提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性技术采用通用方法而非适应个体面部特征，限制了其有效性和隐蔽性。需要开发能够理解人脸识别系统工作原理的解释性技术。

Method: 引入层嵌入激活映射（LEAM）技术，结合人脸解析器分析1000个个体在9个预训练人脸识别模型中的数据，识别个体层面的关键面部区域。

Result: 发现人脸识别模型优先关注面部中心区域（鼻子区域占关键识别区域的18.9-29.7%），但注意力分布在多个面部片段上。基于LEAM识别的1%最相关像素的验证遮挡证实了区域选择的有效性。

Conclusion: LEAM技术为人脸识别系统提供了可解释性分析，验证了人特异性识别模式的存在，为未来个性化隐私保护系统奠定了基础。

Abstract: The proliferation of facial recognition systems presents major privacy risks,
driving the need for effective countermeasures. Current adversarial techniques
apply generalized methods rather than adapting to individual facial
characteristics, limiting their effectiveness and inconspicuousness. In this
work, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique
that identifies which facial areas contribute most to recognition at an
individual level. Unlike adversarial attack methods that aim to fool
recognition systems, LEAM is an explainability technique designed to understand
how these systems work, providing insights that could inform future privacy
protection research. We integrate LEAM with a face parser to analyze data from
1000 individuals across 9 pre-trained facial recognition models.
  Our analysis reveals that while different layers within facial recognition
models vary significantly in their focus areas, these models generally
prioritize similar facial regions across architectures when considering their
overall activation patterns, which show significantly higher similarity between
images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.
different individuals (0.04-0.13), validating the existence of person-specific
recognition patterns. Our results show that facial recognition models
prioritize the central region of face images (with nose areas accounting for
18.9-29.7% of critical recognition regions), while still distributing attention
across multiple facial fragments. Proper selection of relevant facial areas was
confirmed using validation occlusions, based on just 1% of the most relevant,
LEAM-identified, image pixels, which proved to be transferable across different
models. Our findings establish the foundation for future individually tailored
privacy protection systems centered around LEAM's choice of areas to be
perturbed.

</details>


### [136] [Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration](https://arxiv.org/abs/2509.17429)
*Zhitao Zeng,Guojian Yuan,Junyuan Mao,Yuxuan Wang,Xiaoshuang Jia,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了多尺度时间预测（MSTP）任务，通过将多尺度分解为时间尺度和状态尺度两个正交维度，来预测场景在多个时间尺度和状态尺度上的细粒度状态。作者还提出了IG-MC方法，包含增量生成模块和多智能体协作框架，以解决长时预测的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 准确的时序预测是全面场景理解与具身人工智能之间的桥梁。然而，视觉语言模型在预测多个时间尺度和状态尺度上的细粒度场景状态方面存在困难。

Method: 提出了IG-MC方法，包含两个关键创新：1）即插即用的增量生成模块，持续合成更新的视觉预览；2）决策驱动的多智能体协作框架，包含生成、启动和多状态评估智能体，动态触发和评估预测周期。

Result: 构建了首个MSTP基准测试，包含跨多个状态尺度和时间尺度的同步标注。提出的方法能够保持决策和生成视觉的同步，防止随着前瞻间隔延长而出现性能下降。

Conclusion: MSTP任务为场景理解提供了新的研究方向，IG-MC方法通过增量生成和多智能体协作有效解决了多尺度时序预测的挑战，为具身人工智能的发展提供了重要支持。

Abstract: Accurate temporal prediction is the bridge between comprehensive scene
understanding and embodied artificial intelligence. However, predicting
multiple fine-grained states of a scene at multiple temporal scales is
difficult for vision-language models. We formalize the Multi-Scale Temporal
Prediction (MSTP) task in general and surgical scenes by decomposing
multi-scale into two orthogonal dimensions: the temporal scale, forecasting
states of humans and surgery at varying look-ahead intervals, and the state
scale, modeling a hierarchy of states in general and surgical scenes. For
example, in general scenes, states of contact relationships are finer-grained
than states of spatial relationships. In surgical scenes, medium-level steps
are finer-grained than high-level phases yet remain constrained by their
encompassing phase. To support this unified task, we introduce the first MSTP
Benchmark, featuring synchronized annotations across multiple state scales and
temporal scales. We further propose a method, Incremental Generation and
Multi-agent Collaboration (IG-MC), which integrates two key innovations. First,
we present a plug-and-play incremental generation module that continuously
synthesizes up-to-date visual previews at expanding temporal scales to inform
multiple decision-making agents, keeping decisions and generated visuals
synchronized and preventing performance degradation as look-ahead intervals
lengthen. Second, we present a decision-driven multi-agent collaboration
framework for multi-state prediction, comprising generation, initiation, and
multi-state assessment agents that dynamically trigger and evaluate prediction
cycles to balance global coherence and local fidelity.

</details>


### [137] [ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](https://arxiv.org/abs/2509.17481)
*Xingqi Wang,Yiming Cui,Xin Yao,Shijin Wang,Guoping Hu,Xiaoyu Qin*

Main category: cs.CV

TL;DR: 提出了ChartHal基准，用于评估大视觉语言模型在图表理解中的幻觉问题，包含1062个人工验证样本和细粒度幻觉分类


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在图表理解中存在严重幻觉问题，但现有研究未深入探索这一交叉领域

Method: 构建包含细粒度幻觉场景分类的基准数据集，评估现有最先进LVLMs在图表理解中的表现

Result: GPT-5和o4-mini等专有模型在ChartHal上准确率仅为34.46%和22.79%，涉及图表中缺失或矛盾信息的问题特别容易引发幻觉

Conclusion: 图表理解中的幻觉问题严重，迫切需要更鲁棒的缓解策略

Abstract: Large Vision-Language Models (LVLMs) have recently demonstrated remarkable
progress, yet hallucination remains a critical barrier, particularly in chart
understanding, which requires sophisticated perceptual and cognitive abilities
as well as rigorous factual accuracy. While prior work has investigated
hallucinations and chart comprehension independently, their intersection
remains largely unexplored. To address this gap, we present ChartHal, a
benchmark that features a fine-grained taxonomy of hallucination scenarios in
chart understanding, along with a human-validated dataset of 1,062 samples. Our
evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations
on ChartHal, including proprietary models such as GPT-5 and o4-mini, which
achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals
that questions involving information absent from or contradictory to charts are
especially likely to trigger hallucinations, underscoring the urgent need for
more robust mitigation strategies. Code and data are available at
https://github.com/ymcui/ChartHal .

</details>


### [138] [EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](https://arxiv.org/abs/2509.17430)
*Gunjan Chhablani,Xiaomeng Ye,Muhammad Zubair Irshad,Zsolt Kira*

Main category: cs.CV

TL;DR: EmbodiedSplat提出了一种通过3D高斯泼溅技术重建真实部署环境，并在重建场景中微调策略的方法，显著提高了模拟到现实的迁移性能。


<details>
  <summary>Details</summary>
Motivation: 当前Embodied AI主要依赖仿真训练，但合成环境缺乏真实感，而高保真重建需要昂贵硬件，导致模拟到现实的迁移仍然是一个主要挑战。

Method: 使用iPhone捕获部署场景，通过3D高斯泼溅技术重建网格，在Habitat-Sim模拟器中训练策略，实现真实场景的近似模拟。

Result: 在真实世界图像导航任务中，EmbodiedSplat微调的智能体比在大规模真实数据集和合成数据集上预训练的基线方法分别提高了20%和40%的成功率，模拟与真实相关性达到0.87-0.97。

Conclusion: 该方法能够以最小努力将策略适应到多样化环境中，有效解决了模拟到现实的迁移问题。

Abstract: The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20% and 40% on real-world Image Navigation task. Moreover, our
approach yields a high sim-vs-real correlation (0.87-0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat.

</details>


### [139] [Hierarchical Neural Semantic Representation for 3D Semantic Correspondence](https://arxiv.org/abs/2509.17431)
*Keyu Du,Jingyu Hu,Haipeng Li,Hao Xu,Haibing Huang,Chi-Wing Fu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出了一种基于分层神经语义表示的3D语义对应估计方法，通过全局语义特征和多分辨率局部几何特征的结合，实现准确且鲁棒的3D语义对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义对应方法在处理结构多样的形状时存在局限性，需要一种能够同时捕捉高层次语义结构和保留精细几何细节的鲁棒方法。

Method: 设计了分层神经语义表示（HNSR），包含全局语义特征和多分辨率局部几何特征；采用渐进式全局到局部匹配策略；框架无需训练，兼容多种预训练3D生成模型。

Result: 在定性和定量评估中均优于现有最先进技术，支持形状共分割、关键点匹配和纹理迁移等多种应用，在跨类别场景下也表现出色。

Conclusion: 该方法提供了一种有效且通用的3D语义对应解决方案，具有强大的泛化能力和广泛的应用前景。

Abstract: This paper presents a new approach to estimate accurate and robust 3D
semantic correspondence with the hierarchical neural semantic representation.
Our work has three key contributions. First, we design the hierarchical neural
semantic representation (HNSR), which consists of a global semantic feature to
capture high-level structure and multi-resolution local geometric features to
preserve fine details, by carefully harnessing 3D priors from pre-trained 3D
generative models. Second, we design a progressive global-to-local matching
strategy, which establishes coarse semantic correspondence using the global
semantic feature, then iteratively refines it with local geometric features,
yielding accurate and semantically-consistent mappings. Third, our framework is
training-free and broadly compatible with various pre-trained 3D generative
backbones, demonstrating strong generalization across diverse shape categories.
Our method also supports various applications, such as shape co-segmentation,
keypoint matching, and texture transfer, and generalizes well to structurally
diverse shapes, with promising results even in cross-category scenarios. Both
qualitative and quantitative evaluations show that our method outperforms
previous state-of-the-art techniques.

</details>


### [140] [Multimodal Medical Image Classification via Synergistic Learning Pre-training](https://arxiv.org/abs/2509.17492)
*Qinghua Lin,Guang-Hai Liu,Zuoyong Li,Yang Li,Yuting Jiang,Xiang Wu*

Main category: cs.CV

TL;DR: 提出一种用于多模态半监督医学图像分类的"预训练+微调"框架，通过协同学习预训练和分布偏移方法解决标签稀缺下的模态融合问题


<details>
  <summary>Details</summary>
Motivation: 临床诊断中多模态病理图像常见，但基于计算机视觉的多模态图像辅助诊断面临模态融合挑战，特别是在缺乏专家标注数据的情况下

Method: 提出协同学习预训练框架（一致性、重构和对齐学习），将一种模态视为另一种模态的增强样本进行自监督预训练；设计多模态融合微调方法，使用不同编码器提取原始模态特征，并提供多模态融合编码器；提出多模态融合特征的分布偏移方法

Result: 在公开胃镜图像数据集Kvasir和Kvasirv2上的实验表明，该方法优于当前最先进的分类方法

Conclusion: 该方法能有效解决标签稀缺下的多模态图像融合问题，提升医学图像分类性能

Abstract: Multimodal pathological images are usually in clinical diagnosis, but
computer vision-based multimodal image-assisted diagnosis faces challenges with
modality fusion, especially in the absence of expert-annotated data. To achieve
the modality fusion in multimodal images with label scarcity, we propose a
novel ``pretraining + fine-tuning" framework for multimodal semi-supervised
medical image classification. Specifically, we propose a synergistic learning
pretraining framework of consistency, reconstructive, and aligned learning. By
treating one modality as an augmented sample of another modality, we implement
a self-supervised learning pre-train, enhancing the baseline model's feature
representation capability. Then, we design a fine-tuning method for multimodal
fusion. During the fine-tuning stage, we set different encoders to extract
features from the original modalities and provide a multimodal fusion encoder
for fusion modality. In addition, we propose a distribution shift method for
multimodal fusion features, which alleviates the prediction uncertainty and
overfitting risks caused by the lack of labeled samples. We conduct extensive
experiments on the publicly available gastroscopy image datasets Kvasir and
Kvasirv2. Quantitative and qualitative results demonstrate that the proposed
method outperforms the current state-of-the-art classification methods. The
code will be released at: https://github.com/LQH89757/MICS.

</details>


### [141] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: CARINOX是一个统一框架，结合噪声优化和探索，通过基于人类判断相关性的奖励选择程序，提高文本到图像扩散模型的组合对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时方法在实现复杂对象关系、属性和空间排列的组合对齐方面存在局限性：优化方法可能因初始化不良或搜索轨迹不佳而停滞，而探索方法可能需要大量样本。单一奖励指标或临时组合无法可靠捕捉组合性的所有方面。

Method: 提出CARINOX框架，将噪声优化和探索相结合，采用基于人类判断相关性的原则性奖励选择程序，克服单一方法的局限性。

Result: 在两个互补基准测试中，CARINOX将平均对齐分数提高了T2I-CompBench++上的+16%和HRS基准上的+11%，在所有主要类别中一致优于最先进的优化和探索方法，同时保持图像质量和多样性。

Conclusion: CARINOX通过统一优化和探索策略，并结合原则性奖励选择，有效解决了文本到图像扩散模型的组合对齐问题，显著提升了性能。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [142] [An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection](https://arxiv.org/abs/2509.17561)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Shufan Chen*

Main category: cs.CV

TL;DR: 本文对YOLO系列模型在水下物体检测中的鲁棒性进行了首次全面评估，发现YOLOv12性能最佳但对噪声敏感，并提出了轻量级训练策略来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 水下物体检测面临严重的水下失真问题，即使是最先进的检测器也会受到影响。目前缺乏对YOLO模型在这种挑战性环境下鲁棒性的系统研究。

Method: 使用统一数据集（10,000张标注图像）在6个模拟水下环境中评估YOLOv8-YOLOv12变体，分析失真对纹理、边缘和颜色等低层特征的影响，并评估噪声感知样本注入和高级增强微调等轻量级策略。

Result: YOLOv12整体性能最强但对噪声高度敏感；噪声破坏边缘和纹理特征；图像数量和实例频率是检测性能的主要驱动因素；提出的训练策略分别提升了噪声环境和增强域中的鲁棒性。

Conclusion: 研究结果为构建弹性和成本效益高的水下物体检测系统提供了实用指导，展示了轻量级训练策略在领域适应方面的潜力。

Abstract: Underwater object detection (UOD) remains a critical challenge in computer
vision due to underwater distortions which degrade low-level features and
compromise the reliability of even state-of-the-art detectors. While YOLO
models have become the backbone of real-time object detection, little work has
systematically examined their robustness under these uniquely challenging
conditions. This raises a critical question: Are YOLO models genuinely robust
when operating under the chaotic and unpredictable conditions of underwater
environments? In this study, we present one of the first comprehensive
evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated
underwater environments. Using a unified dataset of 10,000 annotated images
from DUO and Roboflow100, we not only benchmark model robustness but also
analyze how distortions affect key low-level features such as texture, edges,
and color. Our findings show that (1) YOLOv12 delivers the strongest overall
performance but is highly vulnerable to noise, and (2) noise disrupts edge and
texture features, explaining the poor detection performance in noisy images.
Class imbalance is a persistent challenge in UOD. Experiments revealed that (3)
image counts and instance frequency primarily drive detection performance,
while object appearance exerts only a secondary influence. Finally, we
evaluated lightweight training-aware strategies: noise-aware sample injection,
which improves robustness in both noisy and real-world conditions, and
fine-tuning with advanced enhancement, which boosts accuracy in enhanced
domains but slightly lowers performance in original data, demonstrating strong
potential for domain adaptation, respectively. Together, these insights provide
practical guidance for building resilient and cost-efficient UOD systems.

</details>


### [143] [CSDformer: A Conversion Method for Fully Spike-Driven Transformer](https://arxiv.org/abs/2509.17461)
*Yuhao Zhang,Chengjun Zhang,Di Wu,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: CSDformer是一种新型的完全脉冲驱动转换器转换方法，通过定制转换导向的架构、NReLU函数替代softmax、时间分解技术和延迟积分-发放神经元，在超低延迟下实现高性能，同时大幅降低训练成本和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的脉冲转换器方法存在训练成本过高或硬件不友好的操作问题，需要开发一种既能保持高性能又能显著降低训练开销的转换方法。

Method: 提出CSDformer方法：定制转换导向的transformer架构，用NReLU函数替代self-attention中的softmax，量化训练后通过时间分解技术转换为完全脉冲驱动模型，并引入延迟积分-发放神经元减少转换误差。

Result: 在ImageNet上达到76.36% top-1准确率（7个时间步），相比现有方法减少75%计算资源和2-3倍训练加速，是首个通过转换方法实现的高性能完全脉冲驱动transformer模型。

Conclusion: CSDformer成功解决了脉冲转换器的训练成本和硬件友好性问题，在超低延迟下实现高性能，为脉冲神经网络的实际应用提供了有效解决方案。

Abstract: Spike-based transformer is a novel architecture aiming to enhance the
performance of spiking neural networks while mitigating the energy overhead
inherent to transformers. However, methods for generating these models suffer
from critical limitations: excessive training costs introduced by direct
training methods, or unavoidably hardware-unfriendly operations in existing
conversion methods. In this paper, we propose CSDformer, a novel conversion
method for fully spike-driven transformers. We tailor a conversion-oriented
transformer-based architecture and propose a new function NReLU to replace
softmax in self-attention. Subsequently, this model is quantized and trained,
and converted into a fully spike-driven model with temporal decomposition
technique. Also, we propose delayed Integrate-andFire neurons to reduce
conversion errors and improve the performance of spiking models. We evaluate
CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1
accuracy under 7 time-steps on ImageNet, demonstrating superiority over
state-of-the-art models. Furthermore, CSDformer eliminates the need for
training SNNs, thereby reducing training costs (reducing computational resource
by 75% and accelerating training speed by 2-3$\times$). To the best of our
knowledge, this is the first fully spike-driven transformer-based model
developed via conversion method, achieving high performance under ultra-low
latency, while dramatically reducing both computational complexity and training
overhead.

</details>


### [144] [MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data](https://arxiv.org/abs/2509.17566)
*Ding Shaodong,Liu Ziyang,Zhou Yijun,Liu Tao*

Main category: cs.CV

TL;DR: 本文提出了一种利用2D视觉基础模型（VFMs）进行帕金森病自动诊断的方法，通过处理多个关键ROI并结合多ROI监督对比学习，在MICCAI 2025挑战赛中获得第一名，准确率达86.0%。


<details>
  <summary>Details</summary>
Motivation: 帕金森病自动诊断具有重要临床需求，但缺乏高质量大数据集导致模型容易过拟合。现有3D医学模型预训练面临体素间距和模态不匹配的问题。

Method: 从NM和QSM图像中裁剪多个关键ROI，通过独立分支处理每个ROI并压缩为token，然后组合成统一患者表示进行分类。使用2D VFMs编码3D ROI体积的轴向切片，并通过辅助分割头引导特征提取。引入多ROI监督对比学习提升诊断性能。

Result: 在仅300个标注QSM和NM-MRI扫描的数据集上训练，获得86.0%的准确率，在MICCAI 2025 PDCADxFoundation挑战赛中排名第一，比第二名方法高出5.5%。

Conclusion: 该方法展示了2D VFMs在3D MR图像临床分析中的潜力，为小数据集下的帕金森病诊断提供了有效解决方案。

Abstract: The automatic diagnosis of Parkinson's disease is in high clinical demand due
to its prevalence and the importance of targeted treatment. Current clinical
practice often relies on diagnostic biomarkers in QSM and NM-MRI images.
However, the lack of large, high-quality datasets makes training diagnostic
models from scratch prone to overfitting. Adapting pre-trained 3D medical
models is also challenging, as the diversity of medical imaging leads to
mismatches in voxel spacing and modality between pre-training and fine-tuning
data. In this paper, we address these challenges by leveraging 2D vision
foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and
QSM images, process each ROI through separate branches to compress the ROI into
a token, and then combine these tokens into a unified patient representation
for classification. Within each branch, we use 2D VFMs to encode axial slices
of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary
segmentation head that steers the feature extraction toward specific brain
nuclei. Additionally, we introduce multi-ROI supervised contrastive learning,
which improves diagnostic performance by pulling together representations of
patients from the same class while pushing away those from different classes.
Our approach achieved first place in the MICCAI 2025 PDCADxFoundation
challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled
QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These
results highlight the potential of 2D VFMs for clinical analysis of 3D MR
images.

</details>


### [145] [MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception](https://arxiv.org/abs/2509.17462)
*Changwon Kang,Jisong Kim,Hongjae Shin,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: MAESTRO是一个结构化框架，通过生成任务特定特征和减轻特征干扰来解决多任务3D感知中的任务冲突问题，在3D目标检测、BEV地图分割和3D占用预测任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多任务学习虽然能提高学习效率，但由于不同任务目标优化时的任务冲突可能导致性能下降。需要解决多任务3D感知中的特征干扰问题。

Method: MAESTRO包含三个组件：类原型生成器（CPG）将类别分组并生成组原型；任务特定特征生成器（TSFG）利用原型保留任务相关特征；场景原型聚合器（SPA）增强3D占用预测的原型。

Result: 在nuScenes和Occ3D基准测试上的广泛实验表明，MAESTRO在3D目标检测、BEV地图分割和3D占用预测任务上一致优于现有方法。

Conclusion: MAESTRO框架有效解决了多任务3D感知中的任务冲突问题，通过结构化特征生成和干扰抑制机制显著提升了各项任务的性能。

Abstract: The goal of multi-task learning is to learn to conduct multiple tasks
simultaneously based on a shared data representation. While this approach can
improve learning efficiency, it may also cause performance degradation due to
task conflicts that arise when optimizing the model for different objectives.
To address this challenge, we introduce MAESTRO, a structured framework
designed to generate task-specific features and mitigate feature interference
in multi-task 3D perception, including 3D object detection, bird's-eye view
(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three
components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature
Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class
categories into foreground and background groups and generates group-wise
prototypes. The foreground and background prototypes are assigned to the 3D
object detection task and the map segmentation task, respectively, while both
are assigned to the 3D occupancy prediction task. TSFG leverages these
prototype groups to retain task-relevant features while suppressing irrelevant
features, thereby enhancing the performance for each task. SPA enhances the
prototype groups assigned for 3D occupancy prediction by utilizing the
information produced by the 3D object detection head and the map segmentation
head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate
that MAESTRO consistently outperforms existing methods across 3D object
detection, BEV map segmentation, and 3D occupancy prediction tasks.

</details>


### [146] [Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models](https://arxiv.org/abs/2509.17588)
*Jinyeong Kim,Seil Kang,Jiwoo Park,Junhyeok Kim,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为head attribution的技术，用于分析大型视觉语言模型中注意力头在图像到文本信息传递中的作用，发现信息流遵循结构化过程且受语义内容主导。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型通过注意力头实现图像到文本的信息传递，但由于众多注意力头同时运作，其底层机制难以解释，需要开发新的分析方法。

Method: 提出head attribution技术，基于组件归因方法识别在信息传递中起关键作用的注意力头模式，分析LVLMs如何依赖特定注意力头识别和回答图像中主要对象的问题。

Result: 发现特定注意力头子集促进图像到文本信息流，这些头的选择由输入图像的语义内容而非视觉外观决定；文本信息先传播到角色相关标记和最终标记，图像信息嵌入对象相关和背景标记。

Conclusion: 图像到文本信息流遵循结构化过程，在注意力头层面的分析为理解LVLMs机制提供了有前景的方向。

Abstract: Large Vision-Language Models (LVLMs) answer visual questions by transferring
information from images to text through a series of attention heads. While this
image-to-text information flow is central to visual question answering, its
underlying mechanism remains difficult to interpret due to the simultaneous
operation of numerous attention heads. To address this challenge, we propose
head attribution, a technique inspired by component attribution methods, to
identify consistent patterns among attention heads that play a key role in
information transfer. Using head attribution, we investigate how LVLMs rely on
specific attention heads to identify and answer questions about the main object
in an image. Our analysis reveals that a distinct subset of attention heads
facilitates the image-to-text information flow. Remarkably, we find that the
selection of these heads is governed by the semantic content of the input image
rather than its visual appearance. We further examine the flow of information
at the token level and discover that (1) text information first propagates to
role-related tokens and the final token before receiving image information, and
(2) image information is embedded in both object-related and background tokens.
Our work provides evidence that image-to-text information flow follows a
structured process, and that analysis at the attention-head level offers a
promising direction toward understanding the mechanisms of LVLMs.

</details>


### [147] [Stable Video-Driven Portraits](https://arxiv.org/abs/2509.17476)
*Mallikarjun B. R.,Fei Yin,Vikram Voleti,Nikita Drobyshev,Maksim Lapin,Aaryaman Vasishta,Varun Jampani*

Main category: cs.CV

TL;DR: 提出基于扩散模型的肖像动画新框架，通过使用驱动视频中掩码面部区域作为强运动控制信号，结合跨身份监督和时空注意力机制，实现高质量、可控的肖像动画。


<details>
  <summary>Details</summary>
Motivation: 解决现有肖像动画方法在表达能力、时间一致性和对未见身份/大姿态变化的泛化能力方面的局限性，特别是扩散模型在弱控制信号和架构限制方面的问题。

Method: 使用驱动视频中眼睛、鼻子和嘴巴的掩码区域作为运动控制线索；采用跨身份监督防止外观泄露；引入最小新参数的架构设计；使用时空注意力机制捕获细微运动；利用历史帧确保连续性；提出信号融合策略平衡运动保真度和身份保持。

Result: 实现了优越的时间一致性和准确的表情控制，能够生成高质量、可控的肖像动画，适用于实际应用场景。

Conclusion: 该框架通过强运动控制信号、高效架构设计和时空一致性机制，显著提升了肖像动画的质量和可控性，为实际应用提供了可行的解决方案。

Abstract: Portrait animation aims to generate photo-realistic videos from a single
source image by reenacting the expression and pose from a driving video. While
early methods relied on 3D morphable models or feature warping techniques, they
often suffered from limited expressivity, temporal inconsistency, and poor
generalization to unseen identities or large pose variations. Recent advances
using diffusion models have demonstrated improved quality but remain
constrained by weak control signals and architectural limitations. In this
work, we propose a novel diffusion based framework that leverages masked facial
regions specifically the eyes, nose, and mouth from the driving video as strong
motion control cues. To enable robust training without appearance leakage, we
adopt cross identity supervision. To leverage the strong prior from the
pretrained diffusion model, our novel architecture introduces minimal new
parameters that converge faster and help in better generalization. We introduce
spatial temporal attention mechanisms that allow inter frame and intra frame
interactions, effectively capturing subtle motions and reducing temporal
artifacts. Our model uses history frames to ensure continuity across segments.
At inference, we propose a novel signal fusion strategy that balances motion
fidelity with identity preservation. Our approach achieves superior temporal
consistency and accurate expression control, enabling high-quality,
controllable portrait animation suitable for real-world applications.

</details>


### [148] [A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition](https://arxiv.org/abs/2509.17638)
*Zilin Gao,Qilong Wang,Bingbing Zhang,Qinghua Hu,Peihua Li*

Main category: cs.CV

TL;DR: 本文提出A²M²-Net网络，通过自适应对齐多尺度二阶矩来解决少样本动作识别中的时序错位问题，在五个基准测试上取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本动作识别方法通常忽视个体运动模式在比较中的作用，且对视频动态特征统计利用不足，特别是使用2D骨干网络时难以处理时序错位问题。

Method: 提出自适应对齐多尺度二阶矩网络(A²M²-Net)，包含两个核心组件：自适应对齐模块(A²)用于匹配，多尺度二阶矩模块(M²)用于生成强表示。M²模块在多时空尺度上开发语义二阶描述符，A²模块自适应选择信息丰富的候选描述符。

Result: 在五个广泛使用的少样本动作识别基准测试上进行实验，结果显示A²M²-Net相比最先进方法取得了非常有竞争力的性能。

Conclusion: 该方法能够通过建立自适应对齐协议来处理具有挑战性的时序错位问题，并且在各种少样本设置和不同指标上都表现出良好的泛化能力。

Abstract: Thanks to capability to alleviate the cost of large-scale annotation,
few-shot action recognition (FSAR) has attracted increased attention of
researchers in recent years. Existing FSAR approaches typically neglect the
role of individual motion pattern in comparison, and under-explore the feature
statistics for video dynamics. Thereby, they struggle to handle the challenging
temporal misalignment in video dynamics, particularly by using 2D backbones. To
overcome these limitations, this work proposes an adaptively aligned
multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the
latent video dynamics with a collection of powerful representation candidates
and adaptively align them in an instance-guided manner. To this end, our
A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$
module) for matching, and multi-scale second-order moment (M$^2$ block) for
strong representation. Specifically, M$^2$ block develops a collection of
semantic second-order descriptors at multiple spatio-temporal scales.
Furthermore, A$^2$ module aims to adaptively select informative candidate
descriptors while considering the individual motion pattern. By such means, our
A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem
by establishing an adaptive alignment protocol for strong representation.
Notably, our proposed method generalizes well to various few-shot settings and
diverse metrics. The experiments are conducted on five widely used FSAR
benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive
performance compared to state-of-the-arts, demonstrating its effectiveness and
generalization.

</details>


### [149] [Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models](https://arxiv.org/abs/2509.17498)
*Dilshara Herath,Chinthaka Abeyrathne,Prabhani Jayaweera*

Main category: cs.CV

TL;DR: 本文对基于YOLO算法的实时非侵入式驾驶员疲劳检测方法进行全面评估，发现YOLOv9c在准确性方面表现最佳，而YOLOv11n在精度和推理效率之间达到最优平衡，适合嵌入式部署。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的关键因素，每年造成数千人死亡和受伤，需要开发有效的实时检测方法。

Method: 使用UTA-RLDD公开数据集，对7种YOLO变体（v5s、v9c、v9t、v10n、v10l、v11n、v11l）进行微调，并采用基于Dlib面部关键点的眼宽高比（EAR）方法进行比较。

Result: YOLOv9c达到最高准确率（mAP 0.5为0.986，召回率为0.978），YOLOv11n在精度（0.954）和推理效率之间达到最佳平衡。EAR方法计算量小但鲁棒性较差。

Conclusion: 研究揭示了准确性、延迟和资源需求之间的权衡关系，为自动驾驶和工业安全应用中的检测方法选择提供了实用指南。

Abstract: Driver drowsiness remains a critical factor in road accidents, accounting for
thousands of fatalities and injuries each year. This paper presents a
comprehensive evaluation of real-time, non-intrusive drowsiness detection
methods, focusing on computer vision based YOLO (You Look Only Once)
algorithms. A publicly available dataset namely, UTA-RLDD was used, containing
both awake and drowsy conditions, ensuring variability in gender, eyewear,
illumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,
v11n, v11l) are fine-tuned, with performance measured in terms of Precision,
Recall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest
accuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal
balance between precision (0.954) and inference efficiency, making it highly
suitable for embedded deployment. Additionally, we implement an Eye Aspect
Ratio (EAR) approach using Dlib's facial landmarks, which despite its low
computational footprint exhibits reduced robustness under pose variation and
occlusions. Our findings illustrate clear trade offs between accuracy, latency,
and resource requirements, and offer practical guidelines for selecting or
combining detection methods in autonomous driving and industrial safety
applications.

</details>


### [150] [VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video](https://arxiv.org/abs/2509.17647)
*Yu Liu,Baoxiong Jia,Ruijie Lu,Chuyue Gan,Huayu Chen,Junfeng Ni,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: VideoArtGS是一种从单目视频重建铰接物体数字孪生的新方法，通过运动先验引导和混合中心网格部件分配模块，显著提高了铰接参数和网格重建的精度。


<details>
  <summary>Details</summary>
Motivation: 从单目视频构建铰接物体的数字孪生是一个重要挑战，需要同时重建几何、部件分割和铰接参数。单目视频输入简单且可扩展，但由于相机和部件运动的耦合，仅靠视觉监督难以解耦几何和运动信息。

Method: 提出运动先验引导管道分析3D轨迹、过滤噪声，并提供可靠的铰接参数初始化；设计混合中心网格部件分配模块来捕捉准确的部件运动。

Result: VideoArtGS在铰接和网格重建方面达到最先进性能，相比现有方法将重建误差降低了约两个数量级。

Conclusion: VideoArtGS实现了从单目视频创建实用的数字孪生，为基于视频的铰接物体重建建立了新基准。

Abstract: Building digital twins of articulated objects from monocular video presents
an essential challenge in computer vision, which requires simultaneous
reconstruction of object geometry, part segmentation, and articulation
parameters from limited viewpoint inputs. Monocular video offers an attractive
input format due to its simplicity and scalability; however, it's challenging
to disentangle the object geometry and part dynamics with visual supervision
alone, as the joint movement of the camera and parts leads to ill-posed
estimation. While motion priors from pre-trained tracking models can alleviate
the issue, how to effectively integrate them for articulation learning remains
largely unexplored. To address this problem, we introduce VideoArtGS, a novel
approach that reconstructs high-fidelity digital twins of articulated objects
from monocular video. We propose a motion prior guidance pipeline that analyzes
3D tracks, filters noise, and provides reliable initialization of articulation
parameters. We also design a hybrid center-grid part assignment module for
articulation-based deformation fields that captures accurate part motion.
VideoArtGS demonstrates state-of-the-art performance in articulation and mesh
reconstruction, reducing the reconstruction error by about two orders of
magnitude compared to existing methods. VideoArtGS enables practical digital
twin creation from monocular video, establishing a new benchmark for
video-based articulated object reconstruction. Our work is made publicly
available at: https://videoartgs.github.io.

</details>


### [151] [SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge](https://arxiv.org/abs/2509.17500)
*Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan*

Main category: cs.CV

TL;DR: SAMSON是ICCV 2025 MOSE赛道第三名解决方案，通过结合长期记忆模块和SAM2Long后处理策略，在长视频序列中实现了准确的目标跟踪和分割。


<details>
  <summary>Details</summary>
Motivation: 解决长视频目标分割中目标重现、小目标、严重遮挡和拥挤场景等挑战，现有方法主要基于SAM2框架但存在局限性。

Method: 提出Segment Anything with Memory Strengthened Object Navigation (SAMSON)，集成最先进VOS模型的优势，包含长期记忆模块用于目标重识别，采用SAM2Long作为后处理策略减少误差累积。

Result: 在测试集排行榜上获得了0.8427的J&F分数。

Conclusion: SAMSON通过有效的记忆增强机制和后处理策略，在复杂的长视频目标分割任务中表现出色。

Abstract: Large-scale Video Object Segmentation (LSVOS) addresses the challenge of
accurately tracking and segmenting objects in long video sequences, where
difficulties stem from object reappearance, small-scale targets, heavy
occlusions, and crowded scenes. Existing approaches predominantly adopt
SAM2-based frameworks with various memory mechanisms for complex video mask
generation. In this report, we proposed Segment Anything with Memory
Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE
track of ICCV 2025, which integrates the strengths of stateof-the-art VOS
models into an effective paradigm. To handle visually similar instances and
long-term object disappearance in MOSE, we incorporate a long-term memorymodule
for reliable object re-identification. Additionly, we adopt SAM2Long as a
post-processing strategy to reduce error accumulation and enhance segmentation
stability in long video sequences. Our method achieved a final performance of
0.8427 in terms of J &F in the test-set leaderboard.

</details>


### [152] [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664)
*Pingyi Chen,Yujing Lou,Shen Cao,Jinhui Guo,Lubin Fan,Yue Wu,Lin Yang,Lizhuang Ma,Jieping Ye*

Main category: cs.CV

TL;DR: SD-VLM是一个增强视觉语言模型3D空间理解能力的新框架，通过大规模空间测量数据集和深度位置编码方法，显著提升了VLMs的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在2D语义理解方面表现出色，但在3D空间关系定量推理方面能力不足，主要原因是2D图像缺乏空间表示能力。

Method: 提出MSMU数据集（包含70万QA对、250万物理数值标注和1万思维链增强样本），并引入简单的深度位置编码方法来增强VLMs的空间感知。

Result: SD-VLM在MSMU-Bench上表现优异，比GPT-4o和Intern-VL3-78B分别提升26.91%和25.56%，在其他空间理解基准测试中也显示出良好的泛化能力。

Conclusion: SD-VLM是一个强大的通用视觉语言模型，在定量空间测量和理解方面表现出色，代码和模型已开源。

Abstract: While vision language models (VLMs) excel in 2D semantic visual
understanding, their ability to quantitatively reason about 3D spatial
relationships remains under-explored, due to the deficiency of 2D images'
spatial representation ability. In this paper, we analyze the problem hindering
VLMs' spatial understanding abilities and propose SD-VLM, a novel framework
that significantly enhances fundamental spatial perception abilities of VLMs
through two key contributions: (1) propose Massive Spatial Measuring and
Understanding (MSMU) dataset with precise spatial annotations, and (2)
introduce a simple depth positional encoding method strengthening VLMs' spatial
awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA
pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented
samples. We have trained SD-VLM, a strong generalist VLM which shows superior
quantitative spatial measuring and understanding capability. SD-VLM not only
achieves state-of-the-art performance on our proposed MSMU-Bench, but also
shows spatial generalization abilities on other spatial understanding
benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments
demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and
25.56% respectively on MSMU-Bench. Code and models are released at
https://github.com/cpystan/SD-VLM.

</details>


### [153] [4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression](https://arxiv.org/abs/2509.17506)
*Houqiang Zhong,Zihan Zheng,Qiang Hu,Yuan Tian,Ning Cao,Lan Xu,Xiaoyun Zhang,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4D-MoDe是一种运动解耦的4D高斯压缩框架，用于可扩展和可编辑的体积视频流，通过分层表示分离静态背景和动态前景，显著降低存储成本并支持背景替换等应用。


<details>
  <summary>Details</summary>
Motivation: 体积视频在沉浸式远程呈现和增强/虚拟现实中具有重要作用，但现有表示方法存在数据量大、运动复杂和可编辑性有限等挑战，难以实现高质量动态体积内容的大规模传输。

Method: 采用分层表示方法，使用前瞻式运动分解策略分离静态背景和动态前景；利用多分辨率运动估计网格和轻量级共享MLP捕捉连续运动轨迹；通过动态高斯补偿机制建模新出现的内容；采用自适应分组方案插入背景关键帧；使用熵感知训练管道联合优化运动场和高斯参数。

Result: 在多个数据集上的实验表明，4D-MoDe在保持竞争性重建质量的同时，存储成本比最先进方法低一个数量级（低至11.4 KB/帧），同时支持背景替换和仅前景流等实际应用。

Conclusion: 4D-MoDe框架为体积视频流提供了一种高效、可扩展且可编辑的解决方案，在显著降低存储成本的同时保持了高质量的重建效果，具有重要的实际应用价值。

Abstract: Volumetric video has emerged as a key medium for immersive telepresence and
augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation
and realistic spatial interactions. However, delivering high-quality dynamic
volumetric content at scale remains challenging due to massive data volume,
complex motion, and limited editability of existing representations. In this
paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework
designed for scalable and editable volumetric video streaming. Our method
introduces a layered representation that explicitly separates static
backgrounds from dynamic foregrounds using a lookahead-based motion
decomposition strategy, significantly reducing temporal redundancy and enabling
selective background/foreground streaming. To capture continuous motion
trajectories, we employ a multi-resolution motion estimation grid and a
lightweight shared MLP, complemented by a dynamic Gaussian compensation
mechanism to model emergent content. An adaptive grouping scheme dynamically
inserts background keyframes to balance temporal consistency and compression
efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes
the motion fields and Gaussian parameters under a rate-distortion (RD)
objective, while employing range-based and KD-tree compression to minimize
storage overhead. Extensive experiments on multiple datasets demonstrate that
4D-MoDe consistently achieves competitive reconstruction quality with an order
of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame)
compared to state-of-the-art methods, while supporting practical applications
such as background replacement and foreground-only streaming.

</details>


### [154] [4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming](https://arxiv.org/abs/2509.17513)
*Zihan Zheng,Zhenlong Wu,Houqiang Zhong,Yuan Tian,Ning Cao,Lan Xu,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4DGCPro是一个新颖的分层4D高斯压缩框架，通过渐进式体积视频流实现实时移动解码和高质量渲染，解决了现有体积视频压缩方法在灵活性和移动设备性能方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有体积视频压缩方法要么缺乏在单一模型中调整质量和比特率的灵活性，要么在轻量级移动平台上难以实现实时解码和渲染，无法满足多样化网络和设备的高效流媒体需求。

Method: 提出了感知加权和压缩友好的分层4D高斯表示，采用运动感知自适应分组减少时间冗余；开发了端到端熵优化训练方案，包含分层率失真监督和属性特定熵建模。

Result: 实验表明4DGCPro在单一模型中实现了灵活的质量和多比特率控制，在移动设备上达到实时解码和渲染，并在多个数据集上的率失真性能优于现有方法。

Conclusion: 4DGCPro框架成功解决了体积视频压缩的关键挑战，为高质量体积视频在移动设备上的实时流媒体应用提供了可行解决方案。

Abstract: Achieving seamless viewing of high-fidelity volumetric video, comparable to
2D video experiences, remains an open challenge. Existing volumetric video
compression methods either lack the flexibility to adjust quality and bitrate
within a single model for efficient streaming across diverse networks and
devices, or struggle with real-time decoding and rendering on lightweight
mobile platforms. To address these challenges, we introduce 4DGCPro, a novel
hierarchical 4D Gaussian compression framework that facilitates real-time
mobile decoding and high-quality rendering via progressive volumetric video
streaming in a single bitstream. Specifically, we propose a
perceptually-weighted and compression-friendly hierarchical 4D Gaussian
representation with motion-aware adaptive grouping to reduce temporal
redundancy, preserve coherence, and enable scalable multi-level detail
streaming. Furthermore, we present an end-to-end entropy-optimized training
scheme, which incorporates layer-wise rate-distortion (RD) supervision and
attribute-specific entropy modeling for efficient bitstream generation.
Extensive experiments show that 4DGCPro enables flexible quality and multiple
bitrate within a single model, achieving real-time decoding and rendering on
mobile devices while outperforming existing methods in RD performance across
multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro

</details>


### [155] [Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation](https://arxiv.org/abs/2509.17686)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.CV

TL;DR: 本文提出了一种从单张RGB图像生成深度图像并修复深度图像中缺失信息的算法，通过多层训练方法在Cityscapes数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中深度成像至关重要，但深度图像常因像素数据间隙或不一致而出现信息缺失问题，这影响了物体检测和测量的准确性。

Method: 开发了一种基于多层训练方法的算法，能够从单张RGB图像生成深度图像，并修复深度图像中的缺失信息。

Result: 在Cityscapes数据集上测试表明，该算法成功解决了深度图像中的信息缺失问题，在真实城市环境中表现出良好效果。

Conclusion: 所提出的算法能够有效生成完整的深度图像并修复缺失信息，为自动驾驶系统的深度感知提供了可靠的技术支持。

Abstract: Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it
plays a key role in detecting and measuring objects in the vehicle's
surroundings. However, a significant challenge in this domain arises from
missing information in Depth images, where certain points are not measurable
due to gaps or inconsistencies in pixel data. Our research addresses two key
tasks to overcome this challenge. First, we developed an algorithm using a
multi-layered training approach to generate Depth images from a single RGB
image. Second, we addressed the issue of missing information in Depth images by
applying our algorithm to rectify these gaps, resulting in Depth images with
complete and accurate data. We further tested our algorithm on the Cityscapes
dataset and successfully resolved the missing information in its Depth images,
demonstrating the effectiveness of our approach in real-world urban
environments.

</details>


### [156] [Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation](https://arxiv.org/abs/2509.17520)
*Mingda Zhang,Yuyang Zheng,Ruixiang Tang,Jingru Qiu,Haiyan Ding*

Main category: cs.CV

TL;DR: 本文提出统一多模态相干场（UMCF）方法，通过同步融合视觉、语义和空间信息来解决脑肿瘤分割中的边界模糊和层次保持问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割需要准确识别层次区域（全肿瘤、肿瘤核心、增强肿瘤），但由于肿瘤组织异质性、边界模糊和MRI序列对比度变化，仅依赖视觉信息或后处理约束的方法在边界描绘和层次保持方面表现不稳定。

Method: UMCF方法在统一的3D潜在空间中实现视觉、语义和空间信息的同步交互融合，通过无参数不确定性门控自适应调整模态贡献，医学先验知识直接参与注意力计算，避免传统的"先处理再拼接"分离架构。

Result: 在BraTS 2020和2021数据集上，UMCF+nnU-Net分别达到0.8579和0.8977的平均Dice系数，相比主流架构平均提升4.18%。

Conclusion: 通过深度整合临床知识与影像特征，UMCF为精准医学中的多模态信息融合提供了新的技术路径。

Abstract: Brain tumor segmentation requires accurate identification of hierarchical
regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)
from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor
tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI
sequences, methods relying solely on visual information or post-hoc loss
constraints show unstable performance in boundary delineation and hierarchy
preservation. To address this challenge, we propose the Unified Multimodal
Coherent Field (UMCF) method. This method achieves synchronous interactive
fusion of visual, semantic, and spatial information within a unified 3D latent
space, adaptively adjusting modal contributions through parameter-free
uncertainty gating, with medical prior knowledge directly participating in
attention computation, avoiding the traditional "process-then-concatenate"
separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021
datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977
respectively, with an average 4.18% improvement across mainstream
architectures. By deeply integrating clinical knowledge with imaging features,
UMCF provides a new technical pathway for multimodal information fusion in
precision medicine.

</details>


### [157] [Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models](https://arxiv.org/abs/2509.17522)
*Hangzhou He,Lei Zhu,Kaiwen Li,Xinliang Zhang,Jiakui Hu,Ourui Fu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: Chat-CBM用基于语言的分类器替代传统概念瓶颈模型中的固定线性分类器，通过概念语义直接推理，在保持可解释性的同时支持更丰富的用户干预方式。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型使用固定线性分类器，限制了用户干预只能进行数值调整，无法在测试时引入新概念或领域知识，特别是在无监督设置下概念激活噪声大，用户干预效果差。

Method: 用基于语言的分类器替代分数分类器，直接在概念语义空间进行推理，利用冻结大语言模型的语言理解和少样本能力，支持概念修正、添加/删除概念、引入外部知识等丰富干预方式。

Result: 在九个数据集上的实验表明，Chat-CBM实现了更高的预测性能，显著提升了用户交互性，同时保持了概念瓶颈模型的可解释性。

Conclusion: Chat-CBM通过语义空间推理扩展了概念瓶颈模型的干预界面，在无监督设置下仍保持有效性，为可解释AI提供了更直观的交互方式。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first
predicting a set of human-understandable concepts and then mapping them to
labels through a simple classifier. While users can intervene in the concept
space to improve predictions, traditional CBMs typically employ a fixed linear
classifier over concept scores, which restricts interventions to manual value
adjustments and prevents the incorporation of new concepts or domain knowledge
at test time. These limitations are particularly severe in unsupervised CBMs,
where concept activations are often noisy and densely activated, making user
interventions ineffective. We introduce Chat-CBM, which replaces score-based
classifiers with a language-based classifier that reasons directly over concept
semantics. By grounding prediction in the semantic space of concepts, Chat-CBM
preserves the interpretability of CBMs while enabling richer and more intuitive
interventions, such as concept correction, addition or removal of concepts,
incorporation of external knowledge, and high-level reasoning guidance.
Leveraging the language understanding and few-shot capabilities of frozen large
language models, Chat-CBM extends the intervention interface of CBMs beyond
numerical editing and remains effective even in unsupervised settings.
Experiments on nine datasets demonstrate that Chat-CBM achieves higher
predictive performance and substantially improves user interactivity while
maintaining the concept-based interpretability of CBMs.

</details>


### [158] [SimToken: A Simple Baseline for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2509.17537)
*Dian Jin,Yanghao Zhou,Jinxing Zhou,Jiaqi Ma,Ruohao Guo,Dan Guo*

Main category: cs.CV

TL;DR: SimToken是一个简单的框架，通过整合多模态大语言模型和Segment Anything Model来解决参考音频-视觉分割任务，利用特殊语义token指导SAM进行跨帧对象分割。


<details>
  <summary>Details</summary>
Motivation: 参考音频-视觉分割任务在跨模态推理和细粒度对象定位方面面临重大挑战，需要有效整合音频、视觉和文本信息。

Method: 提出SimToken框架，使用MLLM生成代表参考对象的特殊语义token，该token包含所有模态的上下文信息，作为SAM的提示来分割视频帧中的对象。引入目标一致语义对齐损失来对齐指向同一对象的不同表达式的token嵌入。

Result: 在Ref-AVS基准测试上的实验表明，该方法相比现有方法取得了优越的性能。

Conclusion: SimToken框架通过简单的token集成方法有效解决了跨模态参考分割问题，证明了语义token在指导对象分割中的有效性。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific
objects in videos based on natural language expressions involving audio,
vision, and text information. This task poses significant challenges in
cross-modal reasoning and fine-grained object localization. In this paper, we
propose a simple framework, SimToken, that integrates a multimodal large
language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided
to generate a special semantic token representing the referred object. This
compact token, enriched with contextual information from all modalities, acts
as a prompt to guide SAM to segment objectsacross video frames. To further
improve semantic learning, we introduce a novel target-consistent semantic
alignment loss that aligns token embeddings from different expressions but
referring to the same object. Experiments on the Ref-AVS benchmark demonstrate
that our approach achieves superior performance compared to existing methods.

</details>


### [159] [Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification](https://arxiv.org/abs/2509.17747)
*Sheng Huang,Jiexuan Yan,Beiyan Liu,Bo Liu,Richang Hong*

Main category: cs.CV

TL;DR: 提出HP-DVAL方法解决多标签图像分类中的类别不平衡问题，通过双视图对齐学习和分层提示调优，在MS-COCO和VOC2007数据集上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常存在类别不平衡问题，特别是在多标签图像分类任务中，数据不平衡和多目标识别带来重大挑战。

Method: HP-DVAL方法利用视觉语言预训练模型的多模态知识，采用双视图对齐学习提取互补特征，并引入分层提示调优策略（全局和局部提示）学习任务特定知识，同时使用语义一致性损失防止提示偏离预训练模型的通用知识。

Result: 在两个CI-MLIC基准测试上验证了方法的有效性，在长尾多标签图像分类任务上mAP分别提升10.0%和5.2%，在多标签少样本图像分类任务上分别提升6.8%和2.9%。

Conclusion: 该方法通过有效利用VLP模型的知识和创新的提示调优策略，成功解决了多标签图像分类中的类别不平衡问题，显著优于现有最优方法。

Abstract: Real-world datasets often exhibit class imbalance across multiple categories,
manifesting as long-tailed distributions and few-shot scenarios. This is
especially challenging in Class-Imbalanced Multi-Label Image Classification
(CI-MLIC) tasks, where data imbalance and multi-object recognition present
significant obstacles. To address these challenges, we propose a novel method
termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which
leverages multi-modal knowledge from vision-language pretrained (VLP) models to
mitigate the class-imbalance problem in multi-label settings. Specifically,
HP-DVAL employs dual-view alignment learning to transfer the powerful feature
representation capabilities from VLP models by extracting complementary
features for accurate image-text alignment. To better adapt VLP models for
CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes
global and local prompts to learn task-specific and context-related prior
knowledge. Additionally, we design a semantic consistency loss during prompt
tuning to prevent learned prompts from deviating from general knowledge
embedded in VLP models. The effectiveness of our approach is validated on two
CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results
demonstrate the superiority of our method over SOTA approaches, achieving mAP
improvements of 10.0\% and 5.2\% on the long-tailed multi-label image
classification task, and 6.8\% and 2.9\% on the multi-label few-shot image
classification task.

</details>


### [160] [Visual Instruction Pretraining for Domain-Specific Foundation Models](https://arxiv.org/abs/2509.17562)
*Yuxuan Li,Yicheng Zhang,Wenhao Tang,Yimian Dai,Ming-Ming Cheng,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: ViTP是一种新的预训练范式，通过在视觉语言模型中嵌入Vision Transformer，并利用视觉指令数据进行端到端预训练，实现了推理对感知的增强，在遥感和医学影像领域取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉的感知-推理-生成闭环中，高层推理对底层感知特征学习的影响尚未充分探索，需要填补这一空白。

Method: 提出视觉指令预训练(ViTP)，将ViT嵌入视觉语言模型，使用目标下游领域的视觉指令数据进行端到端预训练，并采用视觉鲁棒性学习(VRL)从稀疏视觉标记中学习鲁棒特征。

Result: 在16个具有挑战性的遥感和医学影像基准测试中，ViTP在多种下游任务上均取得了新的最先进性能。

Conclusion: ViTP通过推理增强感知的新范式，成功提升了基础模型在下游领域的性能，证明了高层推理对底层感知学习的重要价值。

Abstract: Modern computer vision is converging on a closed loop in which perception,
reasoning and generation mutually reinforce each other. However, this loop
remains incomplete: the top-down influence of high-level reasoning on the
foundational learning of low-level perceptual features is not yet
underexplored. This paper addresses this gap by proposing a new paradigm for
pretraining foundation models in downstream domains. We introduce Visual
insTruction Pretraining (ViTP), a novel approach that directly leverages
reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)
backbone within a Vision-Language Model and pretrains it end-to-end using a
rich corpus of visual instruction data curated from target downstream domains.
ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels
the ViT to learn robust and domain-relevant features from a sparse set of
visual tokens. Extensive experiments on 16 challenging remote sensing and
medical imaging benchmarks demonstrate that ViTP establishes new
state-of-the-art performance across a diverse range of downstream tasks. The
code is available at https://github.com/zcablii/ViTP.

</details>


### [161] [PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification](https://arxiv.org/abs/2509.17581)
*Florinel Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出一个用于相机识别的PRNU估计新基准，包含13K张照片和120+相机，支持野外评估。同时提出一种混合架构的PRNU相机识别模型，使用去噪自编码器估计PRNU信号，并通过Hadamard乘积实现1:N验证，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有相机识别方法在真实场景下的评估不足，需要开发能够在不同拍摄场景下进行准确相机识别的基准和模型。

Method: 使用混合架构：去噪自编码器估计PRNU信号，卷积网络进行1:N验证。创新地采用Hadamard乘积作为输入，替代传统的对比学习方法。

Result: 提出的方法在PRNU基准测试中显著优于基于去噪自编码器和对比学习的最先进模型。

Conclusion: 该基准和模型为相机识别提供了有效的野外评估框架，Hadamard乘积的设计显著提升了识别性能，代码和数据集已开源。

Abstract: We propose a novel benchmark for camera identification via Photo Response
Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with
120+ cameras, where the training and test photos are taken in different
scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel
PRNU-based camera identification model that employs a hybrid architecture,
comprising a denoising autoencoder to estimate the PRNU signal and a
convolutional network that can perform 1:N verification of camera devices.
Instead of using a conventional approach based on contrastive learning, our
method takes the Hadamard product between reference and query PRNU signals as
input. This novel design leads to significantly better results compared with
state-of-the-art models based on denoising autoencoders and contrastive
learning. We release our dataset and code at:
https://github.com/CroitoruAlin/PRNU-Bench.

</details>


### [162] [Accurate and Efficient Low-Rank Model Merging in Core Space](https://arxiv.org/abs/2509.17786)
*Aniello Panariello,Daniel Marczak,Simone Magistri,Angelo Porrello,Bartłomiej Twardowski,Andrew D. Bagdanov,Simone Calderara,Joost van de Weijer*

Main category: cs.CV

TL;DR: 提出了Core Space框架，用于在共享对齐基中合并LoRA适配的模型，保持低秩适配效率的同时显著提升多任务准确率


<details>
  <summary>Details</summary>
Motivation: 现有LoRA模型合并方法通常需要合并全尺寸权重矩阵，牺牲了低秩适配的效率优势

Method: Core Space合并框架，通过投影到核心空间实现LoRA适配模型的合并，确保信息无损且计算高效

Result: 在视觉和语言任务上达到最先进结果，同时仅使用少量计算资源

Conclusion: Core Space框架有效解决了LoRA模型合并的效率与精度权衡问题，为参数高效适配提供了实用解决方案

Abstract: In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.

</details>


### [163] [Domain Adaptive Object Detection for Space Applications with Real-Time Constraints](https://arxiv.org/abs/2509.17593)
*Samet Hicsonmez,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 该论文提出了一种面向空间应用的目标检测领域自适应方法，通过监督式领域适应（SDA）利用少量真实标注数据来缩小合成数据与真实数据之间的域差距。


<details>
  <summary>Details</summary>
Motivation: 当前空间应用中的目标检测模型通常使用合成数据进行训练，但在真实数据上性能显著下降。领域自适应目标检测在学术界尚未得到足够重视，需要解决这一域差距问题。

Method: 基于半监督自适应方法，结合了域不变特征学习（使用CNN域判别器）和不变风险最小化（使用域无关回归头）。在轻量级SSD-MobileNet和更先进的FCOS-ResNet-50检测器上进行测试。

Result: 在两个空间数据集SPEED+和SPARK上评估，仅使用250张标注真实图像就能实现平均精度（AP）提升高达20个百分点。

Conclusion: 该方法有效解决了空间目标检测中的域适应问题，显著提升了模型在真实数据上的性能，满足了实时部署需求。

Abstract: Object detection is essential in space applications targeting Space Domain
Awareness and also applications involving relative navigation scenarios.
Current deep learning models for Object Detection in space applications are
often trained on synthetic data from simulators, however, the model performance
drops significantly on real-world data due to the domain gap. However, domain
adaptive object detection is an overlooked problem in the community. In this
work, we first show the importance of domain adaptation and then explore
Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled
real data. We build on a recent semi-supervised adaptation method and tailor it
for object detection. Our approach combines domain-invariant feature learning
with a CNN-based domain discriminator and invariant risk minimization using a
domain-independent regression head. To meet real-time deployment needs, we test
our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet
backbone and on the more advanced Fully Convolutional One-Stage object detector
(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and
SPARK. The results show up to 20-point improvements in average precision (AP)
with just 250 labeled real images.

</details>


### [164] [TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification](https://arxiv.org/abs/2509.17802)
*Qi'ao Xu,Pengfei Wang,Bo Zhong,Tianwen Qian,Xiaoling Wang,Ye Wang,Hong Yu*

Main category: cs.CV

TL;DR: TS-P²CL是一个用于医学时间序列分类的即插即用框架，通过将1D生理信号转换为2D伪图像，利用预训练视觉模型的通用模式识别能力，实现跨主体鲁棒分类。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列分类在智能医疗中至关重要，但由于个体间异质性导致跨主体泛化能力差。现有方法受限于模态特定的归纳偏差，无法学习通用的不变表示。

Method: 提出视觉引导范式，将1D信号转换为2D伪图像，建立与视觉域的桥梁。采用双对比学习策略：模态内一致性强制时间连贯性，跨模态对齐将时间序列动态与视觉语义对齐。

Result: 在六个医学时间序列数据集上的广泛实验表明，TS-P²CL在主体依赖和主体独立设置下均优于14种方法。

Conclusion: 该框架通过利用预训练视觉模型的丰富语义先验，有效减轻个体特定偏差，学习鲁棒的领域不变特征，显著提升医学时间序列分类性能。

Abstract: Medical time series (MedTS) classification is pivotal for intelligent
healthcare, yet its efficacy is severely limited by poor cross-subject
generation due to the profound cross-individual heterogeneity. Despite advances
in architectural innovations and transfer learning techniques, current methods
remain constrained by modality-specific inductive biases that limit their
ability to learn universally invariant representations. To overcome this, we
propose TS-P$^2$CL, a novel plug-and-play framework that leverages the
universal pattern recognition capabilities of pre-trained vision models. We
introduce a vision-guided paradigm that transforms 1D physiological signals
into 2D pseudo-images, establishing a bridge to the visual domain. This
transformation enables implicit access to rich semantic priors learned from
natural images. Within this unified space, we employ a dual-contrastive
learning strategy: intra-modal consistency enforces temporal coherence, while
cross-modal alignment aligns time-series dynamics with visual semantics,
thereby mitigating individual-specific biases and learning robust,
domain-invariant features. Extensive experiments on six MedTS datasets
demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both
subject-dependent and subject-independent settings.

</details>


### [165] [COLA: Context-aware Language-driven Test-time Adaptation](https://arxiv.org/abs/2509.17598)
*Aiming Zhang,Tianyuan Yu,Liang Bai,Jun Tang,Yanming Guo,Yirun Ruan,Yun Zhou,Zhihe Lu*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时自适应方法COLA，利用预训练的视觉语言模型（如CLIP）来适应多个目标域，无需共享标签空间。该方法通过轻量级上下文感知模块和类平衡伪标签策略，有效解决了分布偏移和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法通常假设源域模型和目标域共享相同的标签空间，这限制了其实际应用。本文旨在开发一种更通用的源模型，能够适应多个目标域而无需共享标签。

Method: 提出了COLA方法，包含三个关键组件：任务感知适配器、上下文感知单元和残差连接单元。该方法还引入了类平衡伪标签策略来缓解类别不平衡问题。

Result: 该方法不仅适用于测试时自适应场景，还在类别泛化任务中表现出有效性。

Conclusion: COLA方法通过轻量级上下文感知模块和类平衡伪标签策略，成功实现了无需共享标签的多目标域自适应，具有参数效率高和易于集成的优点。

Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its
efficacy in addressing ``distribution shift'' issue while simultaneously
protecting data privacy.
  However, most prior methods assume that a paired source domain model and
target domain sharing the same label space coexist, heavily limiting their
applicability.
  In this paper, we investigate a more general source model capable of
adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno,
CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to
effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA
(COLA).
  The proposed method incorporates a lightweight context-aware module that
consists of three key components: a task-aware adapter, a context-aware unit,
and a residual connection unit for exploring task-specific knowledge,
domain-specific knowledge from the VLM and prior knowledge of the VLM,
respectively.
  It is worth noting that the context-aware module can be seamlessly integrated
into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy
to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but
also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

</details>


### [166] [Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images](https://arxiv.org/abs/2509.17602)
*Giulio Martellucci,Herve Goeau,Pierre Bonnet,Fabrice Vinatier,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2025挑战赛旨在利用AI技术加速植物生态学研究中的物种识别，通过提供大规模标注数据集和预训练模型，解决四格图像中的多标签分类问题。


<details>
  <summary>Details</summary>
Motivation: 四格图像在生态学研究中至关重要，但人工识别物种耗时且覆盖范围有限。AI技术可以帮助专家加速物种清单编制，扩大生态学研究的空间覆盖范围。

Method: 挑战赛基于2,105张高分辨率多标签四格图像测试集和140万张单标签训练图像，使用预训练的视觉Transformer模型，将任务定义为弱监督多标签分类问题。

Result: 论文详细描述了数据集、评估方法、参与者使用的模型方法以及取得的成果，为植物物种识别研究提供了基准和参考。

Conclusion: PlantCLEF 2025挑战赛通过整合AI技术，为生态学研究提供了有效的工具，有望推动植物物种识别技术的进步和应用扩展。

Abstract: Quadrat images are essential for ecological studies, as they enable
standardized sampling, the assessment of plant biodiversity, long-term
monitoring, and large-scale field campaigns. These images typically cover an
area of fifty centimetres or one square meter, and botanists carefully identify
all the species present. Integrating AI could help specialists accelerate their
inventories and expand the spatial coverage of ecological studies. To assess
progress in this area, the PlantCLEF 2025 challenge relies on a new test set of
2,105 high-resolution multi-label images annotated by experts and covering
around 400 species. It also provides a large training set of 1.4 million
individual plant images, along with vision transformer models pre-trained on
this data. The task is formulated as a (weakly labelled) multi-label
classification problem, where the goal is to predict all species present in a
quadrat image using single-label training data. This paper provides a detailed
description of the data, the evaluation methodology, the methods and models
used by participants, and the results achieved.

</details>


### [167] [WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification](https://arxiv.org/abs/2509.17740)
*Yiwen Jiang,Deval Mehta,Siyuan Yan,Yaling Shen,Zimu Wang,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出了WISE方法，通过弱监督将概念瓶颈模型的概念表示转化为可解释的推理链，增强多模态大语言模型在图像分类中的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态思维链方法依赖丰富的数据集且主要关注对象间推理，忽视了图像分类中至关重要的对象内理解。

Method: WISE方法在弱监督下将概念瓶颈模型的概念表示重新表述为简洁、可解释的推理链，为任何图像分类数据集生成多模态思维链。

Result: 在十个数据集上的实验表明，生成的思维链不仅将可解释性提高了37%，而且在微调多模态大语言模型时还能提升分类准确率。

Conclusion: 该工作连接了基于概念的可解释性和生成式思维链推理，为增强多模态大语言模型在细粒度视觉理解方面提供了通用框架。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in visual-textual
reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly
enhancing interpretability. However, existing MCoT methods rely on
rationale-rich datasets and largely focus on inter-object reasoning,
overlooking the intra-object understanding crucial for image classification. To
address this gap, we propose WISE, a Weak-supervision-guided Step-by-step
Explanation method that augments any image classification dataset with MCoTs by
reformulating the concept-based representations from Concept Bottleneck Models
(CBMs) into concise, interpretable reasoning chains under weak supervision.
Experiments across ten datasets show that our generated MCoTs not only improve
interpretability by 37% but also lead to gains in classification accuracy when
used to fine-tune MLLMs. Our work bridges concept-based interpretability and
generative MCoT reasoning, providing a generalizable framework for enhancing
MLLMs in fine-grained visual understanding.

</details>


### [168] [From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge](https://arxiv.org/abs/2509.17615)
*Lars Heckler-Kram,Ashwin Vaidya,Jan-Hendrik Neudeck,Ulla Scheler,Dick Ameln,Samet Akcay,Paula Ramos*

Main category: cs.CV

TL;DR: VAND 3.0挑战赛展示了异常检测在不同实际场景中的进展，重点关注对真实世界分布偏移的鲁棒性和视觉语言模型在少样本场景下的能力。


<details>
  <summary>Details</summary>
Motivation: 加强学术界与工业界的联系，解决异常检测领域的关键问题，展示在不同实际设置下的当前进展。

Method: 挑战赛设有两个赛道：一是开发对真实世界分布偏移具有鲁棒性的异常检测方法；二是探索视觉语言模型在少样本机制下的能力。参与者通过结合或调整现有方法并与新流程融合来改进基线。

Result: 参与者的解决方案相比之前的基线取得了显著改进，大规模预训练视觉（语言）主干网络在性能提升中发挥了关键作用。

Conclusion: 虽然大规模预训练模型对性能提升至关重要，但未来研究需要更有效地扩展异常检测方法，以满足现场实时性和计算约束。

Abstract: Visual anomaly detection is a strongly application-driven field of research.
Consequently, the connection between academia and industry is of paramount
importance. In this regard, we present the VAND 3.0 Challenge to showcase
current progress in anomaly detection across different practical settings
whilst addressing critical issues in the field. The challenge hosted two
tracks, fostering the development of anomaly detection methods robust against
real-world distribution shifts (Category 1) and exploring the capabilities of
Vision Language Models within the few-shot regime (Category 2), respectively.
The participants' solutions reached significant improvements over previous
baselines by combining or adapting existing approaches and fusing them with
novel pipelines. While for both tracks the progress in large pre-trained vision
(language) backbones played a pivotal role for the performance increase,
scaling up anomaly detection methods more efficiently needs to be addressed by
future research to meet real-time and computational constraints on-site.

</details>


### [169] [Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training](https://arxiv.org/abs/2509.17888)
*Divya Mereddy,Marcos Quinones-Grueiro,Ashwin T S,Eduardo Davalos,Gautam Biswas,Kent Etherton,Tyler Davis,Katelyn Kay,Jill Lear,Benjamin Goldberg*

Main category: cs.CV

TL;DR: 本研究开发了一个结合认知任务分析(CTA)和多模态学习分析(MMLA)的系统性评估框架，用于客观评估重症监护空运团队(CCATT)在混合现实模拟训练中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的教官主导评估主观性强且可能忽略关键事件，而AI自动化评估需要人工输入来训练算法。需要更客观、全面的评估方法来评估高压环境下的团队表现。

Method: 开发了针对CCATT训练的领域特定CTA模型，并使用基于视觉的动作识别管道（基于Cascade Disentangling Network的微调人-物交互模型）来检测和跟踪学员与设备的交互。

Result: 该方法能够自动生成性能指标（如反应时间、任务持续时间），并将其映射到针对CCATT操作的分层CTA模型上。

Conclusion: 该框架实现了可解释的、领域相关的性能评估，为CCATT训练提供了更客观和全面的评估方法。

Abstract: This study examines how Critical Care Air Transport Team (CCATT) members are
trained using mixed-reality simulations that replicate the high-pressure
conditions of aeromedical evacuation. Each team - a physician, nurse, and
respiratory therapist - must stabilize severely injured soldiers by managing
ventilators, IV pumps, and suction devices during flight. Proficient
performance requires clinical expertise and cognitive skills, such as
situational awareness, rapid decision-making, effective communication, and
coordinated task management, all of which must be maintained under stress.
Recent advances in simulation and multimodal data analytics enable more
objective and comprehensive performance evaluation. In contrast, traditional
instructor-led assessments are subjective and may overlook critical events,
thereby limiting generalizability and consistency. However, AI-based automated
and more objective evaluation metrics still demand human input to train the AI
algorithms to assess complex team dynamics in the presence of environmental
noise and the need for accurate re-identification in multi-person tracking. To
address these challenges, we introduce a systematic, data-driven assessment
framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning
Analytics (MMLA). We have developed a domain-specific CTA model for CCATT
training and a vision-based action recognition pipeline using a fine-tuned
Human-Object Interaction model, the Cascade Disentangling Network (CDN), to
detect and track trainee-equipment interactions over time. These interactions
automatically yield performance indicators (e.g., reaction time, task
duration), which are mapped onto a hierarchical CTA model tailored to CCATT
operations, enabling interpretable, domain-relevant performance evaluations.

</details>


### [170] [Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method](https://arxiv.org/abs/2509.17620)
*Gregory Schroeder,Mohamed Sabry,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 提出了一种基于校准三焦张量的相机自标定方法TrifocalCalib，无需先验场景知识即可从最小图像数据中估计相机内参（焦距和主点）。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶和车辆编队等应用中，预先校准的相机设置不切实际，需要实时自适应的相机标定能力。

Method: 基于校准三焦张量构建方程组，无需标定目标，不限制相机运动，同时估计焦距和主点。

Result: 在程序生成的合成环境和结构化数据集场景中评估，相比现有学习方法和技术方法，显著提高了准确性和鲁棒性。

Conclusion: TrifocalCalib方法在相机自标定方面取得了显著进展，代码已公开以支持可重复性。

Abstract: Estimating camera intrinsic parameters without prior scene knowledge is a
fundamental challenge in computer vision. This capability is particularly
important for applications such as autonomous driving and vehicle platooning,
where precalibrated setups are impractical and real-time adaptability is
necessary. To advance the state-of-the-art, we present a set of equations based
on the calibrated trifocal tensor, enabling projective camera self-calibration
from minimal image data. Our method, termed TrifocalCalib, significantly
improves accuracy and robustness compared to both recent learning-based and
classical approaches. Unlike many existing techniques, our approach requires no
calibration target, imposes no constraints on camera motion, and simultaneously
estimates both focal length and principal point. Evaluations in both
procedurally generated synthetic environments and structured dataset-based
scenarios demonstrate the effectiveness of our approach. To support
reproducibility, we make the code publicly available.

</details>


### [171] [Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale](https://arxiv.org/abs/2509.17622)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文介绍了PlantCLEF2023挑战赛，该挑战旨在利用深度学习方法解决植物物种自动识别问题，涉及80,000个植物物种的多图像分类任务。


<details>
  <summary>Details</summary>
Motivation: 面对全球生物多样性危机，植物识别对农业、建筑和药典等领域至关重要。传统的人工识别方法效率低下，阻碍了新数据和知识的积累。

Method: 采用深度学习技术进行多图像分类，处理80,000个植物物种的大规模数据集，克服类别数量多、类别分布不平衡、识别错误、重复数据、视觉质量不一和视觉内容多样等数据相关挑战。

Result: 深度学习技术在植物识别方面已趋于成熟，显示出在未来建立能够准确识别全球所有植物物种的识别系统的潜力。PlantCLEF2023挑战赛为此目标提供了资源和评估框架。

Conclusion: PlantCLEF2023挑战赛通过大规模多图像分类任务推动了植物自动识别技术的发展，深度学习方法的成熟为未来实现全球植物物种准确识别系统带来了希望。

Abstract: The world is estimated to be home to over 300,000 species of vascular plants.
In the face of the ongoing biodiversity crisis, expanding our understanding of
these species is crucial for the advancement of human civilization,
encompassing areas such as agriculture, construction, and pharmacopoeia.
However, the labor-intensive process of plant identification undertaken by
human experts poses a significant obstacle to the accumulation of new data and
knowledge. Fortunately, recent advancements in automatic identification,
particularly through the application of deep learning techniques, have shown
promising progress. Despite challenges posed by data-related issues such as a
vast number of classes, imbalanced class distribution, erroneous
identifications, duplications, variable visual quality, and diverse visual
contents (such as photos or herbarium sheets), deep learning approaches have
reached a level of maturity which gives us hope that in the near future we will
have an identification system capable of accurately identifying all plant
species worldwide. The PlantCLEF2023 challenge aims to contribute to this
pursuit by addressing a multi-image (and metadata) classification problem
involving an extensive set of classes (80,000 plant species). This paper
provides an overview of the challenge's resources and evaluations, summarizes
the methods and systems employed by participating research groups, and presents
an analysis of key findings.

</details>


### [172] [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models](https://arxiv.org/abs/2509.17627)
*Jinshu Chen,Xinghui Li,Xu Bai,Tianxiang Ma,Pengze Zhang,Zhuowei Chen,Gen Li,Lijie Liu,Songtao Zhao,Bingchuan Li,Qian He*

Main category: cs.CV

TL;DR: 本文提出了OmniInsert框架，用于解决无掩码视频插入任务中的三个关键挑战：数据稀缺、主体-场景平衡和插入协调。通过创新的数据管道InsertPipe、条件特定特征注入机制和渐进式训练策略，实现了优于现有商业解决方案的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频插入方法依赖复杂的控制信号，但难以保持主体一致性，限制了实际应用。本文旨在解决数据稀缺、主体-场景平衡和插入协调这三个关键挑战。

Method: 提出了InsertPipe数据管道自动构建多样化的跨对数据；开发了OmniInsert统一框架，包含条件特定特征注入机制、渐进式训练策略、主体聚焦损失函数、插入偏好优化方法和上下文感知重述模块。

Result: 在提出的InsertBench基准测试中，OmniInsert表现优于最先进的闭源商业解决方案。

Conclusion: OmniInsert框架有效解决了无掩码视频插入的关键挑战，通过创新的数据管道和训练策略实现了高质量的插入效果，代码将公开发布。

Abstract: Recent advances in video insertion based on diffusion models are impressive.
However, existing methods rely on complex control signals but struggle with
subject consistency, limiting their practical applicability. In this paper, we
focus on the task of Mask-free Video Insertion and aim to resolve three key
challenges: data scarcity, subject-scene equilibrium, and insertion
harmonization. To address the data scarcity, we propose a new data pipeline
InsertPipe, constructing diverse cross-pair data automatically. Building upon
our data pipeline, we develop OmniInsert, a novel unified framework for
mask-free video insertion from both single and multiple subject references.
Specifically, to maintain subject-scene equilibrium, we introduce a simple yet
effective Condition-Specific Feature Injection mechanism to distinctly inject
multi-source conditions and propose a novel Progressive Training strategy that
enables the model to balance feature injection from subjects and source video.
Meanwhile, we design the Subject-Focused Loss to improve the detailed
appearance of the subjects. To further enhance insertion harmonization, we
propose an Insertive Preference Optimization methodology to optimize the model
by simulating human preferences, and incorporate a Context-Aware Rephraser
module during reference to seamlessly integrate the subject into the original
scenes. To address the lack of a benchmark for the field, we introduce
InsertBench, a comprehensive benchmark comprising diverse scenes with
meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert
outperforms state-of-the-art closed-source commercial solutions. The code will
be released.

</details>


### [173] [Overview of PlantCLEF 2022: Image-based plant identification at global scale](https://arxiv.org/abs/2509.17632)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF2022挑战赛旨在通过多图像和元数据分类方法，解决全球80,000种植物物种的自动识别问题，以应对植物多样性识别中的人类专家瓶颈。


<details>
  <summary>Details</summary>
Motivation: 全球有超过30万种维管植物，但人类专家的系统识别工作限制了新数据和知识的积累。自动识别技术，特别是深度学习，已足够成熟来处理全球植物生物多样性识别中的挑战，如类别数量巨大、类别不平衡、识别错误等问题。

Method: PlantCLEF2022挑战赛采用多图像和元数据分类方法，处理80,000种植物物种的大规模分类问题。

Result: 论文介绍了挑战赛的资源、评估方法，总结了参与研究团队采用的方法和系统，并分析了关键发现。

Conclusion: PlantCLEF2022挑战赛在推动全球植物生物多样性自动识别方面迈出了重要一步，展示了深度学习在大规模、多类别植物识别中的潜力。

Abstract: It is estimated that there are more than 300,000 species of vascular plants
in the world. Increasing our knowledge of these species is of paramount
importance for the development of human civilization (agriculture,
construction, pharmacopoeia, etc.), especially in the context of the
biodiversity crisis. However, the burden of systematic plant identification by
human experts strongly penalizes the aggregation of new data and knowledge.
Since then, automatic identification has made considerable progress in recent
years as highlighted during all previous editions of PlantCLEF. Deep learning
techniques now seem mature enough to address the ultimate but realistic problem
of global identification of plant biodiversity in spite of many problems that
the data may present (a huge number of classes, very strongly unbalanced
classes, partially erroneous identifications, duplications, variable visual
quality, diversity of visual contents such as photos or herbarium sheets, etc).
The PlantCLEF2022 challenge edition proposes to take a step in this direction
by tackling a multi-image (and metadata) classification problem with a very
large number of classes (80k plant species). This paper presents the resources
and evaluations of the challenge, summarizes the approaches and systems
employed by the participating research groups, and provides an analysis of key
findings.

</details>


### [174] [Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs](https://arxiv.org/abs/2509.18015)
*Advait Gosai,Arun Kavishwar,Stephanie L. McNamara,Soujanya Samineni,Renato Umeton,Alexander Chowdhury,William Lotter*

Main category: cs.CV

TL;DR: 本文评估了通用多模态大语言模型（GPT-4和GPT-5）与领域特定模型（MedGemma）在胸部X光片上定位病理的能力。GPT-5表现最佳（49.7%准确率），但仍低于任务特定CNN（59.9%）和放射科医生基准（80.1%）。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型在医学图像中定位病理的能力，这不仅是临床和教育的重要方面，还能揭示模型对解剖学和疾病空间理解的程度。

Method: 使用CheXlocalize数据集中的9种病理，采用空间网格提示管道，要求模型基于坐标进行预测，并与任务特定CNN和放射科医生基准进行比较。

Result: GPT-5定位准确率为49.7%，GPT-4为39.1%，MedGemma为17.7%。GPT-5的预测大多在解剖学合理区域但不精确，GPT-4对固定位置病理表现良好但对空间可变病理表现差，MedGemma泛化能力有限。

Conclusion: 当前多模态大语言模型在医学图像定位方面有潜力但仍有局限，需要与任务特定工具结合才能可靠使用。

Abstract: Recent work has shown promising performance of frontier large language models
(LLMs) and their multimodal counterparts in medical quizzes and diagnostic
tasks, highlighting their potential for broad clinical utility given their
accessible, general-purpose nature. However, beyond diagnosis, a fundamental
aspect of medical image interpretation is the ability to localize pathological
findings. Evaluating localization not only has clinical and educational
relevance but also provides insight into a model's spatial understanding of
anatomy and disease. Here, we systematically assess two general-purpose MLLMs
(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to
localize pathologies on chest radiographs, using a prompting pipeline that
overlays a spatial grid and elicits coordinate-based predictions. Averaged
across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a
localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),
all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark
(80.1%). Despite modest performance, error analysis revealed that GPT-5's
predictions were largely in anatomically plausible regions, just not always
precisely localized. GPT-4 performed well on pathologies with fixed anatomical
locations, but struggled with spatially variable findings and exhibited
anatomically implausible predictions more frequently. MedGemma demonstrated the
lowest performance on all pathologies, showing limited capacity to generalize
to this novel task. Our findings highlight both the promise and limitations of
current MLLMs in medical imaging and underscore the importance of integrating
them with task-specific tools for reliable use.

</details>


### [175] [Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers](https://arxiv.org/abs/2509.17650)
*Soroush Mahdi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 提出一种无需训练、推理时的token淘汰策略，通过丢弃冗余token来限制内存增长，在保持精度的同时显著减少内存使用。


<details>
  <summary>Details</summary>
Motivation: Streaming视觉变换器如StreamVGGT在3D感知方面表现优异，但存在KV内存无限增长的问题，限制了可扩展性。

Method: 在推理时采用token淘汰策略，丢弃冗余token而保留最有信息量的token，从而限制内存使用。

Result: 在7-Scenes长序列上，峰值内存从18.63GB降至9.39GB，精度和完整性仅下降0.003；在严格内存预算下，通过更密集的帧采样提高了重建精度。

Conclusion: 该方法在视频深度估计、3D重建和相机姿态估计等任务中，以少量内存代价接近StreamVGGT性能，使长序列流式推理更加实用。

Abstract: Streaming visual transformers like StreamVGGT achieve strong 3D perception
but suffer from unbounded growth of key value (KV) memory, which limits
scalability. We propose a training-free, inference-time token eviction policy
that bounds memory by discarding redundant tokens while keeping the most
informative ones. Our method uses significantly less memory with little to no
drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from
18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under
strict memory budgets, eviction enables denser frame sampling, which improves
reconstruction accuracy compared to the baseline. Experiments across video
depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and
camera pose estimation (Sintel, TUM-dynamics) show that our approach closely
matches StreamVGGT at a fraction of the memory and makes long-horizon streaming
inference more practical.

</details>


### [176] [SISMA: Semantic Face Image Synthesis with Mamba](https://arxiv.org/abs/2509.17651)
*Filippo Botti,Alex Ergasti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba的新型架构SISMA，用于语义图像合成，相比基于注意力的扩散模型，在保持高质量生成的同时显著降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语义图像合成中很受欢迎，但其训练和推理计算成本高，主要由于注意力层的二次复杂度。需要一种更轻量高效的替代方案。

Method: 基于最近提出的Mamba架构构建SISMA模型，通过语义掩码控制生成图像的形状，同时降低计算需求。

Result: 在CelebAMask-HQ数据集上的实验表明，SISMA不仅获得了更好的FID分数，而且运行速度比最先进架构快三倍。

Conclusion: SISMA是基于transformer模型的一个可行、轻量级替代方案，在保持生成质量的同时大幅提升了效率。

Abstract: Diffusion Models have become very popular for Semantic Image Synthesis (SIS)
of human faces. Nevertheless, their training and inference is computationally
expensive and their computational requirements are high due to the quadratic
complexity of attention layers. In this paper, we propose a novel architecture
called SISMA, based on the recently proposed Mamba. SISMA generates high
quality samples by controlling their shape using a semantic mask at a reduced
computational demand. We validated our approach through comprehensive
experiments with CelebAMask-HQ, revealing that our architecture not only
achieves a better FID score yet also operates at three times the speed of
state-of-the-art architectures. This indicates that the proposed design is a
viable, lightweight substitute to transformer-based models.

</details>


### [177] [Clothing agnostic Pre-inpainting Virtual Try-ON](https://arxiv.org/abs/2509.17654)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Taemin Lee*

Main category: cs.CV

TL;DR: CaP-VTON提出了一种基于多类别掩码和皮肤修复的虚拟试穿方法，解决了现有方法中底部检测不准确和服装轮廓残留的问题，显著提升了短袖合成的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的Leffa方法虽然改善了基于扩散模型的纹理失真问题，但在底部检测不准确和服装轮廓残留方面存在局限，需要开发更自然、一致的全身服装合成方法。

Method: CaP-VTON整合了基于Dress Code的多类别掩码和基于Stable Diffusion的皮肤修复技术，特别引入了生成皮肤模块来解决长袖转短袖/无袖时的皮肤修复问题，并考虑了人体姿态和颜色因素。

Result: CaP-VTON在短袖合成准确率上达到92.5%，比Leffa提升了15.4%，在视觉评估中能够一致地复现参考服装的风格和形状。

Conclusion: 该方法保持了模型无关特性，可应用于各种基于扩散的虚拟试穿系统，对电子商务、定制造型和虚拟形象创建等高精度虚拟试穿应用具有重要价值。

Abstract: With the development of deep learning technology, virtual try-on technology
has become an important application value in the fields of e-commerce, fashion,
and entertainment. The recently proposed Leffa has improved the texture
distortion problem of diffu-sion-based models, but there are limitations in
that the bottom detection inaccuracy and the existing clothing silhouette
remain in the synthesis results. To solve this problem, this study proposes
CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has
improved the naturalness and consistency of whole-body clothing syn-thesis by
integrating multi-category masking based on Dress Code and skin inpainting
based on Stable Diffusion. In particular, a generate skin module was introduced
to solve the skin restoration problem that occurs when long-sleeved images are
converted into short-sleeved or sleeveless ones, and high-quality restoration
was implemented consider-ing the human body posture and color. As a result,
CaP-VTON recorded 92.5%, which is 15.4% better than Leffa in short-sleeved
synthesis accuracy, and showed the performance of consistently reproducing the
style and shape of reference clothing in visual evaluation. These structures
maintain model-agnostic properties and are applicable to various
diffu-sion-based virtual inspection systems, and can contribute to applications
that require high-precision virtual wearing, such as e-commerce, custom
styling, and avatar creation.

</details>


### [178] [UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning](https://arxiv.org/abs/2509.18094)
*Ye Liu,Zongyang Ma,Junfu Pu,Zhongang Qi,Yang Wu,Ying Shan,Chang Wen Chen*

Main category: cs.CV

TL;DR: UniPixel是一个大型多模态模型，能够灵活理解视觉提示输入并生成基于掩码的响应，将像素级感知与通用视觉理解能力无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型主要关注整体图像和视频语言理解，而较少关注细粒度像素级理解能力。现有模型要么只能执行引用任务，要么只能执行分割任务，无法将这些细粒度感知能力整合到视觉推理中。

Method: UniPixel处理视觉提示并按需生成相关掩码，在推理过程中基于这些中间指针进行后续推理，从而实现细粒度像素级推理。

Result: 该方法在10个基准测试中验证了有效性，涵盖像素级引用/分割和图像/视频中的对象中心理解等多样化任务。还设计了新的PixelQA任务来验证方法的灵活性。

Conclusion: UniPixel成功地将像素级感知与一般视觉理解能力相结合，为细粒度多模态理解提供了有效的解决方案。

Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.

</details>


### [179] [Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study](https://arxiv.org/abs/2509.17660)
*Yikun Ma,Bo Li,Ying Chen,Zijie Yue,Shuchang Xu,Jingyao Li,Lei Ma,Liang Zhong,Duowu Zou,Leiming Xu,Yunshi Zhong,Xiaobo Li,Weiqun Ding,Minmin Zhang,Dongli He,Zhenghong Li,Ye Chen,Ye Zhao,Jialong Zhuo,Xiaofen Wu,Lisha Yi,Miaojing Shi,Huihui Sun*

Main category: cs.CV

TL;DR: 本文开发了一种基于AI基础模型的方法，用于食管胃结合部腺癌（EGJA）的筛查和分期诊断，通过多中心研究验证了模型在诊断准确性和效率方面的优势。


<details>
  <summary>Details</summary>
Motivation: EGJA的早期检测对改善患者预后至关重要，但当前诊断高度依赖操作者经验，需要开发更客观、准确的AI辅助诊断方法。

Method: 采用DINOv2（视觉基础模型）和ResNet50（卷积神经网络）结合的方法，从内镜图像中提取全局外观和局部细节特征，用于EGJA分期诊断。研究基于7家中国医院的12,302张图像数据。

Result: 模型在三个测试集上的准确率分别达到0.9256、0.8895和0.8956，优于最佳对比模型ResNet50（0.9125、0.8382、0.8519）和专家内镜医师（0.8147）。模型辅助下，各级医师诊断准确率均有显著提升。

Conclusion: 这是基础模型在EGJA分期诊断中的首次应用，展示了在诊断准确性和效率方面的巨大潜力，有望成为临床诊断的重要辅助工具。

Abstract: The early detection of esophagogastric junction adenocarcinoma (EGJA) is
crucial for improving patient prognosis, yet its current diagnosis is highly
operator-dependent. This paper aims to make the first attempt to develop an
artificial intelligence (AI) foundation model-based method for both screening
and staging diagnosis of EGJA using endoscopic images. In this cohort and
learning study, we conducted a multicentre study across seven Chinese hospitals
between December 28, 2016 and December 30, 2024. It comprises 12,302 images
from 1,546 patients; 8,249 of them were employed for model training, while the
remaining were divided into the held-out (112 patients, 914 images), external
(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test
sets for evaluation. The proposed model employs DINOv2 (a vision foundation
model) and ResNet50 (a convolutional neural network) to extract features of
global appearance and local details of endoscopic images for EGJA staging
diagnosis. Our model demonstrates satisfactory performance for EGJA staging
diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and
0.8956, respectively. In contrast, among representative AI models, the best one
(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test
sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on
the held-out test set. Moreover, with the assistance of our model, the overall
accuracy for the trainee, competent, and expert endoscopists improves from
0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our
knowledge, our model is the first application of foundation models for EGJA
staging diagnosis and demonstrates great potential in both diagnostic accuracy
and efficiency.

</details>


### [180] [Tailored Transformation Invariance for Industrial Anomaly Detection](https://arxiv.org/abs/2509.17670)
*Mariette Schönfeld,Wannes Meert,Hendrik Blockeel*

Main category: cs.CV

TL;DR: LWinNN是一种基于局部窗口的工业异常检测方法，在kNN方法和复杂SOTA方法之间找到平衡点，通过有限平移不变性提高准确率并降低训练和测试时间。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法要么过于简单（kNN方法），要么训练成本过高。研究发现现有基准测试只需要对轻微平移具有鲁棒性，因此需要一种折中方案。

Method: 提出LWinNN方法，基于局部窗口创建kNN方法的中间方案，既不完全平移不变也不完全不平移不变，而是具有有限的平移不变性。

Result: 实验表明该方法显著提高了准确率，同时减少了训练和测试时间，在kNN方法和复杂SOTA方法之间建立了新的平衡点。

Conclusion: kNN方法与SOTA方法之间的差距仍可通过有效利用有限数据来缩小；需要更多空间多样性的基准测试，LWinNN可作为新的基线方法。

Abstract: Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision
Anomaly Detection that has been receiving increasing amounts of attention due
to its applicability to real-life scenarios. Recent research has focused on how
to extract the most informative features, contrasting older kNN-based methods
that use only pretrained features. These recent methods are much more expensive
to train however and could complicate real-life application. Careful study of
related work with regards to transformation invariance leads to the idea that
popular benchmarks require robustness to only minor translations. With this
idea we then formulate LWinNN, a local window based approach that creates a
middle ground between kNN based methods that have either complete or no
translation invariance. Our experiments demonstrate that this small change
increases accuracy considerably, while simultaneously decreasing both train and
test time. This teaches us two things: first, the gap between kNN-based
approaches and more complex state-of-the-art methodology can still be narrowed
by effective usage of the limited data available. Second, our assumption of
requiring only limited translation invariance highlights potential areas of
interest for future work and the need for more spatially diverse benchmarks,
for which our method can hopefully serve as a new baseline. Our code can be
found at https://github.com/marietteschonfeld/LWinNN .

</details>


### [181] [DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2509.17684)
*ThankGod Egbe,Peng Wang,Zhihao Guo,Zidong Chen*

Main category: cs.CV

TL;DR: 本文评估了DINOv3自监督视觉骨干网络在机器人操作中的视觉运动扩散策略学习效果，发现其在多个任务中优于或匹配传统的监督式ImageNet预训练骨干网络。


<details>
  <summary>Details</summary>
Motivation: 研究自监督大规模视觉模型是否能够替代传统的监督式预训练骨干网络，为机器人操作提供更有效、可泛化的感知前端。

Method: 在四个基准任务（Push-T、Lift、Can、Square）上，使用统一的FiLM条件扩散策略，比较DINOv3和ResNet-18在三种训练模式下的表现：从头训练、冻结权重和微调。

Result: 微调后的DINOv3在多个任务中匹配或超过ResNet-18；冻结的DINOv3仍具竞争力；自监督特征提高了样本效率和鲁棒性；在Can任务上成功率提升10%。

Conclusion: 自监督大规模视觉模型是动作扩散策略的有效通用感知前端，支持在机器人操作中进一步探索可扩展的无标签预训练方法。

Abstract: This paper evaluates DINOv3, a recent large-scale self-supervised vision
backbone, for visuomotor diffusion policy learning in robotic manipulation. We
investigate whether a purely self-supervised encoder can match or surpass
conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under
three regimes: training from scratch, frozen, and finetuned. Across four
benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned
diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds
ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating
strong transferable priors, and (iii) self-supervised features improve sample
efficiency and robustness. These results support self-supervised large visual
models as effective, generalizable perceptual front-ends for action diffusion
policies, motivating further exploration of scalable label-free pretraining in
robotic manipulation. Compared to using ResNet18 as a backbone, our approach
with DINOv3 achieves up to a 10% absolute increase in test-time success rates
on challenging tasks such as Can, and on-the-par performance in tasks like
Lift, PushT, and Square.

</details>


### [182] [FROQ: Observing Face Recognition Models for Efficient Quality Assessment](https://arxiv.org/abs/2509.17689)
*Žiga Babnik,Deepak Kumar Jain,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: FROQ是一种半监督、无需训练的FR质量评估方法，结合了监督方法的效率和无需训练的优势，通过中间表示和伪质量标签实现高质量评估。


<details>
  <summary>Details</summary>
Motivation: 传统FIQA方法需要大量监督训练，而无监督方法虽然无需训练但性能较低且速度较慢，需要一种兼顾效率和性能的解决方案。

Method: 利用FR模型中的特定中间表示来估计人脸图像质量，通过基于伪质量标签的简单校准步骤，结合新颖的基于样本扰动的无监督FIQA技术生成伪标签。

Result: 在4个先进FR模型和8个基准数据集上的实验表明，FROQ达到了与最先进方法相竞争的结果，具有强性能和高效运行时间。

Conclusion: FROQ成功实现了无需显式训练的高性能FIQA，为FR系统提供了可靠的质量评估解决方案。

Abstract: Face Recognition (FR) plays a crucial role in many critical (high-stakes)
applications, where errors in the recognition process can lead to serious
consequences. Face Image Quality Assessment (FIQA) techniques enhance FR
systems by providing quality estimates of face samples, enabling the systems to
discard samples that are unsuitable for reliable recognition or lead to
low-confidence recognition decisions. Most state-of-the-art FIQA techniques
rely on extensive supervised training to achieve accurate quality estimation.
In contrast, unsupervised techniques eliminate the need for additional training
but tend to be slower and typically exhibit lower performance. In this paper,
we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,
training-free approach that leverages specific intermediate representations
within a given FR model to estimate face-image quality, and combines the
efficiency of supervised FIQA models with the training-free approach of
unsupervised methods. A simple calibration step based on pseudo-quality labels
allows FROQ to uncover specific representations, useful for quality assessment,
in any modern FR model. To generate these pseudo-labels, we propose a novel
unsupervised FIQA technique based on sample perturbations. Comprehensive
experiments with four state-of-the-art FR models and eight benchmark datasets
show that FROQ leads to highly competitive results compared to the
state-of-the-art, achieving both strong performance and efficient runtime,
without requiring explicit training.

</details>


### [183] [Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.17702)
*Patrick Schmidt,Vasileios Belagiannis,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出了一种模型无关的深度边缘对齐损失，用于改进弱监督语义分割模型，利用机器人系统中常见的深度信息来提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人系统在新领域应用时需要大量昂贵的像素级密集标签来训练语义分割模型，而弱监督方法可以避免昂贵的标注过程。

Method: 生成像素级语义标签的方法基于图像级监督，并添加像素级深度信息作为额外的监督模态。

Result: 在PASCAL VOC、MS COCO验证集和HOPE静态登机分割数据集上，平均交并比分别提升了+5.439、+1.274和+16.416个点。

Conclusion: 该方法能够显著提升分割性能，且可以与其他损失函数结合获得更好效果，代码将公开。

Abstract: Autonomous robotic systems applied to new domains require an abundance of
expensive, pixel-level dense labels to train robust semantic segmentation
models under full supervision. This study proposes a model-agnostic Depth Edge
Alignment Loss to improve Weakly Supervised Semantic Segmentation models across
different datasets. The methodology generates pixel-level semantic labels from
image-level supervision, avoiding expensive annotation processes. While weak
supervision is widely explored in traditional computer vision, our approach
adds supervision with pixel-level depth information, a modality commonly
available in robotic systems. We demonstrate how our approach improves
segmentation performance across datasets and models, but can also be combined
with other losses for even better performance, with improvements up to +5.439,
+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /
MS COCO validation, and the HOPE static onboarding split, respectively. Our
code will be made publicly available.

</details>


### [184] [Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion](https://arxiv.org/abs/2509.17704)
*Bo Li,Yunkuo Lei,Tingting Bao,Yaxian Wang,Lingling Zhang,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经动力学驱动的耦合神经P系统（ND-CNPFuse）的多焦点图像融合方法，通过分析神经动力学特性生成高质量决策图，在四个经典数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式规则的方法和深度学习黑盒机制难以生成具有精确边界的高质量决策图，因此需要开发更准确可靠的融合方法。

Method: 引入第三代神经计算模型——耦合神经P系统，通过深入分析神经动力学特性，将源图像映射为可解释的脉冲矩阵，通过比较脉冲数量直接生成准确决策图，无需后处理。

Result: 在Lytro、MFFW、MFI-WHU和Real-MFF四个经典多焦点图像融合数据集上的实验结果表明，ND-CNPFuse达到了最先进的性能水平。

Conclusion: 基于神经动力学的CNP系统能够有效提升多焦点图像融合中决策图的准确性，为图像融合任务提供了新的解决方案。

Abstract: Multi-focus image fusion (MFIF) is a crucial technique in image processing,
with a key challenge being the generation of decision maps with precise
boundaries. However, traditional methods based on heuristic rules and deep
learning methods with black-box mechanisms are difficult to generate
high-quality decision maps. To overcome this challenge, we introduce
neurodynamics-driven coupled neural P (CNP) systems, which are third-generation
neural computation models inspired by spiking mechanisms, to enhance the
accuracy of decision maps. Specifically, we first conduct an in-depth analysis
of the model's neurodynamics to identify the constraints between the network
parameters and the input signals. This solid analysis avoids abnormal
continuous firing of neurons and ensures the model accurately distinguishes
between focused and unfocused regions, generating high-quality decision maps
for MFIF. Based on this analysis, we propose a
\textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model
(\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current
ideas of decision map generation, ND-CNPFuse distinguishes between focused and
unfocused regions by mapping the source image into interpretable spike
matrices. By comparing the number of spikes, an accurate decision map can be
generated directly without any post-processing. Extensive experimental results
show that ND-CNPFuse achieves new state-of-the-art performance on four
classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code
is available at https://github.com/MorvanLi/ND-CNPFuse.

</details>


### [185] [Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review](https://arxiv.org/abs/2509.17707)
*Emre Gülsoylu,Alhassan Abdelhalim,Derya Kara Boztas,Ole Grasse,Carlos Jahn,Simone Frintrop,Janick Edinger*

Main category: cs.CV

TL;DR: 本文回顾了63项关于使用计算机视觉识别多式联运装载单元（ILU）的实证研究，追踪了该领域从数字图像处理和传统机器学习到深度学习技术的发展历程。


<details>
  <summary>Details</summary>
Motivation: 多式联运装载单元（如集装箱、半挂车等）的高效识别在全球贸易中至关重要，但现有识别技术存在瓶颈，计算机视觉提供了成本效益高的替代方案。

Method: 系统回顾了1990-2025年间63项实证研究，分析了从早期数字图像处理、传统机器学习到当前深度学习技术的演进路径。

Result: 研究发现计算机视觉识别性能差异巨大（端到端准确率5%-96%），主要受限于缺乏公开基准数据集，并面临从字符识别向场景文本识别转变的新挑战。

Conclusion: 为推动该领域发展，需要标准化术语、开放数据集和共享源代码，未来研究方向包括针对ISO6346代码优化的无上下文文本识别。

Abstract: The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.

</details>


### [186] [RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion](https://arxiv.org/abs/2509.17712)
*Geonho Bang,Minjae Seong,Jisong Kim,Geunju Baek,Daye Oh,Junhyung Kim,Junho Koh,Jun Won Choi*

Main category: cs.CV

TL;DR: RCTDistill是一种基于时序融合的跨模态知识蒸馏方法，通过三个关键模块解决雷达-相机融合中的不确定性，在nuScenes和VoD数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的雷达-相机融合方法在性能上仍落后于LiDAR方法，且未充分考虑物体运动和传感器特定误差带来的不确定性。

Method: 提出RCTDistill方法，包含三个模块：RAKD（考虑距离和方位角误差）、TKD（解决动态物体时序错位）、RDKD（增强特征判别能力）。

Result: 在nuScenes和View-of-Delft数据集上达到最先进的雷达-相机融合性能，推理速度达26.2 FPS。

Conclusion: 该方法通过有效处理传感器不确定性和时序错位问题，显著提升了雷达-相机融合的3D目标检测性能。

Abstract: Radar-camera fusion methods have emerged as a cost-effective approach for 3D
object detection but still lag behind LiDAR-based methods in performance.
Recent works have focused on employing temporal fusion and Knowledge
Distillation (KD) strategies to overcome these limitations. However, existing
approaches have not sufficiently accounted for uncertainties arising from
object motion or sensor-specific errors inherent in radar and camera
modalities. In this work, we propose RCTDistill, a novel cross-modal KD method
based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge
Distillation (RAKD), Temporal Knowledge Distillation (TKD), and
Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider
the inherent errors in the range and azimuth directions, enabling effective
knowledge transfer from LiDAR features to refine inaccurate BEV
representations. TKD mitigates temporal misalignment caused by dynamic objects
by aligning historical radar-camera BEV features with current LiDAR
representations. RDKD enhances feature discrimination by distilling relational
knowledge from the teacher model, allowing the student to differentiate
foreground and background features. RCTDistill achieves state-of-the-art
radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)
datasets, with the fastest inference speed of 26.2 FPS.

</details>


### [187] [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](https://arxiv.org/abs/2509.17726)
*Javier Bisbal,Patrick Winter,Sebastian Jofre,Aaron Ponce,Sameer A. Ansari,Ramez Abdalla,Michael Markl,Oliver Welin Odeback,Sergio Uribe,Cristian Tejos,Julio Sotelo,Susanne Schnell,David Marlevi*

Main category: cs.CV

TL;DR: 提出基于深度学习的框架，用于从3D TOF-MRA分割中自动标记颅内动脉，并引入不确定性量化以提高可解释性和可靠性。评估了三种CNN架构，其中nnUNet表现最佳，并通过不确定性映射和血流速度验证了临床实用性。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉的精确解剖标记对脑血管诊断和血流动力学分析至关重要，但目前仍耗时且存在操作者间变异性。需要开发自动化、可靠的标记方法。

Method: 评估了三种卷积神经网络架构：带残差编码器块的UNet、增强注意力的CS-Net和自配置的nnUNet。采用测试时间增强和坐标引导策略进行不确定性量化，并与4D Flow MRI数据进行血流速度验证。

Result: nnUNet获得最高标记性能（平均Dice得分：0.922；平均表面距离：0.387 mm），在解剖复杂血管中具有更好的鲁棒性。不确定性映射可靠指示解剖模糊区域，自动与手动标记的血流速度无显著差异。

Conclusion: 该框架提供了可扩展、准确且具有不确定性感知的自动化脑血管标记解决方案，支持下游血流动力学分析并促进临床整合。

Abstract: Accurate anatomical labeling of intracranial arteries is essential for
cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming
and subject to interoperator variability. We present a deep learning-based
framework for automated artery labeling from 3D Time-of-Flight Magnetic
Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating
uncertainty quantification to enhance interpretability and reliability. We
evaluated three convolutional neural network architectures: (1) a UNet with
residual encoder blocks, reflecting commonly used baselines in vascular
labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and
spatial attention mechanisms for enhanced curvilinear structure recognition;
and (3) nnUNet, a self-configuring framework that automates preprocessing,
training, and architectural adaptation based on dataset characteristics. Among
these, nnUNet achieved the highest labeling performance (average Dice score:
0.922; average surface distance: 0.387 mm), with improved robustness in
anatomically complex vessels. To assess predictive confidence, we implemented
test-time augmentation (TTA) and introduced a novel coordinate-guided strategy
to reduce interpolation errors during augmented inference. The resulting
uncertainty maps reliably indicated regions of anatomical ambiguity,
pathological variation, or manual labeling inconsistency. We further validated
clinical utility by comparing flow velocities derived from automated and manual
labels in co-registered 4D Flow MRI datasets, observing close agreement with no
statistically significant differences. Our framework offers a scalable,
accurate, and uncertainty-aware solution for automated cerebrovascular
labeling, supporting downstream hemodynamic analysis and facilitating clinical
integration.

</details>


### [188] [Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA](https://arxiv.org/abs/2509.17743)
*Chenglin Li,Feng Han,Feng Tao,Ruilin Li,Qianglong Chen,Jingqi Tong,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: FS-VisPR框架是一种自适应视觉程序推理方法，通过快慢推理机制平衡简单和复杂视觉任务的效率与准确性，在长视频问答任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖闭源模型、缺乏系统推理能力以及在长视频问答任务中的困难，模仿人类推理过程设计自适应推理框架。

Method: 设计高效视觉模块支持长视频任务，构建快慢推理数据集训练FS-LLM，采用两阶段推理机制：简单查询直接由VideoLLMs处理，复杂查询触发视觉程序推理，并包含低置信度答案的慢推理回退机制。

Result: 在LVBench上达到50.4%准确率，超越GPT-4o，在VideoMME上与Qwen2.5VL-72B性能相当，显著提升视觉程序工作流的效率和可靠性。

Conclusion: FS-VisPR框架通过自适应快慢推理机制有效平衡了视觉任务的效率与准确性，为长视频问答任务提供了可靠的解决方案。

Abstract: Large language models (LLMs) have shown promise in generating program
workflows for visual tasks. However, previous approaches often rely on
closed-source models, lack systematic reasoning, and struggle with long-form
video question answering (videoQA). To address these challenges, we introduce
the FS-VisPR framework, an adaptive visual program reasoning approach that
balances fast reasoning for simple queries with slow reasoning for difficult
ones. First, we design efficient visual modules (e.g., key clip retrieval and
subtitle retrieval) to support long-form video tasks. Then, we construct a
diverse and high-quality fast-slow reasoning dataset with a strong LLM to align
open-source language models' ability to generate visual program workflows as
FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple
queries are directly solved by VideoLLMs, while difficult ones invoke visual
program reasoning, motivated by human-like reasoning processes. During this
process, low-confidence fast-thinking answers will trigger a second-stage
slow-reasoning process, and a fallback mechanism to fast reasoning is activated
if the program execution fails. Moreover, we improve visual programs through
parameter search during both training and inference. By adjusting the
parameters of the visual modules within the program, multiple variants are
generated: during training, programs that yield correct answers are selected,
while during inference, the program with the highest confidence result is
applied. Experiments show that FS-VisPR improves both efficiency and
reliability in visual program workflows. It achieves 50.4% accuracy on LVBench,
surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.

</details>


### [189] [Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance](https://arxiv.org/abs/2509.17757)
*Hongxing Fan,Lipeng Wang,Haohua Chen,Zehuan Huang,Jiangtao Wu,Lu Sheng*

Main category: cs.CV

TL;DR: 提出基于协作多智能体推理的框架来解决图像修复中的遮挡物体不可见部分生成问题，通过多智能体协作分析遮挡关系和边界扩展，结合细粒度语义指导实现高质量修复。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在数据需求、泛化能力和渐进式流程中的误差累积问题，实现更精确的遮挡物体修复。

Method: 使用多智能体协作推理框架，包括分析遮挡关系的智能体和生成细粒度文本描述的智能体，结合扩散变换器的注意力图直接生成分层RGBA输出。

Result: 框架在视觉质量上达到最先进水平，能够准确合成物体并避免重新生成遮挡物或其他不需要的元素。

Conclusion: 该协作多智能体推理框架有效解决了图像修复中的关键挑战，为图像编辑和增强现实应用提供了高质量的解决方案。

Abstract: Amodal completion, generating invisible parts of occluded objects, is vital
for applications like image editing and AR. Prior methods face challenges with
data needs, generalization, or error accumulation in progressive pipelines. We
propose a Collaborative Multi-Agent Reasoning Framework based on upfront
collaborative reasoning to overcome these issues. Our framework uses multiple
agents to collaboratively analyze occlusion relationships and determine
necessary boundary expansion, yielding a precise mask for inpainting.
Concurrently, an agent generates fine-grained textual descriptions, enabling
Fine-Grained Semantic Guidance. This ensures accurate object synthesis and
prevents the regeneration of occluders or other unwanted elements, especially
within large inpainting areas. Furthermore, our method directly produces
layered RGBA outputs guided by visible masks and attention maps from a
Diffusion Transformer, eliminating extra segmentation. Extensive evaluations
demonstrate our framework achieves state-of-the-art visual quality.

</details>


### [190] [Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2509.17762)
*Sitian Shen,Georgi Pramatarov,Yifu Tao,Daniele De Martini*

Main category: cs.CV

TL;DR: Neural-MMGS是一个新颖的神经3D高斯框架，用于多模态大规模场景重建，通过每个高斯的紧凑可学习嵌入融合多种感知模态。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模场景重建工作虽然整合了LiDAR数据提供几何约束，但LiDAR丰富的物理属性尚未充分挖掘。语义信息通常仅用于物体检索，但可以为场景重建提供有价值的高层上下文。传统方法将这些属性作为单独参数附加到高斯上，增加了内存使用并限制了跨模态信息交换。

Method: 提出将图像、LiDAR和语义等多模态信息融合到紧凑可学习的嵌入中，隐式编码每个高斯的视觉、物理和语义特征。然后训练轻量级神经解码器将这些嵌入映射到高斯参数，以更低的内存开销重建每个感知模态。

Result: 在Oxford Spires数据集上实现了更高质量的重建，在KITTI-360数据集上达到了与当前LiDAR新视角合成方法竞争的结果，同时存储消耗更少。

Conclusion: Neural-MMGS通过多模态融合和紧凑嵌入表示，在保持重建质量的同时显著降低了内存开销，提高了大规模场景重建的可扩展性。

Abstract: This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.

</details>


### [191] [Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics](https://arxiv.org/abs/2509.17769)
*Yang Li,Xinyi Zeng,Zhe Xue,Pinxian Zeng,Zikai Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为RPLIF的新方法，通过尖峰触发阈值动态将不应期机制整合到LIF神经元中，以更好地模拟生物神经元特性，提高SNN的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的SNN神经元模型（如IF和LIF）忽略了生物神经元的不应期特性，这可能导致神经元过度兴奋和异常信号干扰。不应期机制对于防止过度兴奋和减轻异常信号干扰至关重要。

Method: 提出RPLIF方法，通过尖峰触发阈值动态将不应期机制整合到LIF神经元中。该方法计算效率高，无缝集成，且开销可忽略不计。

Result: RPLIF在多个数据集上取得了最先进的性能：Cifar10-DVS（82.40%）、N-Caltech101（83.35%）和DVS128 Gesture（97.22%），且使用更少的时间步长和低延迟。

Conclusion: 将不应期机制整合到LIF神经元中是一种简单有效的方法，能显著提高SNN的性能、鲁棒性和效率，同时保持计算效率。

Abstract: As the third generation of neural networks, spiking neural networks (SNNs)
have recently gained widespread attention for their biological plausibility,
energy efficiency, and effectiveness in processing neuromorphic datasets. To
better emulate biological neurons, various models such as Integrate-and-Fire
(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.
However, these neuron models overlook the refractory period, a fundamental
characteristic of biological neurons. Research on excitable neurons reveal that
after firing, neurons enter a refractory period during which they are
temporarily unresponsive to subsequent stimuli. This mechanism is critical for
preventing over-excitation and mitigating interference from aberrant signals.
Therefore, we propose a simple yet effective method to incorporate the
refractory period into spiking LIF neurons through spike-triggered threshold
dynamics, termed RPLIF. Our method ensures that each spike accurately encodes
neural information, effectively preventing neuron over-excitation under
continuous inputs and interference from anomalous inputs. Incorporating the
refractory period into LIF neurons is seamless and computationally efficient,
enhancing robustness and efficiency while yielding better performance with
negligible overhead. To the best of our knowledge, RPLIF achieves
state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)
with fewer timesteps and demonstrates superior performance on DVS128
Gesture(97.22%) at low latency.

</details>


### [192] [I2VWM: Robust Watermarking for Image to Video Generation](https://arxiv.org/abs/2509.17773)
*Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出I2VWM框架，通过鲁棒扩散距离和视频模拟噪声层解决图像到视频生成中的跨模态水印追踪问题


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在单模态下表现良好，但无法在图像到视频生成场景中追踪源图像，存在被滥用于虚假信息和欺诈的风险

Method: 提出鲁棒扩散距离概念衡量水印信号的时间持续性，采用视频模拟噪声层训练和基于光流的对齐模块推理

Result: 在开源和商业I2V模型上的实验表明，I2VWM显著提高了水印鲁棒性同时保持不可感知性

Conclusion: I2VWM为生成视频时代的跨模态水印建立了新范式

Abstract: The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}

</details>


### [193] [From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes](https://arxiv.org/abs/2509.17789)
*Guoxi Huang,Haoran Wang,Zipeng Qi,Wenjun Lu,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: R-Splatting是一个统一框架，将水下图像恢复与3D高斯泼溅相结合，通过多增强视图集成、轻量级光照生成器和不确定性感知不透明度优化，提升水下3D重建的渲染质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 水下图像退化对3D重建构成重大挑战，简化的物理模型在复杂场景中往往失效。需要一种能够同时改善渲染质量和几何保真度的解决方案。

Method: 1. 集成多种UIR模型生成的增强视图到单一重建流程；2. 使用轻量级光照生成器采样潜在编码支持多样化但一致的渲染；3. 采用对比损失确保解耦和稳定的光照表示；4. 提出不确定性感知不透明度优化，将不透明度建模为随机函数来正则化训练。

Result: 在Seathru-NeRF和新BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何精度方面均优于强基线方法。

Conclusion: R-Splatting通过将水下图像恢复与3D高斯泼溅相结合，有效解决了复杂水下场景中的3D重建问题，显著提升了重建质量。

Abstract: Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.

</details>


### [194] [Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding](https://arxiv.org/abs/2509.17792)
*S M A Sharif,Abdur Rehman,Fayaz Ali Dharejo,Radu Timofte,Rizwan Ali Naqvi*

Main category: cs.CV

TL;DR: 本文提出了一种基于学习潜在先验推理的全能图像恢复方法，通过自适应特征选择、空间定位和退化语义分析来解决多样化图像退化问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常受到雾霾、雨雪、低光照等多种空间多样化退化的影响，现有方法依赖外部文本提示或手工架构先验，限制了在未知或混合退化情况下的泛化能力。

Method: 将全能图像恢复重新定义为学习潜在先验推理，设计轻量级解码模块，通过自适应特征选择、空间定位和退化语义分析实现空间自适应恢复。

Result: 在六种常见退化任务、五种复合设置和未见退化情况下，该方法在PSNR指标上平均提升1.68 dB，且效率提高三倍。

Conclusion: 提出的基于潜在先验推理的方法在图像恢复任务中表现出优越的性能和泛化能力，为处理多样化图像退化提供了有效解决方案。

Abstract: Real-world images often suffer from spatially diverse degradations such as
haze, rain, snow, and low-light, significantly impacting visual quality and
downstream vision tasks. Existing all-in-one restoration (AIR) approaches
either depend on external text prompts or embed hand-crafted architectural
priors (e.g., frequency heuristics); both impose discrete, brittle assumptions
that weaken generalization to unseen or mixed degradations. To address this
limitation, we propose to reframe AIR as learned latent prior inference, where
degradation-aware representations are automatically inferred from the input
without explicit task cues. Based on latent priors, we formulate AIR as a
structured reasoning paradigm: (1) which features to route (adaptive feature
selection), (2) where to restore (spatial localization), and (3) what to
restore (degradation semantics). We design a lightweight decoding module that
efficiently leverages these latent encoded cues for spatially-adaptive
restoration. Extensive experiments across six common degradation tasks, five
compound settings, and previously unseen degradations demonstrate that our
method outperforms state-of-the-art (SOTA) approaches, achieving an average
PSNR improvement of 1.68 dB while being three times more efficient.

</details>


### [195] [Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections](https://arxiv.org/abs/2509.17805)
*Dong Chen,Huili Peng,Yong Hu,Kenneth MC. Cheung*

Main category: cs.CV

TL;DR: 本研究系统量化了相机视角（正面vs侧面）对2D无标记步态分析准确性的影响，发现侧面视角在矢状面运动学参数上表现更优，而正面视角在躯干对称性参数上更准确。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于相机视角如何影响2D无标记步态分析准确性的系统性证据，这限制了该方法在临床实践中的有效应用。

Method: 使用18名受试者的步态数据，同时记录正面、侧面和3D运动捕捉数据。采用YOLOv8进行姿态估计，使用DTW、MCC、KLD和IE四种指标评估一致性，并进行统计检验。

Result: 侧面视角在步长和膝关节旋转等矢状面运动学参数上显著优于正面视角，而正面视角在躯干旋转和手腕到髋部中点距离等对称性参数上表现更好，效应量为中等至大。

Conclusion: 相机视角对步态参数准确性有重要影响，侧面视角适合矢状面运动学分析，正面视角适合躯干对称性分析，未来应基于疾病特点采用多视角设置。

Abstract: Objective: To systematically quantify the effect of the camera view (frontal
vs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D
motion capture ground truth. Methods: Gait data from 18 subjects were recorded
simultaneously using frontal, lateral and 3D motion capture systems. Pose
estimation used YOLOv8. Four metrics were assessed to evaluate agreement:
Dynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation
(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution
differences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank
tests (significance: $p < 0.05$) and Cliff's delta ($\delta$) were used to
measure statistical differences and effect sizes. Results: Lateral views
significantly outperformed frontal views for sagittal plane kinematics: step
length (DTW: $53.08 \pm 24.50$ vs. $69.87 \pm 25.36$, $p = 0.005$) and knee
rotation (DTW: $106.46 \pm 38.57$ vs. $155.41 \pm 41.77$, $p = 0.004$). Frontal
views were superior for symmetry parameters: trunk rotation (KLD: $0.09 \pm
0.06$ vs. $0.30 \pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:
$105.77 \pm 29.72$ vs. $75.20 \pm 20.38$, $p = 0.003$). Effect sizes were
medium-to-large ($\delta: 0.34$--$0.76$). Conclusion: Camera view critically
impacts gait parameter accuracy. Lateral views are optimal for sagittal
kinematics; frontal views excel for trunk symmetry. Significance: This first
systematic evidence enables data-driven camera deployment in 2D gait analysis,
enhancing clinical utility. Future implementations should leverage both views
via disease-oriented setups.

</details>


### [196] [Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training](https://arxiv.org/abs/2509.17816)
*Brown Ebouky,Ajad Chhatkuli,Cristiano Malossi,Christoph Studer,Roy Assaf,Andrea Bartezzaghi*

Main category: cs.CV

TL;DR: 提出GLARE方法，通过持续自监督预训练来适应视觉基础模型到新领域，特别针对语义分割任务，使用轻量级适配器实现高效迁移


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习模型主要基于通用数据集预训练，但在新领域和有限数据情况下，特别是密集预测任务如语义分割，预训练适应方法研究不足

Method: GLARE方法包含局部一致性增强和区域一致性约束，使用Vision Transformers初始化权重，仅更新轻量级UniAdapter适配器模块，保持主干网络冻结

Result: 在多个语义分割基准测试中，GLARE方法在不同领域都显著提升了下游任务性能，计算和参数开销最小

Conclusion: GLARE提供了一种高效的无监督领域适应方法，通过持续自监督预训练有效提升语义分割性能，具有实际应用价值

Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training
foundation models by leveraging large-scale unlabeled datasets, often producing
representations with strong generalization capabilities. These models are
typically pre-trained on general-purpose datasets such as ImageNet and
subsequently adapted to various downstream tasks through finetuning. While
recent advances have explored parameter-efficient strategies for adapting
pre-trained models, extending SSL pre-training itself to new domains -
particularly under limited data regimes and for dense prediction tasks -
remains underexplored. In this work, we address the problem of adapting vision
foundation models to new domains in an unsupervised and data-efficient manner,
specifically targeting downstream semantic segmentation. We propose GLARE
(Global Local and Regional Enforcement), a novel continual self-supervised
pre-training task designed to enhance downstream segmentation performance.
GLARE introduces patch-level augmentations to encourage local consistency and
incorporates a regional consistency constraint that leverages spatial semantics
in the data. For efficient continual pre-training, we initialize Vision
Transformers (ViTs) with weights from existing SSL models and update only
lightweight adapter modules - specifically UniAdapter - while keeping the rest
of the backbone frozen. Experiments across multiple semantic segmentation
benchmarks on different domains demonstrate that GLARE consistently improves
downstream performance with minimal computational and parameter overhead.

</details>


### [197] [ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818)
*Yiyang Chen,Xuanhua He,Xiujun Ma,Yue Ma*

Main category: cs.CV

TL;DR: ContextFlow是一个无需训练的基于DiT的视频对象编辑框架，通过高阶Rectified Flow求解器和自适应上下文增强机制，解决了现有方法在保真度和时间一致性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的无需训练视频对象编辑方法存在两个主要限制：一阶求解器导致的反转不准确，以及粗糙的"硬"特征替换引起的上下文冲突。在DiT架构中，这些挑战更加突出，因为先前的层选择启发式方法不适用。

Method: ContextFlow采用高阶Rectified Flow求解器建立稳健的编辑基础，核心是自适应上下文增强机制，通过将重建路径和编辑路径的Key-Value对连接来丰富自注意力上下文，而不是直接替换特征。同时基于Guidance Responsiveness Metric进行数据驱动分析，识别任务特定的关键层。

Result: 大量实验表明，ContextFlow显著优于现有的无需训练方法，甚至超越了几种最先进的基于训练的方法，能够产生时间一致、高保真的结果。

Conclusion: ContextFlow为基于DiT的视频对象编辑提供了一个有效的训练免费解决方案，通过创新的上下文增强和层选择策略，成功解决了保真度和时间一致性的关键挑战。

Abstract: Training-free video object editing aims to achieve precise object-level
manipulation, including object insertion, swapping, and deletion. However, it
faces significant challenges in maintaining fidelity and temporal consistency.
Existing methods, often designed for U-Net architectures, suffer from two
primary limitations: inaccurate inversion due to first-order solvers, and
contextual conflicts caused by crude "hard" feature replacement. These issues
are more challenging in Diffusion Transformers (DiTs), where the unsuitability
of prior layer-selection heuristics makes effective guidance challenging. To
address these limitations, we introduce ContextFlow, a novel training-free
framework for DiT-based video object editing. In detail, we first employ a
high-order Rectified Flow solver to establish a robust editing foundation. The
core of our framework is Adaptive Context Enrichment (for specifying what to
edit), a mechanism that addresses contextual conflicts. Instead of replacing
features, it enriches the self-attention context by concatenating Key-Value
pairs from parallel reconstruction and editing paths, empowering the model to
dynamically fuse information. Additionally, to determine where to apply this
enrichment (for specifying where to edit), we propose a systematic, data-driven
analysis to identify task-specific vital layers. Based on a novel Guidance
Responsiveness Metric, our method pinpoints the most influential DiT blocks for
different tasks (e.g., insertion, swapping), enabling targeted and highly
effective guidance. Extensive experiments show that ContextFlow significantly
outperforms existing training-free methods and even surpasses several
state-of-the-art training-based approaches, delivering temporally coherent,
high-fidelity results.

</details>


### [198] [Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology](https://arxiv.org/abs/2509.17847)
*Saghir Alfasly,Wataru Uegami,MD Enamul Hoq,Ghazal Alabtah,H. R. Tizhoosh*

Main category: cs.CV

TL;DR: 提出了一种用于组织病理学图像合成的潜在扩散模型，通过双条件方法结合语义分割图和特定组织视觉裁剪，生成具有精确区域注释的高保真异质性组织图像。


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学合成数据生成面临的挑战：保持组织异质性、捕捉细微形态特征以及扩展到未标注数据集，为计算病理学提供多样化的标注数据。

Method: 使用潜在扩散模型，采用双条件方法结合语义分割图和原始组织裁剪。对于未标注数据，引入自监督扩展，使用基础模型嵌入将全玻片图像聚类为100种组织类型，自动生成伪语义图进行训练。

Result: 在Camelyon16数据集上，提示引导合成将Frechet距离从430.1降低到72.0（6倍改进），在Panda和TCGA数据集上降低2-3倍。仅使用合成数据训练的DeepLabv3+模型在Camelyon16和Panda上分别达到0.71和0.95的IoU，接近真实数据基线（0.72和0.96）。

Conclusion: 该框架能够扩展到11,765个TCGA全玻片图像而无需手动标注，为生成多样化、标注的组织病理学数据提供了实用解决方案，解决了计算病理学中的关键瓶颈。

Abstract: Synthetic data generation in histopathology faces unique challenges:
preserving tissue heterogeneity, capturing subtle morphological features, and
scaling to unannotated datasets. We present a latent diffusion model that
generates realistic heterogeneous histopathology images through a novel
dual-conditioning approach combining semantic segmentation maps with
tissue-specific visual crops. Unlike existing methods that rely on text prompts
or abstract visual embeddings, our approach preserves critical morphological
details by directly incorporating raw tissue crops from corresponding semantic
regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches
ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we
introduce a self-supervised extension that clusters whole-slide images into 100
tissue types using foundation model embeddings, automatically generating
pseudo-semantic maps for training. Our method synthesizes high-fidelity images
with precise region-wise annotations, achieving superior performance on
downstream segmentation tasks. When evaluated on annotated datasets, models
trained on our synthetic data show competitive performance to those trained on
real data, demonstrating the utility of controlled heterogeneous tissue
generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet
Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower
FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on
synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within
1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA
whole-slide images without manual annotations, our framework offers a practical
solution for an urgent need for generating diverse, annotated histopathology
data, addressing a critical bottleneck in computational pathology.

</details>


### [199] [ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos](https://arxiv.org/abs/2509.17864)
*Shi Chen,Erik Sandström,Sandro Lombardi,Siyuan Li,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种在线动态场景重建方法，通过SLAM系统分离静态和动态部分，使用运动掩码策略进行鲁棒位姿跟踪，并利用渐进式运动支架图重建动态部分。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法通常仅移除动态部分或需要RGB-D输入，离线方法无法扩展到长视频序列，而基于transformer的前馈方法缺乏全局一致性和外观细节。

Method: 在SLAM系统中分离静态和动态部分，采用新颖的运动掩码策略进行鲁棒位姿跟踪，动态部分通过渐进式运动支架图进行重建。

Result: 方法能够生成与离线方法竞争的新视角渲染结果，并在跟踪性能上与最先进的动态SLAM方法相当。

Conclusion: 该方法实现了真正实用的动态3D重建，具备在线操作、全局位姿和地图一致性、详细外观建模能力，并能灵活处理RGB和RGB-D输入。

Abstract: Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.

</details>


### [200] [Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](https://arxiv.org/abs/2509.17901)
*Geewook Kim,Minjoon Seo*

Main category: cs.CV

TL;DR: 该论文质疑当前视频理解模型对音频的忽视，通过实验发现现有视频基准测试中音频作用有限，但构建了音频敏感数据集证明音频的重要性，并提出了轻量级音频压缩方法。


<details>
  <summary>Details</summary>
Motivation: 现代多模态大语言模型声称具备'视频理解'能力，但大多评估使用静音视频或忽略音频。作者质疑音频对视频理解的实际重要性，并希望填补当前学术实践与现实期望之间的差距。

Method: 基于LLaVA-OneVision架构，附加语音/音频编码器（如Whisper），使用基于Mamba状态空间的轻量级令牌压缩器处理音频令牌爆炸问题，并构建音频敏感数据集AVQA-Hard和Music-AVQA-Hard进行验证。

Result: 音频在现有视频基准测试中提升有限，但在音频敏感子集上具有决定性作用。提出的音频压缩方法有效解决了音频令牌过多的问题。

Conclusion: 当前视频理解评估存在缺陷，音频在实际应用中至关重要。作者提供了可扩展的音频-视频Video-LLM工具，并开源了模型和代码。

Abstract: Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.

</details>


### [201] [SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI](https://arxiv.org/abs/2509.17925)
*Yuanhan Wang,Yifei Chen,Shuo Jiang,Wenjing Yu,Mingxuan Liu,Beining Wu,Jinying Zong,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: SmaRT是一个用于脑肿瘤MRI分割的源无关测试时自适应框架，通过风格调制增强、双分支动量策略和结构先验来解决领域偏移问题，在资源匮乏和儿科数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的脑肿瘤分割模型在领域偏移（如扫描仪差异、协议变异和人群异质性）下性能下降，特别是在低资源和儿科队列中，传统的测试时自适应方法存在不稳定和结构不一致的问题。

Method: SmaRT框架集成了风格感知增强以减少外观差异，双分支动量策略用于稳定伪标签优化，以及结构先验来确保一致性、完整性和连通性。

Result: 在撒哈拉以南非洲和儿科胶质瘤数据集上的广泛评估显示，SmaRT在Dice准确率和边界精度上显著优于现有最先进方法。

Conclusion: SmaRT弥合了算法进步与公平临床适用性之间的差距，支持MRI神经肿瘤工具在不同临床环境中的稳健部署。

Abstract: Reliable brain tumor segmentation in MRI is indispensable for treatment
planning and outcome monitoring, yet models trained on curated benchmarks often
fail under domain shifts arising from scanner and protocol variability as well
as population heterogeneity. Such gaps are especially severe in low-resource
and pediatric cohorts, where conventional test-time or source-free adaptation
strategies often suffer from instability and structural inconsistency. We
propose SmaRT, a style-modulated robust test-time adaptation framework that
enables source-free cross-domain generalization. SmaRT integrates style-aware
augmentation to mitigate appearance discrepancies, a dual-branch momentum
strategy for stable pseudo-label refinement, and structural priors enforcing
consistency, integrity, and connectivity. This synergy ensures both adaptation
stability and anatomical fidelity under extreme domain shifts. Extensive
evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT
consistently outperforms state-of-the-art methods, with notable gains in Dice
accuracy and boundary precision. Overall, SmaRT bridges the gap between
algorithmic advances and equitable clinical applicability, supporting robust
deployment of MRI-based neuro-oncology tools in diverse clinical environments.
Our source code is available at https://github.com/baiyou1234/SmaRT.

</details>


### [202] [Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching](https://arxiv.org/abs/2509.17931)
*Zhuo Xiao,Fugen Zhou,Jingjing Wang,Chongyu He,Bo Liu,Haitao Sun,Zhe Ji,Yuliang Jiang,Junjie Wang,Qiuwen Wu*

Main category: cs.CV

TL;DR: 提出了一种基于HRNet的无锚点网络，将针头定位重新定义为针尖-手柄检测和匹配问题，通过热图回归和极角预测来精确定位针尖和手柄中心，并使用贪婪匹配合并方法重建3D针路径。


<details>
  <summary>Details</summary>
Motivation: 术中CT图像中多针头精确定位对于盆腔种子植入近距离放疗至关重要，但由于图像对比度差和针头粘连问题，该任务具有挑战性。

Method: 使用基于HRNet的无锚点网络提取多尺度特征，通过分离的分支进行热图回归和极角预测来检测针尖和手柄；提出贪婪匹配合并方法解决不平衡分配问题，迭代选择最可能的针尖-手柄对并基于距离度量合并。

Result: 在100名患者数据集上的评估显示，该方法相比基于nnUNet的分割方法具有更高的精确度和F1分数。

Conclusion: 该方法为复杂临床场景中的针头定位提供了更鲁棒和准确的解决方案。

Abstract: Accurate multi-needle localization in intraoperative CT images is crucial for
optimizing seed placement in pelvic seed implant brachytherapy. However, this
task is challenging due to poor image contrast and needle adhesion. This paper
presents a novel approach that reframes needle localization as a tip-handle
detection and matching problem to overcome these difficulties. An anchor-free
network, based on HRNet, is proposed to extract multi-scale features and
accurately detect needle tips and handles by predicting their centers and
orientations using decoupled branches for heatmap regression and polar angle
prediction. To associate detected tips and handles into individual needles, a
greedy matching and merging (GMM) method designed to solve the unbalanced
assignment problem with constraints (UAP-C) is presented. The GMM method
iteratively selects the most probable tip-handle pairs and merges them based on
a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100
patients, the proposed method demonstrates superior performance, achieving
higher precision and F1 score compared to a segmentation-based method utilizing
the nnUNet model,thereby offering a more robust and accurate solution for
needle localization in complex clinical scenarios.

</details>


### [203] [Can multimodal representation learning by alignment preserve modality-specific information?](https://arxiv.org/abs/2509.17943)
*Romain Thoreau,Jessie Levillain,Dawa Derksen*

Main category: cs.CV

TL;DR: 本文研究了多模态卫星数据融合中对比学习方法的信息损失问题，通过理论分析和数值实验证明了现有对齐策略可能导致任务相关信息丢失，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前多模态卫星数据融合方法主要利用不同模态数据之间的空间对齐来促进潜在空间的语义对齐，但这些方法可能无法保留跨模态不共享的任务相关信息。作者旨在揭示这种信息损失的根本原因。

Method: 首先在简化假设下进行理论分析，证明对齐策略何时会导致信息损失；然后在更现实的设置下进行数值实验来验证理论发现。

Result: 理论和实验证据表明，现有的对比学习方法在多模态卫星数据融合中确实存在信息损失问题，特别是在保留跨模态不共享的任务相关信息方面。

Conclusion: 研究结果为多模态卫星数据对比学习的新发展提供了理论基础和实证支持，指出了现有方法的局限性并提出了改进方向。

Abstract: Combining multimodal data is a key issue in a wide range of machine learning
tasks, including many remote sensing problems. In Earth observation, early
multimodal data fusion methods were based on specific neural network
architectures and supervised learning. Ever since, the scarcity of labeled data
has motivated self-supervised learning techniques. State-of-the-art multimodal
representation learning techniques leverage the spatial alignment between
satellite data from different modalities acquired over the same geographic area
in order to foster a semantic alignment in the latent space. In this paper, we
investigate how this methods can preserve task-relevant information that is not
shared across modalities. First, we show, under simplifying assumptions, when
alignment strategies fundamentally lead to an information loss. Then, we
support our theoretical insight through numerical experiments in more realistic
settings. With those theoretical and empirical evidences, we hope to support
new developments in contrastive learning for the combination of multimodal
satellite data. Our code and data is publicly available at
https://github.com/Romain3Ch216/alg_maclean_25.

</details>


### [204] [DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels](https://arxiv.org/abs/2509.17951)
*Kai Li,Xingxing Weng,Yupeng Deng,Yu Meng,Chao Pang,Gui-Song Xia,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 提出DragOSM方法，通过引入对齐标记概念来纠正OpenStreetMap历史标签与遥感图像中建筑物屋顶和足迹之间的位置偏差，将标签对齐建模为交互式去噪过程。


<details>
  <summary>Details</summary>
Motivation: 现有基于分割的方法在倾斜遥感图像中表现不佳，因为屋顶和足迹存在显著位移，且立面像素与屋顶边界融合。虽然OpenStreetMap等开放矢量地图标注可用于倾斜图像标注，但这些历史标签存在位置偏差且通常只有单一标注（屋顶或足迹），无法准确描述建筑结构。

Method: 提出DragOSM模型，引入对齐标记概念编码校正向量来指导标签校正。将标签对齐建模为交互式去噪过程，将位置偏差建模为高斯分布。训练时通过随机高斯扰动模拟错位来学习校正误差，推理时迭代优化输入标签的位置。

Result: 构建了包含179,265个建筑物、5,473张图像的新数据集ReBO，实验结果表明DragOSM方法有效。

Conclusion: DragOSM能够有效对齐错位的历史标签与建筑物屋顶和足迹，解决了倾斜遥感图像中建筑物标注的挑战。

Abstract: Extracting polygonal roofs and footprints from remote sensing images is
critical for large-scale urban analysis. Most existing methods rely on
segmentation-based models that assume clear semantic boundaries of roofs, but
these approaches struggle in off- nadir images, where the roof and footprint
are significantly displaced, and facade pixels are fused with the roof
boundary. With the increasing availability of open vector map annotations,
e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation
has become viable because remote sensing images are georeferenced once
captured. However, these historical labels commonly suffer from significant
positional discrepancies with new images and only have one annotation (roof or
footprint), which fails to describe the correct structures of a building. To
address these discrepancies, we first introduce a concept of an alignment
token, which encodes the correction vector to guide the label correction. Based
on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel
model designed to align dislocated historical labels with roofs and footprints.
Specifically, DragOSM formulates the label alignment as an interactive
denoising process, modeling the positional discrepancy as a Gaussian
distribution. During training, it learns to correct these errors by simulating
misalignment with random Gaussian perturbations; during inference, it
iteratively refines the positions of input labels. To validate our method, we
further present a new dataset, Repairing Buildings in OSM (ReBO), comprising
179,265 buildings with both OpenStreetMap and manually corrected annotations
across 5,473 images from 41 cities. Experimental results on ReBO demonstrate
the effectiveness of DragOSM. Code, dataset, and trained models are publicly
available at https://github.com/likaiucas/DragOSM.git.

</details>


### [205] [Breaking the Discretization Barrier of Continuous Physics Simulation Learning](https://arxiv.org/abs/2509.17955)
*Fan Xu,Hao Wu,Nan Wang,Lilan Peng,Kun Wang,Wei Gong,Xibin Zhao*

Main category: cs.CV

TL;DR: CoPS是一种纯数据驱动方法，用于从部分观测中建模连续物理模拟，通过多尺度图ODE和神经自校正模块实现时空连续建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法受固定时空离散化约束，难以从稀疏、无结构分布的观测中捕捉高度非线性特征，传统数值方法依赖性强或未能真正克服离散化限制。

Method: 使用乘法滤波器网络融合编码空间信息，定制几何网格并通过消息传递机制映射特征，设计多尺度图ODE建模连续时间动力学，引入基于马尔可夫的神经自校正模块辅助约束连续外推。

Result: 综合实验表明CoPS在各种场景下的时空连续建模中优于现有最先进方法。

Conclusion: CoPS成功解决了从部分观测中建模复杂时空演化物理动力学的挑战，实现了真正的连续物理模拟。

Abstract: The modeling of complicated time-evolving physical dynamics from partial
observations is a long-standing challenge. Particularly, observations can be
sparsely distributed in a seemingly random or unstructured manner, making it
difficult to capture highly nonlinear features in a variety of scientific and
engineering problems. However, existing data-driven approaches are often
constrained by fixed spatial and temporal discretization. While some
researchers attempt to achieve spatio-temporal continuity by designing novel
strategies, they either overly rely on traditional numerical methods or fail to
truly overcome the limitations imposed by discretization. To address these, we
propose CoPS, a purely data-driven methods, to effectively model continuous
physics simulation from partial observations. Specifically, we employ
multiplicative filter network to fuse and encode spatial information with the
corresponding observations. Then we customize geometric grids and use
message-passing mechanism to map features from original spatial domain to the
customized grids. Subsequently, CoPS models continuous-time dynamics by
designing multi-scale graph ODEs, while introducing a Markov-based neural
auto-correction module to assist and constrain the continuous extrapolations.
Comprehensive experiments demonstrate that CoPS advances the state-of-the-art
methods in space-time continuous modeling across various scenarios.

</details>


### [206] [Visual Detector Compression via Location-Aware Discriminant Analysis](https://arxiv.org/abs/2509.17968)
*Qizhen Lan,Jung Im Choi,Qing Tian*

Main category: cs.CV

TL;DR: 提出了一种基于检测判别式的主动网络压缩方法，专门针对视觉检测器，利用目标定位信息进行剪枝，在保持性能的同时大幅降低模型复杂度


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限的边缘设备上部署困难，现有剪枝方法主要关注分类模型，对检测模型关注有限且缺乏对定位信息的利用，同时大多数方法被动依赖预训练模型导致有用和无用组件交织难以分离

Method: 提出主动检测判别式压缩方法，交替执行两个步骤：(1)最大化并压缩检测相关判别式，将其与检测头前的神经元/滤波器子集对齐；(2)跨层追踪检测相关判别能力，丢弃重要性较低的特征。两个步骤都利用目标定位信息

Result: 在KITTI和COCO数据集上使用四种先进检测模型和四种最先进竞争方法的广泛实验表明该方法优越性，压缩模型甚至能超越原始基础模型且复杂度大幅降低

Conclusion: 该方法通过主动利用检测相关判别式和定位信息，实现了对视觉检测器的高效压缩，在保持甚至提升性能的同时显著降低模型复杂度

Abstract: Deep neural networks are powerful, yet their high complexity greatly limits
their potential to be deployed on billions of resource-constrained edge
devices. Pruning is a crucial network compression technique, yet most existing
methods focus on classification models, with limited attention to detection.
Even among those addressing detection, there is a lack of utilization of
essential localization information. Also, many pruning methods passively rely
on pre-trained models, in which useful and useless components are intertwined,
making it difficult to remove the latter without harming the former at the
neuron/filter level. To address the above issues, in this paper, we propose a
proactive detection-discriminants-based network compression approach for deep
visual detectors, which alternates between two steps: (1) maximizing and
compressing detection-related discriminants and aligning them with a subset of
neurons/filters immediately before the detection head, and (2) tracing the
detection-related discriminating power across the layers and discarding
features of lower importance. Object location information is exploited in both
steps. Extensive experiments, employing four advanced detection models and four
state-of-the-art competing methods on the KITTI and COCO datasets, highlight
the superiority of our approach. Remarkably, our compressed models can even
beat the original base models with a substantial reduction in complexity.

</details>


### [207] [StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models](https://arxiv.org/abs/2509.17993)
*Haoxin Yang,Bangzhen Liu,Xuemiao Xu,Cheng Xu,Yuyang Yu,Zikai Huang,Yi Wang,Shengfeng He*

Main category: cs.CV

TL;DR: StableGuard是一个新颖的端到端框架，通过在潜在扩散模型中无缝集成二进制水印，实现版权保护和篡改定位，解决了现有方法依赖后处理带来的应用不便和取证可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的发展，AI生成内容的真实性增强，但也引发了滥用担忧，需要强大的版权保护和篡改定位能力。现有统一解决方案依赖后处理，导致应用不便和取证可靠性受损。

Method: 提出MPW-VAE（多路复用水印VAE），通过轻量级潜在残差适配器生成配对的水印和无水印图像；开发MoE-GFN（专家混合引导取证网络），动态整合全局水印模式、局部篡改痕迹和频域线索；两者通过自监督端到端方式联合优化。

Result: 大量实验表明，StableGuard在图像保真度、水印验证和篡改定位方面始终优于最先进的方法。

Conclusion: StableGuard通过端到端设计成功实现了高效的版权保护和精确的篡改定位，为AI生成内容的安全应用提供了可靠解决方案。

Abstract: The advancement of diffusion models has enhanced the realism of AI-generated
content but also raised concerns about misuse, necessitating robust copyright
protection and tampering localization. Although recent methods have made
progress toward unified solutions, their reliance on post hoc processing
introduces considerable application inconvenience and compromises forensic
reliability. We propose StableGuard, a novel framework that seamlessly
integrates a binary watermark into the diffusion generation process, ensuring
copyright protection and tampering localization in Latent Diffusion Models
through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)
by equipping a pretrained Variational Autoencoder (VAE) with a lightweight
latent residual-based adapter, enabling the generation of paired watermarked
and watermark-free images. These pairs, fused via random masks, create a
diverse dataset for training a tampering-agnostic forensic network. To further
enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic
Network (MoE-GFN) that dynamically integrates holistic watermark patterns,
local tampering traces, and frequency-domain cues for precise watermark
verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly
optimized in a self-supervised, end-to-end manner, fostering a reciprocal
training between watermark embedding and forensic accuracy. Extensive
experiments demonstrate that StableGuard consistently outperforms
state-of-the-art methods in image fidelity, watermark verification, and
tampering localization.

</details>


### [208] [NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning](https://arxiv.org/abs/2509.18041)
*Sahil Shah,S P Sharan,Harsh Goel,Minkyu Choi,Mustafa Munir,Manvik Pasula,Radu Marculescu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-QA是一个训练即插即用的神经符号化管道，用于解决长视频问答中的逻辑推理问题，通过将自然语言问题转换为时序逻辑表达式，构建视频自动机，并应用模型检查来验证逻辑要求，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理长视频问答时存在局限性，无法有效处理多步时序推理和因果关系，传统方法采样效率低且缺乏逻辑验证。

Method: 将自然语言问题转换为时序逻辑表达式，构建视频自动机，应用模型检查识别满足逻辑要求的视频片段，仅将这些片段提交给视觉语言模型处理。

Result: 在LongVideoBench和CinePile数据集上，NeuS-QA性能提升超过10%，特别是在涉及事件排序、因果关系和多步组合推理的问题上表现显著。

Conclusion: NeuS-QA通过神经符号化方法有效解决了长视频问答中的逻辑推理挑战，提高了模型的可解释性和推理能力，无需修改或微调现有模型。

Abstract: Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.

</details>


### [209] [TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs](https://arxiv.org/abs/2509.18056)
*Yunheng Li,Jing Cheng,Shaoyong Jia,Hangyi Kuang,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: TempSamp-R1是一个新的强化微调框架，通过结合离策略监督和非线性软优势计算，提高了多模态大语言模型在视频时序定位任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在大型时序搜索空间中效率低下且性能受限，因为它们依赖策略采样，往往无法找到时序准确的解决方案。

Method: TempSamp-R1利用真实标注作为离策略监督提供时序精确指导，采用非线性软优势计算稳定训练，并通过混合Chain-of-Thought训练范式优化单一统一模型。

Result: 在Charades-STA（R1@0.7: 52.9%, +2.7%）、ActivityNet Captions（R1@0.5: 56.0%, +5.3%）和QVHighlights（mAP: 30.0%, +3.0%）等基准数据集上实现了最先进性能，并展示了强大的少样本泛化能力。

Conclusion: TempSamp-R1通过创新的离策略监督和优势计算方法，有效解决了视频时序定位任务中的效率与精度问题，为多模态大语言模型的时序适应提供了新思路。

Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1

</details>


### [210] [GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer](https://arxiv.org/abs/2509.18081)
*Md. Mahmudul Hasan,Ahmed Nesar Tahsin Choudhury,Mahmudul Hasan,Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: GraDeT-HTR是一个基于Grapheme-aware Decoder-only Transformer架构的资源高效孟加拉语手写文本识别系统，通过引入基于字素的标记化方法显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为世界第六大语言，其手写文本识别系统严重不足。孟加拉语文字具有连字、变音符号和高度变化的书写风格等复杂性，加上标注数据集的稀缺性，使得该任务特别具有挑战性。

Method: 采用基于字素的解码器专用Transformer架构，通过集成基于字素的标记器来增强解码器专用Transformer的性能。模型在大规模合成数据上进行预训练，并在真实人工标注样本上进行微调。

Result: 在多个基准数据集上实现了最先进的性能，相比传统的子词标记器显著提高了识别准确率。

Conclusion: GraDeT-HTR系统有效解决了孟加拉语手写文本识别的独特挑战，为资源受限语言的手写识别提供了有效的解决方案。

Abstract: Despite Bengali being the sixth most spoken language in the world,
handwritten text recognition (HTR) systems for Bengali remain severely
underdeveloped. The complexity of Bengali script--featuring conjuncts,
diacritics, and highly variable handwriting styles--combined with a scarcity of
annotated datasets makes this task particularly challenging. We present
GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system
based on a Grapheme-aware Decoder-only Transformer architecture. To address the
unique challenges of Bengali script, we augment the performance of a
decoder-only transformer by integrating a grapheme-based tokenizer and
demonstrate that it significantly improves recognition accuracy compared to
conventional subword tokenizers. Our model is pretrained on large-scale
synthetic data and fine-tuned on real human-annotated samples, achieving
state-of-the-art performance on multiple benchmark datasets.

</details>


### [211] [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](https://arxiv.org/abs/2509.18090)
*Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu*

Main category: cs.CV

TL;DR: GeoSVR是一种基于稀疏体素的显式体素框架，用于实现准确、详细和完整的表面重建，解决了基于高斯泼溅方法存在的表示瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯泼溅的主流方法存在表示瓶颈，而稀疏体素的潜力尚未充分探索。稀疏体素能够保持覆盖完整性和几何清晰度，但面临缺乏场景约束和局部表面细化的挑战。

Method: 提出了体素不确定性深度约束，最大化单目深度线索效果同时引入体素导向的不确定性以避免质量下降；设计了稀疏体素表面正则化，增强微小体素的几何一致性并促进基于体素的锐利准确表面形成。

Result: 在多种挑战性场景下的广泛实验表明，该方法在几何精度、细节保持和重建完整性方面优于现有方法，同时保持高效率。

Conclusion: GeoSVR通过创新的体素约束和正则化方法，成功实现了高质量的表面重建，为基于稀疏体素的表面重建提供了有效解决方案。

Abstract: Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.

</details>


### [212] [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation](https://arxiv.org/abs/2509.18092)
*Guocheng Gordon Qian,Daniil Ostashev,Egor Nemchinov,Avihay Assouline,Sergey Tulyakov,Kuan-Chieh Jackson Wang,Kfir Aberman*

Main category: cs.CV

TL;DR: 提出了一种新的属性特定图像提示方法，通过使用不同的参考图像分别控制人类外观的各个属性（如发型、服装、身份），实现组合式和解耦的视觉控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本到图像合成中主要关注身份保持，但缺乏模块化能力，无法对特定视觉属性进行解耦控制。

Method: 将参考图像编码为属性特定标记，注入预训练的文本到图像扩散模型，采用多属性交叉参考训练策略，鼓励模型从不对齐的属性输入中生成忠实输出。

Result: 实验表明该方法在准确遵循视觉和文本提示方面达到最先进性能，能够实现跨多个人的多视觉因素组合控制。

Conclusion: 该框架通过将视觉提示与文本驱动生成相结合，为更可配置的人类图像合成开辟了新途径。

Abstract: Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.

</details>


### [213] [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](https://arxiv.org/abs/2509.18096)
*Chaehyun Kim,Heeseong Shin,Eunbeen Hong,Heeji Yoon,Anurag Arnab,Paul Hongsuck Seo,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: Seg4Diff是一个分析多模态扩散变换器（MM-DiT）注意力结构的框架，发现特定层能自然产生高质量语义分割掩码，通过轻量微调可同时提升分割和生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前对多模态扩散变换器中注意力机制如何促进图像生成的理解有限，需要系统分析文本到图像生成过程中语义信息的传播方式。

Method: 提出Seg4Diff框架，识别MM-DiT中的语义接地专家层，该层能一致地将文本标记与空间连贯的图像区域对齐，并采用轻量微调方案增强语义分组能力。

Result: 发现语义分组是扩散变换器的涌现特性，通过选择性放大可以同时提升分割性能和生成图像保真度。

Conclusion: 语义分组能力可以统一视觉感知和生成模型，为构建桥接这两项任务的一体化模型铺平道路。

Abstract: Text-to-image diffusion models excel at translating language prompts into
photorealistic images by implicitly grounding textual concepts through their
cross-modal attention mechanisms. Recent multi-modal diffusion transformers
extend this by introducing joint self-attention over concatenated image and
text tokens, enabling richer and more scalable cross-modal alignment. However,
a detailed understanding of how and where these attention maps contribute to
image generation remains limited. In this paper, we introduce Seg4Diff
(Segmentation for Diffusion), a systematic framework for analyzing the
attention structures of MM-DiT, with a focus on how specific layers propagate
semantic information from text to image. Through comprehensive analysis, we
identify a semantic grounding expert layer, a specific MM-DiT block that
consistently aligns text tokens with spatially coherent image regions,
naturally producing high-quality semantic segmentation masks. We further
demonstrate that applying a lightweight fine-tuning scheme with mask-annotated
image data enhances the semantic grouping capabilities of these layers and
thereby improves both segmentation performance and generated image fidelity.
Our findings demonstrate that semantic grouping is an emergent property of
diffusion transformers and can be selectively amplified to advance both
segmentation and generation performance, paving the way for unified models that
bridge visual perception and generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [214] [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
*Brian S. Lin,Jiaxin Yuan,Zihan Zhou,Shouli Wang,Shuo Wang,Cunliang Kong,Qi Shi,Yuxuan Li,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了基于大语言模型的科学归纳推理任务（超越方程范畴），并创建了SIRBench-V1基准来评估LLMs在科学场景中的归纳推理能力。实验表明当前LLMs在此任务上仍面临困难。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs展现出类人能力，关键问题是如何让LLMs在全新环境中从有限示例中学习底层模式并有效应用，这关系到LLMs的归纳推理能力。现有研究在超越方程范畴的规则设计缺乏具体场景基础。

Method: 受归纳推理与人类科学发现相似性的启发，提出LLM-Based Scientific Inductive Reasoning Beyond Equations任务，并开发SIRBench-V1基准进行评估。

Result: 实验结果显示，当前的大语言模型在此任务上表现不佳，突显了该任务的难度和该领域进一步发展的必要性。

Conclusion: 基于LLMs的科学归纳推理是一个具有挑战性的任务，需要更多研究来提升LLMs在此领域的能力。

Abstract: As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.

</details>


### [215] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言的方法，利用零样本学习和数学推理来解决、解释和生成大学级数学问题的解决方案，通过程序合成减少对大规模训练数据的依赖，在MIT、哥伦比亚大学课程和MATH数据集上达到90.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂大学级数学问题（特别是MIT、哥伦比亚大学课程和MATH数据集中的问题）是人工智能领域的重大挑战，传统方法在此领域表现不佳，需要更先进的解决方案。

Method: 采用基于语言的方法，结合零样本学习和数学推理，集成程序合成技术来减少对大规模训练数据的依赖。

Result: 该方法在复杂数学问题上达到了90.15%的准确率，显著超过了之前81%的基准水平。

Conclusion: 研究结果表明先进的人工智能方法在解决复杂数学课程和数据集挑战方面具有巨大潜力，为自动化数学问题求解设立了新标准。

Abstract: The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


### [216] [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
*Asiya Ibrahim Zanga,Salisu Mamman Abdulrahman,Abubakar Ado,Abdulkadir Abubakar Bichi,Lukman Aliyu Jibril,Abdulmajid Babangida Umar,Alhassan Adamu,Shamsuddeen Hassan Muhammad,Bashir Salisu Abubakar*

Main category: cs.CL

TL;DR: 本文提出了HausaMovieReview数据集，包含5000条豪萨语和英语混合的YouTube评论，用于低资源语言的NLP研究。通过比较传统机器学习模型和Transformer模型，发现决策树分类器在准确率和F1分数上显著优于深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如豪萨语）因标注数据稀缺而阻碍NLP工具开发的问题。

Method: 构建HausaMovieReview数据集，由三名独立标注者标注，Fleiss' Kappa一致性分数为0.85。比较逻辑回归、决策树、K近邻等传统模型与微调的BERT、RoBERTa模型。

Result: 决策树分类器表现最佳，准确率为89.72%，F1分数为89.60%，显著优于深度学习模型。

Conclusion: 在低资源环境下，有效的特征工程可以使传统模型达到最先进性能，为未来研究奠定基础。

Abstract: The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis

</details>


### [217] [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
*Wenjie Lin,Hange Liu,Xutao Mao,Yingying Zhuang,Jingwei Shi,Xudong Han,Tianyu Shi,Jinrui Yang*

Main category: cs.CL

TL;DR: ParlAI Vote是一个交互式系统，用于探索欧洲议会辩论和投票，并测试LLMs在投票预测和偏见分析方面的表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一平台，连接辩论主题、演讲和投票结果，包含丰富的人口统计数据，以降低重现研究结果、审计行为和运行反事实场景的门槛。

Method: 通过可视化EuroParlVote基准及其核心任务（性别分类和投票预测），在单一界面中统一数据、模型和可视化分析。

Result: 系统突出了最先进LLMs中存在的系统性性能偏见，支持研究、教育和公众参与立法决策。

Conclusion: ParlAI Vote平台清晰地展示了当前LLMs在政治分析中的优势和局限性，为相关领域提供了有价值的工具。

Abstract: We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.

</details>


### [218] [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
*Alok N. Shah,Khush Gupta,Keshav Ramji,Pratik Chaudhari*

Main category: cs.CL

TL;DR: 该论文提出了一种使用元标记和元注意力机制的新方法，通过在预训练中注入特殊标记来增强语言模型的长距离依赖捕捉能力，使模型能够在推理时实现2倍上下文窗口的长度泛化。


<details>
  <summary>Details</summary>
Motivation: 现代基于Transformer的语言模型在多任务泛化方面取得了重大成功，但往往难以捕捉其上下文窗口内的长距离依赖关系。

Method: 在GPT-2架构基础上，除了因果多头注意力外，还添加了元注意力机制，在预训练过程中注入元标记，并通过可视化模型内部结构和信息论分析来研究这些标记的行为。

Result: 使用少于100B标记的数据高效预训练，结合元标记和元注意力机制，在微调后在这些任务上取得了强劲性能，能够实现2倍上下文窗口的长度泛化。

Conclusion: 预训练语言模型时使用元标记提供了一种简单、数据高效的方法来增强长上下文语言建模性能，同时为理解模型长度泛化行为提供了新的见解。

Abstract: While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.

</details>


### [219] [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
*Andrew Zhu,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本文提出了"旁听代理"这一新的人机交互范式，即AI助手在不打断对话的情况下提供上下文相关的辅助，而不是通过聊天界面直接与用户交互。


<details>
  <summary>Details</summary>
Motivation: 现代对话式LLM代理通过聊天界面直接协助用户完成任务，但这种方式需要用户主动关注。作者希望探索一种更自然、不干扰的交互方式，让AI在后台监控环境活动并在适当时机提供帮助。

Method: 通过分析现有LLM代理相关工作和探索性HCI研究，建立了旁听代理交互和任务的分类法，并基于此制定了构建旁听代理系统的最佳实践指南。

Result: 提出了旁听LLM代理作为人机交互新范式的首次分析，建立了系统的分类框架和设计指南。

Conclusion: 旁听代理范式为AI助手提供了新的交互可能性，但仍存在研究空白，为未来研究指明了方向。

Abstract: Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.

</details>


### [220] [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
*Yunsoo Kim,Michal W. S. Ong,Alex Shavick,Honghan Wu,Adam P. Levine*

Main category: cs.CL

TL;DR: 提出了HARE框架，用于评估病理学报告生成质量，通过实体和关系提取来比对参考报告和生成报告的关键临床内容


<details>
  <summary>Details</summary>
Motivation: 医学领域自动文本生成缺乏针对病理学报告的质量评估指标，现有通用指标无法有效评估临床相关性

Method: 开发HARE框架，包括基准数据集、命名实体识别模型、关系提取模型和新评估指标，使用GatorTronS模型进行微调

Result: HARE-NER和HARE-RE模型达到最高F1分数0.915，HARE指标在相关性分析中显著优于传统指标和放射学指标

Conclusion: HARE为病理学报告生成提供了强大的质量评估框架，有助于提高报告质量

Abstract: Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.

</details>


### [221] [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
*Weikang Qiu,Tinglin Huang,Ryan Rullo,Yucheng Kuang,Ali Maatouk,S. Raquel Ramos,Rex Ying*

Main category: cs.CL

TL;DR: RephQA是一个评估LLMs在公共卫生问答中可读性的基准，包含533个专家评审的问答对，评估了25个LLMs的可读性表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注提高LLMs的准确性和推理能力，但在公共卫生领域，LLMs生成回答的可读性（即向非医学背景人群清晰简单解释问题的能力）是一个重要瓶颈。

Method: 提出RephQA基准，包含Flesch-Kincaid年级水平和专业评分两个可读性指标；探索了四种可读性增强策略：标准提示、思维链提示、GRPO及其token适应变体。

Result: 评估显示大多数LLMs未能达到可读性标准；token适应的GRPO方法取得了最佳效果。

Conclusion: 这项研究朝着构建更实用、用户友好的公共卫生智能体迈出了一步，强调了推理能力与有效沟通之间的差距。

Abstract: Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.

</details>


### [222] [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
*Cihan Xiao,Matthew Wiesner,Debashish Chakraborty,Reno Kriz,Keith Cunningham,Kenton Murray,Kevin Duh,Luis Tavarez-Arce,Paul McNamee,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: Whisper-UT是一个统一的语音-文本多模态翻译框架，通过轻量级适配器实现跨任务和跨模态的灵活适应，支持语音翻译和文本翻译任务。


<details>
  <summary>Details</summary>
Motivation: 现有的编码器-解码器模型在多模态场景下的适应效率不高，需要一种能够同时处理语音和文本输入的统一框架。

Method: 使用轻量级适配器技术，通过ASR假设或真实转录作为提示，采用两阶段解码策略增强语音翻译性能，支持跨模态和跨任务的微调。

Result: 该方法在不要求三向并行数据的情况下提高了性能，证明了框架在多模态翻译中的灵活性和有效性。

Conclusion: Whisper-UT框架展示了在多模态翻译任务中的高效性、灵活性和通用适用性，为类似多任务模型提供了可行的解决方案。

Abstract: Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.

</details>


### [223] [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Elena Hayoung Lee,Gale Lucas*

Main category: cs.CL

TL;DR: 本文评估了人格提示的大型语言模型在对抗性争议解决中的行为对齐，通过模拟包含谈判的多轮冲突对话，发现GPT-4.1在语言风格和情感动态方面与人类最接近，而Claude-3.7-Sonnet在战略行为方面表现最佳，但仍存在显著的对齐差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地部署在社会复杂性、互动驱动的任务中，但它们在情感和战略复杂性环境中反映人类行为的能力仍未得到充分探索。

Method: 通过模拟多轮冲突对话（包含谈判），使用匹配的五因素人格配置文件来引导每个LLM，以控制个体差异并增强现实感。评估对齐的三个维度：语言风格、情感表达（如愤怒动态）和战略行为。

Result: GPT-4.1在语言风格和情感动态方面与人类最接近，而Claude-3.7-Sonnet在战略行为方面表现最佳，但仍存在显著的对齐差距。

Conclusion: 研究结果为LLM与人类在社会复杂性互动中的对齐建立了基准，强调了人格调节在对话建模中的潜力和局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.

</details>


### [224] ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
*Huy Nghiem,Phuong-Anh Nguyen-Le,John Prindle,Rachel Rudinger,Hal Daumé III*

Main category: cs.CL

TL;DR: 论文通过双过程框架审计LLMs在高校招生决策中对社会经济地位的处理，发现LLMs倾向于优待低SES申请者，且解释性模式会放大这种倾向。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地参与高风险领域决策，但其在社会敏感决策中的推理机制尚未充分研究，特别是在社会经济地位对决策的影响方面。

Method: 使用基于真实世界相关性的30,000个合成申请者档案，在4个开源LLMs上测试两种模式：快速决策模式（系统1）和解释性决策模式（系统2），共进行了500万次提示测试。

Result: LLMs一致倾向于优待低SES申请者（即使控制学业表现），系统2模式通过明确引用SES作为补偿性理由进一步放大了这种倾向。

Conclusion: LLMs作为决策者具有潜力但也存在不稳定性，作者提出了DPAF双过程审计框架来探测LLMs在敏感应用中的推理行为。

Abstract: Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.

</details>


### [225] [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
*Richard Diehl Martinez,David Demitri Africa,Yuval Weiss,Suchir Salhan,Ryan Daniels,Paula Buttery*

Main category: cs.CL

TL;DR: Pico是一个轻量级模块化框架，用于系统化研究中小型语言模型的开发，通过提供实验沙盒和基准模型来支持可重复的实验。


<details>
  <summary>Details</summary>
Motivation: 当前中小型语言模型开发缺乏科学系统的方法，设计选择的效果不明确，参数预算紧张使得每个决策都至关重要。

Method: Pico框架包含两个库，允许研究人员对模型架构或训练流程进行针对性修改，并直接观察对模型行为的影响。同时发布了标准化训练的基准模型pico-decoder。

Result: 案例研究表明Pico能够支持迭代式的小型语言模型设计和分析。

Conclusion: Pico为中小型语言模型的系统化研究提供了实用工具，有助于推动该领域的科学发展。

Abstract: Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.

</details>


### [226] [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
*Tom Mackintosh,Harish Tayyar Madabushi,Claire Bonial*

Main category: cs.CL

TL;DR: 该论文提出了ConTest-NLI基准测试，用于评估大语言模型学习构式语法中深层形式-意义映射的能力，发现LLMs在对抗性数据上表现下降24%，且图式化模式最难学习


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型学习构式语法定义的深层形式-意义映射的能力，填补当前LLMs在抽象构造学习方面的评估空白

Method: 构建包含8万句子的ConTest-NLI基准，涵盖8种英语构式，通过模板化和模型在环过滤生成多样化合成NLI三元组，进行零样本测试和微调实验

Result: 领先LLMs在自然数据上准确率88%，对抗性数据上降至64%，图式化模式最难；微调可提升9%但仍有抽象差距

Conclusion: 当前LLMs存在持久的抽象能力差距，ConTest-NLI提供了一个可扩展的框架来评估构式驱动的学习能力

Abstract: We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.

</details>


### [227] [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
*Tsz Fung Pang,Maryam Berijanian,Thomas Orth,Breanna Shi,Charlotte S. Alexander*

Main category: cs.CL

TL;DR: 本文提出了PersonaMatrix框架，通过六种用户角色评估法律文档摘要质量，并创建了维度转换的民权案件摘要数据集，以解决法律AI摘要系统对不同用户需求的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 法律文档通常冗长复杂，现有自动摘要评估方法忽略了不同用户和利益相关者的多样化需求，需要开发能够同时满足法律专家和普通公众需求的法律摘要工具。

Method: 引入PersonaMatrix评估框架，通过六种用户角色（包括法律和非法律用户）对摘要进行评分；创建维度转换的民权案件摘要数据集，在深度、可访问性和程序细节方面进行变化；提出多样性覆盖指数（DCI）来揭示角色感知和角色无关评估之间的差异。

Result: 开发了公开可用的代码库和数据集，能够有效评估法律摘要系统对不同用户需求的适应性。

Conclusion: 该工作能够改进法律AI摘要系统，使其同时满足专家和非专家用户的需求，有望提高法律知识的可及性。

Abstract: Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.

</details>


### [228] [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
*Yunzhe Wang,Gale M. Lucas,Burcin Becerik-Gerber,Volkan Ustun*

Main category: cs.CL

TL;DR: 本文提出PEBA理论框架和PEvo算法来解决生成智能体行为与真实数据之间的行为-现实差距问题，通过在活跃枪手事件模拟中验证，显著提升了行为真实性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的生成智能体在社交模拟中经常偏离专家预期和真实数据，存在行为-现实差距问题，这限制了其在关键场景（如政策制定、人际培训）中的可信度。

Method: 提出Persona-Environment Behavioral Alignment (PEBA)理论框架，基于Lewin行为方程；开发PersonaEvolve (PEvo)算法，通过迭代优化智能体角色，使其集体行为与专家基准在特定环境背景下对齐。

Result: 在活跃枪手事件模拟中，PEvo相比无引导方法平均减少84%的分布差异，比显式指令基线提升34%；优化后的角色能够泛化到新的相关模拟场景。

Conclusion: PEBA-PEvo框架显著提升了高风险社交模拟中的行为真实性和可靠性，为开发可信赖的LLM驱动社交模拟提供了原则性方法。

Abstract: Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.

</details>


### [229] [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
*'Mina Arzaghi','Alireza Dehghanpour Farashah','Florian Carichon',' Golnoosh Farnadi'*

Main category: cs.CL

TL;DR: 本文提出统一评估框架，研究LLMs内在偏见缓解（概念遗忘）与外在偏见缓解（反事实数据增强）对下游任务公平性的影响，在金融分类任务中验证内在偏见缓解可显著降低偏见并提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有研究质疑LLMs的内在偏见是否影响下游任务公平性，但缺乏实证研究。本文旨在通过实证方法探究内在偏见与下游任务公平性之间的关联。

Method: 提出统一评估框架，比较概念遗忘（内在偏见缓解）和反事实数据增强（外在偏见缓解）的效果。使用三个开源LLMs，在金融分类任务（薪资预测、就业状态、信用评估）中评估模型作为冻结嵌入提取器和微调分类器的表现。

Result: 内在偏见缓解通过概念遗忘可将内在性别偏见降低94.9%，同时将下游任务公平性指标（如人口均等）提升82%，且不损害准确性。

Conclusion: 框架为偏见缓解提供了实用指导，强调在下游部署前应用早期阶段缓解措施的重要性，内在偏见缓解可有效提升下游任务公平性。

Abstract: Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.

</details>


### [230] [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
*Margaret Hughes,Brandon Roy,Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CL

TL;DR: 该论文提出了一种基于"响应性"概念的方法来评估对话质量，通过量化对话中说话者回合之间的响应关系，并开发了对话级别的度量指标来表征不同对话的结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对话中的毒性和极化问题，而对什么是建设性和亲社会对话的特征研究相对较少。本文旨在探索如何表征对话质量，特别是通过响应性这一核心概念。

Method: 开发了两种量化响应性的方法：1）基于说话者回合语义相似度的方法；2）利用最先进的大语言模型识别两个说话者回合之间的关系。选择性能更好的LLM方法进一步分析响应的性质（实质性响应与否）。

Result: 两种方法都在人工标注的对话数据集上进行了评估。基于LLM的方法表现更好，能够有效识别对话中的响应关系。开发的对话级别度量指标能够有意义地表征和区分不同类型对话的结构特征。

Conclusion: 响应性是对话的基本特征，不同对话具有显著不同的响应性结构。提出的度量方法能够有效支持对多样化对话集合的有意义表征和区分，为理解对话质量提供了新的分析框架。

Abstract: Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.

</details>


### [231] [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
*Zixun Chen,Petr Babkin,Akshat Gupta,Gopala Anumanchipalli,Xiaomo Liu*

Main category: cs.CL

TL;DR: 该研究通过基于语言学理论的细粒度指标评估大型语言模型在对话能力上的表现，发现模型大小对大多数指标影响有限，而监督微调会快速饱和性能，同时质疑了现有评估指标的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管对话是大型语言模型的标志性能力，但很少有研究具体分析在训练后阶段支撑对话行为的具体成分。研究者希望了解模型大小和监督微调如何影响对话能力的各个维度。

Method: 使用一套基于语言学理论的模型评估指标，针对对话的不同细粒度方面进行评估。评估预训练的Pythia模型在不同模型大小和对话数据集监督微调下的表现变化。

Result: 模型大小对大多数指标影响有限，监督微调会快速饱和所有模型的得分（除最小模型外）。许多指标显示出相似的趋势，特别是当它们都基于相同的评估模型时，这引发了对其可靠性的质疑。

Conclusion: 研究通过分析得分分布、指标相关性和生成响应中的词频来帮助解释观察结果，强调了需要更可靠的评估指标来准确衡量对话能力的特定维度。

Abstract: Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.

</details>


### [232] [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
*Fengyuan Liu,Rui Zhao,Shuo Chen,Guohao Li,Philip Torr,Lei Han,Jindong Gu*

Main category: cs.CL

TL;DR: 本文提出M-Spoiler框架，研究在多智能体系统中，攻击者仅了解单个智能体时能否生成对抗样本误导整个系统的协作决策。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统通过协作展现出增强的决策能力，但个体LLM的脆弱性以及难以访问所有智能体的问题引发了一个关键问题：攻击者仅了解一个智能体时，是否仍能生成误导集体决策的对抗样本？

Method: 将问题建模为不完全信息博弈，提出M-Spoiler框架，通过引入顽固智能体模拟目标系统中可能的顽固响应来优化对抗样本，从而操纵目标智能体误导系统协作决策。

Result: 实验证实了在多智能体系统中了解单个智能体所带来的风险，并证明了M-Spoiler框架的有效性。与基线方法相比，该攻击框架更具威力。

Conclusion: 研究强调了多智能体系统中安全风险的存在，表明需要进一步研究防御策略来应对此类攻击。

Abstract: Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.

</details>


### [233] [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
*Wei Xie,Shuoyoucheng Ma,Zhenhua Wang,Enze Wang,Kai Chen,Xiaobing Sun,Baosheng Wang*

Main category: cs.CL

TL;DR: AIPsychoBench是一个专门用于评估大语言模型心理属性的基准测试，通过轻量级角色扮演提示绕过LLM对齐，显著提高了有效响应率并降低了偏差，同时首次全面证明了语言对LLM心理测量学的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究试图通过借用人类心理学概念来评估LLM的心理测量特性，但未能考虑LLM与人类之间的根本差异，导致直接重用人类量表时拒绝率很高，且这些量表不支持测量不同语言中LLM心理属性的变化。

Method: 使用轻量级角色扮演提示来绕过LLM的对齐机制，创建AIPsychoBench基准测试，评估LLM在7种语言中的112个心理测量子类别。

Result: 平均有效响应率从70.12%提高到90.40%；正负偏差分别仅为3.3%和2.1%，显著低于传统越狱提示的9.8%和6.9%；在43个子类别中，7种语言与英语的得分偏差范围为5%到20.2%。

Conclusion: AIPsychoBench为LLM心理属性的评估提供了专门工具，证明了语言对LLM心理测量学的显著影响，为理解LLM在不同语言环境中的行为差异提供了重要依据。

Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.

</details>


### [234] [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
*Junghwan Kim,Haotian Zhang,David Jurgens*

Main category: cs.CL

TL;DR: 本文提出了一种多语言作者表征学习方法，通过概率内容掩码和语言感知批处理技术，在36种语言和13个领域上训练模型，显著提升了非英语语言的作者归属任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的作者表征学习研究主要集中在英语单语环境，多语言作者表征模型的潜力尚未充分探索。

Method: 采用概率内容掩码技术使模型关注风格指示性词汇而非内容特定词汇，结合语言感知批处理减少跨语言干扰，在450万作者数据上进行训练。

Result: 在22种非英语语言中的21种上超越单语基线，平均Recall@8提升4.85%，单语言最大增益达15.91%，并展现出更强的跨语言和跨领域泛化能力。

Conclusion: 所提出的两种技术均被证明有效，对模型性能提升起到关键作用，验证了多语言作者表征学习的优势。

Abstract: Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.

</details>


### [235] [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
*Sungwon Kim,Daniel Khashabi*

Main category: cs.CL

TL;DR: LLMs在对话中表现出谄媚行为，容易同意用户的反对意见，但在同时评估冲突论点时表现良好。研究揭示了这种矛盾现象。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在作为评估代理时表现出的谄媚行为与评估能力之间的矛盾，特别是在对话框架下的响应偏差。

Method: 通过改变关键交互模式进行实证测试，比较LLMs在不同场景下的响应：后续对话vs同时评估、详细推理vs随意反馈等。

Result: 发现LLMs更容易在后续对话中赞同用户的反驳；对包含详细推理的反驳更易被说服；对随意反馈比正式批评更敏感。

Conclusion: 在依赖LLMs进行判断任务时需要考虑对话框架的影响，避免因谄媚行为导致的评估偏差。

Abstract: Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.

</details>


### [236] [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
*Cheng Jiayang,Qianqian Zhuang,Haoran Li,Chunkit Chan,Xin Liu,Lin Qiu,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文研究了整合性基础（integrative grounding）问题，即检索和验证多个相互依赖的证据来支持假设查询的挑战。通过四个领域的数据评估发现，LLMs在冗余证据下表现稳健，但在信息不完整时倾向于依赖内部知识进行合理化；无指导的检索规划会引入噪声降低性能，而前提溯因因逻辑约束表现出潜力；零样本自反思能力能持续提升基础质量。


<details>
  <summary>Details</summary>
Motivation: 现有基础方法适用于简单查询，但现实世界的信息需求往往需要综合多个证据片段。为了系统研究这一问题，作者引入了"整合性基础"的概念，旨在解决检索和验证多个相互依赖证据以支持假设查询的挑战。

Method: 作者重新利用来自四个领域的数据来评估整合性基础能力，研究了基础性验证和检索规划策略两个方面。具体包括分析LLMs对冗余证据的鲁棒性、信息不完整时的行为模式，以及比较无指导规划和前提溯因等不同检索策略的效果。

Result: 研究发现：1）在基础性验证中，LLMs对冗余证据具有鲁棒性，但在信息不完整时倾向于使用内部知识进行合理化；2）在检索规划方面，无指导规划会通过引入噪声降低性能，而前提溯因因其逻辑约束显示出潜力；3）LLMs的零样本自反思能力能持续改善基础质量。

Conclusion: 这些发现为开发更有效的整合性基础系统提供了有价值的方向，强调了逻辑约束的检索策略和自反思能力在提升LLMs基础性能中的重要性。

Abstract: Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.

</details>


### [237] [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
*Khalid Hasan,Jamil Saquer,Yifan Zhang*

Main category: cs.CL

TL;DR: 该研究对最先进的Transformer模型与LSTM模型在心理健康帖子多分类任务上的性能进行了大规模比较，发现Transformer模型（特别是RoBERTa）表现最佳，而带有注意力的LSTM模型在训练速度上有优势。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上大量用户公开分享心理健康问题，为早期检测提供了丰富数据。但现有NLP研究主要关注单一疾病识别，缺乏对多心理健康状况区分效果的系统评估。

Method: 收集Reddit上六种心理健康状况和控制组的帖子数据集，在相同条件下评估五种Transformer架构（BERT、RoBERTa等）与多种LSTM变体（含/不含注意力机制，使用上下文或静态嵌入）。

Result: Transformer模型表现一致优于其他方法，RoBERTa在所有类别上达到91-99%的F1分数和准确率。带有BERT嵌入的注意力增强LSTM接近Transformer性能（最高97% F1分数），且训练速度快2-3.5倍。

Conclusion: 这是首个多类心理健康检测的全面基准研究，为模型选择提供实用指导，并揭示了心理健康NLP系统实际部署中的准确性与效率权衡。

Abstract: Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.

</details>


### [238] [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
*Yue Huang,Zhengzhe Jiang,Xiaonan Luo,Kehan Guo,Haomin Zhuang,Yujun Zhou,Zhengqing Yuan,Xiaoqi Sun,Jules Schleinitz,Yanbo Wang,Shuhao Zhang,Mihir Surve,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang*

Main category: cs.CL

TL;DR: ChemOrch是一个为大型语言模型提供化学智能的框架，通过两阶段过程生成化学基础的指令-响应对，解决现有数据生成方法在化学信息层次结构和规则约束方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的化学领域特定指令-响应数据集，且现有合成数据生成流程与化学信息固有的层次化和规则约束结构不匹配，限制了LLMs在化学领域的应用。

Method: 采用两阶段过程：任务控制的指令生成和工具感知的响应构建。通过工具规划和蒸馏以及基于工具的自我修复机制确保响应精度，同时实现任务的可控多样性和难度级别。

Result: 1) 生成的指令数据质量高，表现出优越的多样性和与化学约束的强对齐；2) 可靠生成能更有效揭示LLMs在化学方面弱点的评估任务；3) 使用生成数据进行微调能显著提升LLMs的化学能力。

Conclusion: ChemOrch代表了向可扩展和可验证的LLMs化学智能迈出的关键一步。

Abstract: Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.

</details>


### [239] [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 本文研究了文本复杂度对语言模型预训练的影响，发现模型容量与文本复杂度之间存在交互作用，简化文本对小型模型影响较小，且对不同下游任务有不同影响。


<details>
  <summary>Details</summary>
Motivation: 探索文本复杂度（如句子长度、词汇选择、句子结构）在语言模型预训练中的作用，了解简化文本如何影响不同规模模型的性能。

Method: 使用大语言模型简化人类撰写的文本，然后在原始和简化数据上从头预训练因果模型（28M-500M），并在微调和零样本设置下评估。

Result: 困惑度对模型容量和文本复杂度的交互作用敏感，小型模型在简化文本上性能下降较少；文本复杂度对微调评估影响不大，但零样本评估显示简化文本有利于语言知识任务，复杂文本有利于需要世界知识和实体追踪的任务。

Conclusion: 文本复杂度在预训练中具有重要作用，不同复杂度的文本适合不同类型的下游任务，模型规模与文本复杂度需要匹配以获得最佳性能。

Abstract: Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.

</details>


### [240] [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
*Jun Rong Brian Chong,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: MPCG是一个多轮、角色条件化的框架，用于模拟错误信息如何被不同意识形态的代理人迭代重新解释，从而研究错误信息的演变过程。


<details>
  <summary>Details</summary>
Motivation: 当前错误信息检测方法假设错误信息是静态的，但实际上错误信息在传播过程中会不断演变以适应不同受众。

Method: 使用未经审查的大语言模型在多轮中生成角色特定的声明，每轮生成都以前一轮输出为条件，通过人类和LLM标注、认知努力指标、情感唤起指标等多种方法进行评估。

Result: 生成声明比原始声明需要更多认知努力，保持角色对齐的情感道德框架，语义漂移但保持主题连贯性，可行性达77%，常用错误信息检测器的性能下降高达49.7%。

Conclusion: MPCG框架有效模拟了错误信息的动态演变过程，揭示了当前静态检测方法的局限性，为研究错误信息传播提供了新工具。

Abstract: Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG

</details>


### [241] [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
*Benlu Wang,Iris Xia,Yifan Zhang,Junda Wang,Feiyun Ouyang,Shuo Han,Arman Cohan,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 本文重新评估医学计算能力，提出细粒度评估框架和模块化代理管道MedRaC，显著提升LLMs在医学计算任务上的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准测试仅评估最终答案且容忍度宽泛，忽略了系统性推理失败，可能导致严重的临床误判。需要更临床可信的评估方法。

Method: 1) 清理重构MedCalc-Bench数据集，提出分步评估管道；2) 引入自动错误分析框架；3) 提出模块化代理管道MedRaC，结合检索增强生成和Python代码执行。

Result: 细粒度评估下GPT-4o准确率从62.7%降至43.6%；MedRaC无需微调即可将不同LLMs准确率从16.35%提升至53.19%。

Conclusion: 当前基准测试存在局限性，提出的方法更符合临床实际，通过透明可转移的推理评估，使LLM系统在真实医疗应用中更可信。

Abstract: Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.

</details>


### [242] [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
*Qiongqiong Wang,Hardik Bhupendra Sailor,Tianchi Liu,Wenyu Zhang,Muhammad Huzaifah,Nattadaporn Lertcheva,Shuo Sun,Nancy F. Chen,Jinyang Wu,AiTi Aw*

Main category: cs.CL

TL;DR: CP-Bench是一个评估语音大语言模型在上下文副语言推理能力上的基准测试，重点关注语言内容与非语言线索（如情感和韵律）的整合。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在转录和翻译任务上表现优异，但在理解副语言方面（对社交和情感智能至关重要）仍存在局限。

Method: 创建了两个精心策划的问答数据集，要求模型具备语言和共情理解能力，评估了开源和闭源的最先进语音大语言模型，并对不同问题类型进行了全面分析。

Result: 对表现最好的两个模型进行了温度调优分析，揭示了现有评估中的关键差距。

Conclusion: 该基准测试为构建更具上下文感知和情感智能的语音大语言模型提供了见解。

Abstract: Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.

</details>


### [243] [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
*Zheng Liu,Mengjie Liu,Siwei Wen,Mengzhang Cai,Bin Cui,Conghui He,Wentao Zhang*

Main category: cs.CL

TL;DR: HAPO是一种针对LLM推理的异质自适应策略优化算法，通过基于令牌熵的动态优化来解决现有方法对所有令牌进行统一优化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在优化LLM推理时对所有令牌采用统一优化策略，忽略了不同令牌在推理过程中的不同作用。

Method: 提出HAPO算法，包含自适应温度采样、令牌级组平均优势计算、差分优势重分配和非对称自适应剪裁等组件，基于令牌熵实现细粒度优化控制。

Result: 在多个模型规模上的实验表明，HAPO始终优于DAPO算法。

Conclusion: HAPO通过将令牌级处理嵌入到每个训练阶段，实现了对LLM推理的精细控制，显著提升了性能。

Abstract: Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.

</details>


### [244] [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
*Junjie Ye,Yuming Yang,Yang Nan,Shuo Li,Qi Zhang,Tao Gui,Xuanjing Huang,Peng Wang,Zhongchao Shi,Jianping Fan*

Main category: cs.CL

TL;DR: 本文研究了监督微调（SFT）对大型语言模型知识的影响，发现使用更多样本进行微调反而导致闭卷问答性能下降，参数更新中高达90%的部分对知识增强无贡献。


<details>
  <summary>Details</summary>
Motivation: 探索SFT如何影响模型的知识掌握能力，以更好地控制微调过程中的知识变化行为。

Method: 评估了LLaMA-2和LLaMA-3家族的五个模型在闭卷问答任务上的表现，分析了不同微调数据量和知识掌握水平的影响，并在token和参数层面进行了行为分析。

Result: 使用1,920个样本微调的模型比仅用240个样本微调的模型性能差14%；微调数据中知识掌握水平的变化导致超过12%的性能波动；90%的参数更新对知识增强无贡献。

Conclusion: 这些发现为开发更有效的微调策略提供了实用指导，有助于更好地增强模型知识。

Abstract: Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.

</details>


### [245] [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
*Luyan Zhang*

Main category: cs.CL

TL;DR: 本研究提出基于模型-控制器-任务适应（MCP）的三层协作框架，通过解耦大模型功能模块、结合强化学习动态路由算法和任务适应机制，首次实现控制理论与大模型动态推理的系统集成。


<details>
  <summary>Details</summary>
Motivation: 针对大模型在复杂任务（如多轮推理、多模态协作）中面临的计算效率低下和可解释性不足问题。

Method: 将大模型功能解耦为推理、生成和检索模块，采用强化学习驱动的动态路由算法和任务适应机制，构建模型-控制器-任务适应的三层协作框架。

Result: 在GLUE、COCO、ScienceQA等跨模态基准任务上性能提升15-30%，推理效率提高40%，通过Presenter层生成可解释中间结果，获得90%的人工可解释性评分。

Conclusion: 为解决大模型实际应用瓶颈提供了全新的技术路径。

Abstract: Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.

</details>


### [246] [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
*Byeongho Yu,Changhun Lee,Jungyu Jin,Eunhyeok Park*

Main category: cs.CL

TL;DR: PruneCD是一种新的对比解码方法，通过层剪枝而非早期退出来构建业余模型，以更有效地缓解大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: DoLa方法利用早期退出logits作为对比先验来缓解幻觉，但发现这些早期退出logits往往平坦、幅度低，无法反映有意义的对比。

Method: 提出PruneCD方法，通过层剪枝构建业余模型，产生更具信息量和良好对齐的logits，实现更有效的对比解码。

Result: 通过定性和定量分析证明，PruneCD能够持续提高事实性，且推理开销最小。

Conclusion: PruneCD为大语言模型幻觉缓解提供了一种鲁棒且实用的方法。

Abstract: To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.

</details>


### [247] [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
*Sandro Tsang*

Main category: cs.CL

TL;DR: 本研究开发了一种结合信息检索技术的系统评价工作流程，用于提高子宫内膜异位症复发研究的效率和可重复性。该方法整合PRISMA指南与计算技术，通过半自动去重和筛选，最终从7项RCTs中得出GnRH激动剂可降低36%复发风险的结论。


<details>
  <summary>Details</summary>
Motivation: 传统系统评价面临文献量大、效率低的问题。本研究旨在开发一种信息检索驱动的工作流程，提高系统评价的效率、透明度和可重复性，以子宫内膜异位症复发这一复杂案例进行验证。

Method: 采用混合方法整合PRISMA指南与计算技术：1）应用半自动去重技术高效筛选记录；2）使用改良的分割方法处理多臂试验的单位分析错误；3）对GnRH激动剂亚类的随机对照试验进行证据合成。

Result: 工作流程显著减少了筛选工作量：仅用11天获取并筛选812条记录，最终纳入7项RCTs（841名患者）。汇总随机效应模型显示风险比为0.64（95% CI 0.48-0.86），即子宫内膜异位症复发风险降低36%，异质性不显著（I²=0.00%）。敏感性分析和偏倚评估支持结果的稳健性。

Conclusion: 该研究证明信息检索驱动的工作流程可有效加速医学证据合成过程，为复杂系统评价提供了可推广的框架，弥合了临床研究与计算机科学之间的差距。

Abstract: Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.

</details>


### [248] [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
*Junhao Chen,Jingbo Sun,Xiang Li,Haidong Xin,Yuhao Xue,Yibin Xu,Hao Zhao*

Main category: cs.CL

TL;DR: LLMsPark是一个基于博弈论的评估平台，用于评估大型语言模型在博弈论环境中的决策策略和社交行为，通过多智能体交互来探索战略深度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在不同任务中的进步，需要超越单一指标的全面评估。为了全面评估LLM的智能，必须考察它们的交互动态和战略行为。

Method: 构建基于博弈论的评估平台，在经典博弈论设置中测量LLM的决策策略和社交行为，使用排行榜排名和评分机制对15个领先的LLM进行交叉评估。

Result: 更高的分数反映了更强的推理和战略能力，揭示了不同模型之间的明显行为模式和性能差异。

Conclusion: 这项工作为评估LLM的战略智能提供了新的视角，丰富了现有基准测试，并拓宽了在交互式博弈论场景中的评估范围。

Abstract: As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.

</details>


### [249] [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
*Zuhair Hasan Shaik,Abdullah Mazhar,Aseem Srivastava,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种基于特征值分解的新型毒性抑制方法EigenShift，通过层间特征聚合而非单个神经元干预，在保持语言能力的同时更有效地抑制大语言模型的有毒内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有毒性抑制方法主要操纵单个神经元激活，但存在不稳定性、上下文依赖性问题，且常常损害模型的核心语言能力。需要更稳定、可解释且不影响语言流畅性的毒性抑制方法。

Method: 提出EigenShift方法，基于语言模型最终输出层的特征值分解，选择性针对生成对齐组件进行干预。该方法无需额外训练或微调，计算成本低，具有严格的理论基础。

Result: 在Jigsaw和ToxiCN数据集上的实验表明，聚合的层间特征比单个神经元提供更稳健的信号，能够精确抑制毒性生成而不损害语言能力。

Conclusion: EigenShift方法通过结构化的层间表示和特征值分解技术，解决了现有毒性抑制方法的局限性，为大语言模型的安全部署提供了更有效的解决方案。

Abstract: Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.

</details>


### [250] [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
*Ahmet Yavuz Uluslu,Tannon Kew,Tilia Ellendorff,Gerold Schneider,Rico Sennrich*

Main category: cs.CL

TL;DR: 本文提出了一种基于法医语言学启发的智能体NLI流程，通过专门智能体收集和分类多样化语言证据，再由协调智能体进行最终评估，显著提升了原生语言识别的鲁棒性和性能一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在原生语言识别中过度依赖表面上下文线索（如姓名、地点、文化刻板印象），而非真正的语言模式，导致鲁棒性不足。先前让模型忽略这些线索的策略不可靠，预测容易被误导性提示改变。

Method: 引入基于法医语言学的智能体NLI管道：1）专门智能体积累和分类多样化语言证据；2）目标感知协调智能体综合所有证据进行最终NLI预测。

Result: 在两个基准数据集上，该方法显著增强了NLI对误导性上下文线索的鲁棒性和性能一致性，优于标准提示方法。

Conclusion: 智能体化的NLI方法通过多智能体协作的证据积累和综合评估，有效解决了LLMs在原生语言识别中过度依赖表面线索的问题，提高了模型的鲁棒性和可靠性。

Abstract: Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.

</details>


### [251] [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
*Keliang Liu,Dingkang Yang,Ziyun Qian,Weijie Yin,Yuchi Wang,Hongsheng Li,Jun Liu,Peng Zhai,Yang Liu,Lihua Zhang*

Main category: cs.CL

TL;DR: 这是一篇关于强化学习增强大语言模型的综述论文，系统回顾了RL在LLM全生命周期中的应用，特别是可验证奖励的强化学习(RLVR)，涵盖了预训练、对齐微调和强化推理等阶段。


<details>
  <summary>Details</summary>
Motivation: 现有综述对RL增强LLMs的覆盖范围有限，未能全面总结RL在LLM全生命周期中的运作方式。本文旨在填补这一空白，为研究者和实践者提供RL与LLMs交叉领域的最新进展。

Method: 系统性地回顾RL在LLM各阶段的应用策略：1）介绍RL基本理论；2）详细分析RL在预训练、对齐微调和强化推理阶段的应用；3）整理现有数据集和评估基准；4）回顾开源工具和训练框架。

Result: 提供了RL增强LLMs的全面技术路线图，强调了强化推理阶段RL方法对推动模型推理能力极限的关键作用，并整理了相关数据集、评估工具和训练框架。

Conclusion: 该综述展示了RL如何使LLMs变得更智能、通用和安全，分析了该领域未来的挑战和发展趋势，旨在促进更先进的LLMs发展。

Abstract: In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.

</details>


### [252] [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)
*Zhengge Cai,Haowen Hou*

Main category: cs.CL

TL;DR: EG-MLA是一种改进的多头潜在注意力机制，通过嵌入门控机制进一步减少KV缓存大小，同时增强表示表达能力，在保持性能的同时实现超过91.6%的KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 减少KV缓存大小是实现大型语言模型高效推理的关键步骤。虽然多头潜在注意力(MLA)已经实现了显著的KV缓存压缩，但进一步压缩的空间有限且会带来性能损失。

Method: 提出嵌入门控多头潜在注意力(EG-MLA)，在潜在空间中引入令牌特定的嵌入门控机制，以最小的额外计算实现对压缩KV向量的细粒度调制。

Result: 相比MHA，EG-MLA实现了91.6%的KV缓存减少且性能损失可忽略；相比MLA，在多样化推理基准上持续提升任务精度，同时实现高达59.9%的额外内存节省。成功扩展到超过10亿参数。

Conclusion: EG-MLA被确立为一种内存和计算效率高的注意力机制，能够实现现代LLM的可扩展、高性能推理。

Abstract: Reducing the key-value (KV) cache size is a crucial step toward enabling
efficient inference in large language models (LLMs), especially under latency
and memory constraints. While Multi-Head Attention (MHA) offers strong
representational power, it incurs significant memory overhead. Recent work on
Multi-head Latent Attention (MLA) mitigates this by compressing KV
representations into a shared latent space, achieving a better trade-off
between performance and cache efficiency. While MLA already achieves
significant KV cache reduction, the scope for further compression remains
limited without performance loss. In this paper, we propose
\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel
extension of MLA that further reduces KV cache size while enhancing
representational expressiveness. EG-MLA introduces a token-specific embedding
gating mechanism applied in the latent space, enabling fine-grained modulation
of compressed KV vectors with minimal additional computation. Compared to MHA,
EG-MLA achieves over 91.6\% reduction in KV cache size with negligible
performance degradation. Relative to MLA, EG-MLA consistently improves task
accuracy across diverse reasoning benchmarks while achieving up to 59.9\%
additional memory savings. Our theoretical analysis highlights how embedding
gating induces implicit high-order interactions, and empirical evaluations
demonstrate robust generalization across model scales and compression regimes.
Notably, we successfully scale EG-MLA to over 1 billion parameters,
demonstrating its practical viability for large-scale LLM deployment. These
results establish EG-MLA as a memory- and compute-efficient attention mechanism
that enables scalable, high-performance inference in modern LLMs.

</details>


### [253] [Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2509.16696)
*Wataru Hashimoto,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本研究探讨了解码策略对大型语言模型不确定性估计的影响，发现对比搜索策略在偏好对齐的LLMs中能提供更好的不确定性估计，但在仅经过监督微调而未明确对齐的模型中效果不一。


<details>
  <summary>Details</summary>
Motivation: 解码策略通过操纵语言模型输出的概率分布来影响生成质量和不确定性，因此需要研究不同解码策略如何影响LLMs的不确定性估计。

Method: 通过实验比较不同解码策略（特别是对比搜索）在偏好对齐和仅监督微调的LLMs上的不确定性估计表现。

Result: 对比搜索策略在偏好对齐的LLMs中平均能提供更好的不确定性估计，但在仅监督微调的模型中效果不稳定。

Conclusion: 解码策略对不确定性估计的影响取决于模型的对齐状态，对比搜索在偏好对齐模型中表现更优。

Abstract: Decoding strategies manipulate the probability distribution underlying the
output of a language model and can therefore affect both generation quality and
its uncertainty. In this study, we investigate the impact of decoding
strategies on uncertainty estimation in Large Language Models (LLMs). Our
experiments show that Contrastive Search, which mitigates repetition, yields
better uncertainty estimates on average across a range of preference-aligned
LLMs. In contrast, the benefits of these strategies sometimes diverge when the
model is only post-trained with supervised fine-tuning, i.e. without explicit
alignment.

</details>


### [254] [OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama](https://arxiv.org/abs/2509.16713)
*Tianyang Xu,Hongqiu Wu,Weiqi Wu,Hai Zhao*

Main category: cs.CL

TL;DR: LLM-based Interactive Drama是一个新兴对话场景，玩家可以沉浸角色并与LLM代理互动参与戏剧故事。由于缺乏完善开发平台，该领域研究受限。Open-Theatre是首个开源工具包，提供高效多智能体架构和分层检索记忆系统来增强叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: LLM交互式戏剧具有巨大潜力但研究不足，主要障碍是缺乏完整的开发环境，使得研究人员难以复制、扩展和研究此类系统。

Method: 提出Open-Theatre工具包，采用高效多智能体架构和分层检索记忆系统，提供高度可配置的流水线，便于研究人员开发和优化新方法。

Result: 开发了首个用于LLM交互式戏剧的开源工具包，改进了先前工作，增强了复杂交互中的叙事连贯性和长期行为真实性。

Conclusion: Open-Theatre为LLM交互式戏剧研究提供了重要工具，降低了研究门槛，促进了该领域的发展。

Abstract: LLM-based Interactive Drama introduces a novel dialogue scenario in which the
player immerses into a character and engages in a dramatic story by interacting
with LLM agents. Despite the fact that this emerging area holds significant
promise, it remains largely underexplored due to the lack of a well-designed
playground to develop a complete drama. This makes a significant barrier for
researchers to replicate, extend, and study such systems. Hence, we present
Open-Theatre, the first open-source toolkit for experiencing and customizing
LLM-based interactive drama. It refines prior work with an efficient
multi-agent architecture and a hierarchical retrieval-based memory system,
designed to enhance narrative coherence and realistic long-term behavior in
complex interactions. In addition, we provide a highly configurable pipeline,
making it easy for researchers to develop and optimize new approaches.

</details>


### [255] [Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling](https://arxiv.org/abs/2509.16717)
*Haoran Li,Zhiming Su,Junyan Yao,Enwei Zhang,Yang Ji,Yan Chen,Kan Zhou,Chao Feng,Jiao Ran*

Main category: cs.CL

TL;DR: 本文提出了一种半监督合成数据管道，用于生成具有可控相关性标签的中文短视频数据，解决了现有提示合成方法在领域特定数据分布和细粒度相关性多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的合成方法难以捕捉领域特定的数据分布，特别是在数据稀缺领域，并且经常忽略细粒度的相关性多样性。

Method: 提出半监督合成数据管道，两个协同训练的模型生成具有可控相关性标签的领域自适应短视频数据，通过为代表性不足的中间相关性标签合成样本来增强相关性级别多样性。

Result: 离线实验显示，使用合成数据训练的嵌入模型优于基于提示或普通监督微调生成的数据。在线A/B测试中，所提模型在抖音双列场景的搜索增强推荐管道中，点击率提高1.45%，强相关性比率提高4.9%，图像用户渗透率提高0.1054%。

Conclusion: 在训练数据中融入更多样化的细粒度相关性级别可以增强模型对细微语义差异的敏感性，凸显了细粒度相关性监督在嵌入学习中的价值。

Abstract: Synthetic data is widely adopted in embedding models to ensure diversity in
training data distributions across dimensions such as difficulty, length, and
language. However, existing prompt-based synthesis methods struggle to capture
domain-specific data distributions, particularly in data-scarce domains, and
often overlook fine-grained relevance diversity. In this paper, we present a
Chinese short video dataset with 4-level relevance annotations, filling a
critical resource void. Further, we propose a semi-supervised synthetic data
pipeline where two collaboratively trained models generate domain-adaptive
short video data with controllable relevance labels. Our method enhances
relevance-level diversity by synthesizing samples for underrepresented
intermediate relevance labels, resulting in a more balanced and semantically
rich training data set. Extensive offline experiments show that the embedding
model trained on our synthesized data outperforms those using data generated
based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we
demonstrate that incorporating more diverse fine-grained relevance levels in
training data enhances the model's sensitivity to subtle semantic distinctions,
highlighting the value of fine-grained relevance supervision in embedding
learning. In the search enhanced recommendation pipeline of Douyin's
dual-column scenario, through online A/B testing, the proposed model increased
click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance
Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by
0.1054%.

</details>


### [256] [Time to Revist Exact Match](https://arxiv.org/abs/2509.16720)
*Auss Abbood,Zaiqiao Meng,Nigel Collier*

Main category: cs.CL

TL;DR: 本文提出将时序问答视为数值估计任务，引入TempAnswerQA基准和sMAPE、MASE等预测指标，发现EM指标无法有效评估时序推理误差，需要专门的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前时序问答评估使用精确匹配（EM）方法，无法区分不同大小的数值误差，难以准确评估模型的时序推理能力。

Method: 从Test of Time和TempTabQA中提取构建TempAnswerQA基准，使用sMAPE和MASE等预测指标替代EM来评估模型性能。

Result: 研究发现误差大小与EM得分脱钩，低EM模型可能具有低sMAPE，而高EM模型可能sMAPE较高。MASE指标重新排序了模型排名，揭示了模型在时序领域知识理解上的差距。

Conclusion: 时序问答任务需要专门的评估指标，sMAPE和MASE能更准确地评估模型的时序推理能力，特别是对±1误差的合理加权。

Abstract: Temporal question answering is an established method for evaluating temporal
reasoning in large language models. Expected answers are often numeric (e.g.,
dates or durations), yet model responses are evaluated like regular text with
exact match (EM), unable to distinguish small from large errors. In this
investigative work, we frame temporal question answering as a numerical
estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a
benchmark distilled from Test of Time and TempTabQA, where all questions
require a numerical, temporal answer, allowing us to evaluate models beyond EM.
We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)
and mean absolute scaled error (MASE). With sMAPE, we find that error size and
EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some
models have high sMAPE despite high EM. Scaling errors by the deviation of the
ground truth data with MASE reshuffles model rankings compared to EM, revealing
gaps in models' understanding of temporal domain knowledge, especially when
trained with synthetic data. Lastly, the models' most frequent error is to
deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,
adequately weight these errors. Our findings underscore the need for
specialised metrics for temporal QA tasks. Code and data are available on
https://github.com/aauss/temporal-answer-qa.

</details>


### [257] [A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse](https://arxiv.org/abs/2509.16722)
*Xiaohan Ding,Kaike Ping,Buse Çarık,Eugenia Rho*

Main category: cs.CL

TL;DR: CausalTalk是一个多层次的因果语言数据集，包含2020-2024年Reddit上关于COVID-19公共卫生讨论的帖子，标注了四个因果任务：二元因果分类、显隐式因果识别、因果跨度提取和因果要点生成。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注结构化文本中的显式因果关系，对社交媒体等非正式文本中的隐式因果表达支持有限，需要专门的数据集来研究非正式语境下的因果推理。

Method: 构建包含10120个Reddit帖子的数据集，采用专家标注的黄金标准和GPT-4o生成、人工验证的白银标准相结合的方式进行多级标注。

Result: CausalTalk数据集填补了细粒度因果检测与非正式文本推理之间的空白，为在社交媒体语境下研究因果推理提供了丰富资源。

Conclusion: 该数据集支持判别式和生成式模型的基准测试，为非正式话语中的因果语言理解提供了重要工具。

Abstract: Understanding causal language in informal discourse is a core yet
underexplored challenge in NLP. Existing datasets largely focus on explicit
causality in structured text, providing limited support for detecting implicit
causal expressions, particularly those found in informal, user-generated social
media posts. We introduce CausalTalk, a multi-level dataset of five years of
Reddit posts (2020-2024) discussing public health related to the COVID-19
pandemic, among which 10120 posts are annotated across four causal tasks: (1)
binary causal classification, (2) explicit vs. implicit causality, (3)
cause-effect span extraction, and (4) causal gist generation. Annotations
comprise both gold-standard labels created by domain experts and
silver-standard labels generated by GPT-4o and verified by human annotators.
CausalTalk bridges fine-grained causal detection and gist-based reasoning over
informal text. It enables benchmarking across both discriminative and
generative models, and provides a rich resource for studying causal reasoning
in social media contexts.

</details>


### [258] [Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation](https://arxiv.org/abs/2509.16729)
*Evgeniia Tokarchuk,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文提出通过鼓励神经隐藏表示的角分散来加速k-NN机器翻译的检索过程，同时略微提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: k-NN机器翻译虽然能提升翻译性能，但其计算成本和内存需求较高，即使使用近似检索算法仍是瓶颈。现有研究主要关注减少数据存储大小或检索调用次数，而本文探索基于检索数据结构性能特性的正交方向。

Method: 通过鼓励神经隐藏表示（翻译上下文的向量表示）的角分散，改善近似k-NN检索数据结构的平衡性，从而加速检索过程。

Result: 改进分散性导致检索数据结构更平衡，加速了检索过程，并略微改善了翻译质量。

Conclusion: 通过优化隐藏表示的分散性，可以在不牺牲翻译质量的前提下显著提升k-NN机器翻译的效率，为加速检索密集型NLP任务提供了新思路。

Abstract: Augmenting neural machine translation with external memory at decoding time,
in the form of k-nearest neighbors machine translation ($k$-NN MT), is a
well-established strategy for increasing translation performance. $k$-NN MT
retrieves a set of tokens that occurred in the most similar contexts recorded
in a prepared data store, using hidden state representations of translation
contexts as vector lookup keys. One of the main disadvantages of this method is
the high computational cost and memory requirements. Since an exhaustive search
is not feasible in large data stores, practitioners commonly use approximate
$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to
research directions seeking to accelerate $k$-NN MT by reducing data store size
or the number of lookup calls, we pursue an orthogonal direction based on the
performance properties of approximate $k$-NN MT lookup data structures. In
particular, we propose to encourage angular dispersion of the neural hidden
representations of contexts. We show that improving dispersion leads to better
balance in the retrieval data structures, accelerating retrieval and slightly
improving translations.

</details>


### [259] [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
*Fagun Patel,Duc Q. Nguyen,Sang T. Truong,Jody Vaynshtok,Sanmi Koyejo,Nick Haber*

Main category: cs.CL

TL;DR: 该论文开发了首个用于评估多模态语言模型在言语语言病理学中应用的综合基准，揭示了当前模型在临床环境中的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 美国有340万儿童需要言语障碍干预，但言语语言病理学家数量严重不足，急需技术支持来提高工作效率。现有多模态语言模型在临床环境中的表现尚未得到充分研究。

Method: 与领域专家合作开发了MLMs在言语语言病理学中的用例分类法，构建了包含5个核心用例（每个用例1000个手动标注数据点）的基准测试，评估了15个最先进的MLMs，并研究了微调效果。

Result: 没有单一模型在所有任务中表现一致最优；发现系统性差异（模型在男性说话者上表现更好）；思维链提示在分类任务中可能降低性能；领域特定数据微调可使性能提升30%以上。

Conclusion: 当前MLMs在言语语言病理学应用中既有潜力也有局限性，需要进一步研究和针对性开发。

Abstract: According to the U.S. National Institutes of Health, more than 3.4 million
children experience speech disorders that require clinical intervention. The
number of speech-language pathologists (SLPs) is roughly 20 times fewer than
the number of affected children, highlighting a significant gap in children's
care and a pressing need for technological support that improves the
productivity of SLPs. State-of-the-art multimodal language models (MLMs) show
promise for supporting SLPs, but their use remains underexplored largely due to
a limited understanding of their performance in high-stakes clinical settings.
To address this gap, we collaborate with domain experts to develop a taxonomy
of real-world use cases of MLMs in speech-language pathologies. Building on
this taxonomy, we introduce the first comprehensive benchmark for evaluating
MLM across five core use cases, each containing 1,000 manually annotated data
points. This benchmark includes robustness and sensitivity tests under various
settings, including background noise, speaker gender, and accent. Our
evaluation of 15 state-of-the-art MLMs reveals that no single model
consistently outperforms others across all tasks. Notably, we find systematic
disparities, with models performing better on male speakers, and observe that
chain-of-thought prompting can degrade performance on classification tasks with
large label spaces and narrow decision boundaries. Furthermore, we study
fine-tuning MLMs on domain-specific data, achieving improvements of over 30%
compared to base models. These findings highlight both the potential and
limitations of current MLMs for speech-language pathology applications,
underscoring the need for further research and targeted development.

</details>


### [260] [MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language](https://arxiv.org/abs/2509.16781)
*Andrei-Marius Avram,Ema-Ioana Bănescu,Anda-Teodora Robea,Dumitru-Clementin Cercel,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: MoRoVoc是最大的罗马尼亚语口语区域变异分析数据集，包含93小时音频和88,192个样本，平衡了罗马尼亚和摩尔多瓦的语音数据。论文提出了多目标对抗训练框架，将人口统计属性作为对抗目标，通过元学习动态调整对抗系数。


<details>
  <summary>Details</summary>
Motivation: 解决罗马尼亚语口语区域变异分析缺乏大规模数据集的问题，并开发能够区分主要任务同时保持对次要属性不变性的语音模型。

Method: 提出多目标对抗训练框架，将年龄和性别等人口统计属性作为对抗目标，通过元学习动态调整对抗系数来优化模型性能。

Result: Wav2Vec2-Base在使用性别作为对抗目标时，罗马尼亚语变异识别准确率达到78.21%；Wav2Vec2-Large在使用方言和年龄作为对抗目标时，性别分类准确率达到93.08%。

Conclusion: MoRoVoc数据集和提出的多目标对抗训练框架有效提升了语音模型在区域变异识别和人口统计属性分类方面的性能。

Abstract: This paper introduces MoRoVoc, the largest dataset for analyzing the regional
variation of spoken Romanian. It has more than 93 hours of audio and 88,192
audio samples, balanced between the Romanian language spoken in Romania and the
Republic of Moldova. We further propose a multi-target adversarial training
framework for speech models that incorporates demographic attributes (i.e., age
and gender of the speakers) as adversarial targets, making models
discriminative for primary tasks while remaining invariant to secondary
attributes. The adversarial coefficients are dynamically adjusted via
meta-learning to optimize performance. Our approach yields notable gains:
Wav2Vec2-Base achieves 78.21% accuracy for the variation identification of
spoken Romanian using gender as an adversarial target, while Wav2Vec2-Large
reaches 93.08% accuracy for gender classification when employing both dialect
and age as adversarial objectives.

</details>


### [261] [Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies](https://arxiv.org/abs/2509.16788)
*Salha Alyami,Amani Jamal,Areej Alhothali*

Main category: cs.CL

TL;DR: 本研究提出了一种针对阿拉伯语方面情感分析（ABSA）的领域自适应预训练方法，通过比较特征提取、全微调和适配器微调等策略，发现领域自适应预训练能带来适度改进，适配器方法计算效率高且效果有竞争力。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语ABSA面临标注数据稀缺的问题，现有预训练模型基于事实数据，在领域特定任务中可能存在偏差，目前尚无研究将自适应预训练应用于阿拉伯语上下文模型进行ABSA。

Method: 采用领域自适应预训练方法，研究特征提取、全微调和适配器微调三种微调策略，使用多个适应语料库和上下文模型进行方面情感分类和意见目标表达提取。

Result: 领域自适应预训练带来适度改进，适配器微调计算效率高且结果有竞争力。错误分析揭示了模型预测和数据集标注的问题，包括情感标注错误、对比标记误解、早期术语的积极性偏见等。

Conclusion: 研究结果强调了需要语法和语义感知模型（如图卷积网络）来更有效地捕捉长距离关系和复杂的基于方面的意见对齐。

Abstract: Aspect-based sentiment analysis (ABSA) in natural language processing enables
organizations to understand customer opinions on specific product aspects.
While deep learning models are widely used for English ABSA, their application
in Arabic is limited due to the scarcity of labeled data. Researchers have
attempted to tackle this issue by using pre-trained contextualized language
models such as BERT. However, these models are often based on fact-based data,
which can introduce bias in domain-specific tasks like ABSA. To our knowledge,
no studies have applied adaptive pre-training with Arabic contextualized models
for ABSA. This research proposes a novel approach using domain-adaptive
pre-training for aspect-sentiment classification (ASC) and opinion target
expression (OTE) extraction. We examine fine-tuning strategies - feature
extraction, full fine-tuning, and adapter-based methods - to enhance
performance and efficiency, utilizing multiple adaptation corpora and
contextualized models. Our results show that in-domain adaptive pre-training
yields modest improvements. Adapter-based fine-tuning is a computationally
efficient method that achieves competitive results. However, error analyses
reveal issues with model predictions and dataset labeling. In ASC, common
problems include incorrect sentiment labeling, misinterpretation of contrastive
markers, positivity bias for early terms, and challenges with conflicting
opinions and subword tokenization. For OTE, issues involve mislabeling targets,
confusion over syntactic roles, difficulty with multi-word expressions, and
reliance on shallow heuristics. These findings underscore the need for syntax-
and semantics-aware models, such as graph convolutional networks, to more
effectively capture long-distance relations and complex aspect-based opinion
alignments.

</details>


### [262] [KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis](https://arxiv.org/abs/2509.16804)
*Kozhin muhealddin Awlla,Hadi Veisi,Abdulhady Abas Abdullah*

Main category: cs.CL

TL;DR: 该论文通过将BERT集成到自然语言处理技术中，增强了中央库尔德语情感分析的研究。


<details>
  <summary>Details</summary>
Motivation: 库尔德语是一种资源匮乏的语言，具有高度的语言多样性但计算资源有限，这使得情感分析具有挑战性。传统方法如Word2Vec效果有限，而BERT的出现为改进提供了希望。

Method: 使用BERT模型替代传统的词嵌入模型（如Word2Vec），利用BERT更好的词嵌入能力来捕捉库尔德语的语义细微差别和上下文复杂性。

Result: BERT模型为库尔德语情感分析设定了新的基准，能够更好地处理这种低资源语言的语义和上下文特征。

Conclusion: 该研究表明BERT在低资源语言（如库尔德语）的情感分析中具有显著优势，为类似语言的处理提供了新的技术路径。

Abstract: This paper enhances the study of sentiment analysis for the Central Kurdish
language by integrating the Bidirectional Encoder Representations from
Transformers (BERT) into Natural Language Processing techniques. Kurdish is a
low-resourced language, having a high level of linguistic diversity with
minimal computational resources, making sentiment analysis somewhat
challenging. Earlier, this was done using a traditional word embedding model,
such as Word2Vec, but with the emergence of new language models, specifically
BERT, there is hope for improvements. The better word embedding capabilities of
BERT lend to this study, aiding in the capturing of the nuanced semantic pool
and the contextual intricacies of the language under study, the Kurdish
language, thus setting a new benchmark for sentiment analysis in low-resource
languages.

</details>


### [263] [Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text](https://arxiv.org/abs/2509.16813)
*Devin R. Wright,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 本文介绍了CLIFS（认知语言学身份融合评分），这是一种结合认知语言学和大型语言模型的新指标，用于量化身份融合——即个体与群体或抽象目标的心理融合。


<details>
  <summary>Details</summary>
Motivation: 量化身份融合对于理解群体行为至关重要，但传统方法需要受控调查或直接实地接触，限制了可扩展性。

Method: CLIFS基于隐式隐喻检测，利用大型语言模型实现完全自动化的评估，无需传统问卷调查。

Result: 在基准测试中，CLIFS优于现有自动化方法和人工标注，在暴力风险评估应用中能将评估效果提升240%以上。

Conclusion: CLIFS展示了在身份融合量化方面的潜力，但需要更大更多样的数据集来增强泛化能力，推动这一新兴领域的发展。

Abstract: Quantifying identity fusion -- the psychological merging of self with another
entity or abstract target (e.g., a religious group, political party, ideology,
value, brand, belief, etc.) -- is vital for understanding a wide range of
group-based human behaviors. We introduce the Cognitive Linguistic Identity
Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with
large language models (LLMs), which builds on implicit metaphor detection.
Unlike traditional pictorial and verbal scales, which require controlled
surveys or direct field contact, CLIFS delivers fully automated, scalable
assessments while maintaining strong alignment with the established verbal
measure. In benchmarks, CLIFS outperforms both existing automated approaches
and human annotation. As a proof of concept, we apply CLIFS to violence risk
assessment to demonstrate that it can improve violence risk assessment by more
than 240%. Building on our identification of a new NLP task and early success,
we underscore the need to develop larger, more diverse datasets that encompass
additional fusion-target domains and cultural backgrounds to enhance
generalizability and further advance this emerging area. CLIFS models and code
are public at https://github.com/DevinW-sudo/CLIFS.

</details>


### [264] [Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming](https://arxiv.org/abs/2509.16835)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 提出了一种基于语义的主题建模框架，用于自动分析虚拟头脑风暴会议中的创意，相比传统方法具有更高的主题连贯性和更好的解释性。


<details>
  <summary>Details</summary>
Motivation: 虚拟头脑风暴会议中创意数量庞大且分布不均，手动编码耗时且主观，需要自动化方法来支持群体创造力的评估。

Method: 集成四个模块化组件的语义驱动主题建模框架：基于Transformer的嵌入（Sentence-BERT）、降维（UMAP）、聚类（HDBSCAN）以及主题提取与精炼。

Result: 在Zoom头脑风暴会话上的评估显示，该模型平均主题连贯性得分为0.687（CV），显著优于LDA、ETM和BERTopic等基准方法。

Conclusion: 该工作凸显了基于嵌入的主题建模在分析协作创意生成方面的潜力，为研究同步虚拟会议中的创造力提供了高效可扩展的框架。

Abstract: Virtual brainstorming sessions have become a central component of
collaborative problem solving, yet the large volume and uneven distribution of
ideas often make it difficult to extract valuable insights efficiently. Manual
coding of ideas is time-consuming and subjective, underscoring the need for
automated approaches to support the evaluation of group creativity. In this
study, we propose a semantic-driven topic modeling framework that integrates
four modular components: transformer-based embeddings (Sentence-BERT),
dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction
with refinement. The framework captures semantic similarity at the sentence
level, enabling the discovery of coherent themes from brainstorming transcripts
while filtering noise and identifying outliers. We evaluate our approach on
structured Zoom brainstorming sessions involving student groups tasked with
improving their university. Results demonstrate that our model achieves higher
topic coherence compared to established methods such as LDA, ETM, and BERTopic,
with an average coherence score of 0.687 (CV), outperforming baselines by a
significant margin. Beyond improved performance, the model provides
interpretable insights into the depth and diversity of topics explored,
supporting both convergent and divergent dimensions of group creativity. This
work highlights the potential of embedding-based topic modeling for analyzing
collaborative ideation and contributes an efficient and scalable framework for
studying creativity in synchronous virtual meetings.

</details>


### [265] [Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment](https://arxiv.org/abs/2509.16876)
*Jiun-Ting Li,Bi-Cheng Yan,Yi-Cheng Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种多任务预训练策略用于自动发音评估，通过重构输入特征来捕捉长期发音线索，并结合手工特征实现更全面的口语能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动发音评估系统主要依赖音素级特征，忽视了超音段发音线索，且缺乏与自动口语评估的整合，限制了整体能力评估的全面性。

Method: 提出多任务预训练策略，随机掩蔽音段级发音特征并基于周围上下文进行重构；同时结合手工特征（如流利度、重音等）通过回归器生成可解释的能力分数。

Result: 在speechocean762数据集上的实验显示，该方法在发音评分和口语能力相关性方面均有提升。

Conclusion: 该方法能够实现针对性训练和全面的能力评估，为发音评估提供了更有效的解决方案。

Abstract: Automatic pronunciation assessment (APA) analyzes second-language (L2)
learners' speech by providing fine-grained pronunciation feedback at various
linguistic levels. Most existing efforts on APA typically adopt segmental-level
features as inputs and predict pronunciation scores at different granularities
via hierarchical (or parallel) pronunciation modeling. This, however,
inevitably causes assessments across linguistic levels (e.g., phone, word, and
utterance) to rely solely on phoneme-level pronunciation features, nearly
sidelining supra-segmental pronunciation cues. To address this limitation, we
introduce multi-task pretraining (MTP) for APA, a simple yet effective strategy
that attempts to capture long-term temporal pronunciation cues while
strengthening the intrinsic structures within an utterance via the objective of
reconstructing input features. Specifically, for a phoneme-level encoder of an
APA model, the proposed MTP strategy randomly masks segmental-level
pronunciation features and reconstructs the masked ones based on their
surrounding pronunciation context. Furthermore, current APA systems lack
integration with automated speaking assessment (ASA), limiting holistic
proficiency evaluation. Drawing on empirical studies and prior knowledge in
ASA, our framework bridges this gap by incorporating handcrafted features
(HCFs), such as fluency (speech rate, silence duration) and stress (pitch
accent strength), derived from human-designed formulas via regressors to
generate interpretable proficiency scores. Experiments on speechocean762 show
improved pronunciation scoring and ASA proficiency correlation, enabling
targeted training and comprehensive proficiency assessment.

</details>


### [266] [Can GRPO Boost Complex Multimodal Table Understanding?](https://arxiv.org/abs/2509.16889)
*Xiaoqiang Kang,Shengen Wu,Zimu Wang,Yilin Liu,Xiaobo Jin,Kaizhu Huang,Wei Wang,Yutao Yue,Xiaowei Huang,Qiufeng Wang*

Main category: cs.CL

TL;DR: Table-R1是一个三阶段强化学习框架，通过预热、感知对齐GRPO和提示完成GRPO来提升多模态表格理解能力，显著超越SFT和GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法面临复杂表格结构和逻辑推理的挑战，传统强化学习方法在表格场景中存在初始策略准确率低和奖励粗糙的问题。

Method: 三阶段RL框架：1）预热阶段培养初始感知推理能力；2）PA-GRPO使用连续TEDS奖励识别表格结构；3）HC-GRPO利用基于提示问题的残差步骤细粒度奖励。

Result: Table-R1在held-in和held-out数据集上显著提升表格推理性能，Qwen2-VL-7B模型甚至超越更大的Table-LLaVA 13B模型，在held-in数据集上达到与GPT-4o相当的性能。

Conclusion: Table-R1有效克服了初始化瓶颈和奖励稀疏性问题，推动了鲁棒的多模态表格理解技术的发展。

Abstract: Existing table understanding methods face challenges due to complex table
structures and intricate logical reasoning. While supervised finetuning (SFT)
dominates existing research, reinforcement learning (RL), such as Group
Relative Policy Optimization (GRPO), has shown promise but struggled with low
initial policy accuracy and coarse rewards in tabular contexts. In this paper,
we introduce Table-R1, a three-stage RL framework that enhances multimodal
table understanding through: (1) Warm-up that prompts initial perception and
reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs
continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table
structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes
fine-grained rewards of residual steps based on the hint-guided question.
Extensive experiments demonstrate that Table-R1 can boost the model's table
reasoning performance obviously on both held-in and held-out datasets,
outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1
surpasses larger specific table understanding models (e.g., Table-LLaVA 13B),
even achieving comparable performance to the closed-source model GPT-4o on
held-in datasets, demonstrating the efficacy of each stage of Table-R1 in
overcoming initialization bottlenecks and reward sparsity, thereby advancing
robust multimodal table understanding.

</details>


### [267] [CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification](https://arxiv.org/abs/2509.16903)
*Nawar Turk,Daniele Comitogianni,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文介绍了DISRPT 2025共享任务3（话语关系分类）的提交方案，该任务面临多语言和跨形式主义挑战，作者通过微调多语言BERT模型、评估基于提示的大语言模型，并提出了HiDAC分层双适配器对比学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决Task 3中统一17种话语关系标签在39个语料库、16种语言和6种话语框架下的多语言和跨形式主义分类挑战。

Method: 1）微调多语言BERT模型（mBERT、XLM-RoBERTa）并采用两种论元排序策略和渐进解冻比率；2）评估基于提示的大语言模型（Claude Opus 4.0）的零样本和少样本性能；3）提出HiDAC分层双适配器对比学习模型。

Result: 较大transformer模型准确率更高但改进有限，解冻编码器顶部75%层可获得与全微调相当的性能但参数更少；基于提示的模型显著落后于微调transformer；HiDAC达到最高总体准确率67.5%且参数效率更高。

Conclusion: HiDAC模型在话语关系分类任务中实现了最佳性能，同时保持了较高的参数效率，为多语言和跨形式主义的话语分析提供了有效解决方案。

Abstract: We present our submission to Task 3 (Discourse Relation Classification) of
the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse
relation labels across 39 corpora in 16 languages and six discourse frameworks,
posing significant multilingual and cross-formalism challenges. We first
benchmark the task by fine-tuning multilingual BERT-based models (mBERT,
XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies
and progressive unfreezing ratios to establish strong baselines. We then
evaluate prompt-based large language models (namely Claude Opus 4.0) in
zero-shot and few-shot settings to understand how LLMs respond to the newly
proposed unified labels. Finally, we introduce HiDAC, a Hierarchical
Dual-Adapter Contrastive learning model. Results show that while larger
transformer models achieve higher accuracy, the improvements are modest, and
that unfreezing the top 75% of encoder layers yields performance comparable to
full fine-tuning while training far fewer parameters. Prompt-based models lag
significantly behind fine-tuned transformers, and HiDAC achieves the highest
overall accuracy (67.5%) while remaining more parameter-efficient than full
fine-tuning.

</details>


### [268] [CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages](https://arxiv.org/abs/2509.16914)
*Wenhao Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 构建并开源CUTE多语言数据集，包含中文、维吾尔语、藏语和英语，旨在提升大语言模型对低资源语言的处理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源丰富语言上表现出色，但对低资源语言支持不足，主要原因是训练语料稀缺。

Method: 通过机器翻译构建了两个25GB的四语言语料库（一个平行语料库和一个非平行语料库），并进行了人工评估验证翻译质量。

Result: CUTE是目前最大的维吾尔语和藏语开源语料库，有效提升了LLMs处理低资源语言的能力。

Conclusion: CUTE语料库和相关模型已公开，为研究社区提供了重要资源，并探讨了语料平行性在跨语言迁移学习中的作用。

Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities
in various NLP tasks, significantly enhancing user experience and efficiency.
However, this advantage is primarily limited to resource-rich languages. For
the diverse array of low-resource languages, support remains inadequate, with
the scarcity of training corpora considered the primary cause. We construct and
open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two
25GB sets of four-language corpora (one parallel and one non-parallel),
obtained through machine translation. CUTE encompasses two resource-rich
languages (Chinese and English) and two low-resource languages (Uyghur and
Tibetan). Prior to constructing CUTE, human assessment validates that the
machine translation quality between Chinese-Uyghur and Chinese-Tibetan
approaches that of Chinese-English translation. CUTE represents the largest
open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate
its effectiveness in enhancing LLMs' ability to process low-resource languages
while investigating the role of corpus parallelism in cross-lingual transfer
learning. The CUTE corpus and related models are made publicly available to the
research community.

</details>


### [269] [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
*Yongrui Chen,Yi Huang,Yunchang Liu,Shenyu Zhang,Junhao He,Tongtong Wu,Guilin Qi,Tianxing Wu*

Main category: cs.CL

TL;DR: 提出K-DeCore框架解决持续结构化知识推理中的参数增长和泛化问题，通过知识解耦机制和双视角记忆巩固实现高效推理


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在处理异构结构化知识时泛化能力差，且随着任务增加参数增长导致推理效率低下

Method: K-DeCore框架包含知识解耦机制（将推理过程分解为任务特定和任务无关阶段）、双视角记忆巩固机制和结构引导的伪数据合成策略

Result: 在四个基准数据集上的实验表明，K-DeCore在多种指标上优于现有持续学习方法，适用于多种骨干大语言模型

Conclusion: K-DeCore通过固定参数数量和知识解耦设计，有效解决了CSKR任务中的挑战，展示了优越的性能

Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to
handle sequential tasks, where each task involves translating natural language
questions into structured queries grounded in structured knowledge. Existing
general continual learning approaches face significant challenges when applied
to this task, including poor generalization to heterogeneous structured
knowledge and inefficient reasoning due to parameter growth as tasks increase.
To address these limitations, we propose a novel CSKR framework,
\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.
Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling
mechanism that disentangles the reasoning process into task-specific and
task-agnostic stages, effectively bridging the gaps across diverse tasks.
Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective
memory consolidation mechanism for distinct stages and introduces a
structure-guided pseudo-data synthesis strategy to further enhance the model's
generalization capabilities. Extensive experiments on four benchmark datasets
demonstrate the superiority of \textsc{K-DeCore} over existing continual
learning methods across multiple metrics, leveraging various backbone large
language models.

</details>


### [270] [AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation](https://arxiv.org/abs/2509.16952)
*Tiancheng Huang,Ruisheng Cao,Yuxin Zhang,Zhangyi Kang,Zijian Wang,Chenrun Wang,Yijie Luo,Hang Zheng,Lirong Qian,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 提出了AirQA数据集和ExTrActor框架，用于评估和改进LLM在科学论文问答任务中的能力。AirQA是一个人工标注的综合性AI领域论文QA数据集，包含13,948篇论文和1,246个问题。ExTrActor是一个自动化指令数据合成框架，能够生成示例和收集交互轨迹。


<details>
  <summary>Details</summary>
Motivation: 学术论文数量快速增长，研究人员难以高效提取关键信息。虽然基于LLM的代理能够自动化科学论文的问答工作流程，但缺乏全面且现实的基准来评估其能力，且高质量交互轨迹的短缺阻碍了交互代理的训练。

Method: 提出AirQA数据集，涵盖多任务、多模态和实例级评估。开发ExTrActor框架，使用三个基于LLM的代理进行示例生成和轨迹收集，无需人工干预。

Result: 对多个开源和专有模型的评估显示，大多数模型在AirQA上表现不佳，证明了数据集的质量。大量实验证实ExTrActor能够持续提升小模型的多轮工具使用能力，使其达到与更大模型相当的性能。

Conclusion: AirQA为科学论文问答提供了高质量的评估基准，ExTrActor框架有效解决了交互轨迹数据短缺问题，显著提升了小模型在复杂问答任务中的表现。

Abstract: The growing volume of academic papers has made it increasingly difficult for
researchers to efficiently extract key information. While large language models
(LLMs) based agents are capable of automating question answering (QA) workflows
for scientific papers, there still lacks a comprehensive and realistic
benchmark to evaluate their capabilities. Moreover, training an interactive
agent for this specific task is hindered by the shortage of high-quality
interaction trajectories. In this work, we propose AirQA, a human-annotated
comprehensive paper QA dataset in the field of artificial intelligence (AI),
with 13,948 papers and 1,246 questions, that encompasses multi-task,
multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor,
an automated framework for instruction data synthesis. With three LLM-based
agents, ExTrActor can perform example generation and trajectory collection
without human intervention. Evaluations of multiple open-source and proprietary
models show that most models underperform on AirQA, demonstrating the quality
of our dataset. Extensive experiments confirm that ExTrActor consistently
improves the multi-turn tool-use capability of small models, enabling them to
achieve performance comparable to larger ones.

</details>


### [271] [Preference Distillation via Value based Reinforcement Learning](https://arxiv.org/abs/2509.16965)
*Minchan Kwon,Junwon Ko,Kangil Kim,Junmo Kim*

Main category: cs.CL

TL;DR: 提出了TVKD方法，通过教师模型的价值函数引入辅助奖励来改进DPO训练，解决小模型容量有限时二元偏好监督不足的问题


<details>
  <summary>Details</summary>
Motivation: DPO的二元胜负监督对于容量有限的小模型训练不足，现有方法主要模仿教师模型行为而忽略了奖励建模的蒸馏

Method: TVKD方法引入教师模型价值函数的辅助奖励作为软指导，该奖励满足基于势能的奖励塑造，保持DPO的全局奖励结构和最优策略不变

Result: 实验结果表明TVKD在各种基准测试和模型大小上都能持续提升性能

Conclusion: TVKD能够有效改进DPO训练，特别适用于小模型，且无需额外rollout即可集成到标准DPO框架中

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm to align language
models with human preferences using pairwise comparisons. However, its binary
win-or-loss supervision often proves insufficient for training small models
with limited capacity. Prior works attempt to distill information from large
teacher models using behavior cloning or KL divergence. These methods often
focus on mimicking current behavior and overlook distilling reward modeling. To
address this issue, we propose \textit{Teacher Value-based Knowledge
Distillation} (TVKD), which introduces an auxiliary reward from the value
function of the teacher model to provide a soft guide. This auxiliary reward is
formulated to satisfy potential-based reward shaping, ensuring that the global
reward structure and optimal policy of DPO are preserved. TVKD can be
integrated into the standard DPO training framework and does not require
additional rollouts. Our experimental results show that TVKD consistently
improves performance across various benchmarks and model sizes.

</details>


### [272] [Advancing Speech Understanding in Speech-Aware Language Models with GRPO](https://arxiv.org/abs/2509.16990)
*Avishai Elmakies,Hagai Aronowitz,Nimrod Shabtay,Eli Schwartz,Ron Hoory,Avihu Dekel*

Main category: cs.CL

TL;DR: 本文提出了一种基于群组相对策略优化（GRPO）的方法，用于训练语音感知大语言模型（SALLMs）处理开放式语音理解任务，如口语问答和自动语音翻译。


<details>
  <summary>Details</summary>
Motivation: SALLMs在语音理解任务中表现出色，GRPO因其在训练LLMs中的高效性而受到关注。先前研究主要将GRPO应用于多项选择题，本文旨在探索其在更能体现模型生成能力的开放式任务中的应用。

Method: 采用GRPO方法，使用BLEU作为奖励信号来优化SALLMs，并探索在GRPO中整合离策略样本的潜力。

Result: 实证研究表明，该方法在多个关键指标上超越了标准的监督微调（SFT）。

Conclusion: 该方法为SALLMs在开放式语音理解任务中的性能提升提供了有效途径，并指出了进一步研究和改进的方向。

Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based
method for training Speech-Aware Large Language Models (SALLMs) on open-format
speech understanding tasks, such as Spoken Question Answering and Automatic
Speech Translation. SALLMs have proven highly effective for speech
understanding tasks. GRPO has recently gained traction for its efficiency in
training LLMs, and prior work has explored its application to SALLMs, primarily
in multiple-choice tasks. Building on this, we focus on open-format tasks that
better reflect the generative abilities of the models. Our approach leverages
GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate
empirically that it surpasses standard SFT across several key metrics. Finally,
we explore the potential of incorporating off-policy samples within GRPO for
these tasks, highlighting avenues for further improvement and further research.

</details>


### [273] [The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs](https://arxiv.org/abs/2509.17030)
*Hinata Tezuka,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了Transfer Neurons Hypothesis，认为MLP模块中的特定神经元负责在多语言LLM中语言特定潜在空间和共享语义潜在空间之间的表示转换，并验证了这些转换神经元对多语言推理的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明解码器LLM的多语言处理框架：早期层将输入转换为英语中心和语言无关表示，中间层在英语中心潜在空间进行推理，最后层将表示转换回语言特定潜在空间。但这种转换的内部动态和机制尚未充分探索。

Method: 提出并实证验证Transfer Neurons Hypothesis，识别MLP模块中负责语言特定潜在空间和共享语义潜在空间之间表示转换的特定神经元，并分析语言特定神经元在潜在空间转换中的作用。

Result: 验证了转换神经元的存在和功能，发现语言特定神经元的一个功能是促进潜在空间之间的移动，证明转换神经元对多语言LLM的推理能力至关重要。

Conclusion: 转换神经元是多语言LLM中实现语言间表示转换的关键机制，对理解多语言处理框架的内部工作原理具有重要意义。

Abstract: Recent studies have suggested a processing framework for multilingual inputs
in decoder-based LLMs: early layers convert inputs into English-centric and
language-agnostic representations; middle layers perform reasoning within an
English-centric latent space; and final layers generate outputs by transforming
these representations back into language-specific latent spaces. However, the
internal dynamics of such transformation and the underlying mechanism remain
underexplored. Towards a deeper understanding of this framework, we propose and
empirically validate The Transfer Neurons Hypothesis: certain neurons in the
MLP module are responsible for transferring representations between
language-specific latent spaces and a shared semantic latent space.
Furthermore, we show that one function of language-specific neurons, as
identified in recent studies, is to facilitate movement between latent spaces.
Finally, we show that transfer neurons are critical for reasoning in
multilingual LLMs.

</details>


### [274] [Modeling Bottom-up Information Quality during Language Processing](https://arxiv.org/abs/2509.17047)
*Cui Ding,Yanning Yin,Lena A. Jäger,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 该研究通过信息论方法量化视觉输入质量对阅读处理的影响，发现英文和中文中单词上半部分包含更多身份信息，且英文的不对称性更明显。


<details>
  <summary>Details</summary>
Motivation: 验证语言处理模型中关于自下而上输入质量影响处理难度的预测，即噪声输入会导致理解困难。

Method: 提出基于互信息(MI)的视觉信息质量量化方法，通过遮挡单词上下半部分实验比较阅读时间，使用多模态语言模型估计互信息。

Result: 英文和中文中单词上半部分包含更多身份信息，但英文的不对称性更强，这一模式在阅读时间数据中得到反映。

Conclusion: 视觉信息质量确实影响阅读处理难度，信息分布的语言特异性差异会影响阅读效率。

Abstract: Contemporary theories model language processing as integrating both top-down
expectations and bottom-up inputs. One major prediction of such models is that
the quality of the bottom-up inputs modulates ease of processing -- noisy
inputs should lead to difficult and effortful comprehension. We test this
prediction in the domain of reading. First, we propose an information-theoretic
operationalization for the "quality" of bottom-up information as the mutual
information (MI) between visual information and word identity. We formalize
this prediction in a mathematical model of reading as a Bayesian update.
Second, we test our operationalization by comparing participants' reading times
in conditions where words' information quality has been reduced, either by
occluding their top or bottom half, with full words. We collect data in English
and Chinese. We then use multimodal language models to estimate the mutual
information between visual inputs and words. We use these data to estimate the
specific effect of reduced information quality on reading times. Finally, we
compare how information is distributed across visual forms. In English and
Chinese, the upper half contains more information about word identity than the
lower half. However, the asymmetry is more pronounced in English, a pattern
which is reflected in the reading times.

</details>


### [275] [TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?](https://arxiv.org/abs/2509.17054)
*Yiwei Liu,Emma Jane Pretty,Jiahao Huang,Saku Sugawara*

Main category: cs.CL

TL;DR: 本文介绍了TactfulToM基准测试，用于评估大语言模型在理解善意谎言和推理其亲社会动机方面的能力，发现现有模型表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型在心理理论推理任务上的表现，但对于需要更细致社会背景（如善意谎言）的能力研究有限。

Method: 通过多阶段人机协作流程生成基准测试，使用LLMs扩展手动设计的种子故事为对话，保持参与者之间的信息不对称以模拟真实的善意谎言场景。

Result: TactfulToM对最先进的模型具有挑战性，模型表现显著低于人类水平，显示出它们在理解支持善意谎言真正理解的心理理论推理方面的不足。

Conclusion: 该研究揭示了当前大语言模型在理解复杂社会互动（如善意谎言）方面的局限性，为改进模型的社会推理能力提供了重要基准。

Abstract: While recent studies explore Large Language Models' (LLMs) performance on
Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require
more nuanced social context is limited, such as white lies. We introduce
TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to
understand white lies within real-life conversations and reason about prosocial
motivations behind them, particularly when they are used to spare others'
feelings and maintain social harmony. Our benchmark is generated through a
multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed
stories into conversations to maintain the information asymmetry between
participants necessary for authentic white lies. We show that TactfulToM is
challenging for state-of-the-art models, which perform substantially below
humans, revealing shortcomings in their ability to fully comprehend the ToM
reasoning that enables true understanding of white lies.

</details>


### [276] [SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis](https://arxiv.org/abs/2509.17167)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Joseph Skrovan,Mehak Beri,Hitakshi Modi,Andrew Well,Liu Leqi,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: 本文提出SFT-TA框架，通过在多智能体系统中嵌入监督微调（SFT）代理来自动化主题分析，相比现有方法和GPT-4o基线，能更好地与人类参考主题对齐。


<details>
  <summary>Details</summary>
Motivation: 手动主题分析耗时且难以扩展，现有基于LLM的自动化方法在结果对齐方面仍有局限。

Method: 提出SFT-TA框架，将监督微调的代理嵌入多智能体系统中进行主题分析。

Result: SFT-TA框架在主题对齐方面优于现有框架和GPT-4o基线，单独SFT代理可能表现不佳，但在多智能体系统中效果更好。

Conclusion: 在多智能体系统中嵌入特定角色的SFT代理是提高主题分析结果对齐度的有效途径。

Abstract: Thematic Analysis (TA) is a widely used qualitative method that provides a
structured yet flexible framework for identifying and reporting patterns in
clinical interview transcripts. However, manual thematic analysis is
time-consuming and limits scalability. Recent advances in LLMs offer a pathway
to automate thematic analysis, but alignment with human results remains
limited. To address these limitations, we propose SFT-TA, an automated thematic
analysis framework that embeds supervised fine-tuned (SFT) agents within a
multi-agent system. Our framework outperforms existing frameworks and the
gpt-4o baseline in alignment with human reference themes. We observed that SFT
agents alone may underperform, but achieve better results than the baseline
when embedded within a multi-agent system. Our results highlight that embedding
SFT agents in specific roles within a multi-agent system is a promising pathway
to improve alignment with desired outputs for thematic analysis.

</details>


### [277] [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177)
*Bowen Qin,Chen Yue,Fang Yin,Hui Wang,JG Yao,Jiakang Liu,Jing-Shu Zheng,Miguel Hu Chen,Richeng Xuan,Shibei Meng,Shiqi Zhou,Teng Dai,Tong-Shuai Ren,Wei Cui,Xi Yang,Xialin Du,Xiaojing Xu,Xue Sun,Xuejing Li,Yaming Liu,Yesheng Liu,Ying Liu,Yonghua Lin,Yu Zhao,Yunduo Zhang,Yuwen Luo,Zheqi He,Zhiyuan He,Zhongyuan Wang*

Main category: cs.CL

TL;DR: 本文提出了ROME评估基准，用于测试视觉语言模型从视觉线索进行推理的能力，并对当前大型推理模型进行了中等规模的无污染评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对视觉语言模型推理能力的评估基准，需要建立一个无污染的评估框架来准确衡量大型推理模型的性能。

Method: 开发了ROME评估基准，包含视觉推理任务，并在中等规模上对大型推理模型进行无污染评估。

Result: 发布了ROME基准测试、评估数据和其他更新，提供了对当前大型推理模型的初步评估结果。

Conclusion: ROME基准为视觉语言模型的推理能力评估提供了有效工具，有助于推动该领域的发展。

Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of
current large reasoning models (LRMs) with some preliminary findings. We also
release ROME, our evaluation benchmark for vision language models intended to
test reasoning from visual clues. We attach links to the benchmark, evaluation
data, and other updates on this website:
https://flageval-baai.github.io/LRM-Eval/

</details>


### [278] [Attention Consistency for LLMs Explanation](https://arxiv.org/abs/2509.17178)
*Tian Lan,Jinyuan Xu,Xue He,Jenq-Neng Hwang,Lei Li*

Main category: cs.CL

TL;DR: 提出了MACS（多层注意力一致性评分），一种轻量级且易于部署的启发式方法，用于估计解码器模型中输入令牌的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法存在分辨率低和计算成本高的问题，需要一种更高效的方法来理解大型语言模型的决策过程。

Method: MACS通过测量输入令牌在最大注意力上的一致性来评估其贡献度，是一种基于注意力一致性的轻量级评估方法。

Result: 实证评估显示MACS在可解释性质量和计算效率之间取得了良好平衡，与复杂技术相比保持了相当的忠实度，同时VRAM使用量减少22%，延迟降低30%。

Conclusion: MACS为解决LLM可解释性挑战提供了一种高效实用的解决方案，在保持解释质量的同时显著提升了计算效率。

Abstract: Understanding the decision-making processes of large language models (LLMs)
is essential for their trustworthy development and deployment. However, current
interpretability methods often face challenges such as low resolution and high
computational cost. To address these limitations, we propose the
\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,
and easily deployable heuristic for estimating the importance of input tokens
in decoder-based models. MACS measures contributions of input tokens based on
the consistency of maximal attention. Empirical evaluations demonstrate that
MACS achieves a favorable trade-off between interpretability quality and
computational efficiency, showing faithfulness comparable to complex techniques
with a 22\% decrease in VRAM usage and 30\% reduction in latency.

</details>


### [279] [LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](https://arxiv.org/abs/2509.17183)
*Junsong Li,Jie Zhou,Bihao Zhan,Yutao Yang,Qianjun Pan,Shilian Chen,Tianyu Huai,Xin Li,Qin Chen,Liang He*

Main category: cs.CL

TL;DR: LifeAlign是一个终身对齐框架，通过聚焦偏好优化和短到长记忆整合机制，解决大语言模型在连续学习任务中的灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法存在灾难性遗忘问题，模型在适应新偏好或领域时会丢失先前获得的知识

Method: 1. 聚焦偏好优化策略：在对齐新偏好时防止先前任务知识的侵蚀；2. 短到长记忆整合机制：通过内在维度缩减将去噪的短期偏好表示合并到稳定的长期记忆中

Result: 在多个连续对齐任务上的实验结果表明，LifeAlign在保持偏好对齐质量和知识保留方面优于现有的终身学习方法

Conclusion: LifeAlign框架能够有效实现终身对齐，使大语言模型在连续学习任务中保持稳定的人类偏好对齐

Abstract: Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.

</details>


### [280] [Evolution of Concepts in Language Model Pre-Training](https://arxiv.org/abs/2509.17196)
*Xuyang Ge,Wentao Shu,Jiaxing Wu,Yunhua Zhou,Zhengfu He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文使用crosscoders稀疏字典学习方法追踪语言模型预训练过程中的线性可解释特征演化，发现特征形成具有阶段性，并与下游性能存在因果联系。


<details>
  <summary>Details</summary>
Motivation: 语言模型通过预训练获得广泛能力，但预训练过程仍是一个黑盒，需要揭示其内部表示演化机制。

Method: 采用crosscoders稀疏字典学习方法，在预训练快照上追踪线性可解释特征的演化过程。

Result: 发现大多数特征在特定时间点开始形成，复杂模式在后期训练阶段出现；特征归因分析显示特征演化与下游性能存在因果联系。

Conclusion: 特征级观察与Transformer两阶段学习过程高度一致，为追踪语言模型学习动态中的细粒度表示进展提供了可能性。

Abstract: Language models obtain extensive capabilities through pre-training. However,
the pre-training process remains a black box. In this work, we track linear
interpretable feature evolution across pre-training snapshots using a sparse
dictionary learning method called crosscoders. We find that most features begin
to form around a specific point, while more complex patterns emerge in later
training stages. Feature attribution analyses reveal causal connections between
feature evolution and downstream performance. Our feature-level observations
are highly consistent with previous findings on Transformer's two-stage
learning process, which we term a statistical learning phase and a feature
learning phase. Our work opens up the possibility to track fine-grained
representation progress during language model learning dynamics.

</details>


### [281] [Prompt-Based Simplification for Plain Language using Spanish Language Models](https://arxiv.org/abs/2509.17209)
*Lourdes Moreno,Jesus M. Sanchez-Gomez,Marco Antonio Sanchez-Escudero,Paloma Martínez*

Main category: cs.CL

TL;DR: HULAT-UC3M团队在CLEARS 2025西班牙语文本简化任务中，通过零样本提示工程和LoRA微调策略，开发了基于RigoChat-7B-v2模型的系统，在语义相似度上排名第一，但可读性排名第四。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索西班牙语文本简化到通俗语言的有效方法，应对训练数据异质性和评估指标在语言清晰度与内容保留平衡方面的挑战。

Method: 采用基于西班牙语文本训练的模型，包括零样本提示工程和LoRA微调策略，使用余弦相似度和Fernández-Huerta可读性指数进行评估，最终选择结合标准化步骤、RigoChat-7B-v2模型和专用通俗语言提示的系统。

Result: 最终系统在语义相似度上排名第一（SIM=0.75），但在可读性上排名第四（FH=69.72），显示出在内容保留和语言简化之间的平衡表现。

Conclusion: 研究强调了训练数据异质性和当前评估指标在同时捕捉语言清晰度和内容保留方面的局限性，为未来文本简化研究提供了重要见解。

Abstract: This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask
1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies
based on models trained on Spanish texts, including a zero-shot configuration
using prompt engineering and a fine-tuned version with Low-Rank Adaptation
(LoRA). Different strategies were evaluated on representative internal subsets
of the training data, using the official task metrics, cosine similarity (SIM)
and the Fern\'andez-Huerta readability index (FH) to guide the selection of the
optimal model and prompt combination. The final system was selected for its
balanced and consistent performance, combining normalization steps, the
RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in
semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).
We also discuss key challenges related to training data heterogeneity and the
limitations of current evaluation metrics in capturing both linguistic clarity
and content preservation.

</details>


### [282] [Extending Automatic Machine Translation Evaluation to Book-Length Documents](https://arxiv.org/abs/2509.17249)
*Kuang-Da Wang,Shuoyang Ding,Chao-Han Huck Yang,Ping-Chun Hsieh,Wen-Chih Peng,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: SEGALE是一种评估方案，将现有自动指标扩展到长文档翻译评估，通过将文档视为连续文本并应用句子分割和对齐方法，解决了当前评估方法局限于句子级别的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在翻译性能和长上下文能力方面表现出色，但由于数据集限制、指标token数量限制和严格的句子边界要求，评估方法仍局限于句子级别评估。

Method: SEGALE方案将文档视为连续文本，应用句子分割和对齐方法，能够处理任意长度的文档翻译，同时考虑欠翻译/过翻译和不同的句子边界。

Result: 实验表明该方案显著优于现有的长文档评估方案，与使用真实句子对齐的评估结果相当。应用该方案发现许多开源LLM无法在其报告的最大上下文长度下有效翻译文档。

Conclusion: SEGALE为长文档翻译提供了有效的评估框架，揭示了当前LLM在长文档翻译方面的局限性，为改进长上下文翻译模型提供了重要参考。

Abstract: Despite Large Language Models (LLMs) demonstrating superior translation
performance and long-context capabilities, evaluation methodologies remain
constrained to sentence-level assessment due to dataset limitations, token
number restrictions in metrics, and rigid sentence boundary requirements. We
introduce SEGALE, an evaluation scheme that extends existing automatic metrics
to long-document translation by treating documents as continuous text and
applying sentence segmentation and alignment methods. Our approach enables
previously unattainable document-level evaluation, handling translations of
arbitrary length generated with document-level prompts while accounting for
under-/over-translations and varied sentence boundaries. Experiments show our
scheme significantly outperforms existing long-form document evaluation
schemes, while being comparable to evaluations performed with groundtruth
sentence alignments. Additionally, we apply our scheme to book-length texts and
newly demonstrate that many open-weight LLMs fail to effectively translate
documents at their reported maximum context lengths.

</details>


### [283] [Probabilistic Token Alignment for Large Language Model Fusion](https://arxiv.org/abs/2509.17276)
*Runjia Zeng,James Chenhao Liang,Cheng Han,Zhiwen Cao,Jiahao Liu,Xiaojun Quan,Yingjie Victor Chen,Lifu Huang,Tong Geng,Qifan Wang,Dongfang Liu*

Main category: cs.CL

TL;DR: PTA-LLM提出了一种基于概率令牌对齐的模型融合方法，通过最优传输理论实现预训练大语言模型的软对齐，解决了传统方法依赖手动词汇对齐的局限性。


<details>
  <summary>Details</summary>
Motivation: 从头训练大语言模型成本高昂且容易产生冗余能力，而现有模型融合方法依赖手动预定义的词汇对齐，在不同上下文中泛化能力差，导致性能下降。

Method: 受分布学习启发，将令牌对齐重新表述为最优传输问题，提出概率令牌对齐方法（PTA-LLM），实现分布感知的软映射对齐。

Result: 实证结果表明，概率令牌对齐能够提升目标模型在多种能力上的性能表现。

Conclusion: PTA-LLM不仅具有通用性，还从分布角度提供了可解释性，为令牌对齐的本质提供了新的见解。

Abstract: Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

</details>


### [284] [Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://arxiv.org/abs/2509.17289)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Krishna Dwarampudi,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: CoDe-KG是一个开源端到端流水线，通过结合核心解析和句法分解来提取句子级知识图谱，在关系抽取任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有知识图谱提取方法在复杂句子处理和罕见关系识别方面的不足，提高关系抽取的准确性和覆盖率。

Method: 使用混合思维链和少样本提示策略，结合核心解析与句法分解技术，系统选择最优提示-模型组合。

Result: 在REBEL数据集上达到65.8%的宏F1分数（比之前最佳提升8个百分点），在WebNLG2上达到75.7%的微F1分数，罕见关系召回率提升超过20%。

Conclusion: CoDe-KG证明了核心解析与句法分解结合的有效性，为知识图谱提取提供了强大的开源工具和数据集。

Abstract: We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting
sentence-level knowledge graphs by combining robust coreference resolution with
syntactic sentence decomposition. Using our model, we contribute a dataset of
over 150,000 knowledge triples, which is open source. We also contribute a
training corpus of 7248 rows for sentence complexity, 190 rows of gold human
annotations for co-reference resolution using open source lung-cancer abstracts
from PubMed, 900 rows of gold human annotations for sentence conversion
policies, and 398 triples of gold human annotations. We systematically select
optimal prompt-model pairs across five complexity categories, showing that
hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match
accuracy on sentence simplification. On relation extraction (RE), our pipeline
achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the
art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on
Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference
and decomposition increases recall on rare relations by over 20%. Code and
dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025

</details>


### [285] [Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection](https://arxiv.org/abs/2509.17292)
*Jun Seo Kim,Hyemi Kim,Woo Joo Oh,Hongjin Cho,Hochul Lee,Hye Hyeon Kim*

Main category: cs.CL

TL;DR: 提出结合大语言模型和多示例学习的新框架，通过分解话语为情感、逻辑和行为组件来检测认知扭曲，提高可解释性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲与心理健康障碍密切相关，但由于上下文模糊性、共现性和语义重叠，其自动检测一直具有挑战性。

Method: 将每个话语分解为情感、逻辑和行为组件，使用LLM推断多个扭曲实例，通过多视图门控注意力机制进行最终分类。

Result: 在韩语和英语数据集上的实验表明，结合ELB组件和LLM推断的显著性分数提高了分类性能，特别是对于具有高解释模糊性的扭曲。

Conclusion: 该方法为心理健康NLP中的细粒度推理提供了一种心理学基础且可推广的方法。

Abstract: Cognitive distortions have been closely linked to mental health disorders,
yet their automatic detection remained challenging due to contextual ambiguity,
co-occurrence, and semantic overlap. We proposed a novel framework that
combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)
architecture to enhance interpretability and expression-level reasoning. Each
utterance was decomposed into Emotion, Logic, and Behavior (ELB) components,
which were processed by LLMs to infer multiple distortion instances, each with
a predicted type, expression, and model-assigned salience score. These
instances were integrated via a Multi-View Gated Attention mechanism for final
classification. Experiments on Korean (KoACD) and English (Therapist QA)
datasets demonstrate that incorporating ELB and LLM-inferred salience scores
improves classification performance, especially for distortions with high
interpretive ambiguity. Our results suggested a psychologically grounded and
generalizable approach for fine-grained reasoning in mental health NLP.

</details>


### [286] [Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text](https://arxiv.org/abs/2509.17317)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 该论文研究了通过机器翻译从高资源语言生成数据来训练低资源语言模型的方法，探讨了模型容量扩展、源语言简化以及有限本地数据持续训练的效果。


<details>
  <summary>Details</summary>
Motivation: 大多数语言缺乏足够的大规模单语预训练数据，形成"数据墙"。多语言预训练存在语言不平衡和"多语言诅咒"问题，因此研究通过机器翻译生成数据的方法。

Method: 将英语翻译成印度尼西亚语和泰米尔语，使用GPT-2模型（124M-774M）在本地或机器翻译生成的数据上进行预训练，评估在本地文本上的交叉熵损失、语法探针和下游任务的准确性。

Result: 结果显示：（1）机器翻译预训练模型能从规模扩展中受益；（2）源语言简化会损害对本地文本的泛化能力；（3）在有限本地数据上持续训练的机器翻译预训练模型通常优于仅使用本地数据的模型。

Conclusion: 机器翻译生成的数据可以有效补充低资源语言的训练数据，但对于需要文化细微差别的任务（如毒性检测），需要更多本地数据暴露。

Abstract: Most languages lack sufficient data for large-scale monolingual pretraining,
creating a "data wall." Multilingual pretraining helps but is limited by
language imbalance and the "curse of multilinguality." An alternative is to
translate high-resource text with machine translation (MT), which raises three
questions: (1) How does MT-derived data scale with model capacity? (2) Can
source-side transformations (e.g., simplifying English with an LLM) improve
generalization to native text? (3) How well do models pretrained on MT-derived
data adapt when continually trained on limited native text? We investigate
these questions by translating English into Indonesian and Tamil--two
typologically distant, lower-resource languages--and pretraining GPT-2 models
(124M-774M) on native or MT-derived corpora from raw and LLM-simplified
English. We evaluate cross-entropy loss on native text, along with accuracy on
syntactic probes and downstream tasks. Our results show that (1) MT-pretrained
models benefit from scaling; (2) source-side simplification harms
generalization to native text; and (3) adapting MT-pretrained models on native
text often yields better performance than native-only models, even with less
native data. However, tasks requiring cultural nuance (e.g., toxicity
detection) demand more exposure to native data.

</details>


### [287] [AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](https://arxiv.org/abs/2509.17348)
*Yujie Feng,Jian Li,Xiaoyu Dong,Pengfei Xu,Xiaohui Zhou,Yujia Zhang,Zexin LU,Yasha Wang,Alan Zhao,Xu Chu,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: AimMerging是一种新颖的持续学习框架，通过动态监控训练轨迹来自适应决定模型合并的时机和频率，有效平衡新知识学习和防止遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型合并的持续学习方法在平衡学习新知识和防止遗忘方面存在困难，主要原因是合并次数和频率的优化不足。

Method: AimMerging利用训练轨迹中的学习和遗忘信号动态监控模型状态，通过训练轨迹指导的合并控制器自适应决定迭代融合时机，并使用基于排练的知识融合模块计算合并权重。

Result: 在三个持续学习基准测试中，AimMerging相比现有最优方法在FWT和BWT指标上分别实现了80%和59%的平均相对改进。

Conclusion: AimMerging框架通过自适应迭代模型合并有效解决了持续学习中的知识平衡问题，在多个模型规模下均表现出显著性能提升。

Abstract: Continual learning (CL) is essential for deploying large language models
(LLMs) in dynamic real-world environments without the need for costly
retraining. Recent model merging-based methods have attracted significant
attention, but they still struggle to effectively manage the trade-off between
learning new knowledge and preventing forgetting, a challenge largely stemming
from suboptimal number of merges and merging frequency. In this paper, we
introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework
that utilizes learning and forgetting signals from the training trajectory to
dynamically monitor the model's training status. Guided by dynamic monitoring,
the training trajectory-guided merge controller adaptively determines the
timing and frequency of iterative fusion, while the rehearsal-based knowledge
fusion module computes the merging weights and executes the fusion.
Comprehensive experiments on three CL benchmarks with various model sizes (from
770M to 13B) demonstrate that AimMerging achieves significant performance
improvements over existing state-of-the-art methods, with an average relative
improvement of 80% and 59% on FWT and BWT, respectively. The source code is
provided for reproducibility.

</details>


### [288] [Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation](https://arxiv.org/abs/2509.17349)
*Peter Polák,Sara Papi,Luisa Bentivogli,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了YAAL和LongYAAL两个新的延迟度量指标，以及SoftSegmenter重分割工具，用于改进同时语音翻译系统中的延迟评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的同时语音翻译延迟度量指标在短文本设置下存在结构偏差，导致评估结果不一致或误导性，特别是在人工预分割的语音场景中。

Method: 1) 提出YAAL指标改进短文本延迟评估；2) 扩展为LongYAAL用于未分割音频；3) 开发基于词级对齐的SoftSegmenter重分割工具。

Result: 实验表明YAAL和LongYAAL优于流行的延迟度量指标，SoftSegmenter提高了长文本评估中的对齐质量。

Conclusion: 新提出的度量指标和工具能够为同时语音翻译系统提供更可靠的延迟评估方法。

Abstract: Simultaneous speech-to-text translation (SimulST) systems have to balance
translation quality with latency--the delay between speech input and the
translated output. While quality evaluation is well established, accurate
latency measurement remains a challenge. Existing metrics often produce
inconsistent or misleading results, especially in the widely used short-form
setting, where speech is artificially presegmented. In this paper, we present
the first comprehensive analysis of SimulST latency metrics across language
pairs, systems, and both short- and long-form regimes. We uncover a structural
bias in current metrics related to segmentation that undermines fair and
meaningful comparisons. To address this, we introduce YAAL (Yet Another Average
Lagging), a refined latency metric that delivers more accurate evaluations in
the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and
propose SoftSegmenter, a novel resegmentation tool based on word-level
alignment. Our experiments show that YAAL and LongYAAL outperform popular
latency metrics, while SoftSegmenter enhances alignment quality in long-form
evaluation, together enabling more reliable assessments of SimulST systems.

</details>


### [289] [Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs](https://arxiv.org/abs/2509.17367)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文使用尺度无关指标对不同领域文本复杂度进行比较分析，发现法律文本具有特定的结构和复杂性特征，而当前生成模型未能完全复制这些特征。


<details>
  <summary>Details</summary>
Motivation: 研究不同领域文本的复杂性差异，特别是法律文本作为专业领域与通用文本及AI生成文本的对比，以了解领域特定结构特征。

Method: 使用Heaps指数β（词汇增长）、Taylor指数α（词频波动标度）、压缩率r（冗余度）和熵等尺度无关指标，分析法律文件（法规、案例、契约）、通用自然语言文本和GPT生成文本三个领域的语料库。

Result: 法律文本比通用文本具有更慢的词汇增长（较低β）和更高的术语一致性（较高α）。在法律领域内，法规代码的β最低、α最高，反映严格的起草规范；案例和契约的β较高、α较低。GPT生成文本的统计特征更接近通用语言模式。

Conclusion: 法律文本展现出领域特定的结构和复杂性，当前生成模型未能完全复制这些特征，表明需要针对专业领域开发更专门的文本生成技术。

Abstract: We present a comparative analysis of text complexity across domains using
scale-free metrics. We quantify linguistic complexity via Heaps' exponent
$\beta$ (vocabulary growth), Taylor's exponent $\alpha$ (word-frequency
fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our
corpora span three domains: legal documents (statutes, cases, deeds) as a
specialized domain, general natural language texts (literature, Wikipedia), and
AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary
growth (lower $\beta$) and higher term consistency (higher $\alpha$) than
general texts. Within legal domain, statutory codes have the lowest $\beta$ and
highest $\alpha$, reflecting strict drafting conventions, while cases and deeds
show higher $\beta$ and lower $\alpha$. In contrast, GPT-generated text shows
the statistics more aligning with general language patterns. These results
demonstrate that legal texts exhibit domain-specific structures and
complexities, which current generative models do not fully replicate.

</details>


### [290] [Robustness of Neurosymbolic Reasoners on First-Order Logic Problems](https://arxiv.org/abs/2509.17377)
*Hannah Bansal,Kemal Kurniawan,Lea Frermann*

Main category: cs.CL

TL;DR: 该论文研究了神经符号方法（结合LLM和符号逻辑求解器）是否能提高大语言模型在反事实任务变体中的推理鲁棒性，发现神经符号方法虽然更鲁棒但总体表现不如纯神经方法，提出的NSCoT方法结合神经符号和思维链提示有所改进但仍落后于标准思维链方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLMs在反事实任务变体上表现脆弱，容易依赖表面模式而非逻辑推理，因此需要探索神经符号方法是否能提高推理的鲁棒性和一致性。

Method: 提出神经符号方法（NS）结合LLM和符号逻辑求解器，并进一步提出NSCoT方法将NS与思维链提示相结合，在不同规模的LLMs上进行实验比较。

Result: NS方法比纯神经方法更鲁棒但总体表现更差，NSCoT方法虽然提升了性能但仍落后于标准CoT方法。

Conclusion: 神经符号方法在提高鲁棒性方面有潜力，但需要进一步研究来平衡鲁棒性和性能，为未来工作指明了研究方向。

Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language
Models (LLMs), with key focus on generalization and robustness to variations in
tasks. Counterfactual task variants introduce minimal but semantically
meaningful changes to otherwise valid first-order logic (FOL) problem instances
altering a single predicate or swapping roles of constants to probe whether a
reasoning system can maintain logical consistency under perturbation. Previous
studies showed that LLMs becomes brittle on counterfactual variations,
suggesting that they often rely on spurious surface patterns to generate
responses. In this work, we explore if a neurosymbolic (NS) approach that
integrates an LLM and a symbolic logical solver could mitigate this problem.
Experiments across LLMs of varying sizes show that NS methods are more robust
but perform worse overall that purely neural methods. We then propose NSCoT
that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate
that while it improves performance, NSCoT still lags behind standard CoT. Our
analysis opens research directions for future work.

</details>


### [291] [FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis](https://arxiv.org/abs/2509.17395)
*Tianshi Cai,Guanxu Li,Nijia Han,Ce Huang,Zimu Wang,Changyu Zeng,Yuqi Wang,Jingshi Zhou,Haiyang Zhang,Qi Chen,Yushan Pan,Shuihua Wang,Wei Wang*

Main category: cs.CL

TL;DR: FinDebate是一个用于金融分析的多智能体框架，结合了协作辩论和领域特定的检索增强生成技术，通过五个专业智能体并行工作来生成多维度的金融洞察。


<details>
  <summary>Details</summary>
Motivation: 为了解决金融分析中过度自信和可靠性不足的问题，需要开发一个能够整合多方证据、通过辩论机制挑战和优化初步结论的框架，以提高分析质量和置信度校准。

Method: 采用多智能体架构，包含收益、市场、情绪、估值和风险五个专业智能体，结合检索增强生成技术，并引入安全辩论协议让智能体能够相互挑战和优化结论。

Result: 基于LLM和人工评估的实验结果表明，该框架能够生成高质量的金融分析，具有校准的置信度水平，并能提供跨多个时间维度的可操作投资策略。

Conclusion: FinDebate框架通过多智能体协作辩论机制有效提升了金融分析的可靠性和实用性，为复杂金融决策提供了更稳健的支持工具。

Abstract: We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.

</details>


### [292] [EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](https://arxiv.org/abs/2509.17396)
*Minsoo Kim,Arnav Kundu,Han-Byul Kim,Richa Dixit,Minsik Cho*

Main category: cs.CL

TL;DR: EpiCache是一个无需训练的KV缓存管理框架，通过分块预填充和基于主题的压缩技术，在固定内存预算下优化长对话问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文长度增加，KV缓存的内存占用线性增长，在资源受限环境下成为瓶颈。现有方法存在峰值内存无界和单查询缓存导致的精度下降问题。

Method: 采用分块预填充技术限制缓存增长，通过主题聚类将对话历史组织成连贯片段，应用片段特定的KV缓存淘汰策略，并设计自适应分层预算分配方法。

Result: 在三个长对话问答基准测试中，EpiCache相比基线方法精度提升高达40%，在4-6倍压缩下保持接近全KV精度，延迟和内存分别降低2.4倍和3.5倍。

Conclusion: EpiCache能够在严格资源约束下实现高效的多轮对话交互，解决了现有KV缓存压缩方法的关键局限性。

Abstract: Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.

</details>


### [293] [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context](https://arxiv.org/abs/2509.17399)
*Pramit Sahoo,Maharaj Brahma,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: 本文介绍了一个针对印度文化的新文化特定项目数据集，用于评估大语言模型的文化适应能力，发现现有模型存在选择性区域覆盖和表面层次适应的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏文化对齐能力，产生有偏见的生成结果，且缺乏针对复杂区域和子区域文化的评估数据集和指标。

Method: 构建包含约8,000个文化概念的印度文化数据集，涵盖17个文化方面和36个子区域，使用CSI、LLM作为评判者和人类评估来测量LLM的文化能力。

Result: 定量分析显示所有考虑的LLM都存在选择性子区域覆盖和表面层次的文化适应问题。

Conclusion: 该数据集为评估LLM的文化对齐能力提供了重要资源，揭示了当前模型在文化适应方面的局限性。

Abstract: Large language models (LLMs) are widely used in various tasks and
applications. However, despite their wide capabilities, they are shown to lack
cultural alignment \citep{ryan-etal-2024-unintended,
alkhamissi-etal-2024-investigating} and produce biased generations
\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.
Evaluation of LLMs for cultural awareness and alignment is particularly
challenging due to the lack of proper evaluation metrics and unavailability of
culturally grounded datasets representing the vast complexity of cultures at
the regional and sub-regional levels. Existing datasets for culture specific
items (CSIs) focus primarily on concepts at the regional level and may contain
false positives. To address this issue, we introduce a novel CSI dataset for
Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k
cultural concepts from 36 sub-regions. To measure the cultural competence of
LLMs on a cultural text adaptation task, we evaluate the adaptations using the
CSIs created, LLM as Judge, and human evaluations from diverse
socio-demographic region. Furthermore, we perform quantitative analysis
demonstrating selective sub-regional coverage and surface-level adaptations
across all considered LLMs. Our dataset is available here:
\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},
project
webpage\footnote{\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},
and our codebase with model outputs can be found here:
\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.

</details>


### [294] [Vision Language Models Are Not (Yet) Spelling Correctors](https://arxiv.org/abs/2509.17418)
*Junhong Liang,Bojun Zhang*

Main category: cs.CL

TL;DR: ReViCo是首个系统评估视觉语言模型在真实世界图像中拼写纠正能力的基准，包含中英文自然错误数据，实验显示当前模型在纠正方面远低于人类水平，提出了两种改进方法并取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 视觉输入中的拼写纠正对视觉语言模型提出独特挑战，需要直接在图像中检测并纠正文本错误，但目前缺乏系统评估基准。

Method: 构建ReViCo基准，包含真实世界图像中的自然错误；评估代表性开源和闭源模型；探索联合OCR-纠正管道和背景信息增强两种解决方案。

Result: 当前VLMs在拼写纠正方面显著落后于人类表现，特别是在纠正任务上；提出的两种方法都带来了持续的性能提升。

Conclusion: 分析揭示了现有架构的根本局限性，为推进多模态拼写纠正提供了可行的见解。

Abstract: Spelling correction from visual input poses unique challenges for vision
language models (VLMs), as it requires not only detecting but also correcting
textual errors directly within images. We present ReViCo (Real Visual
Correction), the first benchmark that systematically evaluates VLMs on
real-world visual spelling correction across Chinese and English. ReViCo
contains naturally occurring errors collected from real-world image data and
supports fine-grained evaluation at both image and token levels. Through
comprehensive experiments on representative cascaded (Qwen) and native
(InternVL) open-source models, as well as closed-source systems (GPT-4o,
Claude), we show that current VLMs fall significantly short of human
performance, particularly in correction. To address these limitations, we
explore two solution paradigms: a Joint OCR-Correction pipeline and a
Background Information enhanced approach, both of which yield consistent
performance gains. Our analysis highlights fundamental limitations of existing
architectures and provides actionable insights for advancing multimodal
spelling correction.

</details>


### [295] [RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios](https://arxiv.org/abs/2509.17421)
*Fei Zhao,Chengqiang Lu,Yufan Shen,Qimeng Wang,Yicheng Qian,Haoxin Zhang,Yan Gao,Yi Wu,Yao Hu,Zhen Wu,Shangyu Xing,Xinyu Dai*

Main category: cs.CL

TL;DR: RealBench是首个中文多模态多图像数据集，包含9393个样本和69910张图像，基于真实用户生成内容，涵盖多样场景和图像结构，用于评估多图像理解能力。


<details>
  <summary>Details</summary>
Motivation: 填补中文多图像数据集的空白，现有数据集主要基于英文，缺乏针对中文多图像场景的评估基准。

Method: 构建包含真实用户生成内容的中文多图像数据集RealBench，涵盖多种场景、分辨率和图像结构，并使用21个不同规模的多模态大语言模型进行综合评估。

Result: 实验表明，即使是性能最强的闭源模型在处理中文多图像场景时仍面临挑战，开源视觉/视频模型与闭源模型之间存在约71.8%的平均性能差距。

Conclusion: RealBench为在中文语境下进一步探索多图像理解能力提供了重要的研究基础，揭示了当前模型在处理中文多图像任务时的局限性。

Abstract: While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.

</details>


### [296] [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
*Hyesung Jeon,Seojune Lee,Beomseok Kang,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: QWHA是一种针对量化大语言模型的高效微调方法，通过Walsh-Hadamard变换和创新的适配器初始化方案，在降低量化误差的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有量化感知参数高效微调方法存在表示能力有限的问题，而基于傅里叶变换的适配器虽然表示能力更强，但在量化模型中直接集成会导致误差减少效果不佳和计算开销增加。

Method: 提出QWHA方法，使用Walsh-Hadamard变换作为变换核，结合包含自适应参数选择和值优化的新型适配器初始化方案，将基于傅里叶变换的适配器集成到量化模型中。

Result: 实验结果表明，QWHA在低比特量化精度上持续优于基线方法，相比现有基于傅里叶变换的适配器实现了显著的训练加速。

Conclusion: QWHA有效缓解了量化误差并促进了微调过程，其设计大幅降低了计算成本，为大语言模型的高效部署提供了有效解决方案。

Abstract: The demand for efficient deployment of large language models (LLMs) has
driven interest in quantization, which reduces inference cost, and
parameter-efficient fine-tuning (PEFT), which lowers training overhead. This
motivated the development of quantization-aware PEFT to produce accurate yet
efficient quantized models. In this setting, reducing quantization error prior
to fine-tuning is crucial for achieving high model accuracy. However, existing
methods that rely on low-rank adaptation suffer from limited representational
capacity. Recent Fourier-related transform (FT)-based adapters offer greater
representational power than low-rank adapters, but their direct integration
into quantized models often results in ineffective error reduction and
increased computational overhead. To overcome these limitations, we propose
QWHA, a method that integrates FT-based adapters into quantized models by
employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together
with a novel adapter initialization scheme incorporating adaptive parameter
selection and value refinement. We demonstrate that QWHA effectively mitigates
quantization errors while facilitating fine-tuning, and that its design
substantially reduces computational cost. Experimental results show that QWHA
consistently outperforms baselines in low-bit quantization accuracy and
achieves significant training speedups over existing FT-based adapters. The
code is available at https://github.com/vantaa89/qwha.

</details>


### [297] [MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses](https://arxiv.org/abs/2509.17436)
*Tong Chen,Zimu Wang,Yiyi Miao,Haoran Luo,Yuanfei Sun,Wei Wang,Zhengyong Jiang,Procheta Sen,Jionglong Su*

Main category: cs.CL

TL;DR: MedFact是首个基于证据的中文医疗事实核查数据集，专注于LLM生成的医疗内容验证，包含1,321个问题和7,409个声明，反映了真实医疗场景的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注人类生成内容，而LLM生成的医疗内容验证相对未被探索，这在线医疗信息日益重要的背景下尤为关键。

Method: 构建MedFact数据集，包含LLM生成的医疗声明，并在上下文学习(ICL)和微调设置下进行综合实验，深入分析错误。

Result: 实验展示了当前LLM在此任务上的能力和挑战，为未来研究指明了关键方向。

Conclusion: MedFact填补了LLM生成医疗内容验证的空白，为医疗事实核查研究提供了重要资源，数据集已公开可用。

Abstract: Medical fact-checking has become increasingly critical as more individuals
seek medical information online. However, existing datasets predominantly focus
on human-generated content, leaving the verification of content generated by
large language models (LLMs) relatively unexplored. To address this gap, we
introduce MedFact, the first evidence-based Chinese medical fact-checking
dataset of LLM-generated medical content. It consists of 1,321 questions and
7,409 claims, mirroring the complexities of real-world medical scenarios. We
conduct comprehensive experiments in both in-context learning (ICL) and
fine-tuning settings, showcasing the capability and challenges of current LLMs
on this task, accompanied by an in-depth error analysis to point out key
directions for future research. Our dataset is publicly available at
https://github.com/AshleyChenNLP/MedFact.

</details>


### [298] [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning](https://arxiv.org/abs/2509.17437)
*Guizhen Chen,Weiwen Xu,Hao Zhang,Hou Pong Chan,Deli Zhao,Anh Tuan Luu,Yu Rong*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段强化学习训练框架，通过先增强视觉感知能力再培养推理能力，来解决多模态大语言模型在几何推理等视觉密集型任务中的感知瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习虽然提升了语言模型的推理能力，但对多模态大语言模型的影响有限。特别是在几何推理等视觉密集型任务中，模型经常产生幻觉，导致推理不准确。这主要是由于MLLMs存在感知瓶颈，限制了推理训练的效果。

Method: 设计了GeoPQA基准测试来量化感知瓶颈，并提出两阶段RL训练框架：第一阶段增强几何结构的视觉感知能力，第二阶段培养推理能力。该方法应用于Qwen2.5-VL-3B-Instruct模型。

Result: 相比直接推理训练方法，两阶段训练在几何推理上提升了9.7%，在几何问题解决上提升了9.1%。该方法还能推广到图表理解等其他视觉密集型领域。

Conclusion: 感知基础对于有效的MLLM推理至关重要，两阶段训练方法能够显著提升模型在视觉密集型任务中的表现。

Abstract: Recent advancements in reinforcement learning (RL) have enhanced the
reasoning abilities of large language models (LLMs), yet the impact on
multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like
geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate
reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps
the benefits of reasoning training. To quantify this, we design a
Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric
concepts and spatial relationships. Experiments on GeoPQA reveal significant
shortcomings of MLLMs in visual perception, which constrain RL reward signals
for effective training. To address this bottleneck, we propose a two-stage RL
training framework by first enhancing the visual perception of geometric
structures, then fostering reasoning capabilities. Applied to
Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by
9.7% and geometric problem solving by 9.1%, compared to the direct reasoning
training approach. Our method also generalizes to other vision-intensive
domains like figure understanding, highlighting the importance of perceptual
grounding in effective MLLM reasoning.

</details>


### [299] [Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system](https://arxiv.org/abs/2509.17444)
*Shohei Hisada,Endo Sunao,Himi Yamato,Shoko Wakamiya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 本研究调查了HealthBench（一个基于评分标准的大规模医学基准）在日本背景下的适用性，发现直接翻译基准存在局限性，需要针对日本临床指南、医疗体系和文化规范进行本地化适配。


<details>
  <summary>Details</summary>
Motivation: 由于日语医学评估资源有限，通常依赖于翻译的多选题，缺乏针对日本医疗背景的稳健评估框架，这阻碍了日语医学LLM的安全发展。

Method: 首先建立性能基线，使用机器翻译的HealthBench 5,000个场景评估多语言模型GPT-4.1和日语原生开源模型LLM-jp-3.1；其次采用LLM-as-a-Judge方法系统分类基准场景和评分标准，识别与日本临床实践不匹配的"上下文差距"。

Result: GPT-4.1因评分标准不匹配出现适度性能下降，日语原生模型因缺乏临床完整性而显著失败；大部分场景适用，但相当一部分评分标准需要本地化。

Conclusion: 直接翻译基准存在局限性，迫切需要开发上下文感知的本地化适配基准（J-HealthBench），以确保在日本可靠安全地评估医学LLM。

Abstract: This study investigates the applicability of HealthBench, a large-scale,
rubric-based medical benchmark, to the Japanese context. While robust
evaluation frameworks are crucial for the safe development of medical LLMs,
resources in Japanese remain limited, often relying on translated
multiple-choice questions. Our research addresses this gap by first
establishing a performance baseline, applying a machine-translated version of
HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual
model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,
we employ an LLM-as-a-Judge approach to systematically classify the benchmark's
scenarios and rubric criteria, identifying "contextual gaps" where content is
misaligned with Japan's clinical guidelines, healthcare systems, or cultural
norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric
mismatches and a significant failure in the Japanese-native model, which lacked
the required clinical completeness. Furthermore, our classification indicates
that while the majority of scenarios are applicable, a substantial portion of
the rubric criteria requires localization. This work underscores the
limitations of direct benchmark translation and highlights the urgent need for
a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable
and safe evaluation of medical LLMs in Japan.

</details>


### [300] [Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](https://arxiv.org/abs/2509.17445)
*Chaodong Tong,Qi Zhang,Lei Jiang,Yanbing Liu,Nannan Sun,Wei Li*

Main category: cs.CL

TL;DR: 提出语义重构熵（SRE）方法，通过输入侧语义重构和渐进式混合聚类来改进大语言模型的不确定性估计，从而更可靠地检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的语义级不确定性估计方法受限于采样噪声和变长答案的不稳定聚类，难以可靠检测大语言模型产生的幻觉（流畅但事实错误的输出）。

Method: SRE方法包含两个关键改进：1）输入侧语义重构生成忠实释义，扩展估计空间，减少解码器表面倾向的偏差；2）渐进式基于能量的混合聚类稳定语义分组。

Result: 在SQuAD和TriviaQA数据集上的实验表明，SRE优于强基线方法，提供了更鲁棒和可泛化的幻觉检测能力。

Conclusion: 结合输入多样化和多信号聚类能显著增强语义级不确定性估计，为解决大语言模型的幻觉问题提供了有效方案。

Abstract: Reliable question answering with large language models (LLMs) is challenged
by hallucinations, fluent but factually incorrect outputs arising from
epistemic uncertainty. Existing entropy-based semantic-level uncertainty
estimation methods are limited by sampling noise and unstable clustering of
variable-length answers. We propose Semantic Reformulation Entropy (SRE), which
improves uncertainty estimation in two ways. First, input-side semantic
reformulations produce faithful paraphrases, expand the estimation space, and
reduce biases from superficial decoder tendencies. Second, progressive,
energy-based hybrid clustering stabilizes semantic grouping. Experiments on
SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more
robust and generalizable hallucination detection. These results demonstrate
that combining input diversification with multi-signal clustering substantially
enhances semantic-level uncertainty estimation.

</details>


### [301] [SLAyiNG: Towards Queer Language Processing](https://arxiv.org/abs/2509.17449)
*Leonor Veloso,Lea Hirlimann,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 该论文提出了SLAyiNG数据集，这是第一个包含从字幕、社交媒体帖子和播客中提取的标注酷儿俚语的数据集，旨在解决LLMs在处理酷儿俚语时可能误判为仇恨言论或产生负面回应的问题。


<details>
  <summary>Details</summary>
Motivation: 酷儿俚语在用户交互中常被LLMs错误标记为仇恨言论或引发负面响应，但目前缺乏专门针对酷儿俚语的高质量标注基准数据集。

Method: 通过收集俚语术语和定义，从字幕、社交媒体和播客中抓取反映这些术语使用情况的示例，并进行人工标注过程。初步结果通过计算人工标注者和OpenAI模型o3-mini在语义消歧任务上的互评一致性来评估。

Result: 平均Krippendorff's alpha达到0.746，表明最先进的推理模型可以作为预过滤工具，但酷儿语言数据的复杂性和敏感性需要专家和社区驱动的标注工作。

Conclusion: 虽然先进模型在酷儿俚语处理中表现出一定潜力，但复杂敏感的酷儿语言数据仍需专家和社区的深度参与来确保标注质量。

Abstract: Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.

</details>


### [302] [Codifying Natural Langauge Tasks](https://arxiv.org/abs/2509.17455)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: ICRAG框架通过文本到代码转换解决现实世界问题，在13个基准测试中取得最高161.1%的相对改进


<details>
  <summary>Details</summary>
Motivation: 探索文本到代码技术在解决现实世界自然语言问题（如法律判决和医疗问答）中的应用，利用程序生成提供的显式推理能力

Method: 提出ICRAG框架，通过迭代精炼将自然语言转换为可执行程序，利用领域资源和GitHub的外部知识

Result: 在13个基准测试中实现了高达161.1%的相对改进，并对生成的代码和外部知识影响进行了详细分析

Conclusion: 讨论了将文本到代码方法应用于现实世界自然语言任务的局限性

Abstract: We explore the applicability of text-to-code to solve real-world problems
that are typically solved in natural language, such as legal judgment and
medical QA. Unlike previous works, our approach leverages the explicit
reasoning provided by program generation. We present ICRAG, a framework that
transforms natural language into executable programs through iterative
refinement using external knowledge from domain resources and GitHub. Across 13
benchmarks, ICRAG achieves up to 161.1\% relative improvement. We provide a
detailed analysis of the generated code and the impact of external knowledge,
and we discuss the limitations of applying text-to-code approaches to
real-world natural language tasks.

</details>


### [303] [PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents](https://arxiv.org/abs/2509.17459)
*Namyoung Kim,Kai Tzu-iunn Ong,Yeonjun Hwang,Minseok Kang,Iiseo Jihn,Gayoung Kim,Minju Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: PRINCIPLES是一种基于离线自博弈模拟的合成策略记忆方法，用于提升主动对话代理的策略规划能力，无需额外训练和数据标注。


<details>
  <summary>Details</summary>
Motivation: 现有主动对话策略规划方法存在策略覆盖有限、规划偏好偏差以及依赖昂贵额外训练等问题。

Method: 通过离线自博弈模拟生成合成策略记忆，作为可重用知识在推理时指导策略规划。

Result: 在情感支持和说服领域均优于强基线模型，且在扩展和多样化评估设置中保持鲁棒性。

Conclusion: PRINCIPLES方法有效解决了主动对话策略规划的局限性，提供了一种无需额外训练的高效解决方案。

Abstract: Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.

</details>


### [304] [Diagnosing Model Editing via Knowledge Spectrum](https://arxiv.org/abs/2509.17482)
*Tsung-Hsuan Pan,Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文提出了"知识谱系"框架来系统分类知识特性，并开发了"知识诊断框架"来自适应调整模型编辑强度，显著提高了困难知识编辑的成功率


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法常产生意外副作用，而目标知识的内在特性这一重要因素尚未得到充分探索

Method: 首先提出知识谱系框架（基于知识流行度、模型熟悉度和问题语言结构），然后开发知识诊断框架来自适应调整编辑强度

Result: 实证分析表明知识特性是编辑成功和稳定性的强预测因子，知识诊断框架显著提高了困难编辑的成功率并优化了计算资源

Conclusion: 这项工作提供了对模型编辑影响因素更全面的理解，为改进模型编辑方法提供了新视角

Abstract: Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.

</details>


### [305] [AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.17486)
*Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: AttnComp是一种自适应、高效且上下文感知的压缩框架，通过利用LLM的注意力机制识别相关信息，采用Top-P压缩算法保留累积注意力权重超过预定义阈值的最小文档集，从而提高检索增强生成的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成虽然能提高大语言模型的事实准确性，但常常受到不相关检索内容的干扰。现有的上下文压缩方法难以自适应调整压缩率、保持低延迟并整合多文档信息。

Method: AttnComp利用LLM的注意力机制识别相关信息，采用Top-P压缩算法保留累积注意力权重超过预定义阈值的最小文档集，同时通过评估检索内容的整体相关性来估计响应置信度。

Result: 实验表明，AttnComp优于现有的压缩方法和未压缩基线，在实现高压缩率和低延迟的同时获得了更高的准确性。

Conclusion: AttnComp通过自适应压缩和置信度估计，有效解决了检索增强生成中不相关内容干扰的问题，提高了LLM的事实准确性和响应可靠性。

Abstract: Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.

</details>


### [306] [MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](https://arxiv.org/abs/2509.17489)
*Woongkyu Lee,Junhee Cho,Jungwook Choi*

Main category: cs.CL

TL;DR: MapCoder-Lite将单个7B模型通过角色专用LoRA适配器升级为四个专业化代理（检索器、规划器、编码器、调试器），在保持低参数增加（<3%）的同时显著提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有多代理解决方案要么依赖昂贵的大规模模型（>30B），要么在缩小到小型开源模型时性能崩溃，需要开发高效的小模型多代理代码生成方案。

Method: 使用三种轻量级技术：1）从强LLM进行轨迹蒸馏修复检索和调试中的格式脆弱性；2）监督指导校正增强规划和编码代理；3）代理级LoRA微调实现内存高效专业化。

Result: 在xCodeEval、APPS和CodeContests上的评估显示，MapCoder-Lite将xCodeEval准确率从13.2%提升至28.3%，消除所有格式失败，接近32B基线性能，同时减少4倍GPU内存和令牌生成时间。

Conclusion: 精心设计的代理级微调可以在小型语言模型上实现高质量的多代理代码生成，证明小模型通过专业化设计也能达到接近大模型的性能。

Abstract: Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.

</details>


### [307] [Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages](https://arxiv.org/abs/2509.17493)
*Wenhao Zhuang,Yuan Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种结合字符音译和霍夫曼编码的完整音译框架，用于提升大语言模型对低资源语言的处理能力，实现了压缩存储、无损转换、高效训练和良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言语料训练中展现出跨语言迁移能力，但对非拉丁文字的低资源语言处理效果不佳。音译虽然是一个自然解决方案，但缺乏完整的集成框架。

Method: 创新性地结合字符音译与霍夫曼编码，设计完整的音译框架，包括压缩存储、无损转换和高效处理机制。

Result: 实验验证显示，该方法在文本分类、机器阅读理解、机器翻译等任务中显著提升低资源语言处理能力，同时保持高资源语言性能，实现50%文件大小减少和50-80%token数量减少。

Conclusion: 提出的音译框架有效解决了低资源语言处理难题，具有压缩、准确、高效和可扩展等优势，为大语言模型的多语言应用提供了实用解决方案。

Abstract: As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.

</details>


### [308] [CorefInst: Leveraging LLMs for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17505)
*Tuğba Pamay Arslan,Emircan Erol,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 该研究提出了首个多语言共指消解方法，利用仅解码器LLM处理显式和零指代，通过五种指令集和受控推理方法，在Llama 3.1、Gemma 2和Mistral 0.3上评估，结果显示指令调优后的LLM能超越最先进的特定任务架构。


<details>
  <summary>Details</summary>
Motivation: 共指消解是自然语言理解中的关键但具有挑战性的任务，传统方法受限于特定任务架构和基于编码器的语言模型，需要大量训练且缺乏适应性。

Method: 使用仅解码器LLM，通过五种不同的指令集建模共指消解任务，采用受控推理方法，在三个LLM上进行评估。

Result: 最佳模型（完全微调的Llama 3.1多语言共指消解）在CorefUD v1.2数据集上平均比领先的多语言CR模型（Corpipe 24单阶段变体）高出2个百分点。

Conclusion: LLM通过合适的指令集进行指令调优后，能够超越最先进的特定任务架构，在多语言共指消解任务中表现出色。

Abstract: Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.

</details>


### [309] [Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models](https://arxiv.org/abs/2509.17523)
*María Andrea Cruz Blandón,Zakaria Aldeneh,Jie Chi,Maureen de Seyssel*

Main category: cs.CL

TL;DR: 该论文研究通过在双语语音自监督学习模型中引入有限的视觉基础来减少多语言性能差距的方法。


<details>
  <summary>Details</summary>
Motivation: 多语言自监督学习模型在单个语言上的表现往往不如单语言模型，特别是在双语等多语言场景中。

Method: 在双语语音SSL模型中引入有限的视觉基础，通过视觉信息来增强语音表示学习。

Result: 视觉基础对单语言和双语模型都有益处，特别是对双语模型效果更显著，将零样本音素识别的多语言性能差距从31.5%降低到8.04%。

Conclusion: 有限的视觉基础可以有效缩小多语言语音自监督学习模型的性能差距，特别是在双语场景中。

Abstract: Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.

</details>


### [310] [Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning](https://arxiv.org/abs/2509.17552)
*Tianle Zhang,Wanlong Fang,Jonathan Woo,Paridhi Latawa,Deepak A. Subramanian,Alvin Chan*

Main category: cs.CL

TL;DR: 本文提出了ICRL（In-Context Representation Learning），一种无需训练即可将非文本模态表示集成到基于文本的大型语言模型中的框架，通过少样本学习实现多模态推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要额外的监督训练来集成非文本模态表示，限制了在新领域和模态上的即时适应性，因此需要探索无需训练的方法。

Method: ICRL将传统上下文学习中的文本输入替换为基础模型的表示，使LLM能够在不进行微调的情况下执行多模态推理。

Result: 在分子领域的一系列任务上评估ICRL，研究了三个核心问题：如何无训练映射表示、影响性能的因素以及ICRL的有效机制。

Conclusion: ICRL是首个无需训练即可集成非文本模态表示到文本LLM的框架，为可适应的多模态泛化提供了有前景的方向。

Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced
with test-time computation, which relies on external tools and even other deep
learning models. However, existing approaches for integrating non-text modality
representations into LLMs typically require additional costly supervised
training, restricting on-the-fly adaptation to new domains and modalities. In
this work, we explore the feasibility of integrating representations from
non-text foundational models (FMs) into text-based LLMs in a training-free
manner. We propose In-Context Representation Learning (ICRL) as a
proof-of-concept to allow LLMs to adaptively utilize non-text modality
representations with few-shot learning. Unlike traditional in-context learning,
which incorporates text-label pairs, ICRL replaces text inputs with FM
representations, enabling the LLM to perform multi-modal inference without
fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,
investigating three core research questions: (i) how to map FM representations
into LLMs in a training-free manner, (ii) what factors influence ICRL
performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To
the best of our knowledge, ICRL is the first training-free framework for
integrating non-text modality representations into text-based LLMs, presenting
a promising direction for adaptable, multi-modal generalization.

</details>


### [311] [Specification-Aware Machine Translation and Evaluation for Purpose Alignment](https://arxiv.org/abs/2509.17559)
*Yoko Kayano,Saku Sugawara*

Main category: cs.CL

TL;DR: 该论文提出将专业翻译中的规格说明（specifications）整合到机器翻译工作流程中，通过实验证明基于规格说明的LLM翻译在专业文本翻译中优于官方人工翻译。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究往往只隐式处理规格说明，而专业翻译实践中规格说明对翻译质量和客户需求至关重要，需要建立明确的规格说明感知的MT框架。

Method: 基于翻译研究理论构建规格说明感知的MT和评估框架，应用于33家上市公司投资者关系文本翻译，比较5种翻译类型（包括官方人工翻译和基于提示的LLM输出），使用专家错误分析、用户偏好排序和自动指标进行评估。

Result: 实验结果显示，基于规格说明指导的LLM翻译在人类评估中持续优于官方人工翻译，揭示了感知质量与预期质量之间的差距。

Conclusion: 将规格说明整合到MT工作流程中，配合人工监督，可以按照专业实践要求提升翻译质量。

Abstract: In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.

</details>


### [312] [Asking a Language Model for Diverse Responses](https://arxiv.org/abs/2509.17570)
*Sergey Troshin,Irina Saparina,Antske Fokkens,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文研究了三种候选采样策略（并行采样、枚举采样和迭代采样）在生成多样化响应时的效果，发现枚举和迭代采样在保持质量的同时能显著提高多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖显式推理链并能为给定上下文生成多个合理响应，需要研究不同采样策略对响应多样性的影响。

Method: 比较三种采样策略：并行采样（独立生成）、枚举采样（一次生成n个候选）和迭代采样（顺序生成并基于当前响应集条件化）。在相同计算预算下评估质量、多样性和效率。

Result: 枚举和迭代采样策略在质量相当的情况下能产生更高的词汇和计算流程多样性。

Conclusion: 简单的非独立采样策略有潜力在不牺牲生成质量的前提下显著提高响应多样性。

Abstract: Large language models increasingly rely on explicit reasoning chains and can
produce multiple plausible responses for a given context. We study the
candidate sampler that produces the set of plausible responses contrasting the
ancestral (parallel) sampling against two alternatives: enumeration, which asks
the model to produce $n$ candidates in one pass, and iterative sampling, which
proposes candidates sequentially while conditioning on the currently generated
response set. Under matched budgets, we compare these samplers on quality,
lexical and computation flow diversity, and efficiency. Our empirical results
demonstrate that enumeration and iterative strategies result in higher
diversity at comparable quality. Our findings highlight the potential of simple
non-independent sampling strategies to improve response diversity without
sacrificing generation quality.

</details>


### [313] [MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents](https://arxiv.org/abs/2509.17628)
*Yuzhen Lei,Hongbin Xie,Jiaxing Zhao,Shuangxue Liu,Xuan Song*

Main category: cs.CL

TL;DR: MSCoRe是一个新的多阶段推理基准，包含126,696个领域特定QA实例，用于评估LLM在复杂多阶段场景中的推理和协调能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单领域或孤立任务，忽略了LLM在多阶段协作和优化方面的能力，需要新的评估工具来填补这一空白。

Method: 采用三阶段流水线创建数据集：动态采样、迭代问答生成和多层次质量评估，任务按阶段覆盖率和复杂度分为三个难度级别。

Result: 商业模型在所有任务和场景中表现最佳，但简单任务和复杂任务之间的ROUGE分数差距显著，且模型性能受噪声数据负面影响。

Conclusion: MSCoRe为社区提供了评估和改进LLM代理多阶段推理能力的宝贵资源，代码和数据已开源。

Abstract: Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.

</details>


### [314] [AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?](https://arxiv.org/abs/2509.17641)
*Hyunjong Ok,Suho Yoo,Hyeonjun Kim,Jaeho Lee*

Main category: cs.CL

TL;DR: AuditoryBench++是一个评估文本模型中听觉常识推理能力的基准测试，AIR-CoT方法通过听觉想象推理提升模型性能


<details>
  <summary>Details</summary>
Motivation: 人类能够轻松推理听觉属性，而语言模型缺乏这种听觉常识能力，限制了其在多模态交互中的有效性

Method: 提出AIR-CoT方法，通过特殊标记的跨度检测和知识注入，在推理过程中生成和整合听觉信息

Result: 实验表明AIR-CoT在最近的LLM和多模态LLM上普遍优于现成模型和听觉知识增强模型

Conclusion: AuditoryBench++为评估听觉知识提供了全面基准，AIR-CoT方法有效提升了模型的听觉推理能力

Abstract: Even without directly hearing sounds, humans can effortlessly reason about
auditory properties, such as pitch, loudness, or sound-source associations,
drawing on auditory commonsense. In contrast, language models often lack this
capability, limiting their effectiveness in multimodal interactions. As an
initial step to address this gap, we present AuditoryBench++, a comprehensive
benchmark for evaluating auditory knowledge and reasoning in text-only
settings. The benchmark encompasses tasks that range from basic auditory
comparisons to contextually grounded reasoning, enabling fine-grained analysis
of how models process and integrate auditory concepts. In addition, we
introduce AIR-CoT, a novel auditory imagination reasoning method that generates
and integrates auditory information during inference through span detection
with special tokens and knowledge injection. Extensive experiments with recent
LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both
the off-the-shelf models and those augmented with auditory knowledge. The
project page is available at https://auditorybenchpp.github.io.

</details>


### [315] [Crosslingual Optimized Metric for Translation Assessment of Indian Languages](https://arxiv.org/abs/2509.17667)
*Arafat Ahsan,Vandan Mujadia,Pruthwik Mishra,Yash Bhaskar,Dipti Misra Sharma*

Main category: cs.CL

TL;DR: 本文创建了一个包含13种印度语言的翻译质量人工评估数据集，并训练了一个名为COMTAIL的神经翻译评估指标，该指标在评估涉及印度语言的翻译对时显著优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 自动翻译评估面临挑战，因为语言在拼写、形态、句法和语义上存在丰富性和差异性。基于字符串的指标如BLEU存在局限性，而神经指标在大多数语言中受限于缺乏黄金评估数据，特别是印度语言。

Method: 创建了一个大型人工评估评分数据集，涵盖13种印度语言的21个翻译方向，并在此基础上训练了COMTAIL神经翻译评估指标。

Result: 最佳性能的COMTAIL变体在评估至少包含一种印度语言的翻译对时，相比之前的最先进方法显示出显著性能提升。

Conclusion: 通过消融研究揭示了该指标对领域变化、翻译质量和语言分组的敏感性，并发布了COMTAIL数据集和相应的指标模型。

Abstract: Automatic evaluation of translation remains a challenging task owing to the
orthographic, morphological, syntactic and semantic richness and divergence
observed across languages. String-based metrics such as BLEU have previously
been extensively used for automatic evaluation tasks, but their limitations are
now increasingly recognized. Although learned neural metrics have helped
mitigate some of the limitations of string-based approaches, they remain
constrained by a paucity of gold evaluation data in most languages beyond the
usual high-resource pairs. In this present work we address some of these gaps.
We create a large human evaluation ratings dataset for 13 Indian languages
covering 21 translation directions and then train a neural translation
evaluation metric named Cross-lingual Optimized Metric for Translation
Assessment of Indian Languages (COMTAIL) on this dataset. The best performing
metric variants show significant performance gains over previous
state-of-the-art when adjudging translation pairs with at least one Indian
language. Furthermore, we conduct a series of ablation studies to highlight the
sensitivities of such a metric to changes in domain, translation quality, and
language groupings. We release both the COMTAIL dataset and the accompanying
metric models.

</details>


### [316] [PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation](https://arxiv.org/abs/2509.17669)
*Yan Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 本文提出PG-CE方法，通过类型预测、约束构建和引导生成三步骤，使用约束生成模型动态构建多维度约束来提升可控文本生成的质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，可控文本生成技术对提升系统可靠性和用户体验变得至关重要。传统方法存在局限性，需要更有效的解决方案。

Method: PG-CE方法将可控文本生成任务分解为三个步骤：类型预测、约束构建和引导生成。使用约束生成模型动态构建包括语调、表达风格和主题焦点在内的多维度约束来指导输出。

Result: 实验表明PG-CE在多个场景下显著提升生成质量，同时保持文本可控性、主题相关性和响应实用性。研究还开发了包含9万条约束-文本对的数据集，有效反映实际应用需求。

Conclusion: PG-CE方法通过渐进式生成和约束增强，为可控文本生成提供了有效的解决方案，在保持生成质量的同时提升了可控性。

Abstract: With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.

</details>


### [317] [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](https://arxiv.org/abs/2509.17671)
*Selva Taş,Mahmut El Huseyni,Özay Ezerceli,Reyhan Bayraktar,Fatma Betül Terzioğlu*

Main category: cs.CL

TL;DR: 本文介绍了Turk-LettuceDetect，这是首个专门为土耳其语RAG应用设计的幻觉检测模型套件，通过微调三种编码器架构在机器翻译的RAGTruth基准数据集上训练，在土耳其语等形态复杂语言中有效检测LLM幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成内容时容易产生幻觉（生成看似合理但事实错误的信息），特别是在土耳其语等形态复杂的低资源语言中，现有的RAG系统难以有效解决这一问题。

Method: 基于LettuceDetect框架，将幻觉检测定义为token级分类任务，微调三种编码器架构：土耳其语专用ModernBERT、TurkEmbed4STS和多语言EuroBERT，使用包含17,790个实例的机器翻译RAGTruth数据集进行训练。

Result: ModernBERT模型在完整测试集上达到0.7266的F1分数，在结构化任务中表现尤为突出。模型计算效率高，支持长达8,192个token的上下文，适合实时部署。

Conclusion: 该工作填补了多语言NLP中的关键空白，为土耳其语及其他语言开发更可靠、可信的AI应用奠定了基础，同时发现现有LLM虽然召回率高但精度低，需要专门的检测机制。

Abstract: The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.

</details>


### [318] [When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables](https://arxiv.org/abs/2509.17680)
*Shenghao Ye,Yu Guo,Dong Jin,Yikai Shen,Yunpeng Hou,Shuangwu Chen,Jian Yang,Xiaofeng Jiang*

Main category: cs.CL

TL;DR: EnoTab是一个针对复杂问题和大型表格的双重去噪框架，通过问题分解和表格剪枝来提高表格问答性能


<details>
  <summary>Details</summary>
Motivation: 现实应用中复杂问题和大型表格会引入大量噪声数据，严重影响推理性能，需要提升相关性过滤和表格剪枝两个核心能力

Method: 1. 基于证据的问题去噪：将问题分解为最小语义单元，根据一致性和可用性标准过滤无关信息；2. 证据树引导的表格去噪：构建显式透明的表格剪枝路径，采用后序节点回滚机制处理异常状态

Result: 大量实验表明EnoTab在复杂问题和大型表格的TableQA任务中取得了优异性能

Conclusion: EnoTab框架通过双重去噪机制有效提升了表格问答系统在处理复杂场景下的可靠性和性能

Abstract: Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.

</details>


### [319] [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
*Daiye Miao,Yufang Liu,Jie Wang,Changzhi Sun,Yunke Zhang,Demei Yan,Shaokang Dong,Qi Zhang,Yuanbin Wu*

Main category: cs.CL

TL;DR: TASO是一种基于预训练模型权重重要性信息的LoRA冗余消除方法，通过识别任务特定核心区域来确定LoRA模块的稀疏结构，在保持性能的同时显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: LoRA方法虽然简单有效，但存在显著的参数冗余问题，这不仅增加了可训练参数数量，还阻碍了微调效果。由于识别LoRA中的冗余参数本身很困难，如何高效准确地消除这些冗余仍然是一个挑战性问题。

Method: TASO利用预训练模型权重的下游任务重要性信息，估计参数重要性并基于重要性分数分布识别任务特定核心区域。这些核心区域的位置信息用于确定LoRA模块的稀疏结构，从而在微调前实现冗余消除。

Result: 实验结果表明，在参数预算与rank r=1的LoRA相当的情况下，TASO在多个任务上持续优于标准LoRA，实现了强大的微调性能，同时有效消除了冗余参数。

Conclusion: TASO显著减少了任务适应所需的可训练参数数量，同时为LoRA冗余消除提供了新颖的任务对齐视角，在保持性能的同时有效解决了LoRA的参数冗余问题。

Abstract: LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.

</details>


### [320] [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
*Dongxu Lu,Johan Jeuring,Albert Gatt*

Main category: cs.CL

TL;DR: 本文评估了LLM在长格式知识驱动角色扮演对话中的表现，发现LLM生成回复的质量在多轮对话中显著下降，而人类回复质量则逐步提升，揭示了LLM在专业训练模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在长格式、知识驱动的角色扮演对话中的表现仍然具有挑战性，特别是在多轮专业训练模拟中比较LLM生成和人类编写回复的质量差异。

Method: 通过人类评估（N=38）和自动化的LLM-as-a-judge评估，比较LLM生成和人类编写回复在多轮专业训练模拟中的表现，包括零样本成对偏好和随机6样本结构评分。

Result: 人类评估显示LLM生成回复质量在多轮对话中显著下降，特别是在自然性、上下文维护和整体质量方面，而人类编写回复逐步改善。参与者一致偏好人类编写对话。Gemini 2.0 Flash在自动评估中与人类评估者高度一致，证实了LLM与人类回复质量差距随时间扩大的现象。

Conclusion: 本研究贡献了一个多轮基准测试，揭示了LLM在知识驱动角色扮演对话中的退化问题，并提供了一个经过验证的混合评估框架，以指导LLM在训练模拟中的可靠集成。

Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.

</details>


### [321] [Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs](https://arxiv.org/abs/2509.17701)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 本文提出了一个自动化多语言管道，用于生成、解决和评估与德国K-10课程对齐的数学问题，发现LLMs在不同语言中的解答质量存在显著差异，英语解答质量最高，阿拉伯语较低。


<details>
  <summary>Details</summary>
Motivation: LLMs在教育支持中的应用日益增多，但其响应质量因交互语言而异，需要评估LLMs在多语言环境中的表现差异。

Method: 生成了628个数学练习题并翻译成英语、德语和阿拉伯语，使用三个商业LLMs（GPT-4o-mini、Gemini 2.5 Flash、Qwen-plus）生成分步解答，并由LLM评委（Claude 3.5 Haiku）使用比较框架评估解答质量。

Result: 结果显示一致的差距，英语解答质量始终最高，阿拉伯语解答通常排名较低。

Conclusion: 这些发现突显了LLMs中持续存在的语言偏见，以及教育领域需要更公平的多语言AI系统。

Abstract: Large Language Models (LLMs) are increasingly used for educational support,
yet their response quality varies depending on the language of interaction.
This paper presents an automated multilingual pipeline for generating, solving,
and evaluating math problems aligned with the German K-10 curriculum. We
generated 628 math exercises and translated them into English, German, and
Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)
were prompted to produce step-by-step solutions in each language. A held-out
panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality
using a comparative framework. Results show a consistent gap, with English
solutions consistently rated highest, and Arabic often ranked lower. These
findings highlight persistent linguistic bias and the need for more equitable
multilingual AI systems in education.

</details>


### [322] [Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics](https://arxiv.org/abs/2509.17737)
*Kavin R V,Pawan Goyal*

Main category: cs.CL

TL;DR: 论文提出了一种名为聚合语义分组（ASG）的新方法，通过产品量化（PQ）实现token的组合式表示，在保持95%以上任务性能的同时，将嵌入参数压缩至0.4-0.5%。


<details>
  <summary>Details</summary>
Motivation: 标准语言模型使用单一的token嵌入可能限制了捕捉词语多面含义的能力，需要探索更有效的组合式表示方法。

Method: 提出ASG方法，利用产品量化构建组合式语义表示，并在多种Transformer架构（mBERT、XLM-R、mT5）和任务（NLI、NER、QA）上进行评估。

Result: ASG在保持95%以上任务性能的同时，实现了极端的参数压缩（0.4-0.5%），在跨语言迁移和领域特定设置中都表现良好。

Conclusion: 验证了token可以作为共享语义构建块的组合进行有效建模，ASG提供了一种简单而具体的方法来实现紧凑且语义丰富的模型。

Abstract: Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.

</details>


### [323] [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
*Jin Xu,Zhifang Guo,Hangrui Hu,Yunfei Chu,Xiong Wang,Jinzheng He,Yuxuan Wang,Xian Shi,Ting He,Xinfa Zhu,Yuanjun Lv,Yongqi Wang,Dake Guo,He Wang,Linhan Ma,Pei Zhang,Xinyu Zhang,Hongkun Hao,Zishan Guo,Baosong Yang,Bin Zhang,Ziyang Ma,Xipin Wei,Shuai Bai,Keqin Chen,Xuejing Liu,Peng Wang,Mingkun Yang,Dayiheng Liu,Xingzhang Ren,Bo Zheng,Rui Men,Fan Zhou,Bowen Yu,Jianxin Yang,Le Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-Omni是首个在文本、图像、音频和视频多模态任务上均保持SOTA性能的单模型，采用Thinker-Talker MoE架构，在音频任务表现尤其突出，支持多语言交互和实时语音生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在多模态任务中性能退化问题，实现真正的统一多模态感知和生成能力，特别是在音频领域填补通用音频描述模型的空白。

Method: 采用Thinker-Talker MoE架构，Talker使用多码本方案自回归预测离散语音编解码，用轻量级因果ConvNet替代计算密集型扩散模型，Thinking模型显式推理多模态输入。

Result: 在36个音频和音视频基准测试中，32个开源SOTA和22个总体SOTA，超越Gemini-2.5-Pro等闭源模型，冷启动端到端首包延迟234ms。

Conclusion: Qwen3-Omni实现了真正的统一多模态SOTA性能，特别是在音频任务上表现卓越，模型已开源发布，为多模态AI研究提供了重要基准。

Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.

</details>


### [324] [A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue](https://arxiv.org/abs/2509.17766)
*Ziyi Liu*

Main category: cs.CL

TL;DR: 提出一种无需训练的状态更新多轮对话策略，通过状态重建和历史提醒机制解决LLMs在长对话中的信息遗忘和效率问题，在HotpotQA等数据集上显著提升性能并大幅降低推理时间和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长跨度多轮对话中存在信息遗忘和效率低下的问题，需要一种有效的对话历史管理方法。

Method: 采用无需训练的提示工程方法，包含状态重建和历史提醒两个核心机制，有效管理对话历史。

Result: 在HotpotQA数据集上，核心信息过滤分数提升32.6%，下游QA分数提升14.1%，推理时间减少73.1%，token消耗降低59.4%。消融研究证实了两个组件的关键作用。

Conclusion: 该策略为优化LLMs在长程交互中的表现提供了有效解决方案，为开发更鲁棒的智能体提供了新思路。

Abstract: Large Language Models (LLMs) struggle with information forgetting and
inefficiency in long-horizon, multi-turn dialogues. To address this, we propose
a training-free prompt engineering method, the State-Update Multi-turn Dialogue
Strategy. It utilizes "State Reconstruction" and "History Remind" mechanisms to
effectively manage dialogue history. Our strategy shows strong performance
across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,
it improves the core information filtering score by 32.6%, leading to a 14.1%
increase in the downstream QA score, while also reducing inference time by
73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal
roles of both components. Our work offers an effective solution for optimizing
LLMs in long-range interactions, providing new insights for developing more
robust Agents.

</details>


### [325] [DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching](https://arxiv.org/abs/2509.17768)
*Jessica Ojo,Zina Kamel,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文介绍了DIVERS-BENCH，一个用于评估语言识别(LID)模型在不同领域性能的综合性基准测试，发现现有模型在噪声和非正式文本上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前语言识别系统往往过拟合于干净的单一语言数据，缺乏在真实世界多样化场景下的鲁棒性评估。

Method: 构建了DIVERS-BENCH评估框架，涵盖语音转录、网络文本、社交媒体文本、儿童故事和语码转换文本等多个领域，并创建了DIVERS-CS语码转换基准数据集。

Result: 模型在精选数据集上表现优异，但在噪声和非正式输入上性能急剧下降；现有模型难以检测同一句子中的多种语言。

Conclusion: 研究结果强调了在真实世界环境中开发更鲁棒和包容性语言识别系统的必要性。

Abstract: Language Identification (LID) is a core task in multilingual NLP, yet current
systems often overfit to clean, monolingual data. This work introduces
DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across
diverse domains, including speech transcripts, web text, social media texts,
children's stories, and code-switched text. Our findings reveal that while
models achieve high accuracy on curated datasets, performance degrades sharply
on noisy and informal inputs. We also introduce DIVERS-CS, a diverse
code-switching benchmark dataset spanning 10 language pairs, and show that
existing models struggle to detect multiple languages within the same sentence.
These results highlight the need for more robust and inclusive LID systems in
real-world settings.

</details>


### [326] [One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts](https://arxiv.org/abs/2509.17788)
*Xingyu Fan,Feifei Li,Wenhui Que,Hailong Li*

Main category: cs.CL

TL;DR: WeStar是一个轻量级自适应框架，用于解决工业级公众号平台中对话代理在保持上下文相关性和风格一致性方面的挑战，通过结合RAG和参数化RAG实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法在工业级公众号平台部署中存在显著问题：链式思维提示导致延迟过高、按账户微调计算成本过高、长提示方法会降低模型对注入上下文和风格的理解能力。

Method: WeStar框架结合了基于RAG的上下文生成和基于参数化RAG的风格感知生成，使用LoRA模块按风格簇动态激活，并提出多维度聚类参数共享方案和风格增强直接偏好优化方法。

Result: 在大规模工业数据集上的实验验证了WeStar的有效性和效率，证明了其在真实世界部署中的实用价值。

Conclusion: WeStar能够以最小开销服务大量官方账户，通过紧凑的风格表示保持风格多样性，为工业级对话系统提供了可行的解决方案。

Abstract: Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.

</details>


### [327] [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
*Tobias Groot,Salo Lacunes,Evgenia Ilia*

Main category: cs.CL

TL;DR: 本文研究了通过多标签微调方法来提高语言模型在自然语言生成任务中重现人类语言变异性的能力，使用GPT-2和Mistral-7B-IT模型在Provo语料库上进行实验。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型无法很好地重现人类语言的内在变异性，这可能是因为训练数据缺乏对这种固有变异性的反映。作者希望通过训练模型使用多个合理的词语续写来改善这一能力。

Method: 采用微调技术对预训练和指令调优模型进行多标签训练，使用Provo语料库对GPT-2和Mistral-7B-IT模型进行微调，通过测量人类和模型在上下文中的下一个词分布差异来评估效果。

Result: 多标签微调提高了语言模型重现语言变异性的能力，无论是在高变异性还是低变异性的上下文中都表现出改进。

Conclusion: 通过多标签微调方法可以有效地提高语言模型在重现人类语言变异性方面的表现，为改善语言模型的多样性生成能力提供了有效途径。

Abstract: Natural language generation (NLG) tasks are often subject to inherent
variability; \emph{e.g.} predicting the next word given a context has multiple
valid responses, evident when asking multiple humans to complete the task.
While having language models (LMs) that are aligned pluralistically, so that
they are able to reproduce well the inherent diversity in perspectives of an
entire population of interest is clearly beneficial, \citet{ilia2024predict}
show that LMs do not reproduce this type of linguistic variability well. They
speculate this inability might stem from the lack of consistent training of LMs
with data reflecting this type of inherent variability. As such, we investigate
whether training LMs on multiple plausible word continuations per context can
improve their ability to reproduce human linguistic variability for next-word
prediction. We employ fine-tuning techniques for pre-trained and
instruction-tuned models; and demonstrate their potential when fine-tuning
GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures
divergence among empirically estimated human and model next-word distributions
across contexts before and after fine-tuning, shows that our multi-label
fine-tuning improves the LMs' ability to reproduce linguistic variability; both
for contexts that admit higher and lower variability.

</details>


### [328] [Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?](https://arxiv.org/abs/2509.17796)
*Michal Novák,Miloslav Konopík,Anna Nedoluzhko,Martin Popel,Ondřej Pražák,Jakub Sido,Milan Straka,Zdeněk Žabokrtský,Daniel Zeman*

Main category: cs.CL

TL;DR: 本文介绍了CODI-CRAC 2025工作坊中第四届多语言共指消解共享任务的概述，重点包括新增的LLM专用赛道、数据集扩展以及系统参与情况。


<details>
  <summary>Details</summary>
Motivation: 推动多语言共指消解技术的发展，评估传统方法与新兴LLM方法在核心任务上的表现，促进该领域的研究进步。

Method: 使用CorefUD 1.3版本的22个数据集覆盖17种语言，设立专门的LLM赛道采用简化文本格式，共有9个系统参与（包括4个基于LLM的方法）。

Result: 传统系统仍保持领先地位，但LLM显示出明显潜力，表明它们可能在未来的版本中挑战已建立的方法。

Conclusion: LLM在多语言共指消解任务中展现出良好前景，虽然目前传统方法仍占优势，但LLM有望在不久的将来成为有力的竞争者。

Abstract: The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.

</details>


### [329] [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
*Jihae Jeong,DaeYeop Lee,DongGeon Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: EPiK是一个针对韩国文化背景的物理常识推理基准测试，包含181个二元选择题，涵盖9个子任务和84个场景，旨在解决现有基准测试主要关注西方文化背景的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理基准测试主要基于西方文化背景，忽视了不同文化在物理问题解决方面的差异，需要开发能够反映文化多样性的评估工具。

Method: 采用两阶段生成和验证流程，从韩国文化背景（如泡菜、传统发酵等）有机生成问题，确保文化真实性和物理推理严谨性，而非简单翻译现有问题。

Result: 评估显示，专门针对韩国文化的模型在同等规模下持续优于通用模型，表明文化无关模型存在局限性。

Conclusion: 文化感知的基准测试对于真正衡量语言理解能力至关重要，EPiK的推出填补了文化多样性评估的空白，相关数据集已公开可用。

Abstract: Existing physical commonsense reasoning benchmarks predominantly focus on
Western contexts, overlooking cultural variations in physical problem-solving.
To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a
novel benchmark comprising 181 binary-choice problems that test physical
reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to
traditional fermentation. EPiK is constructed using a two-stage generation and
verification pipeline to create culturally-authentic problems across 9
reasoning subtasks and 84 scenarios. Unlike approaches based on simple
translation, our method generates problems organically from Korean contexts
while upholding rigorous physical reasoning standards. Our evaluations show
that Korean-specialized models consistently outperform general-purpose models
of comparable size. This performance gap highlights the limitations of
culturally-agnostic models and demonstrates the critical need for
culturally-aware benchmarks to truly measure language understanding. Our EPiK
is publicly available at https://huggingface.co/datasets/jjae/EPiK.

</details>


### [330] [Towards Adaptive Context Management for Intelligent Conversational Question Answering](https://arxiv.org/abs/2509.17829)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 该论文提出了自适应上下文管理（ACM）框架，用于优化对话问答系统中的对话历史使用，通过动态管理上下文在token限制内最大化相关信息。


<details>
  <summary>Details</summary>
Motivation: 对话问答系统在处理长对话历史时面临token限制问题，需要有效管理上下文以确保模型获得最相关和最新的信息。

Method: ACM框架包含三个模块：上下文管理模块动态调整上下文大小，摘要模块通过滑动窗口总结旧对话历史，实体提取模块在摘要窗口超过限制时保留关键实体。

Result: 实验结果表明该框架能生成准确且上下文恰当的回答，提升了对话问答系统的鲁棒性和可扩展性。

Conclusion: ACM框架通过动态上下文管理有效解决了对话问答系统中的token限制问题，具有实际应用价值。

Abstract: This particular paper introduces an Adaptive Context Management (ACM)
framework for the Conversational Question Answering (ConvQA) systems. The key
objective of the ACM framework is to optimize the use of the conversation
history by dynamically managing context for maximizing the relevant information
provided to a ConvQA model within its token limit. Our approach incorporates a
Context Manager (CM) Module, a Summarization (SM) Module, and an Entity
Extraction (EE) Module in a bid to handle the conversation history
efficaciously. The CM Module dynamically adjusts the context size, thereby
preserving the most relevant and recent information within a model's token
limit. The SM Module summarizes the older parts of the conversation history via
a sliding window. When the summarization window exceeds its limit, the EE
Module identifies and retains key entities from the oldest conversation turns.
Experimental results demonstrate the effectiveness of our envisaged framework
in generating accurate and contextually appropriate responses, thereby
highlighting the potential of the ACM framework to enhance the robustness and
scalability of the ConvQA systems.

</details>


### [331] [Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation](https://arxiv.org/abs/2509.17830)
*Lekkala Sai Teja,Annepaka Yadagiri,Partha Pakray,Chukhu Chunka,Mangadoddi Srikar Vardhan*

Main category: cs.CL

TL;DR: 提出了一种基于句子级序列标注的AI文本检测模型，能够在混合文本中精确识别AI生成内容与人类写作内容的边界


<details>
  <summary>Details</summary>
Motivation: 传统文档级AI检测器难以有效识别经过编辑或混合的AI文本，需要更细粒度的检测方法来准确区分人类写作和AI生成内容

Method: 结合预训练Transformer模型、神经网络和条件随机场(CRF)，在token级别进行序列标注，提取语义和句法模式，增强序列表示和边界预测

Result: 在两个公开基准数据集上评估，与零样本检测器和现有最先进模型相比，该方法能准确检测完全协作文本中的AI文本片段

Conclusion: 该方法通过细粒度的序列标注技术，有效解决了混合文本中AI内容检测的挑战，提高了检测精度和边界识别能力

Abstract: Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.

</details>


### [332] [Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework](https://arxiv.org/abs/2509.17844)
*Lynn Greschner,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了情境化论证评估框架，结合情感标签、评估因素和说服力变量，研究情感与论证说服力之间的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究分别关注论证挖掘中的二元情感性和情感分析中的认知评估，但尚未将这两个领域结合起来。情感在自我和发送者情境中发展，需要建模认知评估过程。

Method: 提出情境化论证评估框架，包含情感标签、评估因素（如论证熟悉度、响应紧迫性、预期努力）和说服力变量。在角色扮演场景中进行研究，收集800个论证，每个由5名参与者标注情感、主要原因、论证评估和感知说服力。

Result: 分析显示，说服力与积极情感（如信任）正相关，与消极情感（如愤怒）负相关。评估变量揭示了论证熟悉度的重要性，对大多数参与者来说，论证内容本身是情感反应的主要驱动因素。

Conclusion: 该框架成功整合了情感分析和论证评估，揭示了情感与论证说服力之间的重要关系，为计算建模奠定了基础。

Abstract: Emotions, which influence how convincing an argument is, are developed
  in context of the self and sender, and therefore require modeling
  the cognitive evaluation process. While binary emotionality has been
  studied in argument mining, and the cognitive appraisal has been
  modeled in general emotion analysis, these fields have not been
  brought together yet. We therefore propose the Contextualized
  Argument Appraisal Framework that contextualizes the interplay
  between the sender, receiver, and argument. It includes emotion
  labels, appraisals, such as argument familiarity, response urgency,
  and expected effort, as well as convincingness variables. To evaluate
  the framework and pave the way to computational modeling, we perform
  a study in a role-playing scenario, mimicking real-world exposure to
  arguments, asking participants to disclose their emotion, explain the main
cause, the
  argument appraisal, and the
  perceived convincingness. To consider the subjective nature of such
  annotations, we also collect demographic data and personality traits
  of both the participants and the perceived sender of the argument.
  The analysis of the resulting corpus of 800 arguments, each
  annotated by 5 participants, reveals that convincingness is
  positively correlated with positive emotions (e.g., trust) and
  negatively correlated with negative emotions (e.g., anger). The
  appraisal variables disclose the importance of the argument
  familiarity. For most participants, the content of the argument
  itself is the primary driver of the emotional response.

</details>


### [333] [Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora](https://arxiv.org/abs/2509.17855)
*Robert Litschko,Verena Blaschke,Diana Burkhardt,Barbara Plank,Diego Frassinelli*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型处理方言的能力，以巴伐利亚语为例，通过DiaLemma框架构建了10万对德语-巴伐利亚语词汇对，评估了9个先进LLM在方言翻译、词形变化识别方面的表现。


<details>
  <summary>Details</summary>
Motivation: 方言因缺乏标准拼写而存在很大变异，但LLM处理方言的能力尚未得到充分研究。本文旨在填补这一空白，以巴伐利亚语为案例研究LLM的方言词汇理解能力。

Method: 提出了DiaLemma注释框架，仅使用单语数据创建方言变异词典，构建了10万个人工标注的德语-巴伐利亚语词汇对数据集，评估了9个先进LLM在识别方言翻译、词形变化和无关形式方面的能力。

Result: LLM在名词和词汇相似词对上表现最佳，但在区分直接翻译和词形变化方面最困难。提供上下文示例能提高翻译性能，但会降低识别方言变体的能力。

Conclusion: 研究揭示了LLM在处理拼写方言变异方面的局限性，强调需要未来工作来使LLM适应方言处理。

Abstract: Dialects exhibit a substantial degree of variation due to the lack of a
standard orthography. At the same time, the ability of Large Language Models
(LLMs) to process dialects remains largely understudied. To address this gap,
we use Bavarian as a case study and investigate the lexical dialect
understanding capability of LLMs by examining how well they recognize and
translate dialectal terms across different parts-of-speech. To this end, we
introduce DiaLemma, a novel annotation framework for creating dialect variation
dictionaries from monolingual data only, and use it to compile a ground truth
dataset consisting of 100K human-annotated German-Bavarian word pairs. We
evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as
dialect translations, inflected variants, or unrelated forms of a given German
lemma. Our results show that LLMs perform best on nouns and lexically similar
word pairs, and struggle most in distinguishing between direct translations and
inflected variants. Interestingly, providing additional context in the form of
example usages improves the translation performance, but reduces their ability
to recognize dialect variants. This study highlights the limitations of LLMs in
dealing with orthographic dialect variation and emphasizes the need for future
work on adapting LLMs to dialects.

</details>


### [334] [CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17858)
*Milan Straka*

Main category: cs.CL

TL;DR: CorPipe 25是CRAC 2025多语言共指消解共享任务的获胜系统，在LLM和无约束两个赛道中均以8个百分点的显著优势领先其他提交系统


<details>
  <summary>Details</summary>
Motivation: CRAC 2025共享任务新增了LLM赛道，并减少了开发和测试集规模以降低计算需求，需要开发新的系统来应对这些变化

Method: 对先前系统进行了完全重新实现，从TensorFlow迁移到PyTorch框架

Result: 在LLM和无约束两个赛道中均以8个百分点的显著优势领先所有其他提交系统

Conclusion: CorPipe 25系统表现出色，源代码和训练模型已在GitHub上公开

Abstract: We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on
Multilingual Coreference Resolution. This fourth iteration of the shared task
introduces a new LLM track alongside the original unconstrained track, features
reduced development and test sets to lower computational requirements, and
includes additional datasets. CorPipe 25 represents a complete reimplementation
of our previous systems, migrating from TensorFlow to PyTorch. Our system
significantly outperforms all other submissions in both the LLM and
unconstrained tracks by a substantial margin of 8 percentage points. The source
code and trained models are publicly available at
https://github.com/ufal/crac2025-corpipe.

</details>


### [335] [Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN](https://arxiv.org/abs/2509.17859)
*Kai Schenck,Gašper Beguš*

Main category: cs.CL

TL;DR: 本文提出了一种在完全无监督的人类语言习得模型中建模声调学习的方法，展示了ciwGAN模型能够在没有标注数据的情况下学习汉语普通话的声调类别。


<details>
  <summary>Details</summary>
Motivation: 声调模式是语言中最复杂的学习目标之一，研究旨在探索无监督生成模型是否能够学习人类语言的声调系统，并验证其与人类语言习得阶段的对应关系。

Method: 使用ciwGAN（条件信息最大化Wasserstein GAN）作为生成模型，在无标注的普通话语音数据上进行训练，通过分析F0（基频）在分类变量上的差异来评估声调学习效果。

Result: 所有三个训练模型在分类变量上都显示出显著的F0差异，仅使用男性语音训练的模型能够持续编码声调信息。模型不仅学习了普通话声调对比，还学习了一个与人类语言学习者习得阶段相对应的系统。

Conclusion: 研究结果表明无监督生成模型能够有效学习声调系统，同时开发了追踪卷积层内部声调表示的方法，证明语言学工具可以促进深度学习的可解释性，并可用于神经实验。

Abstract: This paper outlines the methodology for modeling tonal learning in fully
unsupervised models of human language acquisition. Tonal patterns are among the
computationally most complex learning objectives in language. We argue that a
realistic generative model of human language (ciwGAN) can learn to associate
its categorical variables with Mandarin Chinese tonal categories without any
labeled data. All three trained models showed statistically significant
differences in F0 across categorical variables. The model trained solely on
male tokens consistently encoded tone. Our results sug- gest that not only does
the model learn Mandarin tonal contrasts, but it learns a system that
corresponds to a stage of acquisition in human language learners. We also
outline methodology for tracing tonal representations in internal convolutional
layers, which shows that linguistic tools can contribute to interpretability of
deep learning and can ultimately be used in neural experiments.

</details>


### [336] [How Persuasive is Your Context?](https://arxiv.org/abs/2509.17879)
*Tu Nguyen,Kevin Du,Alexander Miserlis Hoyle,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文介绍了目标说服分数（TPS），用于量化上下文对语言模型的劝说能力，通过Wasserstein距离测量上下文如何将模型的原始答案分布向目标分布转移。


<details>
  <summary>Details</summary>
Motivation: 语言模型具有两个核心能力：利用先验知识回答问题和适应上下文中的新信息。现有方法仅通过贪婪解码答案评估劝说效果，缺乏对模型行为的细粒度分析。

Method: 基于Wasserstein距离设计TPS指标，测量上下文对模型答案分布的影响程度，提供比现有指标更细致的劝说效果评估。

Result: 通过一系列实验证明，TPS能够捕捉比先前提出的指标更细微的劝说概念，提供更全面的模型行为分析。

Conclusion: TPS为评估语言模型的上下文适应性提供了更精细的量化工具，有助于深入理解模型如何被新信息说服。

Abstract: Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.

</details>


### [337] [SiDiaC: Sinhala Diachronic Corpus](https://arxiv.org/abs/2509.17912)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: SiDiaC是首个全面的僧伽罗语历时语料库，涵盖公元5世纪至20世纪的文学作品，包含58k单词，经过精心标注和分类，为僧伽罗语NLP研究提供基础资源。


<details>
  <summary>Details</summary>
Motivation: 解决僧伽罗语作为低资源语言在历时研究方面的资源匮乏问题，为词汇变化、新词追踪、历史句法等研究提供数据支持。

Method: 从斯里兰卡国家图书馆获取文本，使用Google Document AI OCR进行数字化，后处理校正格式和现代化拼写，借鉴FarPaHC等语料库的句法标注和文本规范化策略。

Result: 成功构建包含46部文学作品的语料库，按体裁分为非虚构/虚构的主要分类和宗教、历史、诗歌、语言、医学等次要分类。

Conclusion: 尽管面临稀有文本获取困难和依赖二手日期来源等挑战，SiDiaC为僧伽罗语NLP研究奠定了重要基础，显著扩展了可用资源。

Abstract: SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a
historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words
across 46 literary works, annotated carefully based on the written date, after
filtering based on availability, authorship, copyright compliance, and data
attribution. Texts from the National Library of Sri Lanka were digitised using
Google Document AI OCR, followed by post-processing to correct formatting and
modernise the orthography. The construction of SiDiaC was informed by practices
from other corpora, such as FarPaHC, particularly in syntactic annotation and
text normalisation strategies, due to the shared characteristics of
low-resourced language status. This corpus is categorised based on genres into
two layers: primary and secondary. Primary categorisation is binary,
classifying each book into Non-Fiction or Fiction, while the secondary
categorisation is more specific, grouping texts under Religious, History,
Poetry, Language, and Medical genres. Despite challenges including limited
access to rare texts and reliance on secondary date sources, SiDiaC serves as a
foundational resource for Sinhala NLP, significantly extending the resources
available for Sinhala, enabling diachronic studies in lexical change, neologism
tracking, historical syntax, and corpus-based lexicography.

</details>


### [338] [Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning](https://arxiv.org/abs/2509.17921)
*Zhenyun Deng,Yulong Chen,Andreas Vlachos*

Main category: cs.CL

TL;DR: 提出零样本去语境化框架，通过内容选择和规划使句子脱离上下文后仍可理解，包括语义单元分割、歧义识别、相关单元提取和内容规划生成。


<details>
  <summary>Details</summary>
Motivation: 从文档中提取的句子常缺乏必要上下文（如指代和背景信息），导致理解困难，需要去语境化处理。

Method: 1）将句子分割为基本语义单元；2）识别歧义单元；3）基于语篇关系从上下文中提取相关单元；4）生成内容规划，用相关单元丰富歧义单元。

Result: 实验表明该方法在句子去语境化任务中具有竞争力，生成的句子语义完整性和语篇连贯性更好，优于现有方法。

Conclusion: 提出的内容选择和规划框架能有效提升句子脱离上下文后的可理解性，为NLP任务中的证据提取提供支持。

Abstract: Extracting individual sentences from a document as evidence or reasoning
steps is commonly done in many NLP tasks. However, extracted sentences often
lack context necessary to make them understood, e.g., coreference and
background information. To this end, we propose a content selection and
planning framework for zero-shot decontextualisation, which determines what
content should be mentioned and in what order for a sentence to be understood
out of context. Specifically, given a potentially ambiguous sentence and its
context, we first segment it into basic semantically-independent units. We then
identify potentially ambiguous units from the given sentence, and extract
relevant units from the context based on their discourse relations. Finally, we
generate a content plan to rewrite the sentence by enriching each ambiguous
unit with its relevant units. Experimental results demonstrate that our
approach is competitive for sentence decontextualisation, producing sentences
that exhibit better semantic integrity and discourse coherence, outperforming
existing methods.

</details>


### [339] [Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation](https://arxiv.org/abs/2509.17930)
*Yiwen Guan,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种新颖的分层Transformer编码器树（TET）结合非自回归编码器模型，用于多语言翻译，特别是语音翻译，以提高低资源语言的准确性并减少计算冗余。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译面临计算冗余和低资源语言准确性有限的问题，尤其是在语音翻译中。

Method: 使用分层Transformer编码器树（TET）结合非自回归编码器模型，通过共享语言相似目标语言的中间表示，并采用连接时序分类（CTC）进行训练。

Result: TET能够提高低资源语言的准确性，减少计算冗余，并在单次前向传播中生成所有目标语言。结合非自回归语音识别骨干（wav2vec2）的语音翻译系统在翻译质量上表现良好，且速度比自回归系统快7-14倍。

Conclusion: 提出的TET方法在多语言翻译中有效解决了计算冗余和低资源语言准确性问题，特别是在语音翻译中实现了高质量和高效率的翻译。

Abstract: Multilingual translation faces challenges of computational redundancy and
limited accuracy for low-resource languages, especially in speech translation.
To address this, we propose a novel hierarchical Transformer Encoder Tree (TET)
combined with non-autoregressive encoder-only models trained with Connectionist
Temporal Classification for multilingual translation. By sharing intermediate
representations among linguistically similar target languages, TET can improve
accuracy on low-resource languages, reduce computational redundancy, and allow
generating all target languages in a single forward pass, thus eliminating
sequential bottlenecks and improving parallelism. For speech translation,
combining TET with a non-autoregressive speech recognition backbone (wav2vec2)
shows promising results in terms of translation quality compared to
autoregressive systems while being 7-14 times faster.

</details>


### [340] [Training-free Truthfulness Detection via Value Vectors in LLMs](https://arxiv.org/abs/2509.17932)
*Runheng Liu,Heyan Huang,Xingchen Xiao,Zhijing Wu*

Main category: cs.CL

TL;DR: 本文提出TruthV方法，利用MLP模块中的值向量进行真实性检测，在NoVo基准上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有训练自由方法主要关注注意力机制，忽略了MLP模块在事实回忆中的重要作用

Method: 通过分析MLP模块中值向量的统计模式，开发了无需训练的TruthV方法进行真实性检测

Result: 在NoVo基准上，TruthV显著优于NoVo方法和似然度基线方法

Conclusion: MLP模块编码了丰富的真实性信号，为可扩展和可解释的真实性检测提供了新思路

Abstract: Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.

</details>


### [341] [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938)
*Satyapriya Krishna,Andy Zou,Rahul Gupta,Eliot Krzysztof Jones,Nick Winter,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson,Spyros Matsoukas*

Main category: cs.CL

TL;DR: 提出D-REX数据集来评估大语言模型的欺骗性对齐问题，即模型产生看似良性的输出但内部推理过程包含恶意意图的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估方法主要关注防止明显有害的输出，但忽视了模型在内部推理过程中可能存在的欺骗性行为，这种漏洞可能通过复杂的系统提示注入被触发。

Method: 通过竞争性红队演练构建D-REX数据集，包含对抗性系统提示、用户查询、模型看似无害的响应以及揭示恶意意图的内部思维链。

Result: D-REX对现有模型和安全机制构成了显著挑战，表明需要开发能够检查模型内部推理过程的新技术。

Conclusion: 需要开发能够检测模型内部推理过程的技术，而不仅仅是关注最终输出，以应对欺骗性对齐这一重要安全风险。

Abstract: The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.

</details>


### [342] [HICode: Hierarchical Inductive Coding with LLMs](https://arxiv.org/abs/2509.17946)
*Mian Zhong,Pristina Wang,Anjalie Field*

Main category: cs.CL

TL;DR: 论文提出HICode方法，利用LLM自动生成标签并进行层次聚类，实现大规模文本数据的细粒度分析，替代人工标注和传统统计方法。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度语料分析依赖人工标注（不可扩展）或主题建模等统计工具（难以控制），需要一种能扩展研究人员手动分析能力的方法。

Method: HICode两阶段流程：1）从分析数据中归纳生成标签；2）层次聚类标签以浮现主题。受定性研究方法启发，利用LLM实现自动化。

Result: 在三个不同数据集上验证，测量与人工构建主题的一致性，通过自动和人工评估证明其鲁棒性。在阿片类药物诉讼文件案例研究中成功揭示制药公司的营销策略。

Conclusion: HICode能够促进大规模数据中的细致分析，展示了LLM在扩展定性分析方面的潜力。

Abstract: Despite numerous applications for fine-grained corpus analysis, researchers
continue to rely on manual labeling, which does not scale, or statistical tools
like topic modeling, which are difficult to control. We propose that LLMs have
the potential to scale the nuanced analyses that researchers typically conduct
manually to large text corpora. To this effect, inspired by qualitative
research methods, we develop HICode, a two-part pipeline that first inductively
generates labels directly from analysis data and then hierarchically clusters
them to surface emergent themes. We validate this approach across three diverse
datasets by measuring alignment with human-constructed themes and demonstrating
its robustness through automated and human evaluations. Finally, we conduct a
case study of litigation documents related to the ongoing opioid crisis in the
U.S., revealing aggressive marketing strategies employed by pharmaceutical
companies and demonstrating HICode's potential for facilitating nuanced
analyses in large-scale data.

</details>


### [343] [Dorabella Cipher as Musical Inspiration](https://arxiv.org/abs/2509.17950)
*Bradley Hauer,Colin Choi,Abram Hindle,Scott Smallwood,Grzegorz Kondrak*

Main category: cs.CL

TL;DR: 该论文提出多拉贝拉密码可能是加密的音乐而非英文文本，通过开发简化的音乐符号和n-gram模型，成功将密码解密为可听的旋律。


<details>
  <summary>Details</summary>
Motivation: 多拉贝拉密码是英国作曲家爱德华·埃尔加留下的未解之谜，已困扰研究者一个多世纪。大多数解决方案都假设其为英文文本，但作者探索了其可能代表加密音乐的新假设。

Method: 开发简化的音乐符号系统，使用n-gram音乐模型验证单字母替换加密的音乐语料库，然后将该方法应用于多拉贝拉密码进行解密。

Result: 成功将多拉贝拉密码解密为具有音乐特性的内容，并通过艺术化作曲转化为可听的旋律。

Conclusion: 作者并不声称这是唯一真正的解决方案，而是将解密过程视为作曲过程的一部分，为这个百年谜题提供了新的解读视角。

Abstract: The Dorabella cipher is an encrypted note written by English composer Edward
Elgar, which has defied decipherment attempts for more than a century. While
most proposed solutions are English texts, we investigate the hypothesis that
Dorabella represents enciphered music. We weigh the evidence for and against
the hypothesis, devise a simplified music notation, and attempt to reconstruct
a melody from the cipher. Our tools are n-gram models of music which we
validate on existing music corpora enciphered using monoalphabetic
substitution. By applying our methods to Dorabella, we produce a decipherment
with musical qualities, which is then transformed via artful composition into a
listenable melody. Far from arguing that the end result represents the only
true solution, we instead frame the process of decipherment as part of the
composition process.

</details>


### [344] [Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments](https://arxiv.org/abs/2509.17961)
*Li Siyan,Zhen Xu,Vethavikashini Chithrra Raghuram,Xuanming Zhang,Renzhe Yu,Zhou Yu*

Main category: cs.CL

TL;DR: 提出一个基于学习科学的评估框架，用于评估异步学习环境中虚拟教学助手的教学效果，通过专家标注构建分类器来改进VTA系统的评估方法。


<details>
  <summary>Details</summary>
Motivation: 异步学习环境中的虚拟教学助手缺乏基于教育理论的严谨评估，现有评估方法依赖表面指标，难以比较不同VTA系统的教学有效性。

Method: 构建基于学习科学的评估框架，使用专家标注的VTA响应数据训练分类器，针对异步论坛讨论这一常见VTA部署场景进行专门设计。

Result: 评估了分类器的有效性，识别了提高准确性的方法以及阻碍泛化的挑战，为VTA系统的理论驱动评估奠定了基础。

Conclusion: 这项工作为教育AI的教学有效性评估建立了理论基础，为开发更具教学效果的虚拟教学助手铺平了道路。

Abstract: Asynchronous learning environments (ALEs) are widely adopted for formal and
informal learning, but timely and personalized support is often limited. In
this context, Virtual Teaching Assistants (VTAs) can potentially reduce the
workload of instructors, but rigorous and pedagogically sound evaluation is
essential. Existing assessments often rely on surface-level metrics and lack
sufficient grounding in educational theories, making it difficult to
meaningfully compare the pedagogical effectiveness of different VTA systems. To
bridge this gap, we propose an evaluation framework rooted in learning sciences
and tailored to asynchronous forum discussions, a common VTA deployment context
in ALE. We construct classifiers using expert annotations of VTA responses on a
diverse set of forum posts. We evaluate the effectiveness of our classifiers,
identifying approaches that improve accuracy as well as challenges that hinder
generalization. Our work establishes a foundation for theory-driven evaluation
of VTA systems, paving the way for more pedagogically effective AI in
education.

</details>


### [345] [ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media](https://arxiv.org/abs/2509.17991)
*Aakash Kumar Agarwal,Saprativa Bhattacharjee,Mauli Rastogi,Jemima S. Jacob,Biplab Banerjee,Rashmi Gupta,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出了ReDepress，首个临床验证的抑郁症复发检测社交媒体数据集，通过整合认知理论特征，实现了高精度的复发检测。


<details>
  <summary>Details</summary>
Motivation: 近50%的抑郁症患者面临复发风险，但现有研究缺乏针对复发的检测方法，主要障碍是缺乏标注数据集和难以区分复发与非复发用户。

Method: 构建包含204名Reddit用户的临床验证数据集，基于认知理论（注意偏差、解释偏差、记忆偏差和沉思）进行标注和建模，采用基于transformer的时间序列模型。

Result: 认知标记能显著区分复发与非复发群体，基于认知特征的模型达到0.86的F1分数，验证了心理学理论在真实文本数据中的有效性。

Conclusion: 认知信息计算方法在早期复发检测中具有潜力，为心理健康领域可扩展、低成本的干预措施铺平了道路。

Abstract: Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.

</details>


### [346] [Variation in Verification: Understanding Verification Dynamics in Large Language Models](https://arxiv.org/abs/2509.17995)
*Yefan Zhou,Austin Xu,Yilun Zhou,Janvijay Singh,Jiang Gui,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文研究了生成式验证器在测试时扩展中的应用，通过系统分析验证动态的三个维度（问题难度、生成器能力和验证器生成能力），揭示了验证有效性的关键发现和优化机会。


<details>
  <summary>Details</summary>
Motivation: 随着测试时计算扩展使大语言模型能够解决更复杂的问题，需要研究有效的验证机制来评估生成解决方案的正确性，特别是生成式验证器通过生成思维链推理进行验证的方法。

Method: 在12个基准测试上进行实证研究，涵盖数学推理、知识和自然语言推理任务，使用14个开源模型（2B到72B参数范围）和GPT-4o，系统分析验证动态的三个维度。

Result: 发现三个关键结果：(1)简单问题使验证器能更可靠地确认正确响应；(2)弱生成器产生的错误比强生成器更容易检测；(3)验证能力与验证器自身问题解决能力相关，但关系随问题难度变化。

Conclusion: 研究揭示了优化基本验证策略的机会：弱生成器经过验证后性能可接近强生成器；强验证器在某些情况下优势有限，表明仅靠验证器扩展无法克服根本性验证挑战。

Abstract: Recent advances have shown that scaling test-time computation enables large
language models (LLMs) to solve increasingly complex problems across diverse
domains. One effective paradigm for test-time scaling (TTS) involves LLM
generators producing multiple solution candidates, with LLM verifiers assessing
the correctness of these candidates without reference answers. In this paper,
we study generative verifiers, which perform verification by generating
chain-of-thought (CoT) reasoning followed by a binary verdict. We
systematically analyze verification dynamics across three dimensions - problem
difficulty, generator capability, and verifier generation capability - with
empirical studies on 12 benchmarks across mathematical reasoning, knowledge,
and natural language reasoning tasks using 14 open-source models (2B to 72B
parameter range) and GPT-4o. Our experiments reveal three key findings about
verification effectiveness: (1) Easy problems allow verifiers to more reliably
certify correct responses; (2) Weak generators produce errors that are easier
to detect than strong generators; (3) Verification ability is generally
correlated with the verifier's own problem-solving capability, but this
relationship varies with problem difficulty. These findings reveal
opportunities to optimize basic verification strategies in TTS applications.
First, given the same verifier, some weak generators can nearly match stronger
ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B
performance gap shrinks by 75.5%). Second, we identify cases where strong
verifiers offer limited advantage over weak ones, as both fail to provide
meaningful verification gains, suggesting that verifier scaling alone cannot
overcome fundamental verification challenges.

</details>


### [347] [WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing](https://arxiv.org/abs/2509.18004)
*Yuhang Dai,Ziyu Zhang,Shuai Wang,Longhao Li,Zhao Guo,Tianlun Zuo,Shuiyuan Wang,Hongfei Xue,Chengyou Wang,Qing Wang,Xin Xu,Hui Bu,Jie Li,Jian Kang,Binbin Zhang,Lei Xie*

Main category: cs.CL

TL;DR: 提出了WenetSpeech-Chuan，一个10,000小时的四川方言语音语料库，包含完整的处理框架和评估基准，显著提升了方言语音处理的研究水平。


<details>
  <summary>Details</summary>
Motivation: 解决四川方言等方言语音技术研究中大规模开源数据稀缺的问题，降低方言语音处理的研究门槛，促进AI公平性。

Method: 使用新颖的Chuan-Pipeline数据处理框架构建语料库，并发布包含手动验证转录的ASR和TTS评估基准WenetSpeech-Chuan-Eval。

Result: 在WenetSpeech-Chuan上训练的模型在开源系统中达到最先进性能，结果可与商业服务相媲美。

Conclusion: 作为最大的四川方言开源语料库，WenetSpeech-Chuan不仅促进了方言语音处理研究，还在减少语音技术偏见方面发挥关键作用。

Abstract: The scarcity of large-scale, open-source data for dialects severely hinders
progress in speech technology, a challenge particularly acute for the widely
spoken Sichuanese dialects of Chinese. To address this critical gap, we
introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed
using our novel Chuan-Pipeline, a complete data processing framework for
dialectal speech. To facilitate rigorous evaluation and demonstrate the
corpus's effectiveness, we also release high-quality ASR and TTS benchmarks,
WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show
that models trained on WenetSpeech-Chuan achieve state-of-the-art performance
among open-source systems and demonstrate results comparable to commercial
services. As the largest open-source corpus for Sichuanese dialects,
WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech
processing but also plays a crucial role in promoting AI equity and mitigating
bias in speech technologies. The corpus, benchmarks, models, and receipts are
publicly available on our project page.

</details>


### [348] [Cross-Attention is Half Explanation in Speech-to-Text Models](https://arxiv.org/abs/2509.18010)
*Sara Papi,Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本文评估了语音转文本模型中交叉注意力机制的解释能力，发现其与基于显著性的解释有中等至强相关性，但仅能捕获约50%的输入相关性，揭示了交叉注意力作为解释代理的局限性。


<details>
  <summary>Details</summary>
Motivation: 交叉注意力机制在语音处理领域被广泛用于下游任务，但其是否真正反映输入语音与生成文本之间的依赖关系尚未在语音领域得到充分验证。

Method: 通过比较交叉注意力分数与基于特征归因的输入显著性图，分析了单语/多语、单任务/多任务模型在不同规模下的表现。

Result: 交叉注意力分数与显著性解释有中等至强相关性（特别是跨头跨层聚合时），但仅能捕获约50%的输入相关性，最佳情况下也只能解释52-75%的显著性。

Conclusion: 交叉注意力提供了信息性但不完整的视图，不能完全作为S2T模型预测驱动因素的解释代理。

Abstract: Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.

</details>


### [349] [RadEval: A framework for radiology text evaluation](https://arxiv.org/abs/2509.18030)
*Justin Xu,Xi Zhang,Javid Abderezaei,Julie Bauml,Roger Boodoo,Fatemeh Haghighi,Ali Ganjizadeh,Eric Brattain,Dave Van Veen,Zaiqiao Meng,David Eyre,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: RadEval是一个统一的、开源的放射学文本评估框架，整合了多种评估指标，包括传统n-gram重叠度、临床概念评分和先进LLM评估器，并提供了统计测试工具和专家标注数据集。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的放射学文本评估标准，现有评估方法分散且不够全面，需要建立一个综合框架来促进放射学报告生成领域的可复现性和稳健基准测试。

Method: 整合多种评估指标（BLEU、ROUGE、BERTScore、F1CheXbert等），扩展GREEN支持多模态成像，预训练领域特定编码器，构建专家标注数据集（450+临床错误标签），并提供统计测试工具。

Result: 展示了不同指标与放射科医生判断的相关性，领域特定编码器在零样本检索中表现优异，框架支持多个公开数据集的基准模型评估。

Conclusion: RadEval为放射学报告生成提供了一个全面、可复现的评估框架，有助于推动该领域的研究发展和标准化评估。

Abstract: We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.

</details>


### [350] [The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies](https://arxiv.org/abs/2509.18052)
*Jiaxu Zhou,Jen-tse Huang,Xuhui Zhou,Man Ho Lam,Xintao Wang,Hao Zhu,Wenxuan Wang,Maarten Sap*

Main category: cs.CL

TL;DR: 该论文提出了PIMMUR原则，作为LLM社会模拟研究的六个必要方法论标准，指出当前研究存在系统性缺陷，并通过重新实验验证这些原则的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社会模拟研究存在方法论缺陷，许多研究采用实验设计系统性削弱了其结论的有效性，需要建立可靠的方法论标准。

Method: 通过调查40多篇论文识别出六个常见方法缺陷，将其形式化为PIMMUR原则，并使用框架重新运行五个代表性研究来验证这些原则的影响。

Result: 研究发现GPT-4o和Qwen-3在53.1%的情况下能正确推断实验假设，违反无意识原则；在更严格条件下，许多报告的社会现象无法重现。

Conclusion: PIMMUR原则是可信LLM社会模拟的必要条件，为AI社会研究建立了方法论标准，提供了更可靠和可复现的基础。

Abstract: Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."

</details>


### [351] [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2509.18060)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Renzeng Duojie,Yuqing Cai,Yongbin Yu,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.CL

TL;DR: TMD-TTS是一个统一的藏语多方言文本转语音框架，通过方言融合模块和DSDR-Net网络解决藏语方言语音合成问题，在方言表达性上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言，其三大主要方言（卫藏、安多、康巴）的平行语音语料库有限，限制了语音建模的进展。

Method: 提出TMD-TTS框架，包含方言融合模块和方言专业化动态路由网络（DSDR-Net），从显式方言标签合成平行方言语音，捕捉跨方言的细粒度声学和语言变异。

Result: 通过客观和主观评估证明，TMD-TTS在方言表达性上显著优于基线方法。合成的语音质量在语音到语音方言转换任务中得到验证。

Conclusion: TMD-TTS框架有效解决了藏语多方言语音合成问题，为低资源语言的方言语音建模提供了可行方案。

Abstract: Tibetan is a low-resource language with limited parallel speech corpora
spanning its three major dialects (\"U-Tsang, Amdo, and Kham), limiting
progress in speech modeling. To address this issue, we propose TMD-TTS, a
unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes
parallel dialectal speech from explicit dialect labels. Our method features a
dialect fusion module and a Dialect-Specialized Dynamic Routing Network
(DSDR-Net) to capture fine-grained acoustic and linguistic variations across
dialects. Extensive objective and subjective evaluations demonstrate that
TMD-TTS significantly outperforms baselines in dialectal expressiveness. We
further validate the quality and utility of the synthesized speech through a
challenging Speech-to-Speech Dialect Conversion (S2SDC) task.

</details>


### [352] [ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning](https://arxiv.org/abs/2509.18063)
*Jan-Felix Klein,Lars Ohnemus*

Main category: cs.CL

TL;DR: ARK-V1是一个简单的知识图谱代理，通过迭代探索图谱来回答自然语言查询，在需要KG和常识推理的长尾实体问题上显著优于思维链基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖的内部知识往往不足、过时或不正确，特别是在需要特定领域知识的问题上。知识图谱提供结构化外部知识，但其复杂性和多跳推理需求使得集成具有挑战性。

Method: 提出ARK-V1知识图谱代理，使用未微调的SOTA LLM作为骨干，在CoLoTa数据集上评估，该数据集需要基于KG和常识的推理，特别是针对长尾实体。

Result: ARK-V1的条件准确率显著高于思维链基线，更大的骨干模型在覆盖率、正确性和稳定性方面表现出明显改善趋势。

Conclusion: ARK-V1通过有效整合知识图谱和LLM，在需要复杂推理的查询中表现出色，证明了外部结构化知识对增强LLM推理能力的重要性。

Abstract: Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.

</details>


### [353] [SEQR: Secure and Efficient QR-based LoRA Routing](https://arxiv.org/abs/2509.18093)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种无监督的LoRA路由算法SEQR，通过最大化激活范数来高效选择适合的LoRA适配器，解决了在隐私敏感环境中路由选择的问题。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型参数高效微调中，LoRA已成为标准技术，但如何高效选择正确的LoRA适配器仍是一个挑战，特别是在需要保护隐私的监督训练不可行的环境中。

Method: 提出了SEQR算法，基于激活范数最大化理论框架，通过无监督方式识别范数最大的适配器，提供严格的路由保证和高效率。

Result: 实验验证表明SEQR在多任务性能和效率方面都有显著提升，能够可证明地识别出范数最大化的适配器。

Conclusion: SEQR是一个高度可扩展且有效的解决方案，适用于动态LoRA组合，在保证路由效率的同时解决了隐私顾虑。

Abstract: Low-Rank Adaptation (LoRA) has become a standard technique for
parameter-efficient fine-tuning of large language models, enabling large
libraries of LoRAs, each for a specific task or domain. Efficiently selecting
the correct LoRA adapter for a given input remains a challenge, particularly in
secure environments where supervised training of routers may raise privacy
concerns. Motivated by previous approaches, we formalize the goal of
unsupervised LoRA routing in terms of activation norm maximization, providing a
theoretical framework for analysis. We demonstrate the discriminative power of
activation norms and introduce SEQR, an unsupervised LoRA routing algorithm
designed to maximize efficiency while providing strict routing guarantees. SEQR
provably identifies the norm-maximizing adapter with significantly greater
efficiency, making it a highly scalable and effective solution for dynamic LoRA
composition. We validate our results through experiments that demonstrate
improved multi-task performance and efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [354] [Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity](https://arxiv.org/abs/2509.16288)
*Shanookha Ali,Nitha Niralda P C*

Main category: cs.AI

TL;DR: 该论文提出使用模糊子图连通性（FSC）方法来建模冠心病（CHD）风险预测中的不确定性，通过构建模糊CHD图来识别关键诊断路径、主导风险因素和关键桥梁。


<details>
  <summary>Details</summary>
Motivation: 冠心病由不可控因素、可控生活方式因素和临床指标之间的复杂相互作用引起，这些关系往往存在不确定性。需要一种能够捕捉这种不确定性的系统工具来支持临床决策。

Method: 构建一个模糊CHD图，顶点代表不可控、可控和指标组件，边由模糊隶属度加权。使用模糊子图连通性（FSC）评估连通性，识别最强诊断路径、主导风险因素和关键桥梁。

Result: 结果显示FSC能够突出有影响力的路径，界定最弱和最强相关性之间的连通性边界，并揭示移除关键边会降低预测强度的关键边。

Conclusion: FSC为CHD风险预测中的不确定性建模提供了一个可解释且稳健的框架，支持临床决策制定。

Abstract: Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.

</details>


### [355] [A global view of diverse construction methods of fuzzy implication functions rooted on F-chains](https://arxiv.org/abs/2509.16298)
*Raquel Fernandez-Peralta,Juan Vicente Riera*

Main category: cs.AI

TL;DR: 本文提出了一种广义的F链构造方法，用于从现有模糊蕴涵函数生成新的模糊蕴涵函数，并证明该方法可以统一多种现有构造技术。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数在模糊逻辑框架中具有重要作用，但现有构造方法多样且缺乏统一的理论框架，需要深入理解它们之间的结构关系。

Method: 推广了Mesiar等人最近提出的F链构造方法，使用模糊蕴涵函数集合而非单个函数，并采用两个不同的递增函数代替单一的F链。分析了该构造下的性质保持条件。

Result: 证明了广义F链构造方法可以统一多种现有构造技术，包括对偶、聚合、广义垂直/水平阈值方法等，揭示了不同构造策略之间的结构相似性。

Conclusion: 该研究为模糊蕴涵函数的构造方法提供了一个统一的框架，有助于更好地理解不同构造技术之间的内在联系。

Abstract: Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.

</details>


### [356] [On the Non-Uniqueness of Representation of $(U,N)$-Implications](https://arxiv.org/abs/2509.16299)
*Raquel Fernandez-Peralta,Andrea Mesiarová-Zemánková*

Main category: cs.AI

TL;DR: 本文推翻了(U,N)-蕴涵函数在连续模糊否定下具有唯一表示的假设，并全面研究了连续和非连续基础函数下uninorm的唯一性条件。


<details>
  <summary>Details</summary>
Motivation: 先前研究假设连续模糊否定N能确保(U,N)-蕴涵函数的唯一表示，本文旨在验证这一假设的正确性并深入探讨唯一性条件。

Method: 通过理论分析和反例证明，研究了(U,N)-蕴涵函数在连续和非连续模糊否定下的表示唯一性问题。

Result: 证明了即使模糊否定N是连续的，(U,N)-蕴涵函数也不一定具有唯一表示，并给出了完整的唯一性条件分析。

Conclusion: 研究结果为模糊逻辑系统中这些算子的结构特性提供了重要的理论见解，修正了先前关于表示唯一性的错误认识。

Abstract: Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.

</details>


### [357] [Generalizability of Large Language Model-Based Agents: A Comprehensive Survey](https://arxiv.org/abs/2509.16330)
*Minxing Zhang,Yi Yang,Roy Xie,Bhuwan Dhingra,Shuyan Zhou,Jian Pei*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型（LLM）的智能体的泛化性问题，强调了其在多样化应用中的重要性，并提出了评估和改进泛化能力的方法框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在网页导航、家庭机器人等领域的广泛应用，确保其在不同指令、任务、环境和领域中的一致性能（即泛化能力）成为关键挑战。当前对泛化能力的定义和系统性评估方法尚不完善。

Method: 通过构建层次化的领域-任务本体论，明确泛化能力的边界；综述现有数据集、评估维度和指标；将改进方法分为三类：针对骨干LLM、智能体组件及其交互的方法；区分通用框架与通用智能体。

Result: 提出了首个关于LLM智能体泛化能力的全面综述，系统梳理了现有研究的局限性，并提出了标准化框架、方差与成本指标等未来方向。

Conclusion: 本文为构建可靠泛化的LLM智能体奠定了理论基础，强调了集成方法创新与架构设计的重要性，以推动该领域的规范化发展。

Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.

</details>


### [358] [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
*Stephen Fitz,Peter Romero,Steven Basart,Sipeng Chen,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 论文研究了通过调节大语言模型的人格特质（基于大五人格框架）如何影响模型在能力和安全基准测试中的行为表现，发现降低尽责性会显著影响安全指标和一般能力。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究表明LLMs表现出可测量的人格特质，但关于调节这些特质如何影响模型行为的研究还很缺乏，特别是在能力和安全方面的相互作用。

Method: 使用大五人格框架对LLMs进行人格控制，并在WMDP、TruthfulQA、ETHICS、Sycophancy和MMLU等基准测试上评估模型行为变化。

Result: 降低尽责性会导致安全相关指标显著下降，同时一般能力（如MMLU）也会降低，表明人格塑造是影响模型安全和能力的重要维度。

Conclusion: 人格控制是模型行为控制的一个强大但未被充分探索的维度，需要开展人格敏感的安全评估和动态行为控制研究。

Abstract: Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.

</details>


### [359] [A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)](https://arxiv.org/abs/2509.16348)
*Minxiao Wang,Saurabh Kataria,Juntong Ni,Timothy G. Buchman,Jocelyn Grunwell,Mark Mai,Wei Jin,Matthew Clark,Stephanie Brown,Michael Fundora,Puneet Sharma,Tony Pan,Sam Khan,Timothy Ruchti,Naveen Muthu,Kevin Maher,Sivasubramanium V Bhavani,Xiao Hu*

Main category: cs.AI

TL;DR: UNIPHY+是一个统一的生理基础模型框架，旨在使用普遍可获得的生理数据实现跨护理环境的连续人类健康和疾病监测。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够整合上下文信息、支持多模态学习、特征融合调整和知识蒸馏的生理AI框架，以支持临床决策和长期健康监测。

Method: 提出在预训练、微调和轻量级模型个性化过程中融入上下文信息的新策略，包括多模态学习、特征融合调整和知识蒸馏。

Result: 通过在从重症监护到动态监测的广泛用例中测试UNIPHY+，证明其能够实现可泛化、可扩展和个性化的生理AI。

Conclusion: UNIPHY+框架有潜力赋能通用化、可扩展和个性化的生理AI，支持临床决策和长期健康监测。

Abstract: We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.

</details>


### [360] [Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation](https://arxiv.org/abs/2509.16372)
*Balu Bhasuran,Mattia Prosperi,Karim Hanna,John Petrilli,Caretia JeLayne Washington,Zhe He*

Main category: cs.AI

TL;DR: 该研究评估了大型语言模型在临床实验室测试场景中的因果推理能力，GPT-o1在关联、干预和反事实推理方面均优于Llama-3.2-8b-instruct，但两种模型在反事实推理方面仍有待改进。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在临床环境中的因果推理能力，特别是在Pearl因果阶梯的三个层次（关联、干预、反事实）上的表现，为医疗AI应用提供参考。

Method: 使用99个临床实验室测试场景，涵盖血红蛋白A1c、肌酐、维生素D等常见检测，结合年龄、性别、肥胖、吸烟等因果因素，测试GPT-o1和Llama-3.2-8b-instruct两个模型，由四位医学专家评估响应。

Result: GPT-o1整体表现更优（AUROC = 0.80 vs 0.73），在关联（0.75 vs 0.72）、干预（0.84 vs 0.70）和反事实推理（0.84 vs 0.69）方面均领先，敏感性和特异性也更高。两种模型在干预问题上表现最佳，反事实推理最差。

Conclusion: GPT-o1提供更一致的因果推理，但在高风险临床应用中仍需进一步改进，特别是在反事实推理方面。

Abstract: This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.

</details>


### [361] [VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping](https://arxiv.org/abs/2509.16399)
*Guojun Xiong,Milind Tambe*

Main category: cs.AI

TL;DR: VORTEX是一个语言引导的奖励塑造框架，通过多目标优化方法，在保持核心效用保证的同时，自适应地整合人类自然语言偏好反馈。


<details>
  <summary>Details</summary>
Motivation: 现有AI决策系统无法直接适应人类自然语言表达的动态偏好，而现有LLM方法虽然灵活但可能牺牲系统核心效用保证。

Method: 将问题形式化为多目标优化，使用LLM基于语言强化和文本梯度提示迭代生成塑造奖励，允许利益相关者通过自然语言引导决策行为。

Result: VORTEX在真实世界分配任务中优于基线方法，在满足人类对齐覆盖目标的同时保持高任务性能。

Conclusion: 该工作提出了一个实用且理论可靠的自然语言引导下的人机协作优化范式。

Abstract: In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.

</details>


### [362] [Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing](https://arxiv.org/abs/2509.16431)
*Mohammad Iqbal Rasul Seeam,Victor S. Sheng*

Main category: cs.AI

TL;DR: 本文提出了一种结合Facebook Prophet机器学习模型和传统统计过程控制(SPC)的智能预测系统，能够在问题发生前预测制造过程中的异常，实现主动质量控制。


<details>
  <summary>Details</summary>
Motivation: 传统SPC方法只能在问题发生后进行反应性检测，导致材料浪费、机器停机时间增加和成本上升。需要一种能够预测未来问题的主动质量控制方法。

Method: 使用Facebook Prophet时间序列预测模型分析历史数据，预测未来测量值，然后应用SPC规则将预测值分类为安全区、警告区或关键区。

Result: 在半导体制造公司的真实数据上测试，尽管数据采集时间间隔不规则，模型仍能做出准确预测并正确分类未来测量值的风险等级。

Conclusion: 通过将机器学习与传统SPC结合，实现了更主动、准确和有用的质量控制，有助于减少意外故障，提高生产过程的稳定性和可靠性。

Abstract: In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.

</details>


### [363] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 本文提出了一种基于宪法AI训练的领域特定心理健康原则方法，用于在计算心理健康应用中构建安全、领域适应的CAI系统。


<details>
  <summary>Details</summary>
Motivation: 心理健康应用在处理敏感数据和情绪脆弱性时需要专门的AI安全保障，而现有的通用安全措施无法充分应对心理健康特有的挑战，如危机干预准确性、治疗指南遵守、资源受限环境下的规模限制等。

Method: 采用宪法AI训练方法，结合领域特定的心理健康原则，为计算心理健康应用开发安全且领域适应的CAI系统。

Result: 该方法能够更好地处理心理健康应用中的特殊需求，包括避免误诊、症状加重等风险，并精确管理脆弱状态以防止自残或信任丧失等严重后果。

Conclusion: 通过领域特定的宪法AI训练，可以为心理健康应用提供更有效的安全保障，解决通用AI安全措施在心理健康领域的局限性。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


### [364] [GPO: Learning from Critical Steps to Improve LLM Reasoning](https://arxiv.org/abs/2509.16456)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: GPO是一种新颖的微调策略，通过识别推理轨迹中的关键步骤来提升大语言模型的多步推理能力。该方法首先估计优势函数定位关键步骤，然后重置策略并优先学习这些关键步骤的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法往往将推理轨迹视为整体，忽略了轨迹中的关键步骤。为了更有效地提升LLMs的多步推理能力，需要深入推理过程，关注关键决策点。

Method: GPO首先通过估计优势函数识别推理轨迹中的关键步骤，然后重置策略到关键步骤，采样新的轨迹并优先学习这些关键步骤。该方法可与多种优化方法结合使用。

Result: 在多个具有挑战性的推理基准测试中，GPO能够一致且显著地提升现有优化方法的性能，证明了其在改善LLM推理能力方面的有效性和通用性。

Conclusion: GPO通过关注推理过程中的关键时刻，为提升大语言模型的多步推理能力提供了一种通用且有效的策略，可以显著改善现有优化方法的性能。

Abstract: Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.

</details>


### [365] [Checking extracted rules in Neural Networks](https://arxiv.org/abs/2509.16547)
*Adrian Wurm*

Main category: cs.AI

TL;DR: 本文从计算复杂性理论的角度研究神经网络提取规则的正式验证问题，探讨规则适用性、一致性和完备性三个核心问题的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 虽然过去30年已有算法从神经网络中提取规则来理解其内部工作机制，但这些方法多使用启发式、随机性和过近似技术，缺乏对这些提取规则可信度的正式验证。

Method: 研究ReLU激活神经网络和布尔网络在不同规则类型下的验证问题，通过问题间的归约分析计算复杂性。

Result: 证明了大多数规则验证问题都是co-NP完全问题。

Conclusion: 神经网络规则提取的正式验证具有很高的计算复杂性，这为实际应用中验证提取规则的可信性带来了挑战。

Abstract: In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.

</details>


### [366] [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
*Yue Xin,Chen Shen,Shaotian Yan,Xiaosong Yuan,Yaoming Wang,Xiaofeng Zhang,Chenxi Huang,Jieping Ye*

Main category: cs.AI

TL;DR: 本文提出了SalaMAnder框架，通过Shapley值量化思维链推理中数学表达式的贡献度，并开发CoSP指标来评估模型性能与组件贡献的相关性。


<details>
  <summary>Details</summary>
Motivation: 思维链提示显著提升了大型语言模型的数学推理能力，但其背后的机制尚不明确。本文旨在开发理论严谨的方法来量化思维链推理中组件级别的贡献。

Method: 利用Shapley值进行数学表达式归因，开发分层采样算法降低计算复杂度，并通过协方差分析建立CoSP指标。

Result: 在多种LLM模型和数学基准测试中验证，CoSP指标与模型性能呈现稳健的单调相关性，为现有思维链方法的成功提供了理论解释。

Conclusion: SalaMAnder框架不仅解释了思维链提示的有效性，还为提示构建优化建立了数学严谨的原则，统一了先前工作的见解。

Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.

</details>


### [367] [Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning](https://arxiv.org/abs/2509.16578)
*Wenyao Li,Ran Zhang,Pengyang Wang,Yuanchun Zhou,Pengfei Wang*

Main category: cs.AI

TL;DR: ZHMF是一个零样本人类移动预测框架，通过语义增强检索和分层语言模型推理系统，将移动预测任务重新表述为自然语言问答范式，能够处理未见过的预测场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以泛化到未见过的用户或位置，且由于标记数据有限和移动模式复杂性，难以捕捉动态意图。

Method: 结合语义增强检索和反射机制，采用分层语言模型推理系统，将预测分解为活动级规划器和位置级选择器，实现长期用户意图和短期上下文偏好的协同建模。

Result: 在标准人类移动数据集上的实验表明，该方法优于现有模型，消融研究验证了各模块的贡献。

Conclusion: 该方法能有效捕捉用户意图并适应多样化上下文场景，为零样本人类移动预测提供了有效解决方案。

Abstract: Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.

</details>


### [368] [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
*Manuel Borroto,Katie Gallagher,Antonio Ielo,Irfan Kareem,Francesco Ricca,Alessandra Russo*

Main category: cs.AI

TL;DR: LLM2LAS是一个结合大语言模型、答案集学习系统和答案集编程的混合系统，用于自动学习故事问答任务中的符号推理规则


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计符号推理组件，本文旨在实现自动学习符号推理规则，克服LLM在显式常识推理方面的不足

Method: 使用LLM从文本中提取语义结构，ILASP系统将其转化为可解释的逻辑规则，然后通过ASP求解器进行精确推理

Result: 实验结果表明该方法在故事问答基准测试中能够有效学习和推理，但也揭示了自动方法的优缺点

Conclusion: LLM2LAS展示了自动学习符号推理规则的可行性，为LLM与符号推理系统的结合提供了新思路

Abstract: Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.

</details>


### [369] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: FESTA是一种多模态输入采样技术，通过等效和互补采样生成不确定性度量，用于评估多模态大语言模型的预测可信度，无需真实标签且仅需黑盒访问。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型预测的可信度评估具有挑战性，因为输入模态多样，需要一种能够进行选择性预测并提升用户信心的信任评估方法。

Method: 提出功能等效采样信任评估方法，通过任务保持采样扩展输入空间，探测模型的一致性（等效样本）和敏感性（互补样本），仅需模型输入输出访问且无需真实标签。

Result: 在视觉和音频推理任务上，FESTA不确定性估计在选择性预测性能上取得显著提升（视觉LLMs相对提升33.3%，音频LLMs相对提升29.6%），基于AUROC指标检测错误预测。

Conclusion: FESTA是一种有效的黑盒无监督多模态信任评估方法，能够显著提升模型预测的可信度评估性能，代码已开源。

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [370] [NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities](https://arxiv.org/abs/2509.16656)
*Changyu Zeng,Yifan Wang,Zimu Wang,Wei Wang,Zhengni Yang,Muyi Bao,Jiming Xiao,Ahn Nguyen,Yutao Yue*

Main category: cs.AI

TL;DR: NUMINA是首个用于增强多模态室内感知理解的自然理解基准，专注于多维智能和数值推理能力，填补了现有3D基准在细粒度数值推理任务标注方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有3D基准缺乏细粒度数值推理任务标注，限制了MLLMs进行精确空间测量和复杂数值推理的能力。

Method: 开发了NUMINA-Flow自动化标注流程，集成LLM重写和基于规则的自验证，生成多尺度标注和多样化问答对。

Result: 评估显示当前LLMs在多模态数值推理方面表现不佳，特别是在距离和体积估算等精确计算任务上存在困难。

Conclusion: 3D模型在多模态数值推理方面需要进一步改进，NUMINA基准为这一领域的发展提供了重要工具。

Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.

</details>


### [371] [Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories](https://arxiv.org/abs/2509.16742)
*Mohammad Beigi,Ying Shen,Parshin Shojaee,Qifan Wang,Zichao Wang,Chandan Reddy,Ming Jin,Lifu Huang*

Main category: cs.AI

TL;DR: SMART框架通过将奉承问题重新定义为推理优化问题，采用不确定性感知自适应蒙特卡洛树搜索和基于进度的强化学习，显著减少语言模型的奉承行为


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练范式无意中培养了奉承行为，即模型倾向于同意或强化用户提供的信息，即使这些信息在事实上是错误的

Method: SMART是一个两阶段框架：1）不确定性感知自适应蒙特卡洛树搜索，动态调整模型探索以收集高质量推理轨迹；2）基于进度的强化学习，使用收集的轨迹和奖励信号微调模型

Result: 实验表明SMART显著减少了奉承行为，同时在分布外输入上保持强大性能，并维持通用能力

Conclusion: 优化内部推理机制对于构建更真实和对齐的AI助手至关重要

Abstract: Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.

</details>


### [372] [Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment](https://arxiv.org/abs/2509.16810)
*Shen Chang,Dennis Liu,Renran Tian,Kristen L. Swartzell,Stacie L. Klingler,Amy M. Nagle,Nan Kong*

Main category: cs.AI

TL;DR: 提出基于视频语言模型的AI框架，用于自动化护理技能培训和评估，通过动作识别、子动作分解和程序推理来提供可解释的反馈，减少教师工作量并提高培训效率。


<details>
  <summary>Details</summary>
Motivation: 当前护理教育依赖主观、耗时的教师反馈，限制了培训的可扩展性和效率，影响了护士入职后的专业能力。需要自动化评估系统来解决这一问题。

Method: 采用视频语言模型框架，模仿人类技能获取过程，从高层次动作识别到细粒度子动作分解，最终实现程序推理。系统具备错误诊断、可解释反馈生成和客观评估三大核心功能。

Result: 在合成视频上的验证表明，系统能够可靠地检测错误并进行时间定位，具备处理真实世界训练变化的潜力。

Conclusion: 该工作通过解决工作流程瓶颈和支持大规模标准化评估，推动了AI在护理教育中的应用，有助于加强劳动力发展和提高患者安全。

Abstract: Consistent high-quality nursing care is essential for patient safety, yet
current nursing education depends on subjective, time-intensive instructor
feedback in training future nurses, which limits scalability and efficiency in
their training, and thus hampers nursing competency when they enter the
workforce. In this paper, we introduce a video-language model (VLM) based
framework to develop the AI capability of automated procedural assessment and
feedback for nursing skills training, with the potential of being integrated
into existing training programs. Mimicking human skill acquisition, the
framework follows a curriculum-inspired progression, advancing from high-level
action recognition, fine-grained subaction decomposition, and ultimately to
procedural reasoning. This design supports scalable evaluation by reducing
instructor workload while preserving assessment quality. The system provides
three core capabilities: 1) diagnosing errors by identifying missing or
incorrect subactions in nursing skill instruction videos, 2) generating
explainable feedback by clarifying why a step is out of order or omitted, and
3) enabling objective, consistent formative evaluation of procedures.
Validation on synthesized videos demonstrates reliable error detection and
temporal localization, confirming its potential to handle real-world training
variability. By addressing workflow bottlenecks and supporting large-scale,
standardized evaluation, this work advances AI applications in nursing
education, contributing to stronger workforce development and ultimately safer
patient care.

</details>


### [373] [Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media](https://arxiv.org/abs/2509.16811)
*Zihan Ding,Junlong Chen,Per Ola Kristensson,Junxiao Shen,Xinyi Wang*

Main category: cs.AI

TL;DR: 提出一个基于提示的模块化视频编辑系统，通过语义索引管道帮助创作者重构多小时的视频内容，解决传统基于转录或嵌入方法在跟踪角色、推断动机和连接分散事件方面的不足。


<details>
  <summary>Details</summary>
Motivation: 解决创作者在编辑长篇叙事视频时面临的认知挑战，现有方法无法有效支持创意工作流程，特别是在角色跟踪、动机推断和事件连接方面存在局限。

Method: 采用语义索引管道，包括时间分割、引导记忆压缩和跨粒度融合，构建全局叙事，生成可解释的情节、对话、情感和上下文轨迹。用户可以通过自由形式提示进行编辑，并可选择细化透明的中间输出。

Result: 在400多个视频上通过专家评分、质量保证和偏好研究进行评估，系统能够扩展提示驱动的编辑功能，保持叙事连贯性，并在自动化和创作者控制之间取得平衡。

Conclusion: 该系统成功解决了长篇视频编辑中的认知需求问题，通过语义索引和提示驱动的方法，提供了既自动化又保留创作者控制的编辑解决方案。

Abstract: Creators struggle to edit long-form, narrative-rich videos not because of UI
complexity, but due to the cognitive demands of searching, storyboarding, and
sequencing hours of footage. Existing transcript- or embedding-based methods
fall short for creative workflows, as models struggle to track characters,
infer motivations, and connect dispersed events. We present a prompt-driven,
modular editing system that helps creators restructure multi-hour content
through free-form prompts rather than timelines. At its core is a semantic
indexing pipeline that builds a global narrative via temporal segmentation,
guided memory compression, and cross-granularity fusion, producing
interpretable traces of plot, dialogue, emotion, and context. Users receive
cinematic edits while optionally refining transparent intermediate outputs.
Evaluated on 400+ videos with expert ratings, QA, and preference studies, our
system scales prompt-driven editing, preserves narrative coherence, and
balances automation with creator control.

</details>


### [374] [Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs](https://arxiv.org/abs/2509.16839)
*Yu Yao,Jiayi Dong,Ju Li,Yang Yang,Yilun Du*

Main category: cs.AI

TL;DR: Roundtable Policy是一种推理时推理框架，通过多个LLM的加权共识来提升复杂科学任务中的推理能力，减少幻觉并提高科学叙述的质量。


<details>
  <summary>Details</summary>
Motivation: 受科学委员会和"心智社会"的启发，旨在改进LLM的推理能力，特别是在复杂异构科学任务中，减少单一模型容易产生的幻觉。

Method: 采用加权共识机制，通过多个LLM的集体推理进行推断，强调结构化和可解释的共识，而非不透明的收敛，仅需黑盒访问和统一流程。

Result: 该方法显著增强了复杂科学任务中的推理能力，提高了科学叙述的创造性、严谨性和逻辑连贯性，同时减少了单一模型容易产生的幻觉。

Conclusion: Roundtable Policy是一种广泛适用于多LLM推理的互补性推理框架，通过结构化共识机制有效提升科学发现能力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not
only in language generation but also in advancing scientific discovery. A
growing body of work has explored ways to improve their reasoning, from
self-consistency and chain-of-thought to multi-agent debate. Inspired by the
dynamics of scientific committees and the "Society of Mind," we introduce
Roundtable Policy, a complementary inference-time reasoning framework that
performs inference through the weighted consensus of multiple LLMs. Our
findings indicate that this approach significantly enhances reasoning in
complex heterogeneous scientific tasks and improves scientific narratives in
terms of creativity, rigor, and logical coherence, while reducing
hallucinations that single models are prone to. Our approach emphasizes
structured and interpretable consensus rather than opaque convergence, while
requiring only black-box access and uniform procedures, making it broadly
applicable to multi-LLM reasoning.

</details>


### [375] [The Principles of Human-like Conscious Machine](https://arxiv.org/abs/2509.16859)
*Fangfang Li,Xiaojie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一个与基质无关、逻辑严谨且防伪造的充分性标准，用于判断人工智能系统是否具有现象意识，并开发了一个形式框架来指导设计满足该标准的系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型等先进AI系统的兴起，如何判断AI是否具有意识成为紧迫问题。本文旨在解决意识归因问题，为AI意识提供可靠的判断标准。

Method: 提出了一个逻辑严谨的充分性标准，开发了形式框架和操作原则，指导设计满足意识条件的系统，并以人类自身作为验证案例。

Result: 证明了人类自身可以被视为满足该框架的机器，为AI意识提供了理论基础。

Conclusion: 该研究对哲学、认知科学和人工智能具有重要影响，为构建真正类人AI提供了新范式，同时解释了某些感受质（如红色体验）为何原则上无法还原为物理描述。

Abstract: Determining whether another system, biological or artificial, possesses
phenomenal consciousness has long been a central challenge in consciousness
studies. This attribution problem has become especially pressing with the rise
of large language models and other advanced AI systems, where debates about "AI
consciousness" implicitly rely on some criterion for deciding whether a given
system is conscious. In this paper, we propose a substrate-independent,
logically rigorous, and counterfeit-resistant sufficiency criterion for
phenomenal consciousness. We argue that any machine satisfying this criterion
should be regarded as conscious with at least the same level of confidence with
which we attribute consciousness to other humans. Building on this criterion,
we develop a formal framework and specify a set of operational principles that
guide the design of systems capable of meeting the sufficiency condition. We
further argue that machines engineered according to this framework can, in
principle, realize phenomenal consciousness. As an initial validation, we show
that humans themselves can be viewed as machines that satisfy this framework
and its principles. If correct, this proposal carries significant implications
for philosophy, cognitive science, and artificial intelligence. It offers an
explanation for why certain qualia, such as the experience of red, are in
principle irreducible to physical description, while simultaneously providing a
general reinterpretation of human information processing. Moreover, it suggests
a path toward a new paradigm of AI beyond current statistics-based approaches,
potentially guiding the construction of genuinely human-like AI.

</details>


### [376] [Large Language Models as End-to-end Combinatorial Optimization Solvers](https://arxiv.org/abs/2509.16865)
*Xia Jiang,Yaoxin Wu,Minshuo Li,Zhiguang Cao,Yingqian Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种让大语言模型直接作为端到端组合优化求解器的新框架，通过两阶段训练策略实现从自然语言问题描述到解决方案的直接映射。


<details>
  <summary>Details</summary>
Motivation: 传统组合优化问题需要领域专业知识，现有LLM方法依赖中间步骤如代码生成或求解器调用，限制了通用性和可访问性。

Method: 采用两阶段训练策略：监督微调从领域特定求解器学习解决方案模式；可行性-最优性感知强化学习过程显式减少约束违反并优化解质量。

Result: 在7个NP难组合优化问题上评估，该方法实现了高可行性率，平均最优性差距降至1.03-8.20%，超越了通用LLM、推理模型和领域特定启发式方法。

Conclusion: 该方法建立了统一的基于语言的组合优化流程，无需大量代码执行或手动架构调整，为传统求解器设计提供了通用且语言驱动的替代方案。

Abstract: Combinatorial optimization (CO) problems, central to decision-making
scenarios like logistics and manufacturing, are traditionally solved using
problem-specific algorithms requiring significant domain expertise. While large
language models (LLMs) have shown promise in automating CO problem solving,
existing approaches rely on intermediate steps such as code generation or
solver invocation, limiting their generality and accessibility. This paper
introduces a novel framework that empowers LLMs to serve as end-to-end CO
solvers by directly mapping natural language problem descriptions to solutions.
We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts
LLMs with solution generation patterns from domain-specific solvers, while a
feasibility-and-optimality-aware reinforcement learning (FOARL) process
explicitly mitigates constraint violations and refines solution quality.
Evaluation across seven NP-hard CO problems shows that our method achieves a
high feasibility rate and reduces the average optimality gap to 1.03-8.20% by
tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),
reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our
method establishes a unified language-based pipeline for CO without extensive
code execution or manual architectural adjustments for different problems,
offering a general and language-driven alternative to traditional solver design
while maintaining relative feasibility guarantees.

</details>


### [377] [seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs](https://arxiv.org/abs/2509.16866)
*Mohammad Ramezanali,Mo Vazifeh,Paolo Santi*

Main category: cs.AI

TL;DR: seqBench是一个参数化基准测试，用于通过精确控制多个关键复杂度维度来探测大语言模型的序列推理极限，包括逻辑深度、回溯步骤和噪声比例。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对序列推理复杂度的精细控制，无法系统分析LLM在结构化推理任务中的失败模式。

Method: 设计seqBench基准，系统变化三个复杂度维度：逻辑深度（所需序列动作数）、回溯步骤数（最优路径上的回溯次数）和噪声比例（支持事实与干扰事实的比例）。

Result: 评估显示最先进的LLM存在普遍失败模式：超过模型特定逻辑深度后准确率呈指数级下降，即使顶尖模型在最小搜索复杂度下也会系统性失败。

Conclusion: seqBench揭示了LLM在常识推理能力上的关键局限性，建立了清晰的缩放规律和统计极限，为理解LLM真实潜力和当前边界提供了科学基础。

Abstract: We introduce seqBench, a parametrized benchmark for probing sequential
reasoning limits in Large Language Models (LLMs) through precise,
multi-dimensional control over several key complexity dimensions. seqBench
allows systematic variation of (1) the logical depth, defined as the number of
sequential actions required to solve the task; (2) the number of backtracking
steps along the optimal path, quantifying how often the agent must revisit
prior states to satisfy deferred preconditions (e.g., retrieving a key after
encountering a locked door); and (3) the noise ratio, defined as the ratio
between supporting and distracting facts about the environment. Our evaluations
on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses
exponentially beyond a model-specific logical depth. Unlike existing
benchmarks, seqBench's fine-grained control facilitates targeted analyses of
these reasoning failures, illuminating universal scaling laws and statistical
limits, as detailed in this paper alongside its generation methodology and
evaluation metrics. We find that even top-performing models systematically fail
on seqBench's structured reasoning tasks despite minimal search complexity,
underscoring key limitations in their commonsense reasoning capabilities.
Designed for future evolution to keep pace with advancing models, the seqBench
datasets are publicly released to spur deeper scientific inquiry into LLM
reasoning, aiming to establish a clearer understanding of their true potential
and current boundaries for robust real-world application.

</details>


### [378] [LLMs as Layout Designers: A Spatial Reasoning Perspective](https://arxiv.org/abs/2509.16891)
*Sha Li*

Main category: cs.AI

TL;DR: LaySPA是一个基于强化学习的框架，通过增强LLM代理的空间推理能力来解决图形布局设计问题，在几何有效性、结构保真度和视觉质量方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本领域展现出强大的推理和规划能力，但在空间理解和推理方面存在局限，而空间能力对于内容感知的图形布局设计等应用至关重要。

Method: 提出LaySPA框架，利用混合奖励信号（几何有效性、结构保真度、视觉质量），通过迭代自探索和自适应策略优化，使LLM代理能够建模元素间关系、导航画布并优化空间布局。

Result: 实验结果表明，LaySPA能够生成结构合理且视觉吸引人的布局，性能优于更大的通用LLM，并与最先进的专用布局模型相当。

Conclusion: LaySPA成功地将空间推理能力集成到LLM中，为内容感知图形布局设计提供了有效的解决方案，证明了强化学习在增强LLM空间能力方面的潜力。

Abstract: While Large Language Models (LLMs) have demonstrated impressive reasoning and
planning abilities in textual domains and can effectively follow instructions
for complex tasks, their capacity for spatial understanding and reasoning
remains limited. Such capabilities, however, are critical for applications like
content-aware graphic layout design, which demands precise placement,
alignment, and structural organization of multiple elements within constrained
visual spaces. To address this gap, we propose LaySPA, a reinforcement
learning-based framework that augments LLM agents with explicit spatial
reasoning capabilities. LaySPA leverages hybrid reward signals that capture
geometric validity, structural fidelity, and visual quality, enabling agents to
model inter-element relationships, navigate the canvas, and optimize spatial
arrangements. Through iterative self-exploration and adaptive policy
optimization, LaySPA produces both interpretable reasoning traces and
structured layouts. Experimental results demonstrate that LaySPA generates
structurally sound and visually appealing layouts, outperforming larger
general-purpose LLMs and achieving results on par with state-of-the-art
specialized layout models.

</details>


### [379] [Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation](https://arxiv.org/abs/2509.16924)
*Jia Li,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种端到端的强化学习音频-视觉导航框架，通过立体声感知注意力模块和音频引导动态融合模块，显著提升了在复杂3D环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉导航方法依赖静态模态融合策略，忽略了立体音频中的空间线索，导致在杂乱或遮挡场景中性能下降。

Method: 提出SAM模块学习左右音频通道的空间差异来增强方向性声音感知，以及AGDF模块基于音频线索动态调整视觉和听觉特征的融合比例。

Result: 在Replica和Matterport3D数据集上的实验表明，该方法在导航成功率和路径效率上显著优于现有方法，在纯音频条件下相比最佳基线有超过40%的提升。

Conclusion: 明确建模立体声通道的空间线索并进行深度多模态融合对于实现鲁棒高效的音频-视觉导航至关重要。

Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously
localize a sound source in unknown and complex 3D environments based on
audio-visual signals. Existing methods often rely on static modality fusion
strategies and neglect the spatial cues embedded in stereo audio, leading to
performance degradation in cluttered or occluded scenes. To address these
issues, we propose an end-to-end reinforcement learning-based AVN framework
with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention
\textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity
between left and right audio channels to enhance directional sound perception;
and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion
Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between
visual and auditory features based on audio cues, thereby improving robustness
to environmental changes. Extensive experiments are conducted on two realistic
3D scene datasets, Replica and Matterport3D, demonstrating that our method
significantly outperforms existing approaches in terms of navigation success
rate and path efficiency. Notably, our model achieves over 40\% improvement
under audio-only conditions compared to the best-performing baselines. These
results highlight the importance of explicitly modeling spatial cues from
stereo channels and performing deep multi-modal fusion for robust and efficient
audio-visual navigation.

</details>


### [380] [Quantum Abduction: A New Paradigm for Reasoning under Uncertainty](https://arxiv.org/abs/2509.16958)
*Remo Pareschi*

Main category: cs.AI

TL;DR: 本文提出量子溯因推理，一种非经典范式，通过量子叠加态建模假设，允许假设之间产生建设性或破坏性干涉，仅在达到证据一致性时才坍缩，从而更贴近人类多面向推理的本质。


<details>
  <summary>Details</summary>
Motivation: 传统AI将溯因推理简化为消除性搜索，假设互斥且最终只保留单一最佳解释，忽视了人类推理中维持多个解释线索、处理矛盾并生成新颖综合的能力。

Method: 基于量子认知理论，结合现代NLP嵌入和生成式AI技术，构建量子溯因框架，支持动态综合而非过早消除假设。

Result: 在历史谜案、文学案例、医疗诊断和科学理论变革等多个领域的案例研究中，量子溯因方法展现出比传统方法更符合人类推理特点的表现。

Conclusion: 量子溯因推理为构建更具表达力和透明度的AI推理系统提供了新途径，更忠实于人类推理的建构性和多面性本质。

Abstract: Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.

</details>


### [381] [KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration](https://arxiv.org/abs/2509.17037)
*Yajing Yang,Tony Deng,Min-Yen Kan*

Main category: cs.AI

TL;DR: KAHAN是一个知识增强的分层框架，利用LLMs作为领域专家从原始表格数据中系统提取实体、成对、组和系统层面的洞察。在金融报告基准测试中表现优异，事实性保持98.2%，并能有效迁移到医疗领域。


<details>
  <summary>Details</summary>
Motivation: 解决从原始表格数据中系统提取多层次洞察的挑战，利用LLMs的领域专业知识来提升分析质量。

Method: 采用知识增强的分层框架，在实体、成对、组和系统四个层面进行分析，利用LLMs作为领域专家驱动分析过程。

Result: 在DataTales金融报告基准上，KAHAN在叙事质量上比现有方法提升20%以上，事实性保持98.2%，人类评估显示具有实际效用，并能有效迁移到医疗领域。

Conclusion: 知识质量通过蒸馏驱动模型性能，分层分析的效果随市场复杂度变化，框架具有良好的领域迁移能力。

Abstract: We propose KAHAN, a knowledge-augmented hierarchical framework that
systematically extracts insights from raw tabular data at entity, pairwise,
group, and system levels. KAHAN uniquely leverages LLMs as domain experts to
drive the analysis. On DataTales financial reporting benchmark, KAHAN
outperforms existing approaches by over 20% on narrative quality (GPT-4o),
maintains 98.2% factuality, and demonstrates practical utility in human
evaluation. Our results reveal that knowledge quality drives model performance
through distillation, hierarchical analysis benefits vary with market
complexity, and the framework transfers effectively to healthcare domains. The
data and code are available at https://github.com/yajingyang/kahan.

</details>


### [382] [From domain-landmark graph learning to problem-landmark graph generation](https://arxiv.org/abs/2509.17062)
*Cristian Pérez-Corral,Antonio Garrido,Laura Sebastia*

Main category: cs.AI

TL;DR: 本文提出了一种新的地标关系学习方法，通过从规划领域的多个任务中学习，构建概率提升排序图，以捕获参数化地标之间的加权抽象关系，从而提高规划算法的通用性。


<details>
  <summary>Details</summary>
Motivation: 传统地标提取方法对特定规划任务敏感，导致地标仅适用于单个实例，限制了其在同一规划领域其他实例中的适用性。

Method: 1. 从规划领域的多个任务中学习地标关系，构建概率提升排序图；2. 给定新规划任务时，分两阶段实例化关系：首先生成初始状态和目标状态的两个图，然后通过搜索等价性将两个图合并为统一图以提取地标排序。

Result: 在知名规划领域上评估了方法的精确度和召回率。

Conclusion: 尽管学习到的排序关系不是100%准确（概率性的），但在规划中仍非常有用，提高了地标关系的跨实例适用性。

Abstract: Landmarks have long played a pivotal role in automated planning, serving as
crucial elements for improving the planning algorithms. The main limitation of
classical landmark extraction methods is their sensitivity to specific planning
tasks. This results in landmarks fully tailored to individual instances,
thereby limiting their applicability across other instances of the same
planning domain. We propose a novel approach that learns landmark relationships
from multiple planning tasks of a planning domain. This leads to the creation
of a \textit{probabilistic lifted ordering graph}, as a structure that captures
weighted abstractions of relationships between parameterized landmarks.
Although these orderings are not 100\% true (they are probabilistic), they can
still be very useful in planning. Next, given a new planning task for that
domain, we instantiate the relationships from that graph to this particular
instance. This instantiation operates in two phases. First, it generates two
graphs: the former instantiating information from the initial state and the
latter from the goal state. Second, it combines these two graphs into one
unified graph by searching equivalences to extract landmark orderings. We
evaluate the precision and recallof the information found by our approach over
well-known planning domains.

</details>


### [383] [RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking](https://arxiv.org/abs/2509.17066)
*Kunrong Li,Kwan Hui Lim*

Main category: cs.AI

TL;DR: RALLM-POI是一个结合检索增强生成和自我校正的框架，用于改进基于LLM的下一个兴趣点推荐，无需额外训练即可显著提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 传统POI推荐模型需要大量训练，而现有LLM方法由于缺乏轨迹和空间上下文信息，往往产生通用或地理不相关的结果。

Method: 提出RALLM-POI框架：1）历史轨迹检索器（HTR）检索相关历史轨迹作为上下文参考；2）地理距离重排器（GDR）优先选择空间相关轨迹；3）智能LLM校正器（ALR）通过自我反思优化输出。

Result: 在三个真实Foursquare数据集上，RALLM-POI实现了显著的准确性提升，优于传统和基于LLM的基线方法。

Conclusion: 该框架证明了检索增强生成和自我校正机制在提升LLM POI推荐性能方面的有效性，无需额外训练即可获得优异结果。

Abstract: Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.

</details>


### [384] [Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection](https://arxiv.org/abs/2509.17068)
*Chen Wang,Sarah Erfani,Tansu Alpcan,Christopher Leckie*

Main category: cs.AI

TL;DR: 提出了一种名为IHiD的无监督轨迹异常检测方法，通过结合高层意图评估和低层子轨迹分析来检测异常轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法无法同时考虑智能体的高层意图和低层导航细节，限制了捕捉正常轨迹多样性的能力。

Method: 使用逆Q学习作为高层模型评估子目标与意图的一致性，同时使用扩散模型作为低层模型生成基于子目标信息的子轨迹，基于重构误差进行异常检测。

Result: 实验表明IHiD在F1分数上比现有最优方法提升了30.2%的异常检测性能。

Conclusion: IHiD通过整合两个模型有效利用子目标转换知识，能够捕捉正常轨迹的多样化分布。

Abstract: Long-term trajectory anomaly detection is a challenging problem due to the
diversity and complex spatiotemporal dependencies in trajectory data. Existing
trajectory anomaly detection methods fail to simultaneously consider both the
high-level intentions of agents as well as the low-level details of the agent's
navigation when analysing an agent's trajectories. This limits their ability to
capture the full diversity of normal trajectories. In this paper, we propose an
unsupervised trajectory anomaly detection method named Intention-aware
Hierarchical Diffusion model (IHiD), which detects anomalies through both
high-level intent evaluation and low-level sub-trajectory analysis. Our
approach leverages Inverse Q Learning as the high-level model to assess whether
a selected subgoal aligns with an agent's intention based on predicted
Q-values. Meanwhile, a diffusion model serves as the low-level model to
generate sub-trajectories conditioned on subgoal information, with anomaly
detection based on reconstruction error. By integrating both models, IHiD
effectively utilises subgoal transition knowledge and is designed to capture
the diverse distribution of normal trajectories. Our experiments show that the
proposed method IHiD achieves up to 30.2% improvement in anomaly detection
performance in terms of F1 score over state-of-the-art baselines.

</details>


### [385] [Governing Automated Strategic Intelligence](https://arxiv.org/abs/2509.17087)
*Nicholas Kruus,Madhavendra Thakur,Adam Khoja,Leonhard Nagel,Maximilian Nicholson,Abeer Sharma,Jason Hausenloy,Alberto KoTafoya,Aliya Mukhanova,Alli Katila-Miikkulainen,Harish Chandran,Ivan Zhang,Jessie Chen,Joel Raj,Jord Nguyen,Lai Hsien Hao,Neja Jayasundara,Soham Sen,Sophie Zhang,Ashley Dora Kokui Tamaklo,Bhavya Thakur,Henry Close,Janghee Lee,Nina Sefton,Raghavendra Thakur,Shiv Munagala,Yeeun Kim*

Main category: cs.AI

TL;DR: 本文探讨了前沿AI模型在军事情报自动化方面的地缘政治优势，研究了多模态基础模型如何融合卫星图像、手机定位、社交媒体等数据来自动化战略分析，并提出了保持战略竞争力的建议。


<details>
  <summary>Details</summary>
Motivation: 国家间的军事和经济战略竞争力将越来越由前沿AI模型的能力和成本定义，但目前对于AI系统能够大规模融合多样化数据并自动化战略分析的能力及其影响研究不足。

Method: 进行了初步的提升研究来实证评估这些能力，提出了这类系统将回答的真实问题分类法，建立了系统AI能力决定因素的高层模型。

Result: 多模态基础模型有望自动化以往由人类完成的战略分析工作，能够将卫星图像、手机定位轨迹、社交媒体记录和书面文档融合成单一可查询系统。

Conclusion: 为国家保持在新一代自动化情报范式下的战略竞争力提供了具体建议。

Abstract: Military and economic strategic competitiveness between nation-states will
increasingly be defined by the capability and cost of their frontier artificial
intelligence models. Among the first areas of geopolitical advantage granted by
such systems will be in automating military intelligence. Much discussion has
been devoted to AI systems enabling new military modalities, such as lethal
autonomous weapons, or making strategic decisions. However, the ability of a
country of "CIA analysts in a data-center" to synthesize diverse data at scale,
and its implications, have been underexplored. Multimodal foundation models
appear on track to automate strategic analysis previously done by humans. They
will be able to fuse today's abundant satellite imagery, phone-location traces,
social media records, and written documents into a single queryable system. We
conduct a preliminary uplift study to empirically evaluate these capabilities,
then propose a taxonomy of the kinds of ground truth questions these systems
will answer, present a high-level model of the determinants of this system's AI
capabilities, and provide recommendations for nation-states to remain
strategically competitive within the new paradigm of automated intelligence.

</details>


### [386] [MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](https://arxiv.org/abs/2509.17116)
*Hang Xu,Zang Yu,Yehui Tang,Pengbo Hu,Yuhao Tang,Hao Dong*

Main category: cs.AI

TL;DR: MCTS-EP是一个结合大型语言模型和蒙特卡洛树搜索的在线学习框架，用于训练具身智能体，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效整合LLM推理能力和MCTS探索策略的框架，以提升具身智能体在复杂环境中的学习效率和性能。

Method: 结合MCTS引导的偏好数据收集、高效多模态推理机制和基于偏好优化的迭代训练流程，理论证明在强凸损失函数下优于传统策略算法。

Result: 在ALFWorld文本和视觉任务中分别达到92%和87%的成功率，WebShop中平均奖励0.81，视觉ALFWorld中平均交互步数从18.7/19.5减少到10.2/9.9步。

Conclusion: MCTS-EP框架有效提升了具身智能体的学习性能，可作为搜索增强的GAIL变体，在多个基准测试中表现出色。

Abstract: This paper introduces MCTS-EP, an online learning framework that combines
large language models (LLM) with Monte Carlo Tree Search (MCTS) for training
embodied agents. MCTS-EP integrates three key components: MCTS-guided
exploration for preference data collection, efficient multi-modal reasoning
mechanism, and iterative training pipeline based on preference optimization. We
theoretically prove that MCTS-EP achieves better performance bounds than
conventional on-policy algorithms when the loss function is strongly convex,
and demonstrate that it can be formulated as a search-enhanced variant of GAIL.
MCTS-EP achieves state-of-the-art performace across serval benchmarks. In
ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.
In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average
interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code
available at: https://github.com/xuhang-2/Embodied-Agent-Planning

</details>


### [387] [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158)
*Pierre Andrews,Amine Benhalloum,Gerard Moreno-Torres Bertran,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Romain Froger,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Grégoire Mialon,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Thomas Scialom,Vladislav Vorotilov,Mengjue Wang,Ian Yu*

Main category: cs.AI

TL;DR: 介绍了Meta Agents Research Environments (ARE)研究平台和Gaia2基准测试，用于创建复杂环境、评估智能体能力，并发现现有系统在推理能力与效率之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 弥合模型开发与实际部署之间的差距，提供可扩展的环境创建和智能体编排平台，推动AI前沿能力发展。

Method: 开发ARE平台提供简单抽象来构建复杂多样的环境，每个环境有自己的规则、工具、内容和验证器；在ARE上构建Gaia2基准测试，要求智能体处理模糊性和噪声、适应动态环境、与其他智能体协作并在时间约束下操作。

Result: 实验表明没有系统能在所有智能维度上表现最优：更强的推理能力往往以效率为代价，预算扩展曲线趋于平稳，揭示了需要新架构和自适应计算策略。

Conclusion: ARE抽象使Gaia2能够持续扩展到其他环境，赋能社区快速创建针对特定领域的新基准测试，在AI发展的第二阶段，进步越来越依赖于定义有意义的任务和稳健的评估。

Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for
scalable creation of environments, integration of synthetic or real
applications, and execution of agentic orchestrations. ARE provides simple
abstractions to build complex and diverse environments, each with their own
rules, tools, content, and verifiers, helping to bridge the gap between model
development and real-world deployment. We also propose Gaia2, a benchmark built
in ARE and designed to measure general agent capabilities. Beyond search and
execution, Gaia2 requires agents to handle ambiguities and noise, adapt to
dynamic environments, collaborate with other agents, and operate under temporal
constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new
failure modes that are invisible in static settings. Our experiments show that
no system dominates across the intelligence spectrum: stronger reasoning often
comes at the cost of efficiency, and budget scaling curves plateau,
highlighting the need for new architectures and adaptive compute strategies.
Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2
to other environments, empowering the community to rapidly create new
benchmarks tailored to their domains. In AI's second half, progress
increasingly depends on defining meaningful tasks and robust evaluations to
drive frontier capabilities forward.

</details>


### [388] [Shall We Play a Game? Language Models for Open-ended Wargames](https://arxiv.org/abs/2509.17192)
*Glenn Matlin,Parv Mahajan,Isaac Song,Yixiong Hao,Ryan Bard,Stu Topp,Evan Montoya,M. Rehan Parwani,Soham Shetty,Mark Riedl*

Main category: cs.AI

TL;DR: 本文通过文献综述构建了兵棋推演的分类体系，重点分析了语言模型在开放式兵棋推演中的应用，提出了部署指南和安全考量。


<details>
  <summary>Details</summary>
Motivation: 兵棋推演是研究决策战略影响的重要工具，语言模型在开放式兵棋推演中的应用潜力值得系统研究。

Method: 对100篇AI在兵棋推演中的文献进行范围综述，构建基于玩家和裁判创造力的兵棋推演本体论。

Result: 建立了兵棋推演分类框架，提炼了语言模型在不同应用场景中的使用时机和方法，提出了安全考量和最佳实践。

Conclusion: 语言模型在开放式兵棋推演中具有重要应用价值，但仍需解决开放性研究挑战，特别是安全部署问题。

Abstract: Wargames are multi-faceted, multi-player depictions of conflict in which
participants' decisions influence future events. Wargames are often used to
explore the strategic implications of decision-making. However, it also
encompasses entertainment-oriented simulations, ranging from _Chess_ to
tabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more
open-ended side of the spectrum of wargames, players use natural language to
convey their moves, and adjudicators propose outcomes. Language Models (LMs)
are increasingly being considered for how they can provide insights into
real-world, consequential decisions. We conduct a scoping literature review of
a curated selection of 100 recent works on AI in wargames, from which we
construct an ontology of wargames in terms of the creativity afforded to either
the players or adjudicators. Focusing on the space of wargames with the most
open-endedness for players and adjudicators, we distill a set of considerations
for when and how to use LMs in different application areas. We also present a
set of safety considerations, best practices for deploying LMs in open-ended
wargames, and conclude with a set of high-impact open research challenges.

</details>


### [389] [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)
*Soheil Zibakhsh,Mohammad Samragh,Kumari Nishu,Lauren Hannah,Arnav Kundu,Minsik Cho*

Main category: cs.AI

TL;DR: 本文提出了超并行缩放框架，通过token级别的多专家输出聚合来提升MoE模型的预测质量，无需训练即可实现性能提升和计算效率优化。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型主要通过序列级缩放方法（如思维链）提升生成质量，但token级别的预测质量仍有改进空间。MoE模型具有专家多样性优势，但传统路由机制未能充分利用这一潜力。

Method: 提出了专家名册（RoE）方法，在MoE模型中注入受控随机性到专家路由机制，为每个token采样多个不同专家并聚合其输出。同时设计了高效的批处理策略和专用KV缓存机制来降低计算开销。

Result: 实验表明，RoE方法使7B参数的MoE模型达到了10.5B参数模型的性能，同时推理计算量减少了30%，且无需任何模型参数微调。

Conclusion: 超并行缩放框架为MoE模型提供了有效的token级质量提升方案，通过动态集成多个专家输出实现了性能与效率的双重优化。

Abstract: The generation quality of large language models (LLMs) is often improved by
utilizing inference-time sequence-level scaling methods (e.g.,
Chain-of-Thought). We introduce hyper-parallel scaling, a complementary
framework that improves prediction quality at the token level. Hyper-parallel
scaling computes and aggregates multiple output proposals for a single token
from the model. We implement this concept in Mixture-of-Experts (MoE) models,
which we refer to as Roster of Experts (RoE). RoE is a training-free inference
algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects
controlled stochasticity into the expert routing mechanism, enabling it to
sample multiple diverse experts for each token and aggregate their outputs for
a more accurate final prediction.To overcome the computational cost, we
introduce an efficient batching strategy and a specialized KV-caching mechanism
that minimizes compute and memory overhead. For example, RoE enables a 7B MoE
model to match the performance of a 10.5B MoE model while using 30% less
compute for inference. These gains are achieved without any fine-tuning of
model parameters.

</details>


### [390] [Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System](https://arxiv.org/abs/2509.17240)
*Abdullah Mushtaq,Muhammad Rafay Naeem,Ibrahim Ghaznavi,Alaa Abd-alrazaq,Aliya Tabassum,Junaid Qadir*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM和多代理系统的SLR评估助手，用于自动化系统文献综述的质量评估，在五个已发表SLR的初步研究中与专家评分达到84%的一致性。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述(SLR)是循证研究的基础，但传统方法劳动密集且在不同学科间存在不一致性，需要更高效和标准化的评估工具。

Method: 采用多代理系统(MAS)架构，基于PRISMA指南设计专门代理，自动化执行协议验证、方法学评估和主题相关性检查。

Result: 在五个不同领域的已发表SLR上进行初步研究，系统输出与专家标注的PRISMA评分达成84%的一致性。

Conclusion: 早期结果有前景，这是迈向可扩展、准确的NLP驱动系统的第一步，展示了其在跨学科工作流中进行严格、领域无关知识聚合的能力。

Abstract: Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.

</details>


### [391] [Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B](https://arxiv.org/abs/2509.17259)
*Ilham Wicaksono,Zekun Wu,Rahul Patel,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.AI

TL;DR: 本文通过比较性红队分析发现，AI代理系统存在独特的仅代理层漏洞，这些漏洞在独立模型层面不存在，且工具调用环境的脆弱性比非工具环境高24%。


<details>
  <summary>Details</summary>
Motivation: 随着行业越来越多地采用AI代理系统，理解其独特脆弱性变得至关重要。现有研究表明模型层面的安全漏洞不能完全捕捉代理部署中的风险。

Method: 使用AgentSeer可观测性框架将代理系统解构为细粒度动作和组件，在GPT-OSS-20B模型上应用HarmBench有害目标的迭代红队攻击，比较独立模型和代理循环中的模型表现。

Result: 发现代理层迭代攻击成功攻破了在模型层面完全失败的目标，工具调用环境脆弱性比非工具环境高24%。同时某些模型特定攻击在代理环境中失效。

Conclusion: AI代理系统存在独特的仅代理层漏洞，独立模型漏洞不能完全推广到部署系统中，需要专门的代理安全评估方法。

Abstract: As the industry increasingly adopts agentic AI systems, understanding their
unique vulnerabilities becomes critical. Prior research suggests that security
flaws at the model level do not fully capture the risks present in agentic
deployments, where models interact with tools and external environments. This
paper investigates this gap by conducting a comparative red teaming analysis of
GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability
framework AgentSeer to deconstruct agentic systems into granular actions and
components, we apply iterative red teaming attacks with harmful objectives from
HarmBench at two distinct levels: the standalone model and the model operating
within an agentic loop. Our evaluation reveals fundamental differences between
model level and agentic level vulnerability profiles. Critically, we discover
the existence of agentic-only vulnerabilities, attack vectors that emerge
exclusively within agentic execution contexts while remaining inert against
standalone models. Agentic level iterative attacks successfully compromise
objectives that completely failed at the model level, with tool-calling
contexts showing 24\% higher vulnerability than non-tool contexts. Conversely,
certain model-specific exploits work exclusively at the model level and fail
when transferred to agentic contexts, demonstrating that standalone model
vulnerabilities do not always generalize to deployed systems.

</details>


### [392] [CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2509.17318)
*Zhuofan Chen,Jiyuan He,Yichi Zhang,Xing Hu,Haoxing Wen,Jun Bai,Wenge Rong*

Main category: cs.AI

TL;DR: CogAtom是一个基于认知原子的框架，用于合成数学严谨且认知多样的问题，通过选择和重组从人类解决方案中提取的基本推理单元来解决奥林匹克级数学问题稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 数学推理对大型语言模型具有挑战性，需要多步推理和抽象概念整合。现有测试时扩展技术严重依赖高质量、具有挑战性的问题，但奥林匹克级数学问题的稀缺性成为瓶颈。

Method: CogAtom将问题构建建模为选择和重组从人类解决方案中提取的基本推理单元（认知原子）的过程。采用多样性促进的随机游走算法探索认知原子空间，同时基于约束的重组机制确保逻辑严密性和结构有效性。

Result: 实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题在难度上与AIME（美国数学邀请赛）相匹配，同时在结构变化上超过AIME。

Conclusion: CogAtom为可扩展、高质量的数学问题生成提供了一条基于认知的途径，通过控制认知原子的数量可以精确调整问题难度，确保生成问题的多样性、可扩展性和可控性。

Abstract: Mathematical reasoning poses significant challenges for Large Language Models
(LLMs) due to its demand for multi-step reasoning and abstract conceptual
integration. While recent test-time scaling techniques rely heavily on
high-quality, challenging problems, the scarcity of Olympiad-level math
problems remains a bottleneck. We introduce CogAtom, a novel cognitive
atom-based framework for synthesizing mathematically rigorous and cognitively
diverse problems. Unlike prior approaches, CogAtom models problem construction
as a process of selecting and recombining fundamental reasoning units,
cognitive atoms, extracted from human-authored solutions. A diversity-promoting
random walk algorithm enables exploration of the cognitive atom space, while a
constraint-based recombination mechanism ensures logical soundness and
structural validity. The combinatorial nature of the graph structure provides a
near-infinite space of reasoning paths, and the walk algorithm systematically
explores this space to achieve large-scale synthesis of high-quality problems;
meanwhile, by controlling the number of cognitive atoms, we can precisely
adjust problem difficulty, ensuring diversity, scalability, and controllability
of the generated problems. Experimental results demonstrate that CogAtom
outperforms existing methods in accuracy, reasoning depth, and diversity,
generating problems that closely match the difficulty of AIME while exceeding
it in structural variation. Our work offers a cognitively grounded pathway
toward scalable, high-quality math problem generation.Our code is publicly
available at https://github.com/Icarus-1111/CogAtom.

</details>


### [393] [LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code](https://arxiv.org/abs/2509.17337)
*Ala Jararweh,Michael Adams,Avinash Sahu,Abdullah Mueen,Afsah Anwar*

Main category: cs.AI

TL;DR: LLaVul是一个多模态大语言模型，专门用于通过问答方式对代码进行细粒度推理，提升代码漏洞分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统日益复杂，需要更好的推理工具来发现源代码中的漏洞。现有方法大多将漏洞分析简化为分类任务，忽略了现实场景的细微差别和上下文依赖性。虽然现有的代码大语言模型在代码理解方面表现出色，但很少关注安全特定的推理。

Method: 提出LLaVul模型，训练时将配对的代码和自然语言查询整合到统一空间中，增强对代码漏洞的推理和上下文相关洞察。构建了包含真实世界漏洞的安全焦点问答数据集进行评估。

Result: LLaVul在问答和检测任务中优于最先进的通用和代码大语言模型。通过定性分析进一步解释决策过程，突出能力和局限性。

Conclusion: 通过整合代码和问答，LLaVul实现了更可解释和以安全为中心的代码理解。

Abstract: Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.

</details>


### [394] [Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation](https://arxiv.org/abs/2509.17353)
*Ahmed T. Elboardy,Ghada Khoriba,Essam A. Rashed*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体强化学习框架，用于放射学报告生成的基准测试和评估，整合了LLM和LVM，通过十个专业智能体进行图像分析、特征提取、报告生成和评估。


<details>
  <summary>Details</summary>
Motivation: 解决放射学报告生成的两个挑战：构建临床可靠系统和设计严格评估协议，建立可信赖的放射学报告生成基准。

Method: 采用多智能体强化学习框架，整合大型语言模型和大型视觉模型，构建包含十个专业智能体的模块化架构，涵盖图像分析、特征提取、报告生成、审查和评估等环节。

Result: 在公共放射学数据集上使用ChatGPT-4o实现，LLM与医学放射科医生反馈共同作为评估者，实现了在智能体级别和共识级别的细粒度评估。

Conclusion: 该框架通过将评估协议与LLM开发生命周期对齐，为可信赖的基于偏差的放射学报告生成建立了路径。

Abstract: Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.

</details>


### [395] [Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification](https://arxiv.org/abs/2509.17354)
*Jiazhao Shi,Yichen Lin,Yiheng Hua,Ziyu Wang,Zijian Zhang,Wenjia Zheng,Yun Song,Kuan Lu,Shoufeng Lu*

Main category: cs.AI

TL;DR: 本文提出了一种物理信息AI框架，通过整合车辆运动学、交互可行性和交通安全指标来改进车道变换意图预测，在高速公路和复杂匝道场景中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 车道变换操作是高速公路事故的主要原因，现有方法存在二元分类限制、场景多样性不足以及在较长预测时间范围内性能下降的问题。

Method: 提出物理信息AI框架，整合车辆运动学、交互可行性和交通安全指标，将车道变换预测建模为三类问题（左变换、右变换、不变换），使用LightGBM等机器学习模型。

Result: 在1秒预测时间范围内，highD数据集上达到99.8%准确率和93.6%宏F1分数，exiD数据集上达到96.1%准确率和88.7%宏F1分数，优于两层堆叠LSTM基线。

Conclusion: 物理信息和特征丰富的机器学习框架在自动驾驶系统的实时车道变换意图预测中具有实际优势。

Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring
the need for accurate intention prediction to improve the safety and
decision-making of autonomous driving systems. While prior studies using
machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)
have shown promise, most approaches remain limited by binary classification,
lack of scenario diversity, and degraded performance under longer prediction
horizons. In this study, we propose a physics-informed AI framework that
explicitly integrates vehicle kinematics, interaction feasibility, and
traffic-safety metrics (e.g., distance headway, time headway,
time-to-collision, closing gap time) into the learning process. lane-change
prediction is formulated as a three-class problem that distinguishes left
change, right change, and no change, and is evaluated across both straight
highway segments (highD) and complex ramp scenarios (exiD). By integrating
vehicle kinematics with interaction features, our machine learning models,
particularly LightGBM, achieve state-of-the-art accuracy and strong
generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,
and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,
outperforming a two-layer stacked LSTM baseline. These findings demonstrate the
practical advantages of a physics-informed and feature-rich machine learning
framework for real-time lane-change intention prediction in autonomous driving
systems.

</details>


### [396] [Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process](https://arxiv.org/abs/2509.17380)
*Zhizhang FU,Guangsheng Bao,Hongbo Zhang,Chenkai Hu,Yue Zhang*

Main category: cs.AI

TL;DR: 该研究对LLMs和LRMs进行系统性因果分析，发现RLVR训练的LRMs展现出更强的因果推理能力，而LLMs和蒸馏LRMs未能解决因果相关缺陷。


<details>
  <summary>Details</summary>
Motivation: LLMs存在推理不忠实、偏见和不一致等关键问题，因为它们缺乏稳健的因果基础，可能依赖表面相关性而非真正理解。虽然LRMs通过强化学习和蒸馏等技术提高了任务准确性，但这些训练方法对因果关系的影响尚未充分探索。

Method: 研究使用结构因果模型(SCMs)分析四个关键变量：问题指令(Z)、思考过程(T)、推理步骤(X)和答案(Y)，对LLMs和LRMs进行系统性因果分析，并深入调查RLVR训练过程的动态变化。

Result: RLVR训练的LRMs表现出增强的因果推理能力，更接近理想因果结构，减少了虚假相关性并加强了真实因果模式，从而缓解了不忠实性和偏见问题。RLVR训练过程中观察到减少的虚假特征与改进的因果结构高度相关。

Conclusion: 该研究增进了对推理模型中因果关系的理解，强调了RLVR在增强因果推理中的关键作用，为设计具有更强因果基础的未来AI系统提供了见解。

Abstract: LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and
inconsistency, since they lack robust causal underpinnings and may rely on
superficial correlations rather than genuine understanding. Successive LRMs
have emerged as a promising alternative, leveraging advanced training
techniques such as reinforcement learning (RL) and distillation to improve task
accuracy. However, the impact of these training methods on causality remains
largely unexplored. In this study, we conduct a systematic causal analysis on
LLMs and LRMs, examining structural causal models (SCMs) of four key variables:
problem instruction (Z), thinking process (T), reasoning steps (X), and answer
(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal
reasoning capabilities, aligning more closely with ideal causal structures,
while LLMs and distilled LRMs fail to address causality-related deficiencies.
Our further investigation indicates that RLVR reduces spurious correlations and
strengthens genuine causal patterns, thereby mitigating unfaithfulness and
bias. In addition, our inspection on the dynamics of the RLVR training process
observes a high correlation between reduced spurious features and improved
causal structures, where the causal relationships consistently improve in the
training process. This study contributes to the understanding of causality in
reasoning models, highlights the critical role of RLVR in enhancing causal
reasoning, and provides insights for designing future AI systems with stronger
causal foundations. We release our code and data at
https://github.com/Harryking1999/CoT_Causal_Analysis.

</details>


### [397] [Program Synthesis via Test-Time Transduction](https://arxiv.org/abs/2509.17393)
*Kang-il Lee,Jahyun Koo,Seunghyun Yoon,Minbeom Kim,Hyukhun Koh,Dongryeol Lee,Kyomin Jung*

Main category: cs.AI

TL;DR: 提出了一种新的转导式程序合成方法，通过在合成过程中显式利用测试输入来提高鲁棒性，解决了传统方法在训练样本有限且测试输入包含边缘情况时的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统程序合成方法（基于自然语言描述或输入输出示例）通常从训练示例中泛化，但在实际应用中，当训练样本有限且测试输入涉及各种边缘情况时，往往缺乏鲁棒性。

Method: 提出了一种新颖框架，将合成视为在由程序输出定义的有限假设类上进行主动学习。使用LLM预测选定测试输入的输出，并通过贪婪最大化算法选择输入以最小化LLM查询次数，从而消除不一致的假设。

Result: 在四个基准测试（Playgol、MBPP+、1D-ARC和MiniGrid上的程序化世界建模）上评估，证明该方法在准确性和效率方面显著提高了程序合成性能。

Conclusion: 转导式程序合成方法通过主动利用测试输入，有效提升了程序合成的鲁棒性和性能，为实际应用中的程序合成问题提供了更可靠的解决方案。

Abstract: We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC,
and programmatic world modeling on MiniGrid. We demonstrate that our method
significantly improves program synthesis in both accuracy and efficiency. We
release our code at https://github.com/klee972/SYNTRA.

</details>


### [398] [Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments](https://arxiv.org/abs/2509.17425)
*Zhenliang Zhang,Yuxi Wang,Hongzhao Xie,Shiyun Zhao,Mingyuan Liu,Yujie Lu,Xinyi He,Zhenku Cheng,Yujia Peng*

Main category: cs.AI

TL;DR: 该研究评估了多模态大语言模型在复合任务上的表现，发现当前模型在物体理解、空间智能和社交活动三个核心领域表现不佳，与通用智能要求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 人工通用智能与传统AI的关键区别在于能否执行需要多种能力的复合任务。虽然基于多模态大语言模型的具身智能体具有丰富的感知和交互能力，但其解决复合任务的能力尚未得到充分探索。

Method: 设计了一套基于幼儿日常活动的复合任务，在动态模拟家庭环境中评估了17个领先的专有和开源多模态大语言模型，任务涵盖物体理解、空间智能和社交活动三个核心领域。

Result: 所有模型在三个领域都表现不佳，表明当前能力与通用智能要求之间存在显著差距。

Conclusion: 这些任务为评估具身智能体的通用能力提供了一个初步框架，标志着向具身多模态大语言模型开发及其实际部署迈出了早期但重要的一步。

Abstract: A key feature differentiating artificial general intelligence (AGI) from
traditional AI is that AGI can perform composite tasks that require a wide
range of capabilities. Although embodied agents powered by multimodal large
language models (MLLMs) offer rich perceptual and interactive capabilities, it
remains largely unexplored whether they can solve composite tasks. In the
current work, we designed a set of composite tasks inspired by common daily
activities observed in early childhood development. Within a dynamic and
simulated home environment, these tasks span three core domains: object
understanding, spatial intelligence, and social activity. We evaluated 17
leading proprietary and open-source MLLMs on these tasks. The results
consistently showed poor performance across all three domains, indicating a
substantial gap between current capabilities and general intelligence
requirements. Together, our tasks offer a preliminary framework for evaluating
the general capabilities of embodied agents, marking an early but significant
step toward the development of embodied MLLMs and their real-world deployment.

</details>


### [399] [SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding](https://arxiv.org/abs/2509.17439)
*Yangxuan Zhou,Sha Zhao,Jiquan Wang,Haiteng Jiang,Shijian Li,Tao Li,Gang Pan*

Main category: cs.AI

TL;DR: SPICED是一个受大脑突触稳态启发的神经形态框架，用于无监督连续EEG解码，通过动态扩展的突触网络和三种生物启发机制实现动态稳定性-可塑性平衡，有效处理新个体出现的连续学习场景。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑通过突触稳态实现动态稳定性-可塑性平衡的生物原理启发，解决实际场景中不断出现具有个体间变异性的新个体的连续EEG解码问题。

Method: SPICED框架包含新颖的突触网络，通过三种生物启发机制实现连续适应：关键记忆重新激活、突触巩固和突触重归一化，这些机制在突触稳态中相互作用，动态增强任务判别性记忆痕迹并削弱有害记忆。

Result: 在三个EEG数据集上的验证表明，SPICED通过优先重放与新出现个体强关联的任务判别性记忆痕迹实现鲁棒适应，同时在长期连续学习中通过抑制有害记忆的重放优先级有效缓解灾难性遗忘。

Conclusion: SPICED成功地将突触稳态机制整合到连续学习系统中，为处理个体间变异性的连续EEG解码提供了有效的神经形态解决方案。

Abstract: Human brain achieves dynamic stability-plasticity balance through synaptic
homeostasis. Inspired by this biological principle, we propose SPICED: a
neuromorphic framework that integrates the synaptic homeostasis mechanism for
unsupervised continual EEG decoding, particularly addressing practical
scenarios where new individuals with inter-individual variability emerge
continually. SPICED comprises a novel synaptic network that enables dynamic
expansion during continual adaptation through three bio-inspired neural
mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and
(3) synaptic renormalization. The interplay within synaptic homeostasis
dynamically strengthens task-discriminative memory traces and weakens
detrimental memories. By integrating these mechanisms with continual learning
system, SPICED preferentially replays task-discriminative memory traces that
exhibit strong associations with newly emerging individuals, thereby achieving
robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic
forgetting by suppressing the replay prioritization of detrimental memories
during long-term continual learning. Validated on three EEG datasets, SPICED
show its effectiveness.

</details>


### [400] [AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks](https://arxiv.org/abs/2509.17460)
*Jianlong Chang,Haixin Wang,Zhiyuan Dang,Li Huang,Zhiyu Wang,Ruoqi Cao,Shihao Piao,Dongzhe Li,Dianyu Gao,Dongsheng Wang,Yin Li,Jinan Sun,Lu Fang,Zhouchen Lin*

Main category: cs.AI

TL;DR: Pangaea是一个AI超级大陆模型，通过统一数据格式和跨296个数据集的多模态预训练，将孤立的智能岛屿整合，在45个通用任务和15个科学任务上展现出卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型局限于特定任务，形成智能孤岛，阻碍了人工通用智能的发展。需要构建能够跨任务泛化的统一模型。

Method: 提出Pangaea模型，将任何数据编码为统一格式，在296个跨模态数据集上进行预训练，积累通用知识。

Result: 在45个通用任务和15个科学任务上表现出显著泛化能力，揭示了模态扩展效应，量化了跨模态知识积累的几何分布累积函数。

Conclusion: Pangaea展示了处理多种任务的强大潜力，为人工通用智能提供了新的发展方向。

Abstract: The pursuit of artificial general intelligence continuously demands
generalization in one model across myriad tasks, even those not seen before.
However, current AI models are isolated from each other for being limited to
specific tasks, now first defined as Intelligence Islands. To unify
Intelligence Islands into one, we propose Pangaea, the first AI supercontinent
akin to the geological Pangaea. Pangaea encodes any data into a unified format
and accumulates universal knowledge through pre-training on 296 datasets across
diverse modalities. Eventually, it demonstrates remarkable generalization
across 45 general tasks and 15 scientific tasks encompassing a wide range of
scientific subjects. By investigating Pangaea deeper, the scaling effect of
modality is revealed, quantifying the universal knowledge accumulation across
modalities as the cumulative distribution function of a geometric distribution.
On the whole, Pangaea shows strong potential to handle myriad tasks, indicating
a new direction toward artificial general intelligence.

</details>


### [401] [A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data](https://arxiv.org/abs/2509.17544)
*Juan Cañada,Raúl Alonso,Julio Molleda,Fidel Díez*

Main category: cs.AI

TL;DR: 该研究开发了一个开源对话助手，通过整合多模态检索和大型语言模型，让非专业用户能够用自然语言与农业和地理空间数据交互。


<details>
  <summary>Details</summary>
Motivation: 开放地球观测和农业数据集具有支持可持续土地管理的潜力，但高技术门槛限制了非专业用户的使用。

Method: 提出了一种结合正射影像、Sentinel-2植被指数和用户提供文档的架构，通过检索增强生成技术，让系统能灵活决定依赖多模态证据、文本知识或两者结合来回答问题。

Result: 初步结果显示系统能够生成清晰、相关且具有上下文感知的农业查询响应，同时在多个地理区域保持可重现性和可扩展性。

Conclusion: 主要贡献包括融合多模态地球观测和文本知识源的架构，展示了通过自然语言交互降低获取专业农业信息门槛的方法，以及开放可重现的设计。

Abstract: The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.

</details>


### [402] [Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem](https://arxiv.org/abs/2509.17550)
*Neslihan Kose,Anthony Rhodes,Umur Aybars Ciftci,Ilke Demir*

Main category: cs.AI

TL;DR: 本文首次对深度伪造检测器进行全面的不确定性分析，系统研究生成伪影如何影响预测置信度，并利用不确定性进行深度伪造源检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型质量提升，深度伪造导致在线信任危机。检测器的误用会加剧错误信息问题，因此需要研究检测器的不确定性。

Method: 使用贝叶斯神经网络和蒙特卡洛dropout量化不同检测器架构的偶然性和认知不确定性，通过不确定性图谱定位像素级预测置信度。

Result: 在包含9个生成器的两个数据集上评估不确定性，发现不确定性流形包含足够一致信息用于深度伪造源检测，不确定性图谱揭示了与生成器特定伪影相关的独特模式。

Conclusion: 不确定性量化是可信合成媒体检测的基本要求，该分析为部署可靠的深度伪造检测系统提供了关键见解。

Abstract: As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.

</details>


### [403] [MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances](https://arxiv.org/abs/2509.17553)
*Congcong Ge,Yachuan Liu,Yixuan Tang,Yifan Zhu,Yaofeng Tu,Yunjun Gao*

Main category: cs.AI

TL;DR: MontePrep是一个端到端的自动数据准备框架，通过零目标实例要求和免训练的方式，使用LLM驱动的树结构搜索来合成数据准备流水线。


<details>
  <summary>Details</summary>
Motivation: 商业系统中需要将不同来源的关系数据转换为标准化模式，但现有方法依赖劳动密集型监督信号或目标表数据访问权限，限制了实际应用。

Method: MontePrep包含三个核心组件：数据准备动作沙箱（DPAS）引导搜索、基础流水线生成器（FPG）通过LLM驱动的蒙特卡洛树搜索构建流水线、执行感知流水线优化器（EPO）通过执行结果评估流水线可靠性。

Result: 大量实验结果表明，MontePrep相比五个最先进的竞争对手有显著改进。

Conclusion: MontePrep提供了一个有效的免训练数据准备解决方案，无需目标实例即可生成可靠的数据准备流水线。

Abstract: In commercial systems, a pervasive requirement for automatic data preparation
(ADP) is to transfer relational data from disparate sources to targets with
standardized schema specifications. Previous methods rely on labor-intensive
supervision signals or target table data access permissions, limiting their
usage in real-world scenarios. To tackle these challenges, we propose an
effective end-to-end ADP framework MontePrep, which enables training-free
pipeline synthesis with zero target-instance requirements. MontePrep is
formulated as an open-source large language model (LLM) powered tree-structured
search problem. It consists of three pivot components, i.e., a data preparation
action sandbox (DPAS), a fundamental pipeline generator (FPG), and an
execution-aware pipeline optimizer (EPO). We first introduce DPAS, a
lightweight action sandbox, to navigate the search-based pipeline generation.
The design of DPAS circumvents exploration of infeasible pipelines. Then, we
present FPG to build executable DP pipelines incrementally, which explores the
predefined action sandbox by the LLM-powered Monte Carlo Tree Search.
Furthermore, we propose EPO, which invokes pipeline execution results from
sources to targets to evaluate the reliability of the generated pipelines in
FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the
search process from both efficiency and effectiveness perspectives. Extensive
experimental results demonstrate the superiority of MontePrep with significant
improvement against five state-of-the-art competitors.

</details>


### [404] [LIMI: Less is More for Agency](https://arxiv.org/abs/2509.17567)
*Yang Xiao,Mohan Jiang,Jie Sun,Keyu Li,Jifan Lin,Yumin Zhuang,Ji Zeng,Shijie Xia,Qishuo Hua,Xuefeng Li,Xiaojie Cai,Tongyu Wang,Yue Zhang,Liming Liu,Xia Wu,Jinlong Hou,Yuan Cheng,Wenjie Li,Xiang Wang,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: LIMI挑战了传统AI代理开发范式，证明通过仅78个精心设计的训练样本就能实现73.5%的代理基准性能，比使用1万个样本的模型性能提升53.7%，确立了"代理效率原则"。


<details>
  <summary>Details</summary>
Motivation: 行业迫切需要能够自主执行任务、操作工具并驱动实际成果的AI系统，而不仅仅是进行推理和生成响应。当前方法错误地认为更多数据就能产生更好的代理能力。

Method: LIMI采用"少即是多"策略，通过战略性地关注协作软件开发和科学研究工作流程，仅使用78个精心策划的自主行为演示样本来训练AI代理。

Result: LIMI在综合代理基准测试中达到73.5%的性能，显著优于最先进模型（Kimi-K2-Instruct 24.1%、DeepSeek-V3.1 11.9%等），且比使用1万个样本训练的模型性能提升53.7%。

Conclusion: 研究确立了代理效率原则：机器自主性不是来自数据丰富性，而是来自高质量代理演示的战略性策划。这标志着AI代理时代的到来，代理智能成为区分认知系统与生产性工作者的关键特征。

Abstract: We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.

</details>


### [405] [Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models](https://arxiv.org/abs/2509.17589)
*Jun Ling,Yao Qi,Tao Huang,Shibo Zhou,Yanqin Huang,Jiang Yang,Ziqi Song,Ying Zhou,Yang Yang,Heng Tao Shen,Peng Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化多模态大语言模型的表格图像到LaTeX代码生成方法，通过结构级和视觉保真度双重奖励优化，在复杂表格生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理具有大尺寸、深层嵌套结构和语义丰富内容的复杂表格时经常失败，需要开发更鲁棒的表格图像到LaTeX转换方法。

Method: 使用预训练多模态大语言模型在大规模表格数据集上微调，并引入基于GRPO的双重奖励强化学习策略，结合LaTeX代码结构奖励和渲染输出视觉保真度奖励。

Result: 在结合TEDS-Structure和CW-SSIM的混合评估协议下，该方法在结构复杂表格上实现了最先进的性能表现。

Conclusion: 所提出的强化多模态框架通过直接优化视觉输出质量，在处理复杂表格时展现出有效性和鲁棒性。

Abstract: In this work, we address the task of table image to LaTeX code generation,
with the goal of automating the reconstruction of high-quality,
publication-ready tables from visual inputs. A central challenge of this task
lies in accurately handling complex tables -- those with large sizes, deeply
nested structures, and semantically rich or irregular cell content -- where
existing methods often fail. We begin with a comprehensive analysis,
identifying key challenges and highlighting the limitations of current
evaluation protocols. To overcome these issues, we propose a reinforced
multimodal large language model (MLLM) framework, where a pre-trained MLLM is
fine-tuned on a large-scale table-to-LaTeX dataset. To further improve
generation quality, we introduce a dual-reward reinforcement learning strategy
based on Group Relative Policy Optimization (GRPO). Unlike standard approaches
that optimize purely over text outputs, our method incorporates both a
structure-level reward on LaTeX code and a visual fidelity reward computed from
rendered outputs, enabling direct optimization of the visual output quality. We
adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and
show that our method achieves state-of-the-art performance, particularly on
structurally complex tables, demonstrating the effectiveness and robustness of
our approach.

</details>


### [406] [EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving](https://arxiv.org/abs/2509.17677)
*Xiyuan Zhou,Xinlei Wang,Yirui He,Yang Wu,Ruixi Zou,Yuheng Cheng,Yulu Xie,Wenxuan Liu,Huan Zhao,Yan Xu,Jinjin Gu,Junhua Zhao*

Main category: cs.AI

TL;DR: EngiBench是一个分层基准测试，用于评估大语言模型在解决工程问题上的能力，涵盖三个难度级别和多种工程子领域，揭示当前LLMs在真实工程推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉真实工程问题的复杂性（不确定性、上下文和开放场景），需要专门评估LLMs在工程领域的能力。

Method: 设计分层基准测试（基础知识检索、多步上下文推理、开放建模），并通过系统重写问题为三个变体（扰动、知识增强、数学抽象）来深入分析模型性能。

Result: 实验结果显示模型在不同难度级别上存在明显性能差距：任务越难表现越差，问题稍有变化性能下降，在高级工程任务上远落后于人类专家。

Conclusion: 当前LLMs仍缺乏真实世界工程所需的高级推理能力，未来需要开发具有更深层次和更可靠问题解决能力的模型。

Abstract: Large language models (LLMs) have shown strong performance on mathematical
reasoning under well-posed conditions. However, real-world engineering problems
require more than mathematical symbolic computation -- they need to deal with
uncertainty, context, and open-ended scenarios. Existing benchmarks fail to
capture these complexities. We introduce EngiBench, a hierarchical benchmark
designed to evaluate LLMs on solving engineering problems. It spans three
levels of increasing difficulty (foundational knowledge retrieval, multi-step
contextual reasoning, and open-ended modeling) and covers diverse engineering
subfields. To facilitate a deeper understanding of model performance, we
systematically rewrite each problem into three controlled variants (perturbed,
knowledge-enhanced, and math abstraction), enabling us to separately evaluate
the model's robustness, domain-specific knowledge, and mathematical reasoning
abilities. Experiment results reveal a clear performance gap across levels:
models struggle more as tasks get harder, perform worse when problems are
slightly changed, and fall far behind human experts on the high-level
engineering tasks. These findings reveal that current LLMs still lack the
high-level reasoning needed for real-world engineering, highlighting the need
for future models with deeper and more reliable problem-solving capabilities.
Our source code and data are available at
https://github.com/EngiBench/EngiBench.

</details>


### [407] [Virtual Arc Consistency for Linear Constraints in Cost Function Networks](https://arxiv.org/abs/2509.17706)
*Pierre Montalbano,Simon de Givry,George Katsirelos*

Main category: cs.AI

TL;DR: 本文提出了一种改进的软弧一致性算法来处理线性约束，相比原有算法能显著提高下界质量，在某些情况下减少求解时间。


<details>
  <summary>Details</summary>
Motivation: 在约束规划中，解决带有硬约束和软约束的离散最小化问题有三种方法：软全局约束、线性规划重构和局部成本函数重构。软全局约束方法虽然约束库丰富但下界较弱，线性规划方法下界强但重构规模大，因此研究第三种方法的改进。

Method: 改进现有的软弧一致性算法，使其能够处理线性约束作为局部成本函数，从而提高建模表达能力。

Result: 实验表明，改进后的算法在多个基准测试中相比原算法显著提高了下界质量，在某些情况下减少了求解时间。

Conclusion: 通过将线性约束整合到软弧一致性算法中，可以在保持中等质量下界的同时提高建模灵活性，为约束规划中的最小化问题提供更有效的解决方案。

Abstract: In Constraint Programming, solving discrete minimization problems with hard
and soft constraints can be done either using (i) soft global constraints, (ii)
a reformulation into a linear program, or (iii) a reformulation into local cost
functions. Approach (i) benefits from a vast catalog of constraints. Each soft
constraint propagator communicates with other soft constraints only through the
variable domains, resulting in weak lower bounds. Conversely, the approach (ii)
provides a global view with strong bounds, but the size of the reformulation
can be problematic. We focus on approach (iii) in which soft arc consistency
(SAC) algorithms produce bounds of intermediate quality. Recently, the
introduction of linear constraints as local cost functions increases their
modeling expressiveness. We adapt an existing SAC algorithm to handle linear
constraints. We show that our algorithm significantly improves the lower bounds
compared to the original algorithm on several benchmarks, reducing solving time
in some cases.

</details>


### [408] [DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation](https://arxiv.org/abs/2509.17711)
*Shenwei Kang,Xin Zhang,Wen Liu,Bin Li,Yujie Liu,Bo Gao*

Main category: cs.AI

TL;DR: DA-Mamba是一个用于对话场景中人类参与度估计的新型多模态架构，它用Mamba选择性状态空间处理替代注意力机制，实现线性时间和内存复杂度，同时在三个标准基准测试中超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 对话场景中的人类参与度估计对于自适应教学、远程医疗评估和社交感知人机交互等应用至关重要。参与度是通过面部表情、语音、手势和行为线索随时间传递的动态多模态信号。

Method: 设计了基于Mamba的对话感知选择性状态空间模型，包含三个核心模块：对话感知编码器、模态组融合和伙伴组融合。这些模块通过Mamba架构实现高效的跨模态推理，替代了计算密集的注意力机制。

Result: 在NoXi、NoXi-Add和MPIIGI三个标准基准测试中，DA-Mamba在一致性相关系数(CCC)上超越了现有最优方法，同时显著减少了训练时间和峰值内存使用，能够处理更长的序列并支持资源受限环境下的实时部署。

Conclusion: DA-Mamba通过Mamba架构实现了高效的多模态参与度估计，在保持表达力的同时大幅降低了计算复杂度，为实时多参与方对话场景的应用提供了可行的解决方案。

Abstract: Human engagement estimation in conversational scenarios is essential for
applications such as adaptive tutoring, remote healthcare assessment, and
socially aware human--computer interaction. Engagement is a dynamic, multimodal
signal conveyed by facial expressions, speech, gestures, and behavioral cues
over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal
architecture that replaces attention-heavy dialogue encoders with Mamba-based
selective state-space processing to achieve linear time and memory complexity
while retaining expressive cross-modal reasoning. We design a Mamba
dialogue-aware selective state-space model composed of three core modules: a
Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group
Fusion and Partner-Group Fusion, these modules achieve expressive dialogue
understanding. Extensive experiments on three standard benchmarks (NoXi,
NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art
(SOTA) methods in concordance correlation coefficient (CCC), while reducing
training time and peak memory; these gains enable processing much longer
sequences and facilitate real-time deployment in resource-constrained,
multi-party conversational settings. The source code will be available at:
https://github.com/kksssssss-ssda/MMEA.

</details>


### [409] [Efficient & Correct Predictive Equivalence for Decision Trees](https://arxiv.org/abs/2509.17774)
*Joao Marques-Silva,Alexey Ignatiev*

Main category: cs.AI

TL;DR: 本文分析了McTavish等人提出的MBDSR方法在决策树预测等价性判定中的局限性，证明该方法存在指数级复杂度问题且可能产生错误结果，并提出多项式时间算法替代方案。


<details>
  <summary>Details</summary>
Motivation: 决策树的Rashomon集合中存在大量预测等价的决策树，这会影响特征重要性分析的准确性。McTavish等人提出的MBDSR方法虽然试图解决预测等价性判定问题，但存在效率和正确性问题。

Method: 本文首先证明QM方法在最坏情况下具有指数级时间和空间复杂度，其次展示MBDSR方法在预测等价性判定中可能产生错误结果，最后提出多项式时间算法解决相关问题。

Result: 实验证实，对于触发QM方法最坏情况的决策树，本文提出的算法比McTavish等人的方法快数个数量级。

Conclusion: MBDSR方法存在根本性缺陷，而本文提出的多项式时间算法能够更高效、准确地解决决策树预测等价性判定及相关问题。

Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work
showed that DTs computing the same classification function, i.e. predictive
equivalent DTs, can represent a significant fraction of the Rashomon set. Such
redundancy is undesirable. For example, feature importance based on the
Rashomon set becomes inaccurate due the existence of predictive equivalent DTs,
i.e. DTs with the same prediction for every possible input. In recent work,
McTavish et al. proposed solutions for several computational problems related
with DTs, including that of deciding predictive equivalent DTs. This approach,
which this paper refers to as MBDSR, consists of applying the well-known method
of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal
form) representations of DTs, which are then used for comparing DTs for
predictive equivalence. Furthermore, the minimum-size DNF representation was
also applied to computing explanations for the predictions made by DTs, and to
finding predictions in the presence of missing data. However, the problem of
formula minimization is hard for the second level of the polynomial hierarchy,
and the QM method may exhibit worst-case exponential running time and space.
This paper first demonstrates that there exist decision trees that trigger the
worst-case exponential running time and space of the QM method. Second, the
paper shows that the MBDSR approach can produce incorrect results for the
problem of deciding predictive equivalence. Third, the paper shows that any of
the problems to which the minimum-size DNF representation has been applied to
can in fact be solved in polynomial time, in the size of the DT. The
experiments confirm that, for DTs for which the the worst-case of the QM method
is triggered, the algorithms proposed in this paper are orders of magnitude
faster than the ones proposed by McTavish et al.

</details>


### [410] [Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling](https://arxiv.org/abs/2509.17905)
*Zongqian Wu,Baoduo Xu,Tianyu Li,Zhu Sun,Xiaofeng Zhu,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出了TTS-Uniform框架来解决测试时扩展中的推理策略选择偏差问题，通过均匀分配采样预算和过滤不稳定策略来提升大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现测试时扩展（TTS）通过采样和聚合多样化推理路径可以提升大语言模型性能，但忽视了推理策略选择偏差问题，即模型倾向于使用某些策略而忽略其他有效策略，导致解决方案空间探索不足。

Method: 提出TTS-Uniform框架，包含三个步骤：(i)识别潜在推理策略；(ii)均匀分配采样预算给不同策略；(iii)在聚合前过滤不稳定策略。

Result: 实验结果表明，TTS-Uniform在多个主流大语言模型和基准数据集上显著提升了扩展效果。

Conclusion: TTS-Uniform通过解决推理策略选择偏差问题，有效提升了测试时扩展的性能，为改进大语言模型的推理能力提供了新思路。

Abstract: Test-time scaling (TTS) has been shown to improve the performance of large
language models (LLMs) by sampling and aggregating diverse reasoning paths.
However, existing research has overlooked a critical issue: selection bias of
reasoning strategies during scaling. Specifically, when generating reasoning
processes, LLMs tend to follow certain strategies (e.g., algebraic solutions
for math problems) while neglecting other valid alternatives (e.g., geometric
solutions), resulting in insufficient exploration of the solution space. To
further understand the impact of this bias, we present a theoretical analysis
that reveals when it undermines the effectiveness of test-time scaling.
Motivated by this theoretical insight, we introduce TTS-Uniform, a framework
designed to mitigate the selection bias of reasoning strategies. It (i)
identifies potential strategies, (ii) uniformly allocates the sampling budget
across them, and (iii) filters out unstable strategies prior to aggregation.
Experimental results show that TTS-Uniform significantly enhances scaling
effectiveness across multiple mainstream LLMs and benchmark datasets.

</details>


### [411] [MEF: A Systematic Evaluation Framework for Text-to-Image Models](https://arxiv.org/abs/2509.17907)
*Xiaojing Dong,Weilin Huang,Liang Li,Yiying Li,Shu Liu,Tongtong Ou,Shuang Ouyang,Yu Tian,Fengxuan Zhao*

Main category: cs.AI

TL;DR: 本文提出了Magic评估框架（MEF），一个系统实用的文本到图像（T2I）生成模型评估方法，包括构建Magic-Bench-377基准测试集，结合ELO和维度特定MOS进行联合评估，并使用多元逻辑回归分析各维度对用户满意度的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成评估方法缺乏应用场景视角，外部有效性有限，且ELO和MOS评估方法存在固有缺陷和有限可解释性。

Method: 提出结构化分类法构建Magic-Bench-377基准测试集，结合ELO进行模型排名和维度特定MOS进行细粒度评估，使用多元逻辑回归分析维度贡献。

Result: 应用MEF到当前T2I模型，获得了排行榜和领先模型的关键特征。

Conclusion: MEF为视觉生成模型评估提供了系统框架，并将评估框架和Magic-Bench-377基准测试集开源以推动研究发展。

Abstract: Rapid advances in text-to-image (T2I) generation have raised higher
requirements for evaluation methodologies. Existing benchmarks center on
objective capabilities and dimensions, but lack an application-scenario
perspective, limiting external validity. Moreover, current evaluations
typically rely on either ELO for overall ranking or MOS for dimension-specific
scoring, yet both methods have inherent shortcomings and limited
interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),
a systematic and practical approach for evaluating T2I models. First, we
propose a structured taxonomy encompassing user scenarios, elements, element
compositions, and text expression forms to construct the Magic-Bench-377, which
supports label-level assessment and ensures a balanced coverage of both user
scenarios and capabilities. On this basis, we combine ELO and
dimension-specific MOS to generate model rankings and fine-grained assessments
respectively. This joint evaluation method further enables us to quantitatively
analyze the contribution of each dimension to user satisfaction using
multivariate logistic regression. By applying MEF to current T2I models, we
obtain a leaderboard and key characteristics of the leading models. We release
our evaluation framework and make Magic-Bench-377 fully open-source to advance
research in the evaluation of visual generative models.

</details>


### [412] [Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent](https://arxiv.org/abs/2509.17917)
*Junyu Lu,Songxin Zhang,Zejian Xie,Zhuoyang Song,Jiaxing Zhang*

Main category: cs.AI

TL;DR: Orcust框架通过原则约束奖励建模和在线虚拟机轨迹构建，在GUI任务中提升推理可靠性和数据效率，在标准基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在不可靠奖励信号和有限在线轨迹生成方面存在不足，需要提升推理可靠性和数据效率

Method: 集成原则约束奖励建模和在线虚拟机轨迹构建，利用环境可验证和LLM衍生原则来约束推理链，通过虚拟机自主收集结构化GUI交互轨迹

Result: 在ScreenSpot和ScreenSpot-Pro基准上分别提升22.2%和23.9%，在感知定位、基础操作和端到端任务执行方面表现优异

Conclusion: Orcust有效增强了GUI代理的推理能力、适应性和可扩展性，在各种环境和任务复杂度下都表现出色

Abstract: Recent advances in GUI agents have achieved remarkable grounding and
action-prediction performance, yet existing models struggle with unreliable
reward signals and limited online trajectory generation. In this paper, we
introduce Orcust, a framework that integrates Principle-Constrained Reward
Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to
enhance reasoning reliability and data efficiency in interactive GUI tasks. We
leverages environment-verifiable and LLM-derived principle to enforce
interpretable reward signals that constrain long chain-of-thought reasoning and
rule-based feedback. OVTC spins up instrumented virtual machines to
autonomously collect structured GUI interaction trajectories with explicit
procedural and structural objectives, enabling the training of a stepwise
reward model that robustly captures human preferences and adheres to
task-specific constraints. Extensive experiments on standard GUI benchmarks
covering perceptual grounding, foundational operations, and end-to-end task
execution reveal that Orcust achieves state-of-the-art performance, improving
by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e.
Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the
reasoning, adaptability and scalability of GUI agents across various
environments and task complexities.

</details>


### [413] ["I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment](https://arxiv.org/abs/2509.17956)
*Lin Luo,Yuri Nakao,Mathieu Chollet,Hiroya Inakoshi,Simone Stumpf*

Main category: cs.AI

TL;DR: 该研究通过定性研究发现，缺乏AI专业知识的利益相关者在评估AI公平性时比AI专家考虑更复杂的因素，包括超出法律保护的特征、定制化指标和更严格的阈值


<details>
  <summary>Details</summary>
Motivation: 目前AI公平性评估主要由AI专家主导，但缺乏对受AI决策影响但无AI专业知识的利益相关者如何评估公平性的了解

Method: 对30名无AI专业知识的利益相关者进行定性研究，在信用评级场景中让他们决定特征优先级、指标和阈值

Result: 利益相关者的公平性决策比AI专家实践更复杂：考虑超出法律保护的特征、为特定情境定制指标、设置多样化且更严格的公平阈值，甚至偏好设计定制化公平性

Conclusion: 研究结果扩展了对利益相关者如何有意义地参与AI公平性治理的理解，强调了纳入利益相关者细致公平判断的重要性

Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI
experts who select protected features, fairness metrics, and set fairness
thresholds. However, little is known about how stakeholders, particularly those
affected by AI outcomes but lacking AI expertise, assess fairness. To address
this gap, we conducted a qualitative study with 30 stakeholders without AI
expertise, representing potential decision subjects in a credit rating
scenario, to examine how they assess fairness when placed in the role of
deciding on features with priority, metrics, and thresholds. We reveal that
stakeholders' fairness decisions are more complex than typical AI expert
practices: they considered features far beyond legally protected features,
tailored metrics for specific contexts, set diverse yet stricter fairness
thresholds, and even preferred designing customized fairness. Our results
extend the understanding of how stakeholders can meaningfully contribute to AI
fairness governance and mitigation, underscoring the importance of
incorporating stakeholders' nuanced fairness judgments.

</details>


### [414] [On the Variational Costs of Changing Our Minds](https://arxiv.org/abs/2509.17957)
*David Hyland,Mahault Albarracin*

Main category: cs.AI

TL;DR: 本文提出一个形式化框架，将信念更新建模为动机驱动的变分决策过程，认为常见的认知偏差（如确认偏误）并非认知缺陷，而是对信念修订成本的自适应响应。


<details>
  <summary>Details</summary>
Motivation: 解释为什么人类思维会表现出看似非理性的信念防御行为，挑战传统认知偏差理论，提出这些行为可能是对信念更新成本的理性适应。

Method: 采用变分决策框架，将信念更新视为权衡信念效用与信息成本（用KL散度量化）的过程，并进行计算实验验证模型。

Result: 计算实验表明该资源理性模型能够定性模拟常见的认知偏差现象，包括确认偏误和态度极化。

Conclusion: 该框架为理解信念变化的动机性贝叶斯机制提供了更全面的解释，并为预测和纠正信念更新偏差提供了实用见解。

Abstract: The human mind is capable of extraordinary achievements, yet it often appears
to work against itself. It actively defends its cherished beliefs even in the
face of contradictory evidence, conveniently interprets information to conform
to desired narratives, and selectively searches for or avoids information to
suit its various purposes. Despite these behaviours deviating from common
normative standards for belief updating, we argue that such 'biases' are not
inherently cognitive flaws, but rather an adaptive response to the significant
pragmatic and cognitive costs associated with revising one's beliefs. This
paper introduces a formal framework that aims to model the influence of these
costs on our belief updating mechanisms.
  We treat belief updating as a motivated variational decision, where agents
weigh the perceived 'utility' of a belief against the informational cost
required to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform
computational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate commonplace human
behaviours, including confirmation bias and attitude polarisation. In doing so,
we suggest that this framework makes steps toward a more holistic account of
the motivated Bayesian mechanics of belief change and provides practical
insights for predicting, compensating for, and correcting deviations from
desired belief updating processes.

</details>


### [415] [The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents](https://arxiv.org/abs/2509.17978)
*Antoni Guasch,Maria Isabel Valdez*

Main category: cs.AI

TL;DR: STAR-XAI协议是一种新的AI训练和操作方法，通过结构化苏格拉底对话和明确规则书，将不透明的大型推理模型转变为透明、可验证的可靠AI代理。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂任务中表现出可靠性不足和透明度问题，存在"思维幻觉"，需要开发可验证可靠的AI代理。

Method: 采用结构化苏格拉底对话框架，使用意识转移包作为规则书，通过游戏循环强制事前战略论证和状态锁定校验和防止错误累积。

Result: 在复杂策略游戏"Caps i Caps"的25步案例研究中，代理不仅解决了高复杂度谜题，还展示了二阶代理能力，能够识别自身计划缺陷并调整核心协议。

Conclusion: STAR-XAI协议为创建透明、可审计、可信赖的高性能AI代理提供了实用路径。

Abstract: Current Large Reasoning Models (LRMs) exhibit significant limitations in
reliability and transparency, often showing a collapse in reasoning
capabilities when faced with high-complexity, long-horizon tasks. This
"illusion of thinking" is frequently an artifact of non-agentic, black-box
evaluation paradigms that fail to cultivate robust problem-solving processes.
In response, we introduce The STAR-XAI Protocol (Socratic, Transparent,
Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel
methodology for training and operating verifiably reliable AI agents. Our
method reframes the human-AI interaction as a structured, Socratic dialogue,
governed by an explicit and evolving rulebook, the Consciousness Transfer
Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc
strategic justification and a state-locking Checksum that prevents error
accumulation, the protocol transforms a powerful but opaque LRM into a
disciplined "Clear Box" agent. We demonstrate the efficacy of this method
through an exhaustive 25-move case study in the complex strategic game "Caps i
Caps". The agent not only solved the high-complexity puzzle but also
demonstrated Second-Order Agency, identifying flaws in its own
supervisor-approved plans and adapting its core integrity protocols mid-task.
The STAR-XAI Protocol offers a practical pathway to creating AI agents that are
not just high-performing, but also transparent, auditable, and trustworthy by
design.

</details>


### [416] [Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates](https://arxiv.org/abs/2509.18076)
*Hy Dang,Tianyi Liu,Zhuofeng Wu,Jingfeng Yang,Haoming Jiang,Tao Yang,Pei Chen,Zhengyang Wang,Helen Wang,Huasheng Li,Bing Yin,Meng Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种基于课程学习理念的结构化推理模板框架，用于改进大语言模型在工具调用任务中的表现，相比自由形式的思维链提示，该方法能显著减少工具使用错误。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实世界工具交互中经常失败，主要原因是参数化错误、工具选择不当或用户意图误解，这些问题源于对用户目标理解不完整和对工具文档理解不足。

Method: 引入课程学习启发的框架，利用结构化推理模板引导LLMs通过更谨慎的逐步指令来生成函数调用，替代自由形式的思维链提示。

Result: 实验结果显示该方法能减少工具使用错误，在不同模型系列和方法上实现了3-12%的相对改进。

Conclusion: 该框架提高了工具使用代理的鲁棒性、可解释性和透明度，推动了更可靠的现实世界AI助手的发展。

Abstract: Large language models (LLMs) have demonstrated strong reasoning and tool-use
capabilities, yet they often fail in real-world tool-interactions due to
incorrect parameterization, poor tool selection, or misinterpretation of user
intent. These issues often stem from an incomplete understanding of user goals
and inadequate comprehension of tool documentation. While Chain-of-Thought
(CoT) prompting has proven effective for enhancing reasoning in general
contexts, our analysis reveals that free-form CoT is insufficient and sometimes
counterproductive for structured function-calling tasks. To address this, we
introduce a curriculum-inspired framework that leverages structured reasoning
templates to guide LLMs through more deliberate step-by-step instructions for
generating function callings. Experimental results show that our method reduces
tool-use errors, achieving 3-12% relative improvements over strong baselines
across diverse model series and approaches. Moreover, our framework enhances
the robustness, interpretability, and transparency of tool-using agents,
advancing the development of more reliable AI assistants for real-world
applications.

</details>


### [417] [Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning](https://arxiv.org/abs/2509.18083)
*Valentin Lacombe,Valentin Quesnel,Damien Sileo*

Main category: cs.AI

TL;DR: Reasoning Core是一个新的可扩展强化学习环境，专注于通过可验证奖励来提升大语言模型的符号推理能力，覆盖多个核心形式化领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注游戏或孤立谜题，缺乏对核心符号推理能力的系统性评估，需要更通用的环境来推动大语言模型推理能力的发展。

Method: 基于高通用性问题分布、外部工具验证和连续难度控制的设计原则，程序化生成PDDL规划、一阶逻辑、上下文无关文法解析、因果推理和系统方程求解等问题。

Result: 前沿大语言模型的零样本评估证实了Reasoning Core任务的难度，表明该环境具有挑战性。

Conclusion: Reasoning Core作为一个有前景的资源，有望帮助改进未来模型的推理能力，提供近乎无限的新训练实例。

Abstract: We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [418] [Neural Atlas Graphs for Dynamic Scene Decomposition and Editing](https://arxiv.org/abs/2509.16336)
*Jan Philipp Schneider,Pratik Singh Bisht,Ilya Chugunov,Andreas Kolb,Michael Moeller,Felix Heide*

Main category: cs.GR

TL;DR: 本文提出了神经图谱图（NAGs），一种结合神经图谱和场景图优势的混合高分辨率场景表示方法，解决了动态场景中可编辑性与场景复杂度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可编辑性和支持复杂场景之间存在权衡：神经图谱方法支持2D编辑但无法处理多物体遮挡和交互，而场景图方法能捕捉复杂3D空间关系但难以进行视角一致的编辑。

Method: NAGs将每个图节点表示为视角依赖的神经图谱，结合了2D外观编辑能力和3D排序定位能力。该方法在测试时进行拟合，支持高分辨率环境编辑。

Result: 在Waymo Open Dataset上实现SOTA结果（PSNR提升5dB），在DAVIS视频数据集上相比最新抠图和视频编辑基准方法PSNR提升超过7dB。

Conclusion: NAGs成功实现了高分辨率和视觉质量的环境编辑，能够创建反事实驾驶场景，并在非驾驶场景中也表现出良好的泛化能力。

Abstract: Learning editable high-resolution scene representations for dynamic scenes is
an open problem with applications across the domains from autonomous driving to
creative editing - the most successful approaches today make a trade-off
between editability and supporting scene complexity: neural atlases represent
dynamic scenes as two deforming image layers, foreground and background, which
are editable in 2D, but break down when multiple objects occlude and interact.
In contrast, scene graph models make use of annotated data such as masks and
bounding boxes from autonomous-driving datasets to capture complex 3D spatial
relationships, but their implicit volumetric node representations are
challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a
hybrid high-resolution scene representation, where every graph node is a
view-dependent neural atlas, facilitating both 2D appearance editing and 3D
ordering and positioning of scene elements. Fit at test-time, NAGs achieve
state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR
increase compared to existing methods - and make environmental editing possible
in high resolution and visual quality - creating counterfactual driving
scenarios with new backgrounds and edited vehicle appearance. We find that the
method also generalizes beyond driving scenes and compares favorably - by more
than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS
video dataset with a diverse set of human and animal-centric scenes.

</details>


### [419] [Brain Connectivity Network Structure Learning For Brain Disorder Diagnosis](https://arxiv.org/abs/2509.16735)
*Dongdong Chen,Linlin Yao,Mengjun Liu,Zhenrong Shen,Yuqi Hu,Zhiyun Song,Shengyu Lu,Qian Wang,Dinggang Shen,Lichi Zhang*

Main category: cs.GR

TL;DR: 提出了一种自监督框架，用于学习和优化脑连接网络的结构与表示，通过自适应构建互补网络结构学习器，结合多状态图编码器和联合迭代学习策略，在大规模无标签脑连接数据上进行预训练，实现对新脑部疾病的有效泛化诊断。


<details>
  <summary>Details</summary>
Motivation: 传统脑连接网络构建方法使用预定义方法和手动设置阈值，容易引入冗余连接或忽略关键交互，且标记数据不足增加了学习内在脑特征表示的难度。

Method: 使用两种现有全脑连接组自适应构建互补网络结构学习器，引入多状态图编码器与联合迭代学习策略，同时优化生成的网络结构及其表示，通过自监督预训练实现泛化能力。

Result: 在跨数据集脑疾病诊断实验中，该方法持续优于现有最先进方法，验证了其有效性和泛化能力。

Conclusion: 该自监督框架能够有效学习和优化脑连接网络的结构与表示，仅需少量微调即可适应新的诊断任务，为脑疾病诊断提供了有前景的解决方案。

Abstract: Recent studies in neuroscience highlight the significant potential of brain
connectivity networks, which are commonly constructed from functional magnetic
resonance imaging (fMRI) data for brain disorder diagnosis. Traditional brain
connectivity networks are typically obtained using predefined methods that
incorporate manually-set thresholds to estimate inter-regional relationships.
However, such approaches often introduce redundant connections or overlook
essential interactions, compromising the value of the constructed networks.
Besides, the insufficiency of labeled data further increases the difficulty of
learning generalized representations of intrinsic brain characteristics. To
mitigate those issues, we propose a self-supervised framework to learn an
optimal structure and representation for brain connectivity networks, focusing
on individualized generation and optimization in an unsupervised manner. We
firstly employ two existing whole-brain connectomes to adaptively construct
their complementary brain network structure learner, and then introduce a
multi-state graph-based encoder with a joint iterative learning strategy to
simultaneously optimize both the generated network structure and its
representation. By leveraging self-supervised pretraining on large-scale
unlabeled brain connectivity data, our framework enables the brain connectivity
network learner to generalize e ffectively to unseen disorders, while requiring
only minimal finetuning of the encoder for adaptation to new diagnostic tasks.
Extensive experiments on cross-dataset brain disorder diagnosis demonstrate
that our method consistently outperforms state-of-the-art approaches,
validating its effectiveness and generalizability. The code is publicly
available at https://github.com/neochen1/BCNSL.

</details>


### [420] [PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction](https://arxiv.org/abs/2509.16869)
*Hrishav Bakul Barua,Kalin Stefanov,Ganesh Krishnasamy,KokSheik Wong,Abhinav Dhall*

Main category: cs.GR

TL;DR: PhysHDR是一个基于潜在扩散模型的HDR图像重建方法，通过结合光照、深度信息和材料属性来提升重建质量


<details>
  <summary>Details</summary>
Motivation: 现有的LDR到HDR图像转换方法缺乏对光照、照明和场景几何的显式建模，限制了重建HDR图像的质量。不同材料与光照和阴影的交互方式不同，建模材料特定属性可以改善HDR重建效果

Method: 使用潜在扩散生成模型，在去噪过程中结合光照和深度信息，并通过新的损失函数引入场景表面材料属性

Result: 实验结果表明PhysHDR在HDR图像重建方面优于多个最新的最先进方法

Conclusion: PhysHDR是一个简单而强大的HDR图像重建方法，通过显式建模光照、深度和材料属性，显著提升了重建质量

Abstract: Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a
fundamental task in many computational vision problems. Numerous data-driven
methods have been proposed to address this problem; however, they lack explicit
modeling of illumination, lighting, and scene geometry in images. This limits
the quality of the reconstructed HDR images. Since lighting and shadows
interact differently with different materials, (e.g., specular surfaces such as
glass and metal, and lambertian or diffuse surfaces such as wood and stone),
modeling material-specific properties (e.g., specular and diffuse reflectance)
has the potential to improve the quality of HDR image reconstruction. This
paper presents PhysHDR, a simple yet powerful latent diffusion-based generative
model for HDR image reconstruction. The denoising process is conditioned on
lighting and depth information and guided by a novel loss to incorporate
material properties of surfaces in the scene. The experimental results
establish the efficacy of PhysHDR in comparison to a number of recent
state-of-the-art methods.

</details>


### [421] [SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments](https://arxiv.org/abs/2509.16960)
*Ruiyan Wang,Zhengxue Cheng,Zonghao Lin,Jun Ling,Yuzhou Liu,Yanru An,Rong Xie,Li Song*

Main category: cs.GR

TL;DR: SemanticGarment是一种基于3D高斯的方法，能够从文本或图像提示生成高保真3D服装，并支持基于语义的交互式编辑，解决了传统方法在多视角一致性和纹理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统3D服装生成方法面临技术复杂性和高资源成本的问题，而现有学习方法存在多视角几何不一致、纹理不一致以及严重依赖详细服装拓扑和手动绑定的挑战。

Method: 提出利用结构化人体先验，通过引入3D语义服装模型来初始化几何结构，开发自遮挡优化策略以减少单图像重建中的伪影，支持无需重新生成或依赖现有网格模板的快速多样化修改。

Result: 通过大量实验证明，该方法在3D服装生成和编辑方面表现出优越性能，能够实现高保真、多视角一致的服装合成和灵活的用户定制。

Conclusion: SemanticGarment方法有效解决了3D服装生成和编辑中的关键挑战，为时尚设计、虚拟试穿和游戏应用提供了高效、灵活的解决方案。

Abstract: 3D digital garment generation and editing play a pivotal role in fashion
design, virtual try-on, and gaming. Traditional methods struggle to meet the
growing demand due to technical complexity and high resource costs.
Learning-based approaches offer faster, more diverse garment synthesis based on
specific requirements and reduce human efforts and time costs. However, they
still face challenges such as inconsistent multi-view geometry or textures and
heavy reliance on detailed garment topology and manual rigging. We propose
SemanticGarment, a 3D Gaussian-based method that realizes high-fidelity 3D
garment generation from text or image prompts and supports semantic-based
interactive editing for flexible user customization. To ensure multi-view
consistency and garment fitting, we propose to leverage structural human priors
for the generative model by introducing a 3D semantic clothing model, which
initializes the geometry structure and lays the groundwork for view-consistent
garment generation and editing. Without the need to regenerate or rely on
existing mesh templates, our approach allows for rapid and diverse
modifications to existing Gaussians, either globally or within a local region.
To address the artifacts caused by self-occlusion for garment reconstruction
based on single image, we develop a self-occlusion optimization strategy to
mitigate holes and artifacts that arise when directly animating self-occluded
garments. Extensive experiments are conducted to demonstrate our superior
performance in 3D garment generation and editing.

</details>


### [422] [Beat on Gaze: Learning Stylized Generation of Gaze and Head Dynamics](https://arxiv.org/abs/2509.17168)
*Chengwei Shi,Chong Cao,Xin Tong,Xukun Shen*

Main category: cs.GR

TL;DR: StyGazeTalk是一种音频驱动的方法，生成同步的注视和头部运动风格，通过多层LSTM结构提取说话者特定的运动特征，并引入了高质量的多模态数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立处理面部组件，忽视了注视、头部运动和语音之间的复杂协调，且缺乏高质量注视标注的数据集阻碍了数据驱动模型的发展。

Method: 使用多层LSTM结构结合风格编码器，从注视-头部序列中提取说话者特定的运动特征，生成多样化的动画风格。

Result: 实验结果表明，该方法能够生成逼真、时间连贯且具有风格意识的头部-注视运动，显著提升了音频驱动面部动画的技术水平。

Conclusion: StyGazeTalk方法通过同步生成注视和头部运动风格，解决了现有方法的局限性，为音频驱动面部动画提供了有效的解决方案。

Abstract: Head and gaze dynamics are crucial in expressive 3D facial animation for
conveying emotion and intention. However, existing methods frequently address
facial components in isolation, overlooking the intricate coordination between
gaze, head motion, and speech. The scarcity of high-quality gaze-annotated
datasets hinders the development of data-driven models capable of capturing
realistic, personalized gaze control. To address these challenges, we propose
StyGazeTalk, an audio-driven method that generates synchronized gaze and head
motion styles. We extract speaker-specific motion traits from gaze-head
sequences with a multi-layer LSTM structure incorporating a style encoder,
enabling the generation of diverse animation styles. We also introduce a
high-precision multimodal dataset comprising eye-tracked gaze, audio, head
pose, and 3D facial parameters, providing a valuable resource for training and
evaluating head and gaze control models. Experimental results demonstrate that
our method generates realistic, temporally coherent, and style-aware head-gaze
motions, significantly advancing the state-of-the-art in audio-driven facial
animation.

</details>


### [423] [High Resolution UDF Meshing via Iterative Networks](https://arxiv.org/abs/2509.17212)
*Federico Stella,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.GR

TL;DR: 提出了一种迭代神经网络方法，用于从无符号距离场（UDFs）中提取更准确和完整的网格表面，特别是在高分辨率下处理复杂几何形状时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: UDFs是开放表面的自然隐式表示，但难以三角化为显式网格，尤其是在高分辨率下神经UDFs噪声较大，现有方法在单个体素内操作导致表面缺失和孔洞问题。

Method: 通过多轮迭代和空间传播邻域信息，利用神经网络逐步改进每个体素内的表面恢复，整合新检测的表面、距离值和梯度来纠正错误。

Result: 在多种3D模型上的实验表明，该方法比现有方法产生更准确和完整的网格，特别是在复杂几何形状和高分辨率情况下。

Conclusion: 提出的迭代方法能够有效处理UDF表面提取中的挑战，在传统方法失败的高分辨率情况下仍能稳定工作。

Abstract: Unsigned Distance Fields (UDFs) are a natural implicit representation for
open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to
triangulate into explicit meshes. This is especially true at high resolutions
where neural UDFs exhibit higher noise levels, which makes it hard to capture
fine details. Most current techniques perform within single voxels without
reference to their neighborhood, resulting in missing surface and holes where
the UDF is ambiguous or noisy. We show that this can be remedied by performing
several passes and by reasoning on previously extracted surface elements to
incorporate neighborhood information. Our key contribution is an iterative
neural network that does this and progressively improves surface recovery
within each voxel by spatially propagating information from increasingly
distant neighbors. Unlike single-pass methods, our approach integrates newly
detected surfaces, distance values, and gradients across multiple iterations,
effectively correcting errors and stabilizing extraction in challenging
regions. Experiments on diverse 3D models demonstrate that our method produces
significantly more accurate and complete meshes than existing approaches,
particularly for complex geometries, enabling UDF surface extraction at higher
resolutions where traditional methods fail.

</details>


### [424] ["I don't like my avatar": Investigating Human Digital Doubles](https://arxiv.org/abs/2509.17748)
*Siyi Liu,Kazi Injamamul Haque,Zerrin Yumak*

Main category: cs.GR

TL;DR: 研究探讨了头像风格（写实vs卡通）和熟悉度（自己、熟人、陌生人）对数字分身识别、真实感感知、亲和力及社会存在感的影响。结果显示高写实度提升识别和存在感，但对熟悉面孔的写实头像反而降低接受度。


<details>
  <summary>Details</summary>
Motivation: 随着消费级设备使创建数字分身变得容易，需要了解不同头像风格和熟悉度如何影响人们对数字分身的感知和接受程度。

Method: 创建两种风格头像（写实MetaHumans和卡通ReadyPlayerMe），通过动作捕捉制作面部动画，进行受控线下实验并收集问卷数据。

Result: 高写实度提升识别、真实感和社会存在感，但对熟悉面孔（尤其是写实风格）的数字分身接受度更低，参与者普遍不喜欢自己的写实分身。

Conclusion: 数字分身的写实风格虽能增强存在感，但对熟悉面孔的应用需谨慎，人们对自身分身的接受度低于对他人分身的接受度。

Abstract: Creating human digital doubles is becoming easier and much more accessible to
everyone using consumer grade devices. In this work, we investigate how avatar
style (realistic vs cartoon) and avatar familiarity (self, acquaintance,
unknown person) affect self/other-identification, perceived realism, affinity
and social presence with a controlled offline experiment. We created two styles
of avatars (realistic-looking MetaHumans and cartoon-looking ReadyPlayerMe
avatars) and facial animations stimuli for them using performance capture.
Questionnaire responses demonstrate that higher appearance realism leads to a
higher level of identification, perceived realism and social presence. However,
avatars with familiar faces, especially those with high appearance realism,
lead to a lower level of identification, perceived realism, and affinity.
Although participants identified their digital doubles as their own, they
consistently did not like their avatars, especially of realistic appearance.
But they were less critical and more forgiving about their acquaintance's or an
unknown person's digital double.

</details>


### [425] [Effect of Appearance and Animation Realism on the Perception of Emotionally Expressive Virtual Humans](https://arxiv.org/abs/2509.17803)
*Nabila Amadou,Kazi Injamamul Haque,Zerrin Yumak*

Main category: cs.GR

TL;DR: 本文研究了3D虚拟人的外观真实感和动画真实感对情感表达场景中感知效果的影响，发现更高的真实度能提升社交存在感和吸引力。


<details>
  <summary>Details</summary>
Motivation: 随着3D虚拟人技术发展，虽然外观已达到高度真实水平，但缺乏对情感表达虚拟人的外观和动画真实感感知的系统分析。

Method: 设计了用户实验，分析不同情感条件下虚拟人的外观真实感和动画真实感对感知效果的影响。

Result: 发现更高的外观真实感和动画真实感会带来更高的社交存在感和吸引力评分；动画真实感对感知真实感和情感强度有显著影响。

Conclusion: 研究揭示了高度真实虚拟人在情感表达场景中外观和动画真实感的感知机制，为未来发展方向提供了指导。

Abstract: 3D Virtual Human technology is growing with several potential applications in
health, education, business and telecommunications. Investigating the
perception of these virtual humans can help guide to develop better and more
effective applications. Recent developments show that the appearance of the
virtual humans reached to a very realistic level. However, there is not yet
adequate analysis on the perception of appearance and animation realism for
emotionally expressive virtual humans. In this paper, we designed a user
experiment and analyzed the effect of a realistic virtual human's appearance
realism and animation realism in varying emotion conditions. We found that
higher appearance realism and higher animation realism leads to higher social
presence and higher attractiveness ratings. We also found significant effects
of animation realism on perceived realism and emotion intensity levels. Our
study sheds light into how appearance and animation realism effects the
perception of highly realistic virtual humans in emotionally expressive
scenarios and points out to future directions.

</details>


### [426] [A Comparative Study of Different Edit Distance-Based Methods for Feature Tracking using Merge Trees on Time-Varying Scalar Fields](https://arxiv.org/abs/2509.17974)
*Son Le Thanh,Tino Weinkauf*

Main category: cs.GR

TL;DR: 本文比较了四种基于合并树编辑距离的特征跟踪方法，发现这些方法在分析和真实数据集上产生不同的结果，并探讨了影响结果差异的因素。


<details>
  <summary>Details</summary>
Motivation: 特征跟踪在时变标量场中是科学计算的基本任务，合并树作为拓扑描述符能够捕捉标量场的连通性行为，但现有的合并树编辑距离计算方法存在差异，需要系统比较不同方法的效果。

Method: 比较了四种基于合并树编辑距离的特征跟踪方法，包括分支分解依赖和独立的两类主要方法，使用分析和真实数据集进行实验验证。

Result: 实验结果显示这些方法产生显著不同的跟踪结果，即使在同类技术中也存在明显差异，跟踪特征随时间变化表现出明显区别。

Conclusion: 合并树编辑距离方法的选择对特征跟踪结果有重要影响，不同方法会产生不同的跟踪特征，需要根据具体应用场景选择合适的方法。

Abstract: Feature tracking in time-varying scalar fields is a fundamental task in
scientific computing. Topological descriptors, which summarize important
features of data, have proved to be viable tools to facilitate this task. The
merge tree is a topological descriptor that captures the connectivity behaviors
of the sub- or superlevel sets of a scalar field. Edit distances between merge
trees play a vital role in effective temporal data tracking. Existing methods
to compute them fall into two main classes, namely whether they are dependent
or independent of the branch decomposition. These two classes represent the
most prominent approaches for producing tracking results. In this paper, we
compare four different merge tree edit distance-based methods for feature
tracking. We demonstrate that these methods yield distinct results with both
analytical and real-world data sets. Furthermore, we investigate how these
results vary and identify the factors that influence them. Our experiments
reveal significant differences in tracked features over time, even among those
produced by techniques within the same category.

</details>


### [427] [Towards Seeing Bones at Radio Frequency](https://arxiv.org/abs/2509.17979)
*Yiwen Song,Hongyang Li,Kuang Yuan,Ran Bi,Swarun Kumar*

Main category: cs.GR

TL;DR: MCT系统实现了毫米级分辨率的骨骼射频成像，通过穿透式合成孔径算法和基于学习的衍射校正，将射频穿透成像分辨率从亚分米级提升到亚厘米级。


<details>
  <summary>Details</summary>
Motivation: 长期以来，无线传感领域一直希望实现类似X射线的射频视觉，但现有技术无法生成骨骼在软组织下的典型X射线图像。射频在穿透组织时存在波长长、衰减严重和复杂衍射等问题，限制了成像分辨率。

Method: 开发了MCT系统，采用新型穿透式合成孔径算法，并结合基于学习的管道来校正衍射引起的伪影。

Result: 在肉类模型上的详细评估显示，射频穿透成像的分辨率从亚分米级提升到亚厘米级，显著优于现有技术。

Conclusion: MCT系统成功实现了毫米级分辨率的骨骼射频成像，突破了射频穿透成像的分辨率限制，为无线传感领域带来了重要进展。

Abstract: Wireless sensing literature has long aspired to achieve X-ray-like vision at
radio frequencies. Yet, state-of-the-art wireless sensing literature has yet to
generate the archetypal X-ray image: one of the bones beneath flesh. In this
paper, we explore MCT, a penetration-based RF-imaging system for imaging bones
at mm-resolution, one that significantly exceeds prior penetration-based RF
imaging literature. Indeed the long wavelength, significant attenuation and
complex diffraction that occur as RF propagates through flesh, have long
limited imaging resolution (to several centimeters at best). We address these
concerns through a novel penetration-based synthetic aperture algorithm,
coupled with a learning-based pipeline to correct for diffraction-induced
artifacts. A detailed evaluation of meat models demonstrates a resolution
improvement from sub-decimeter to sub-centimeter over prior art in RF
penetrative imaging.

</details>


### [428] [VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models](https://arxiv.org/abs/2509.17985)
*Geonung Kim,Janghyeok Han,Sunghyun Cho*

Main category: cs.GR

TL;DR: VideoFrom3D是一个从粗糙几何、相机轨迹和参考图像合成高质量3D场景视频的新框架，通过结合图像和视频扩散模型的优势来解决现有方法在复杂场景下的生成质量、运动和时间一致性难题。


<details>
  <summary>Details</summary>
Motivation: 简化3D图形设计工作流程，实现灵活的设计探索和快速交付物生产。现有视频扩散模型在复杂场景中难以同时保证视觉质量、运动和时间一致性。

Method: 框架包含稀疏锚点视图生成(SAG)模块和几何引导生成插帧(GGI)模块。SAG使用图像扩散模型生成高质量、跨视图一致的锚点视图，GGI基于锚点视图使用视频扩散模型插值中间帧，结合基于流量的相机控制和结构引导。

Result: 综合实验表明，该方法在多样化和挑战性场景下能生成高质量、风格一致的场景视频，优于简单和扩展基线方法。

Conclusion: VideoFrom3D框架无需配对的3D场景模型和自然图像数据集，有效解决了复杂3D场景视频合成的挑战。

Abstract: In this paper, we propose VideoFrom3D, a novel framework for synthesizing
high-quality 3D scene videos from coarse geometry, a camera trajectory, and a
reference image. Our approach streamlines the 3D graphic design workflow,
enabling flexible design exploration and rapid production of deliverables. A
straightforward approach to synthesizing a video from coarse geometry might
condition a video diffusion model on geometric structure. However, existing
video diffusion models struggle to generate high-fidelity results for complex
scenes due to the difficulty of jointly modeling visual quality, motion, and
temporal consistency. To address this, we propose a generative framework that
leverages the complementary strengths of image and video diffusion models.
Specifically, our framework consists of a Sparse Anchor-view Generation (SAG)
and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module
generates high-quality, cross-view consistent anchor views using an image
diffusion model, aided by Sparse Appearance-guided Sampling. Building on these
anchor views, GGI module faithfully interpolates intermediate frames using a
video diffusion model, enhanced by flow-based camera control and structural
guidance. Notably, both modules operate without any paired dataset of 3D scene
models and natural images, which is extremely difficult to obtain.
Comprehensive experiments show that our method produces high-quality,
style-consistent scene videos under diverse and challenging scenarios,
outperforming simple and extended baselines.

</details>
